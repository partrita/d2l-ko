<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>일반화 (Generalization) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/generalization.html" class="active"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="일반화-generalization"><a class="header" href="#일반화-generalization">일반화 (Generalization)</a></h1>
<p>:label:<code>sec_generalization_basics</code></p>
<p>기말고사를 성실히 준비하는 두 대학생을 생각해 보십시오.
일반적으로 이 준비는 예년의 시험을 치르며 실력을 연습하고 테스트하는 것으로 구성됩니다.
그럼에도 불구하고 과거 시험에서 잘하는 것이 정작 중요한 때에 잘할 것이라는 보장은 아닙니다.
예를 들어, 준비 과정이 예년의 시험 문제 정답을 모두 외우는 것이었던 학생 'Extraordinary Ellie'를 상상해 보십시오.
비록 Ellie가 비범한 기억력을 타고나서 <em>이전에 본</em> 어떤 문제의 답도 완벽하게 기억해낼 수 있더라도, 새로운 (<em>이전에 본 적 없는</em>) 문제에 직면하면 얼어붙을지도 모릅니다.
그에 비해, 암기 능력은 비슷하게 부족하지만 패턴을 파악하는 재주가 있는 또 다른 학생 'Inductive Irene'을 상상해 보십시오.
만약 시험이 정말로 전년도 문제를 재활용한 것이라면, Ellie가 Irene을 가뿐히 이길 것입니다.
Irene의 추론된 패턴이 90% 정확한 예측을 낸다고 해도 Ellie의 100% 암기를 결코 따라올 수 없습니다.
그러나 시험이 전적으로 새로운 문제로 구성된다면, Irene은 여전히 90%의 평균을 유지할 수 있습니다.</p>
<p>머신러닝 과학자로서 우리의 목표는 <em>패턴</em>을 발견하는 것입니다.
하지만 우리가 단순히 데이터를 암기한 것이 아니라 진정으로 <em>일반적인</em> 패턴을 발견했는지 어떻게 확신할 수 있을까요?
대부분의 경우, 우리의 예측은 모델이 그러한 패턴을 발견했을 때만 유용합니다.
우리는 어제의 주가가 아니라 내일의 주가를 예측하고 싶어 합니다.
이전에 본 환자들에 대해 이미 진단된 질병을 인식할 필요는 없으며, 그보다 이전에 본 적 없는 환자들에게서 이전에 진단되지 않은 질병을 인식해야 합니다.
패턴을 발견하여 어떻게 <em>일반화</em>할 것인가 하는 이 문제는 머신러닝의 근본적인 문제이며, 아마도 모든 통계학의 근본적인 문제일 것입니다.
우리는 이 문제를 과학 전체를 집어삼키는 훨씬 더 거대한 질문의 한 조각으로 던질 수 있습니다: 우리가 특정한 관찰에서 더 일반적인 진술로 도약하는 것이 정당화되는 때는 언제일까요?</p>
<p>실제 생활에서 우리는 한정된 데이터 모음을 사용하여 모델을 맞춰야 합니다.
그 데이터의 일반적인 규모는 도메인에 따라 크게 다릅니다.
많은 중요한 의학 문제의 경우, 수천 개의 데이터 포인트만 사용할 수 있습니다.
희귀 질병을 연구할 때는 수백 개에 접근할 수 있다면 운이 좋은 편일 수도 있습니다.
대조적으로, 레이블이 지정된 사진으로 구성된 가장 큰 공개 데이터셋(예: ImageNet :cite:<code>Deng.Dong.Socher.ea.2009</code>)은 수백만 개의 이미지를 포함합니다.
그리고 Flickr YFC100M 데이터셋과 같은 일부 레이블이 지정되지 않은 이미지 모음은 심지어 1억 개 이상의 이미지를 포함할 수 있습니다 :cite:<code>thomee2016yfcc100m</code>.
그러나 이 극한의 규모에서도 사용 가능한 데이터 포인트의 수는 메가픽셀 해상도의 모든 가능한 이미지 공간에 비하면 무한히 작습니다.
한정된 샘플로 작업할 때마다, 우리는 훈련 데이터에는 잘 맞지만 일반화 가능한 패턴을 발견하는 데 실패할 수 있다는 위험을 염두에 두어야 합니다.</p>
<p>기저의 분포보다 훈련 데이터에 더 가깝게 맞추는 현상을 *과대적합(overfitting)*이라고 하며, 과대적합과 싸우는 기술을 종종 <em>정규화(regularization)</em> 방법이라고 합니다.
이것이 통계적 학습 이론(statistical learning theory, :citet:<code>Vapnik98,boucheron2005theory</code> 참조)에 대한 적절한 소개를 대신할 수는 없지만, 여러분이 시작하기에 충분한 직관을 제공할 것입니다.
우리는 책 전반에 걸쳐 많은 장에서 일반화를 다시 다룰 것이며, 다양한 모델에서 일반화의 기초가 되는 원리에 대해 알려진 내용과 실무적인 작업에서 일반화를 개선하는 것으로 (경험적으로) 밝혀진 휴리스틱 기술들을 모두 살펴볼 것입니다.</p>
<h2 id="훈련-오차와-일반화-오차-training-error-and-generalization-error"><a class="header" href="#훈련-오차와-일반화-오차-training-error-and-generalization-error">훈련 오차와 일반화 오차 (Training Error and Generalization Error)</a></h2>
<p>표준 지도 학습 설정에서, 우리는 훈련 데이터와 테스트 데이터가 <em>동일한</em> 분포에서 <em>독립적으로</em> 추출되었다고 가정합니다.
이를 일반적으로 <em>IID 가정</em>이라고 합니다.
이 가정이 강력하기는 하지만, 그러한 가정이 없다면 우리는 아무것도 할 수 없다는 점에 유의할 가치가 있습니다.
분포 $P(X,Y)$에서 샘플링된 훈련 데이터가 <em>다른 분포</em> $Q(X,Y)$에 의해 생성된 테스트 데이터에 대해 예측하는 방법을 알려줄 것이라고 왜 믿어야 할까요?
그러한 도약을 하는 데는 $P$와 $Q$가 어떻게 관련되어 있는지에 대한 강력한 가정이 필요한 것으로 밝혀졌습니다.
나중에 분포의 이동을 허용하는 몇 가지 가정에 대해 논의하겠지만, 먼저 $P(\cdot) = Q(\cdot)$인 IID 사례를 이해해야 합니다.</p>
<p>우선, 훈련 데이터셋에서 계산되는 <em>통계량</em>인 <em>훈련 오차(training error)</em> $R_\textrm{emp}$와, 기저의 분포에 대해 취해진 <em>기댓값</em>인 <em>일반화 오차(generalization error)</em> $R$을 구별해야 합니다.
일반화 오차는 모델을 동일한 기저 데이터 분포에서 추출된 추가 데이터 예제의 무한한 스트림에 적용했을 때 보게 될 오차라고 생각할 수 있습니다.
공식적으로 훈련 오차는 (:numref:<code>sec_linear_regression</code>과 동일한 표기법을 사용하여) <em>합</em>으로 표현됩니다:</p>
<p>$$R_\textrm{emp}[\mathbf{X}, \mathbf{y}, f] = \frac{1}{n} \sum_{i=1}^n l(\mathbf{x}^{(i)}, y^{(i)}, f(\mathbf{x}^{(i)})),$$</p>
<p>반면 일반화 오차는 적분으로 표현됩니다:</p>
<p>$$R[p, f] = E_{(\mathbf{x}, y) \sim P} [l(\mathbf{x}, y, f(\mathbf{x}))] =
\int \int l(\mathbf{x}, y, f(\mathbf{x})) p(\mathbf{x}, y) ;d\mathbf{x} dy.$$</p>
<p>문제는 우리가 일반화 오차 $R$을 정확하게 계산할 수 없다는 것입니다.
아무도 우리에게 밀도 함수 $p(\mathbf{x}, y)$의 정확한 형태를 말해주지 않습니다.
게다가 우리는 무한한 데이터 포인트 스트림을 샘플링할 수 없습니다.
따라서 실제로는 훈련 세트에서 제외된 무작위 예제 선택 $\mathbf{X}'$ 및 레이블 $\mathbf{y}'$로 구성된 독립적인 테스트 세트에 모델을 적용하여 일반화 오차를 <em>추정</em>해야 합니다.
이는 경험적 훈련 오차를 계산하는 데 사용된 것과 동일한 공식을 테스트 세트 $\mathbf{X}', \mathbf{y}'$에 적용하는 것으로 구성됩니다.</p>
<p>결정적으로, 테스트 세트에서 분류기를 평가할 때 우리는 <em>고정된</em> 분류기(테스트 세트의 샘플에 의존하지 않음)로 작업하고 있으므로, 그 오차를 추정하는 것은 단순히 평균 추정 문제입니다.
하지만 훈련 세트에 대해서는 그렇게 말할 수 없습니다.
우리가 얻게 되는 모델은 훈련 세트의 선택에 명시적으로 의존하므로, 훈련 오차는 일반적으로 기저 모집단에 대한 실제 오차의 편향된 추정치가 될 것입니다.
일반화의 중심 질문은 언제 우리의 훈련 오차가 모집단 오차(따라서 일반화 오차)와 가까울 것으로 기대해야 하는가입니다.</p>
<h3 id="모델-복잡도-model-complexity"><a class="header" href="#모델-복잡도-model-complexity">모델 복잡도 (Model Complexity)</a></h3>
<p>고전 이론에서 단순한 모델과 풍부한 데이터가 있을 때, 훈련 오차와 일반화 오차는 가까워지는 경향이 있습니다.
하지만 더 복잡한 모델 및/또는 더 적은 예제로 작업할 때, 훈련 오차는 내려가지만 일반화 갭(generalization gap)은 커질 것으로 예상합니다.
이는 놀라운 일이 아닙니다.
어떤 $n$개 예제의 데이터셋에 대해서도, 설령 무작위로 할당되었더라도 임의의 레이블에 완벽하게 맞출 수 있는 파라미터 세트를 찾을 수 있을 만큼 표현력이 뛰어난 모델 클래스를 상상해 보십시오.
이 경우 훈련 데이터를 완벽하게 맞췄다고 한들, 일반화 오차에 대해 무엇을 결론지을 수 있을까요?
우리가 아는 한, 우리의 일반화 오차는 무작위 추측보다 나을 것이 없을지도 모릅니다.</p>
<p>일반적으로 우리 모델 클래스에 어떠한 제한도 없다면, 훈련 데이터에만 맞춘 것을 근거로 모델이 일반화 가능한 패턴을 발견했다고 결론지을 수 없습니다 :cite:<code>vapnik1994measuring</code>.
반면에 만약 우리 모델 클래스가 임의의 레이블에 맞출 능력이 없었다면, 그것은 반드시 패턴을 발견했어야 합니다.
모델 복잡도에 대한 학습 이론적 아이디어는 반증 가능성(falsifiability)의 기준을 공식화한 영향력 있는 과학 철학자 칼 포퍼(Karl Popper)의 아이디어에서 영감을 얻었습니다.
포퍼에 따르면, 모든 관찰을 설명할 수 있는 이론은 결코 과학적 이론이 아닙니다!
결국 어떤 가능성도 배제하지 못했다면 세상에 대해 무엇을 말해준 것일까요?
요컨대 우리가 원하는 것은 우리가 생각할 수 있는 어떤 관찰도 설명할 수는 <em>없지만</em>, 그럼에도 불구하고 우리가 <em>실제로</em> 내놓은 관찰들과는 우연히 호환되는 가설입니다.</p>
<p>이제 무엇이 적절한 모델 복잡도 개념을 구성하는지는 복잡한 문제입니다.
종종 더 많은 파라미터를 가진 모델이 더 많은 수의 임의로 할당된 레이블에 맞출 수 있습니다.
하지만 이것이 반드시 사실인 것은 아닙니다.
예를 들어 커널 방법은 무한한 수의 파라미터 공간에서 작동하지만, 그 복잡도는 다른 수단에 의해 제어됩니다 :cite:<code>Scholkopf.Smola.2002</code>.
종종 유용하다고 증명되는 복잡도의 한 가지 개념은 파라미터가 취할 수 있는 값의 범위입니다.
여기서 파라미터가 임의의 값을 취하도록 허용되는 모델이 더 복잡할 것입니다.
우리는 다음 섹션에서 첫 번째 실용적인 정규화 기술인 *가중치 감쇠(weight decay)*를 소개할 때 이 아이디어를 다시 다룰 것입니다.
특히, 상당히 다른 모델 클래스(예: 결정 트리 vs. 신경망) 간에 복잡도를 비교하는 것은 어려울 수 있습니다.</p>
<p>이 시점에서 심층 신경망을 소개할 때 다시 다룰 또 다른 중요한 점을 강조해야 합니다.
모델이 임의의 레이블에 맞출 수 있을 때, 낮은 훈련 오차가 반드시 낮은 일반화 오차를 의미하는 것은 아닙니다.
<em>하지만 그렇다고 해서 반드시 높은 일반화 오차를 의미하는 것도 아닙니다!</em>
우리가 자신 있게 말할 수 있는 전부는 낮은 훈련 오차만으로는 낮은 일반화 오차를 증명하기에 충분하지 않다는 것입니다.
심층 신경망은 바로 그러한 모델인 것으로 밝혀졌습니다: 실제로는 일반화가 잘 되지만, 훈련 오차만으로는 많은 결론을 내리기에는 너무 강력합니다.
이러한 경우 우리는 사후에 일반화를 증명하기 위해 홀드아웃(holdout) 데이터에 더 많이 의존해야 합니다.
홀드아웃 데이터, 즉 검증 세트에서의 오차를 *검증 오차(validation error)*라고 합니다.</p>
<h2 id="과소적합-또는-과대적합-underfitting-or-overfitting"><a class="header" href="#과소적합-또는-과대적합-underfitting-or-overfitting">과소적합 또는 과대적합? (Underfitting or Overfitting?)</a></h2>
<p>훈련 오차와 검증 오차를 비교할 때, 우리는 두 가지 일반적인 상황을 유념해야 합니다.
먼저, 훈련 오차와 검증 오차가 모두 상당하지만 그 사이의 격차가 거의 없는 경우를 주의해야 합니다.
모델이 훈련 오차를 줄이지 못한다면, 이는 우리 모델이 모델링하려는 패턴을 포착하기에 너무 단순하다(즉, 충분히 표현력이 없다)는 의미일 수 있습니다.
게다가 훈련 오차와 일반화 오차 사이의 <em>일반화 갭</em>($R_\textrm{emp} - R$)이 작기 때문에, 더 복잡한 모델을 사용해도 괜찮을 것이라고 믿을 근거가 있습니다.
이 현상을 *과소적합(underfitting)*이라고 합니다.</p>
<p>반면에 위에서 논의했듯이, 훈련 오차가 검증 오차보다 현저히 낮은 경우를 주의해야 하며, 이는 심각한 *과대적합(overfitting)*을 나타냅니다.
과대적합이 항상 나쁜 것만은 아니라는 점에 유의하십시오.
특히 딥러닝에서 가장 우수한 예측 모델은 종종 홀드아웃 데이터보다 훈련 데이터에서 훨씬 더 나은 성능을 보입니다.
궁극적으로 우리는 대개 일반화 오차를 낮추는 것에 관심을 가지며, 그 갭에 대해서는 그것이 그 목적에 장애가 될 때만 신경을 씁니다.
만약 훈련 오차가 0이라면, 일반화 갭은 정확히 일반화 오차와 같으며 우리는 오직 갭을 줄임으로써만 진전을 이룰 수 있습니다.</p>
<h3 id="다항식-곡선-맞춤-polynomial-curve-fitting"><a class="header" href="#다항식-곡선-맞춤-polynomial-curve-fitting">다항식 곡선 맞춤 (Polynomial Curve Fitting)</a></h3>
<p>:label:<code>subsec_polynomial-curve-fitting</code></p>
<p>과대적합과 모델 복잡도에 대한 몇 가지 고전적인 직관을 설명하기 위해 다음을 고려해 보십시오:
단일 특성 $x$와 그에 해당하는 실수 값 레이블 $y$로 구성된 훈련 데이터가 주어졌을 때, 레이블 $y$를 추정하기 위해 $d$차 다항식을 찾으려고 합니다.</p>
<p>$$\hat{y}= \sum_{i=0}^d x^i w_i$$</p>
<p>이는 특성이 $x$의 거듭제곱으로 주어지고 모델의 가중치가 $w_i$로 주어지며, 모든 $x$에 대해 $x^0 = 1$이므로 편향이 $w_0$으로 주어지는 선형 회귀 문제일 뿐입니다.
이것은 선형 회귀 문제이므로 제곱 오차를 손실 함수로 사용할 수 있습니다.</p>
<p>고차 다항식 함수는 저차 다항식 함수보다 파라미터가 더 많고 모델 함수의 선택 범위가 더 넓기 때문에 더 복잡합니다.
훈련 데이터셋을 고정했을 때, 고차 다항식 함수는 항상 저차 다항식에 비해 더 낮은(최악의 경우 동일한) 훈련 오차를 달성해야 합니다.
사실, 각 데이터 예제가 서로 다른 $x$ 값을 가질 때마다 데이터 예제 수와 동일한 차수의 다항식 함수는 훈련 세트를 완벽하게 맞출 수 있습니다.
우리는 :numref:<code>fig_capacity_vs_error</code>에서 다항식 차수(모델 복잡도)와 과소적합 및 과대적합 사이의 관계를 비교합니다.</p>
<p><img src="../img/capacity-vs-error.svg" alt="과소적합과 과대적합에 대한 모델 복잡도의 영향." />
:label:<code>fig_capacity_vs_error</code></p>
<h3 id="데이터셋-크기-dataset-size"><a class="header" href="#데이터셋-크기-dataset-size">데이터셋 크기 (Dataset Size)</a></h3>
<p>위의 경계가 이미 나타내듯이, 유념해야 할 또 다른 큰 고려 사항은 데이터셋 크기입니다.
모델을 고정했을 때, 훈련 데이터셋의 샘플 수가 적을수록 과대적합이 발생할 가능성이 더 높고 더 심각해집니다.
훈련 데이터의 양을 늘리면 일반적으로 일반화 오차가 감소합니다.
게다가 일반적으로 데이터는 많을수록 좋습니다.
고정된 작업과 데이터 분포에 대해, 모델 복잡도는 데이터의 양보다 더 빠르게 증가해서는 안 됩니다.
데이터가 많아지면 더 복잡한 모델을 맞추려고 시도할 수 있습니다.
충분한 데이터가 없으면 단순한 모델을 이기기가 더 어려울 수 있습니다.
많은 작업에서 딥러닝은 수만 개의 훈련 예제를 사용할 수 있을 때만 선형 모델보다 뛰어난 성능을 보입니다.
부분적으로 현재 딥러닝의 성공은 인터넷 기업, 저렴한 저장소, 연결된 장치 및 경제의 광범위한 디지털화에서 비롯된 막대한 데이터셋의 풍부함에 크게 힘입었습니다.</p>
<h2 id="모델-선택-model-selection"><a class="header" href="#모델-선택-model-selection">모델 선택 (Model Selection)</a></h2>
<p>:label:<code>subsec_generalization-model-selection</code></p>
<p>일반적으로 우리는 다양한 방식(서로 다른 아키텍처, 훈련 목표, 선택된 특성, 데이터 전처리, 학습률 등)으로 서로 다른 여러 모델을 평가한 후에만 최종 모델을 선택합니다.
많은 모델 중에서 선택하는 것을 적절하게 *모델 선택(model selection)*이라고 합니다.</p>
<p>원칙적으로 우리는 모든 하이퍼파라미터를 선택할 때까지 테스트 세트를 건드려서는 안 됩니다.
만약 모델 선택 과정에서 테스트 데이터를 사용했다면, 테스트 데이터에 과대적합될 위험이 있습니다.
그렇게 되면 심각한 문제에 빠지게 됩니다.
훈련 데이터에 과대적합되더라도 우리를 정직하게 유지해 줄 테스트 데이터에 대한 평가가 항상 있지만, 테스트 데이터에 과대적합되면 우리가 그것을 어떻게 알 수 있겠습니까?
복잡도가 엄격하게 제어될 수 있는 모델에서조차 이것이 어떻게 황당한 결과로 이어질 수 있는지에 대한 예는 :citet:<code>ong2005learning</code>를 참조하십시오.</p>
<p>따라서 모델 선택을 위해 테스트 데이터에 결코 의존해서는 안 됩니다.
그렇다고 모델을 훈련하는 데 사용하는 바로 그 데이터에서 일반화 오차를 추정할 수 없기 때문에 훈련 데이터에만 전적으로 의존할 수도 없습니다.</p>
<p>실제 응용 분야에서 그림은 더 흐릿해집니다.
이상적으로는 가장 좋은 모델을 평가하거나 소수의 모델을 서로 비교하기 위해 테스트 데이터를 단 한 번만 건드려야 하겠지만, 실제 테스트 데이터는 한 번 사용한 후에 폐기되는 경우가 거의 없습니다.
우리는 매번 실험할 때마다 새로운 테스트 세트를 마련할 여력이 거의 없습니다.
사실, 수십 년 동안 벤치마크 데이터를 재활용하는 것은 <a href="https://paperswithcode.com/sota/image-classification-on-imagenet">이미지 분류</a> 및 <a href="https://paperswithcode.com/sota/image-classification-on-mnist">광학 문자 인식</a>과 같은 알고리즘 개발에 상당한 영향을 미칠 수 있습니다.</p>
<p><em>테스트 세트에서의 훈련</em> 문제를 해결하기 위한 일반적인 관행은 데이터를 세 가지로 나누어 훈련 및 테스트 데이터셋 외에 *검증 세트(validation set)*를 포함하는 것입니다.
그 결과 검증 데이터와 테스트 데이터 사이의 경계가 걱정스러울 정도로 모호해지는 불분명한 상황이 벌어집니다.
이 책의 실험에서 명시적으로 언급되지 않는 한, 우리는 실제로는 훈련 데이터와 검증 데이터로 작업하고 있으며 진정한 테스트 세트는 없습니다.
따라서 이 책의 각 실험에서 보고된 정확도는 실제로는 검증 정확도이며 진정한 테스트 세트 정확도가 아닙니다.</p>
<h3 id="교차-검증-cross-validation"><a class="header" href="#교차-검증-cross-validation">교차 검증 (Cross-Validation)</a></h3>
<p>훈련 데이터가 부족할 때, 우리는 적절한 검증 세트를 구성할 만큼 충분한 데이터를 떼어놓을 여유조차 없을 수도 있습니다.
이 문제에 대한 한 가지 대중적인 해결책은 *$K$-겹 교차 검증($K$-fold cross-validation)*을 사용하는 것입니다.
여기서 원래 훈련 데이터는 겹치지 않는 $K$개의 부분 집합으로 나뉩니다.
그런 다음 모델 훈련과 검증이 $K$번 실행되는데, 매번 $K-1$개의 부분 집합에서 훈련하고 다른 부분 집합(해당 라운드에서 훈련에 사용되지 않은 것)에서 검증합니다.
마지막으로 $K$번의 실험 결과를 평균내어 훈련 및 검증 오차를 추정합니다.</p>
<h2 id="요약-summary"><a class="header" href="#요약-summary">요약 (Summary)</a></h2>
<p>이 섹션에서는 머신러닝에서 일반화의 토대 중 일부를 살펴보았습니다.
이러한 아이디어 중 일부는 심층 모델로 갈수록 복잡해지고 직관에 반하게 됩니다. 여기서는 모델이 데이터를 심하게 과대적합할 수 있으며, 관련 복잡도 개념이 암시적이고 직관에 반할 수 있습니다(예: 파라미터가 더 많은 더 큰 아키텍처가 더 잘 일반화됨).
몇 가지 경험 법칙을 남겨드립니다:</p>
<ol>
<li>모델 선택을 위해 검증 세트(또는 <em>$K$-겹 교차 검증</em>)를 사용하십시오.</li>
<li>복잡한 모델은 종종 더 많은 데이터를 필요로 합니다.</li>
<li>관련 복잡도 개념에는 파라미터 수와 파라미터가 허용되는 값의 범위가 모두 포함됩니다.</li>
<li>다른 모든 조건이 동일할 때, 데이터가 많을수록 거의 항상 더 나은 일반화로 이어집니다.</li>
<li>이 모든 일반화에 대한 이야기는 IID 가정을 전제로 합니다. 만약 이 가정을 완화하여 훈련 기간과 테스트 기간 사이에 분포가 이동하는 것을 허용한다면, 추가적인 (아마도 더 완만한) 가정 없이는 일반화에 대해 아무것도 말할 수 없습니다.</li>
</ol>
<h2 id="연습-문제-exercises"><a class="header" href="#연습-문제-exercises">연습 문제 (Exercises)</a></h2>
<ol>
<li>다항식 회귀 문제를 정확하게 풀 수 있는 때는 언제입니까?</li>
<li>종속 확률 변수로 인해 문제를 IID 데이터로 취급하는 것이 바람직하지 않은 예시를 최소 다섯 가지 드십시오.</li>
<li>훈련 오차가 0이 될 것으로 기대할 수 있습니까? 어떤 상황에서 일반화 오차가 0이 되는 것을 볼 수 있을까요?</li>
<li>$K$-겹 교차 검증이 왜 계산 비용이 매우 많이 들까요?</li>
<li>$K$-겹 교차 검증 오차 추정치가 왜 편향되어 있을까요?</li>
<li>VC 차원(VC dimension)은 일련의 함수들의 함수에 의해 임의의 레이블 {± 1}로 분류될 수 있는 최대 포인트 수로 정의됩니다. 이것이 함수 클래스가 얼마나 복잡한지 측정하는 데 좋은 아이디어가 아닌 이유는 무엇일까요? 힌트: 함수의 크기를 고려하십시오.</li>
<li>매니저가 현재 알고리즘이 잘 작동하지 않는 어려운 데이터셋을 줍니다. 더 많은 데이터가 필요하다는 것을 그에게 어떻게 정당화하시겠습니까? 힌트: 데이터를 늘릴 수는 없지만 줄일 수는 있습니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/96">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/97">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/234">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17978">토론</a>
:end_tab:</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_linear-regression/linear-regression-concise.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_linear-regression/weight-decay.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_linear-regression/linear-regression-concise.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_linear-regression/weight-decay.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
