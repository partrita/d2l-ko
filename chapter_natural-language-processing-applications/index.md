# 자연어 처리: 응용 (Natural Language Processing: Applications)
:label:`chap_nlp_app`

우리는 :numref:`chap_nlp_pretrain`에서 텍스트 시퀀스의 토큰을 표현하고 그 표현을 훈련하는 방법을 보았습니다.
이러한 사전 훈련된 텍스트 표현은 다양한 다운스트림 자연어 처리 작업을 위해 다양한 모델에 공급될 수 있습니다.

사실,
이전 챕터에서는 딥러닝 아키텍처를 설명하기 위해
*사전 훈련 없이* 일부 자연어 처리 응용 프로그램을 이미 논의했습니다.
예를 들어, :numref:`chap_rnn`에서,
우리는 RNN에 의존하여 단편 소설과 같은 텍스트를 생성하는 언어 모델을 설계했습니다.
:numref:`chap_modern_rnn` 및 :numref:`chap_attention-and-transformers`에서,
우리는 또한 기계 번역을 위해 RNN 및 주의 메커니즘을 기반으로 한 모델을 설계했습니다.

그러나 이 책은 그러한 모든 응용 프로그램을 포괄적으로 다루려는 의도는 아닙니다.
대신,
우리의 초점은 *자연어 처리 문제를 해결하기 위해 언어의 (심층) 표현 학습을 적용하는 방법*에 있습니다.
사전 훈련된 텍스트 표현이 주어졌을 때,
이 장에서는 각각 단일 텍스트와 텍스트 쌍의 관계를 분석하는
두 가지 인기 있고 대표적인
다운스트림 자연어 처리 작업인
감정 분석과 자연어 추론을 탐구할 것입니다.

![사전 훈련된 텍스트 표현은 다양한 다운스트림 자연어 처리 응용 프로그램을 위해 다양한 딥러닝 아키텍처에 공급될 수 있습니다. 이 장에서는 다양한 다운스트림 자연어 처리 응용 프로그램을 위한 모델을 설계하는 방법에 중점을 둡니다.](../img/nlp-map-app.svg)
:label:`fig_nlp-map-app`

:numref:`fig_nlp-map-app`에 묘사된 바와 같이,
이 장에서는 MLP, CNN, RNN 및 주의와 같은 다양한 유형의 딥러닝 아키텍처를 사용하여 자연어 처리 모델을 설계하는 기본 아이디어를 설명하는 데 중점을 둡니다.
:numref:`fig_nlp-map-app`의 모든 응용 프로그램에 대해 모든 사전 훈련된 텍스트 표현을 모든 아키텍처와 결합하는 것이 가능하지만,
우리는 몇 가지 대표적인 조합을 선택합니다.
구체적으로, 우리는 감정 분석을 위해 RNN 및 CNN을 기반으로 한 인기 있는 아키텍처를 탐구할 것입니다.
자연어 추론의 경우, 우리는 텍스트 쌍을 분석하는 방법을 보여주기 위해 주의와 MLP를 선택합니다.
마지막으로, 우리는 시퀀스 수준(단일 텍스트 분류 및 텍스트 쌍 분류)
및 토큰 수준(텍스트 태깅 및 질문 응답)과 같은
광범위한 자연어 처리 응용 프로그램을 위해 사전 훈련된 BERT 모델을 미세 조정하는 방법을 소개합니다.
구체적인 경험적 사례로,
우리는 자연어 추론을 위해 BERT를 미세 조정할 것입니다.

:numref:`sec_bert`에서 소개했듯이,
BERT는 광범위한 자연어 처리 응용 프로그램을 위해 최소한의 아키텍처 변경을 요구합니다.
그러나 이 이점은 다운스트림 응용 프로그램을 위해
엄청난 수의 BERT 파라미터를 미세 조정하는 비용을 수반합니다.
공간이나 시간이 제한된 경우,
MLP, CNN, RNN 및 주의를 기반으로 한 제작 모델이 더 실현 가능합니다.
다음으로, 감정 분석 응용 프로그램부터 시작하여
각각 RNN 및 CNN을 기반으로 한 모델 설계를 설명합니다.

```toc
:maxdepth: 2

sentiment-analysis-and-dataset
sentiment-analysis-rnn
sentiment-analysis-cnn
natural-language-inference-and-dataset
natural-language-inference-attention
finetuning-bert
natural-language-inference-bert
```