# 파라미터 서버 (Parameter Servers)
:label:`sec_parameterserver`

단일 GPU에서 다중 GPU로, 그리고 다중 GPU를 포함하는 다중 서버로 이동함에 따라(아마도 모두 여러 랙과 네트워크 스위치에 걸쳐 분산되어 있을 것입니다), 분산 및 병렬 훈련을 위한 우리의 알고리즘은 훨씬 더 정교해져야 합니다. 서로 다른 상호 연결은 매우 다른 대역폭을 갖기 때문에 세부 사항이 중요합니다(예: NVLink는 적절한 설정에서 6개 링크를 가로질러 최대 100 GB/s를 제공할 수 있고, PCIe 4.0(16레인)은 32 GB/s를 제공하는 반면, 고속 100GbE 이더넷조차도 10 GB/s에 불과합니다). 동시에 통계 모델러가 네트워킹 및 시스템 전문가가 되기를 기대하는 것은 비합리적입니다.

파라미터 서버의 핵심 아이디어는 분산 잠재 변수 모델의 맥락에서 :citet:`Smola.Narayanamurthy.2010`에서 도입되었습니다. push 및 pull 시맨틱에 대한 설명은 :citet:`Ahmed.Aly.Gonzalez.ea.2012`에서 이어졌고, 시스템 및 오픈 소스 라이브러리에 대한 설명은 :citet:`Li.Andersen.Park.ea.2014`에서 이어졌습니다. 다음에서는 효율성을 위해 필요한 구성 요소들에 대해 동기를 부여할 것입니다.


## 데이터 병렬 훈련 (Data-Parallel Training)

분산 훈련에 대한 데이터 병렬 훈련 접근 방식을 검토해 봅시다. 실전에서 구현하기가 현저히 간단하기 때문에 이 섹션에서는 다른 모든 것들을 제외하고 이것만 사용할 것입니다. 요즘 GPU는 메모리가 풍부하기 때문에 다른 병렬 처리 전략이 선호되는 사용 사례(그래프 상의 딥러닝 제외)는 거의 없습니다. :numref:`fig_parameterserver`는 우리가 :numref:`sec_multi_gpu`에서 구현한 데이터 병렬화의 변형을 설명합니다. 여기서 핵심 측면은 업데이트된 파라미터가 모든 GPU에 재브로드캐스트되기 전에 단일 GPU(GPU 0)에서 기울기의 집계가 발생한다는 것입니다.

![왼쪽: 단일 GPU 훈련. 오른쪽: 다중 GPU 훈련의 변형: (1) 손실과 기울기를 계산함, (2) 모든 기울기가 하나의 GPU에서 집계됨, (3) 파라미터 업데이트가 발생하고 파라미터가 모든 GPU에 재분배됨.](../img/ps.svg)
:label:`fig_parameterserver`

지나고 나서 보면, GPU 0에서 집계하기로 한 결정은 다소 임의적으로 보입니다. 결국 CPU에서 집계할 수도 있습니다. 사실, 일부 파라미터는 한 GPU에서 집계하고 다른 일부는 다른 GPU에서 집계하기로 결정할 수도 있습니다. 최적화 알고리즘이 이를 지원한다면 그렇게 하지 못할 실제 이유는 없습니다. 예를 들어 연관된 기울기 $\mathbf{g}_1, \ldots, \mathbf{g}_4$를 가진 네 개의 파라미터 벡터가 있다면, 각 $\mathbf{g}_i$ ($i = 1, \ldots, 4$)에 대해 하나의 GPU에서 기울기를 집계할 수 있습니다.


이 추론은 자의적이고 경박해 보일 수 있습니다. 결국 수학은 전반적으로 동일하기 때문입니다. 하지만 우리는 :numref:`sec_hardware`에서 논의한 대로 서로 다른 버스가 서로 다른 대역폭을 갖는 실제 물리적 하드웨어를 다루고 있습니다.
실제 4-way GPU 서버를 고려해 보십시오 :numref:`fig_bw_hierarchy`. 특히 연결이 잘 되어 있다면 100 GbE 네트워크 카드를 가질 수도 있습니다. 더 일반적인 수치는 100 MB/s에서 1 GB/s의 유효 대역폭을 가진 1~10 GbE 범위입니다.
CPU는 모든 GPU를 직접 연결하기에 PCIe 레인이 너무 적기 때문에(예: 소비자급 Intel CPU는 24개 레인을 가짐), 우리는 [멀티플렉서](https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches)가 필요합니다. 16x Gen3 링크에서 CPU로부터의 대역폭은 16 GB/s입니다. 이는 또한 *각* GPU가 스위치에 연결되는 속도이기도 합니다. 이는 장치들끼리 통신하는 것이 더 효과적임을 의미합니다.

![4-way GPU 서버.](../img/bw-hierarchy.svg)
:label:`fig_bw_hierarchy`

논의를 위해 기울기가 160 MB라고 가정해 봅시다. 이 경우 나머지 3개의 GPU에서 네 번째 GPU로 기울기를 보내는 데 30ms가 걸립니다(각 전송에는 10ms = 160 MB / 16 GB/s가 소요됨). 가중치 벡터를 다시 전송하기 위해 또 다른 30ms를 더하면 총 60ms에 도달합니다.
모든 데이터를 CPU로 보낸다면 네 개의 GPU *각각*이 데이터를 CPU로 보내야 하므로 40ms의 페널티가 발생하여 총 80ms가 됩니다. 마지막으로 기울기를 각각 40 MB씩 4개 부분으로 나눌 수 있다고 가정해 봅시다. 이제 PCIe 스위치가 모든 링크 간에 풀 대역폭 연산을 제공하므로, 각 부분을 서로 다른 GPU에서 *동시에* 집계할 수 있습니다. 30ms 대신 7.5ms가 소요되어 동기화 작업에 총 15ms가 걸립니다. 요컨대, 파라미터를 어떻게 동기화하느냐에 따라 동일한 작업이 15ms에서 80ms까지 걸릴 수 있습니다. :numref:`fig_ps_distributed`는 파라미터 교환을 위한 서로 다른 전략을 묘사합니다.

![파라미터 동기화 전략.](../img/ps-distributed.svg)
:label:`fig_ps_distributed`

성능을 향상시키는 데 있어 우리가 처분할 수 있는 또 다른 도구가 있음에 유의하십시오: 심층 네트워크에서 상단부터 하단까지 모든 기울기를 계산하는 데는 시간이 좀 걸립니다. 다른 파라미터 그룹에 대해 여전히 계산하느라 바쁜 동안에도 이미 계산된 일부 파라미터 그룹에 대해 기울기 동기화를 시작할 수 있습니다. [Horovod](https://github.com/horovod/horovod)에서 이를 수행하는 방법에 대한 자세한 내용은 예: :citet:`Sergeev.Del-Balso.2018`를 참조하십시오.

## 링 동기화 (Ring Synchronization)

현대 딥러닝 하드웨어에서의 동기화와 관련하여, 우리는 종종 상당히 맞춤화된 네트워크 연결성을 마주하게 됩니다. 예를 들어 AWS p3.16xlarge 및 NVIDIA DGX-2 인스턴스는 :numref:`fig_nvlink`의 연결 구조를 공유합니다. 각 GPU는 최상으로 16 GB/s로 작동하는 PCIe 링크를 통해 호스트 CPU에 연결됩니다. 추가적으로 각 GPU는 또한 6개의 NVLink 연결을 가지며, 각 연결은 양방향으로 300 Gbit/s를 전송할 수 있습니다. 이는 방향당 링크당 약 18 GB/s에 해당합니다. 요컨대, 집계된 NVLink 대역폭은 PCIe 대역폭보다 현저히 높습니다. 질문은 이를 어떻게 가장 효율적으로 사용하느냐입니다.

![8개의 V100 GPU 서버에서의 NVLink 연결성 (이미지 제공: NVIDIA).](../img/nvlink.svg)
:label:`fig_nvlink`

최적의 동기화 전략은 네트워크를 두 개의 링으로 분해하고 이를 사용하여 데이터를 직접 동기화하는 것으로 밝혀졌습니다 :cite:`Wang.Li.Liberty.ea.2018`. :numref:`fig_nvlink_twoloop`은 네트워크가 이중 NVLink 대역폭을 가진 하나의 링(1-2-3-4-5-6-7-8-1)과 일반 대역폭을 가진 다른 하나(1-4-6-3-5-8-2-7-1)로 분해될 수 있음을 보여줍니다. 이 경우 효율적인 동기화 프로토콜을 설계하는 것은 간단하지 않습니다.

![NVLink 네트워크를 두 개의 링으로 분해.](../img/nvlink-twoloop.svg)
:label:`fig_nvlink_twoloop`


다음 사고 실험을 고려해 보십시오: $n$개의 컴퓨팅 노드(또는 GPU)로 구성된 링이 주어졌을 때, 우리는 첫 번째 노드에서 두 번째 노드로 기울기를 보낼 수 있습니다. 거기서 로컬 기울기에 더해져 세 번째 노드로 전송되고, 이런 식으로 계속됩니다. $n-1$단계 후에 집계된 기울기는 마지막으로 방문한 노드에서 찾을 수 있습니다. 즉, 기울기를 집계하는 시간은 노드 수에 따라 선형적으로 증가합니다. 하지만 이렇게 하면 알고리즘이 상당히 비효율적입니다. 결국 어느 시점에서도 통신하는 노드는 하나뿐이기 때문입니다. 기울기를 $n$개의 덩어리로 나누고 노드 $i$에서 시작하여 덩어리 $i$를 동기화하기 시작하면 어떨까요?
각 덩어리의 크기가 $1/n$이므로 이제 총 시간은 $(n-1)/n \approx 1$입니다. 즉, 기울기를 집계하는 데 소요되는 시간은 링의 크기가 커짐에 따라 *증가하지 않습니다*. 이는 꽤 놀라운 결과입니다. :numref:`fig_ringsync`는 $n=4$개 노드에서의 단계 시퀀스를 보여줍니다.

![4개 노드에서의 링 동기화. 각 노드는 조립된 기울기가 오른쪽 이웃에서 발견될 때까지 기울기의 일부를 왼쪽 이웃으로 전송하기 시작합니다.](../img/ringsync.svg)
:label:`fig_ringsync`

8개의 V100 GPU 전체에서 160 MB를 동기화하는 동일한 예제를 사용하면 약 $2 \cdot 160 \textrm{MB} / (3 \cdot 18 \textrm{GB/s}) \approx 6 \textrm{ms}$에 도달합니다. 이는 이제 8개의 GPU를 사용하고 있음에도 불구하고 PCIe 버스를 사용하는 것보다 낫습니다. 실전에서 이러한 수치들은 조금 더 나쁜데, 딥러닝 프레임워크가 종종 통신을 큰 버스트 전송으로 조립하는 데 실패하기 때문입니다.

링 동기화가 다른 동기화 알고리즘과 근본적으로 다르다는 일반적인 오해가 있음에 유의하십시오. 유일한 차이점은 단순한 트리 구조와 비교할 때 동기화 경로가 다소 더 정교하다는 점뿐입니다.

## 다중 기기 훈련 (Multi-Machine Training)

여러 대의 기기에서 분산 훈련을 하는 것은 추가적인 과제를 더합니다: 우리는 어떤 경우에는 한 자릿수 이상 더 느릴 수 있는 상대적으로 낮은 대역폭의 패브릭을 통해서만 연결된 서버들과 통신해야 합니다.
장치들 간의 동기화는 까다롭습니다. 결국 훈련 코드를 실행하는 서로 다른 기기들은 미묘하게 다른 속도를 가질 것이기 때문입니다. 따라서 동기식 분산 최적화를 사용하려면 이들을 *동기화*해야 합니다. :numref:`fig_ps_multimachine`은 분산 병렬 훈련이 어떻게 발생하는지 설명합니다.

1. 각 기기에서 (서로 다른) 데이터 배치를 읽고, 여러 GPU에 분할하여 GPU 메모리로 전송합니다. 거기서 각 GPU 배치에 대해 별도로 예측과 기울기를 계산합니다.
2. 모든 로컬 GPU로부터의 기울기가 하나의 GPU에서 집계됩니다(또는 그 일부가 서로 다른 GPU들에서 집계됩니다).
3. 기울기가 CPU로 전송됩니다.
4. CPU는 모든 기울기를 집계하는 중앙 파라미터 서버로 기울기를 보냅니다.
5. 그런 다음 집계된 기울기를 사용하여 파라미터를 업데이트하고, 업데이트된 파라미터를 개별 CPU로 다시 브로드캐스트합니다.
6. 정보가 하나(또는 여러 개)의 GPU로 전송됩니다.
7. 업데이트된 파라미터가 모든 GPU에 퍼집니다.

![다중 기기 다중 GPU 분산 병렬 훈련.](../img/ps-multimachine.svg)
:label:`fig_ps_multimachine`

이러한 각 연산은 상당히 직관적으로 보입니다. 그리고 실제로 단일 기기 *내에서* 효율적으로 수행될 수 있습니다. 하지만 여러 기기를 살펴보면 중앙 파라미터 서버가 병목 현상이 됨을 알 수 있습니다. 결국 서버당 대역폭이 제한되어 있으므로, $m$명의 작업자(workers)에 대해 모든 기울기를 서버로 보내는 데 걸리는 시간은 $\mathcal{O}(m)$입니다. 서버 수를 $n$으로 늘림으로써 이 장벽을 뚫을 수 있습니다. 이 시점에서 각 서버는 파라미터의 $\mathcal{O}(1/n)$만 저장하면 되므로, 업데이트 및 최적화를 위한 총 시간은 $\mathcal{O}(m/n)$이 됩니다.
두 수치를 일치시키면 우리가 얼마나 많은 작업자를 다루든 관계없이 일정한 확장을 얻을 수 있습니다. 실전에서 우리는 작업자와 서버로 *동일한* 기기를 사용합니다. :numref:`fig_ps_multips`는 이 설계를 보여줍니다(자세한 내용은 :cite:`Li.Andersen.Park.ea.2014` 참조).
특히 여러 기기가 비합리적인 지연 없이 작동하도록 보장하는 것은 간단하지 않습니다. 

![상단: 단일 파라미터 서버는 대역폭이 한정되어 있어 병목 현상이 됩니다. 하단: 여러 파라미터 서버가 집계된 대역폭으로 파라미터의 일부를 저장합니다.](../img/ps-multips.svg)
:label:`fig_ps_multips`

## 키-값 저장소 (Key--Value Stores)

분산 다중 GPU 훈련에 필요한 단계들을 실전에서 구현하는 것은 비자명합니다.
이것이 공통적인 추상화, 즉 재정의된 업데이트 시맨틱을 가진 *키-값 저장소(key--value store)*의 추상화를 사용하는 것이 유익한 이유입니다.


많은 작업자와 많은 GPU에 걸쳐 기울기 $i$에 대한 계산은 다음과 같이 정의될 수 있습니다.

$$\mathbf{g}_{i} = \sum_{k \in \textrm{workers}} \sum_{j \in \textrm{GPUs}} \mathbf{g}_{ijk},$$ 

여기서 $\mathbf{g}_{ijk}$는 작업자 $k$의 GPU $j$에서 분할된 기울기 $i$의 일부입니다.
이 연산의 핵심 측면은 그것이 *교환 법칙이 성립하는 축소(commutative reduction)*라는 점입니다. 즉, 많은 벡터를 하나로 바꾸고 연산이 적용되는 순서가 중요하지 않다는 것입니다. 이는 우리가 어떤 기울기가 언제 수신되는지에 대해 세밀하게 제어할 필요가 없기 때문에 우리의 목적에 아주 좋습니다. 게다가 이 연산은 서로 다른 $i$ 간에 독립적임에 유의하십시오.

이를 통해 다음과 같은 두 가지 연산을 정의할 수 있습니다: 기울기를 누적하는 *push*, 그리고 집계된 기울기를 검색하는 *pull*입니다. 우리는 많은 서로 다른 기울기 세트(결국 많은 레이어가 있으므로)를 가지고 있기 때문에, 키 $i$로 기울기를 인덱싱해야 합니다. Dynamo :cite:`DeCandia.Hastorun.Jampani.ea.2007` 등에서 도입된 키-값 저장소와의 이러한 유사성은 우연이 아닙니다. 이들도 특히 여러 서버에 걸쳐 파라미터를 분산시키는 것과 관련하여 많은 유사한 특성을 만족하기 때문입니다.


키-값 저장소를 위한 push 및 pull 연산은 다음과 같이 설명됩니다:

* **push(key, value)**는 특정 기울기(값)를 작업자로부터 공통 저장소로 보냅니다. 거기서 값은 예를 들어 합산됨으로써 집계됩니다.
* **pull(key, value)**은 모든 작업자로부터의 기울기를 결합한 후와 같이 공통 저장소로부터 집계된 값을 검색합니다.

동기화에 대한 모든 복잡성을 간단한 push 및 pull 연산 뒤에 숨김으로써, 우리는 최적화를 간단한 용어로 표현하고 싶어 하는 통계 모델러의 관심사와 분산 동기화에 내재된 복잡성을 다뤄야 하는 시스템 엔지니어의 관심사를 분리할 수 있습니다.

## 요약 (Summary)

* 동기화는 서버 내의 특정 네트워크 인프라와 연결성에 고도로 적응해야 합니다. 이는 동기화에 걸리는 시간에 큰 차이를 만들 수 있습니다.
* 링 동기화는 p3 및 DGX-2 서버에 최적일 수 있습니다. 다른 서버들의 경우에는 아마 그렇지 않을 수도 있습니다.
* 대역폭 증가를 위해 여러 파라미터 서버를 추가할 때 계층적 동기화 전략이 잘 작동합니다.


## 연습 문제 (Exercises)

1. 링 동기화를 더욱 강화할 수 있습니까? 힌트: 양방향으로 메시지를 보낼 수 있습니다.
1. (계산이 여전히 진행 중인 동안) 비동기 통신을 허용하는 것이 가능합니까? 성능에 어떤 영향을 미칩니까?
1. 오래 실행되는 계산 중에 서버를 잃으면 어떻게 될까요? 계산을 완전히 다시 시작하는 것을 피하기 위해 *결함 허용(fault tolerance)* 메커니즘을 어떻게 설계할 수 있을까요?


[Discussions](https://discuss.d2l.ai/t/366)