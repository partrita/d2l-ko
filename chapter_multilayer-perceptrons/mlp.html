<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>다층 퍼셉트론 (Multilayer Perceptrons) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/mlp.html" class="active"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="다층-퍼셉트론-multilayer-perceptrons"><a class="header" href="#다층-퍼셉트론-multilayer-perceptrons">다층 퍼셉트론 (Multilayer Perceptrons)</a></h1>
<p>:label:<code>sec_mlp</code></p>
<p>:numref:<code>sec_softmax</code>에서 우리는 소프트맥스 회귀를 소개하고, 알고리즘을 밑바닥부터 구현(:numref:<code>sec_softmax_scratch</code>)하거나 고수준 API를 사용(:numref:<code>sec_softmax_concise</code>)하여 구현했습니다. 이를 통해 저해상도 이미지에서 10가지 의류 카테고리를 인식할 수 있는 분류기를 훈련했습니다.
그 과정에서 데이터를 다루고, 출력을 유효한 확률 분포로 강제 변환하고, 적절한 손실 함수를 적용하고, 모델의 파라미터에 대해 이를 최소화하는 방법을 배웠습니다.
이제 단순한 선형 모델의 맥락에서 이러한 메커니즘을 마스터했으므로, 이 책의 주된 관심사이며 비교적 풍부한 모델 클래스인 심층 신경망에 대한 탐구를 시작할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
from jax import grad, vmap
</code></pre>
<h2 id="은닉층-hidden-layers"><a class="header" href="#은닉층-hidden-layers">은닉층 (Hidden Layers)</a></h2>
<p>우리는 :numref:<code>subsec_linear_model</code>에서 아핀 변환을 편향이 추가된 선형 변환으로 설명했습니다.
시작하기 위해, :numref:<code>fig_softmaxreg</code>에 설명된 소프트맥스 회귀 예제에 해당하는 모델 아키텍처를 상기해 보십시오.
이 모델은 단일 아핀 변환과 그 뒤를 잇는 소프트맥스 연산을 통해 입력을 출력에 직접 매핑합니다.
만약 우리의 레이블이 정말로 단순한 아핀 변환에 의해 입력 데이터와 관련이 있다면 이 접근 방식만으로 충분할 것입니다.
그러나 (아핀 변환에서의) 선형성은 <em>강력한</em> 가정입니다.</p>
<h3 id="선형-모델의-한계-limitations-of-linear-models"><a class="header" href="#선형-모델의-한계-limitations-of-linear-models">선형 모델의 한계 (Limitations of Linear Models)</a></h3>
<p>예를 들어, 선형성은 *단조성(monotonicity)*이라는 <em>더 약한</em> 가정을 내포합니다. 즉, 특성의 증가가 항상 모델 출력의 증가를 유발하거나(해당 가중치가 양수인 경우), 항상 모델 출력의 감소를 유발해야 합니다(해당 가중치가 음수인 경우).
때로는 이것이 타당합니다.
예를 들어, 개인이 대출을 갚을지 예측하려고 할 때, 다른 모든 조건이 동일하다면 소득이 높은 신청자가 낮은 신청자보다 항상 갚을 가능성이 더 높다고 합리적으로 가정할 수 있습니다.
단조적이기는 하지만, 이 관계가 상환 확률과 선형적으로 관련이 있을 가능성은 낮습니다. 소득이 0달러에서 5만 달러로 증가하는 것은 100만 달러에서 105만 달러로 증가하는 것보다 상환 가능성 증가에 더 큰 영향을 미칠 가능성이 높습니다.
이를 처리하는 한 가지 방법은 로지스틱 맵(따라서 결과 확률의 로그)을 사용하여 선형성이 더 그럴듯해지도록 결과를 후처리하는 것일 수 있습니다.</p>
<p>단조성을 위반하는 예제를 쉽게 생각해 낼 수 있다는 점에 유의하십시오.
예를 들어 체온의 함수로 건강을 예측하고 싶다고 가정해 봅시다.
체온이 37°C(98.6°F) 이상인 정상 체온을 가진 개인의 경우, 체온이 높을수록 위험이 큽니다.
그러나 체온이 37°C 아래로 떨어지면 체온이 낮을수록 위험이 큽니다!
다시 말하지만, 37°C로부터의 거리를 특성으로 사용하는 것과 같은 기발한 전처리를 통해 문제를 해결할 수도 있습니다.</p>
<p>하지만 고양이와 개 이미지를 분류하는 것은 어떨까요?
위치 (13, 17)에 있는 픽셀의 강도를 높이는 것이 이미지가 개를 묘사할 가능성을 항상 증가(또는 항상 감소)시켜야 할까요?
선형 모델에 의존하는 것은 고양이와 개를 구별하는 유일한 요구 사항이 개별 픽셀의 밝기를 평가하는 것이라는 암묵적인 가정에 해당합니다.
이 접근 방식은 이미지를 반전시켜도 범주가 보존되는 세상에서는 실패할 운명입니다.</p>
<p>그럼에도 불구하고 이전 예제들과 비교할 때 여기에서의 선형성이 명백히 터무니없음에도 불구하고, 간단한 전처리 수정으로 문제를 해결할 수 있을지는 덜 명확합니다.
그 이유는 픽셀의 중요성이 맥락(주변 픽셀의 값)에 복잡한 방식으로 의존하기 때문입니다.
데이터에 대한 표현이 존재하여 특성 간의 관련 상호 작용을 고려하고 그 위에 선형 모델이 적합할 수도 있겠지만, 우리는 단순히 그것을 손으로 계산하는 방법을 모릅니다.
심층 신경망을 사용하여, 우리는 관찰 데이터를 통해 은닉층을 통한 표현과 그 표현에 작용하는 선형 예측기를 공동으로 학습합니다.</p>
<p>이 비선형성 문제는 적어도 한 세기 동안 연구되어 왔습니다 :cite:<code>Fisher.1928</code>. 예를 들어, 가장 기본적인 형태의 결정 트리는 클래스 멤버십을 결정하기 위해 일련의 이진 결정을 사용합니다 :cite:<code>quinlan2014c4</code>. 마찬가지로 커널 방법은 수십 년 동안 비선형 의존성을 모델링하는 데 사용되었습니다 :cite:<code>Aronszajn.1950</code>. 이것은 비모수 스플라인 모델 :cite:<code>Wahba.1990</code>과 커널 방법 :cite:<code>Scholkopf.Smola.2002</code>으로 이어졌습니다. 이것은 또한 뇌가 아주 자연스럽게 해결하는 문제이기도 합니다. 결국 뉴런은 다른 뉴런으로 공급되고, 이는 다시 다른 뉴런으로 공급됩니다 :cite:<code>Cajal.Azoulay.1894</code>.
결과적으로 우리는 상대적으로 단순한 변환의 시퀀스를 갖게 됩니다.</p>
<h3 id="은닉층-통합하기-incorporating-hidden-layers"><a class="header" href="#은닉층-통합하기-incorporating-hidden-layers">은닉층 통합하기 (Incorporating Hidden Layers)</a></h3>
<p>우리는 하나 이상의 은닉층을 통합하여 선형 모델의 한계를 극복할 수 있습니다.
가장 쉬운 방법은 많은 완전 연결 레이어를 서로 쌓는 것입니다.
각 레이어는 출력을 생성할 때까지 그 위의 레이어로 공급됩니다.
처음 $L-1$개의 레이어를 표현으로 생각하고 마지막 레이어를 선형 예측기로 생각할 수 있습니다.
이 아키텍처는 일반적으로 *다층 퍼셉트론(multilayer perceptron)*이라고 불리며 종종 <em>MLP</em>로 축약됩니다 (:numref:<code>fig_mlp</code>).</p>
<p><img src="../img/mlp.svg" alt="5개의 은닉 유닛이 있는 은닉층을 가진 MLP." />
:label:<code>fig_mlp</code></p>
<p>이 MLP는 4개의 입력, 3개의 출력을 가지며, 은닉층에는 5개의 은닉 유닛이 있습니다.
입력 레이어는 어떠한 계산도 포함하지 않으므로, 이 네트워크로 출력을 생성하려면 은닉층과 출력 레이어 모두에 대한 계산을 구현해야 합니다. 따라서 이 MLP의 레이어 수는 2개입니다.
두 레이어 모두 완전 연결되어 있다는 점에 유의하십시오.
모든 입력은 은닉층의 모든 뉴런에 영향을 미치고, 이들 각각은 다시 출력 레이어의 모든 뉴런에 영향을 미칩니다. 아쉽게도 아직 끝난 것은 아닙니다.</p>
<h3 id="선형에서-비선형으로-from-linear-to-nonlinear"><a class="header" href="#선형에서-비선형으로-from-linear-to-nonlinear">선형에서 비선형으로 (From Linear to Nonlinear)</a></h3>
<p>이전과 마찬가지로, 행렬 $\mathbf{X} \in \mathbb{R}^{n \times d}$를 각 예제가 $d$개의 입력(특성)을 가진 $n$개 예제의 미니배치라고 표시합니다.
$h$개의 은닉 유닛을 가진 은닉층이 있는 1-은닉층 MLP의 경우, $\mathbf{H} \in \mathbb{R}^{n \times h}$를 은닉층의 출력으로 표시하며, 이는 *은닉 표현(hidden representations)*입니다.
은닉층과 출력 레이어 모두 완전 연결되어 있으므로, 은닉층 가중치 $\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$와 편향 $\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$, 그리고 출력층 가중치 $\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$와 편향 $\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$를 갖습니다.
이를 통해 1-은닉층 MLP의 출력 $\mathbf{O} \in \mathbb{R}^{n \times q}$를 다음과 같이 계산할 수 있습니다:</p>
<p>$$
\begin{aligned}
\mathbf{H} &amp; = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \
\mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.<br />
\end{aligned}
$$</p>
<p>은닉층을 추가한 후, 이제 우리 모델은 추가적인 파라미터 세트를 추적하고 업데이트해야 합니다.
그렇다면 그 대가로 무엇을 얻었을까요?
위에서 정의된 모델에서는 <em>수고에 대해 아무것도 얻지 못했다</em>는 사실을 알게 되어 놀랄 수도 있습니다!
이유는 명백합니다.
위의 은닉 유닛은 입력의 아핀 함수로 주어지고, 출력(소프트맥스 전)은 단지 은닉 유닛의 아핀 함수일 뿐입니다.
아핀 함수의 아핀 함수는 그 자체로 아핀 함수입니다.
게다가 우리 선형 모델은 이미 모든 아핀 함수를 표현할 수 있었습니다.</p>
<p>이를 공식적으로 확인하기 위해 위 정의에서 은닉층을 붕괴시켜 파라미터 $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$와 $\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$를 가진 동등한 단일 레이어 모델을 얻을 수 있습니다:</p>
<p>$$
\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.
$$</p>
<p>다층 아키텍처의 잠재력을 실현하기 위해, 우리는 한 가지 핵심 요소가 더 필요합니다: 아핀 변환 후 각 은닉 유닛에 적용될 비선형 <em>활성화 함수(activation function)</em> $\sigma$입니다. 예를 들어 인기 있는 선택은 인수에 요소별로 작동하는 ReLU(rectified linear unit) 활성화 함수 :cite:<code>Nair.Hinton.2010</code> $\sigma(x) = \mathrm{max}(0, x)$입니다.
활성화 함수 $\sigma(\cdot)$의 출력을 *활성화(activations)*라고 합니다.
일반적으로 활성화 함수가 있으면 더 이상 MLP를 선형 모델로 붕괴시킬 수 없습니다:</p>
<p>$$
\begin{aligned}
\mathbf{H} &amp; = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \
\mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.<br />
\end{aligned}
$$</p>
<p>$\mathbf{X}$의 각 행은 미니배치의 예제에 해당하므로, 약간의 표기법 남용과 함께 비선형성 $\sigma$가 입력에 행별 방식으로, 즉 한 번에 한 예제씩 적용되도록 정의합니다.
:numref:<code>subsec_softmax_vectorization</code>에서 행별 연산을 나타낼 때 소프트맥스에 대해 동일한 표기법을 사용했다는 점에 유의하십시오.
우리가 사용하는 활성화 함수는 단순히 행별이 아니라 요소별로 적용되는 경우가 꽤 많습니다. 이는 레이어의 선형 부분을 계산한 후, 다른 은닉 유닛이 취한 값을 보지 않고 각 활성화를 계산할 수 있음을 의미합니다.</p>
<p>더 일반적인 MLP를 구축하기 위해, 우리는 이러한 은닉층을 계속 쌓을 수 있습니다.
예를 들어 $\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$ 및 $\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$와 같이 차곡차곡 쌓아 훨씬 더 표현력이 풍부한 모델을 만들 수 있습니다.</p>
<h3 id="보편적-근사자-universal-approximators"><a class="header" href="#보편적-근사자-universal-approximators">보편적 근사자 (Universal Approximators)</a></h3>
<p>우리는 뇌가 매우 정교한 통계 분석을 할 수 있다는 것을 알고 있습니다. 따라서 심층 네트워크가 <em>얼마나 강력할 수 있는지</em> 물어볼 가치가 있습니다. 이 질문은 MLP의 맥락에서 :citet:<code>Cybenko.1989</code>에 의해, 그리고 단일 은닉층을 가진 방사형 기저 함수(RBF) 네트워크로 볼 수 있는 방식으로 재생 커널 힐베르트 공간의 맥락에서 :citet:<code>micchelli1984interpolation</code>에 의해 여러 번 대답되었습니다.
이러한 (및 관련 결과)는 단일 은닉층 네트워크라도 충분한 노드(어쩌면 터무니없이 많은)와 올바른 가중치 세트가 주어지면 어떤 함수든 모델링할 수 있음을 시사합니다.
하지만 실제로 그 함수를 학습하는 것은 어려운 부분입니다.
신경망을 C 프로그래밍 언어와 비슷하다고 생각할 수 있습니다.
다른 모든 현대 언어와 마찬가지로 이 언어는 모든 계산 가능한 프로그램을 표현할 수 있습니다.
하지만 사양을 충족하는 프로그램을 실제로 생각해 내는 것이 어려운 부분입니다.</p>
<p>게다가 단일 은닉층 네트워크가 어떤 함수든 학습할 <em>수</em> 있다고 해서 모든 문제를 하나로 해결하려고 해서는 안 됩니다. 사실 이 경우 커널 방법은 무한 차원 공간에서도 <em>정확하게</em> 문제를 해결할 수 있으므로 훨씬 더 효과적입니다 :cite:<code>Kimeldorf.Wahba.1971,Scholkopf.Herbrich.Smola.2001</code>.
실제로 우리는 (넓은 것보다는) 더 깊은 네트워크를 사용하여 많은 함수를 훨씬 더 간결하게 근사할 수 있습니다 :cite:<code>Simonyan.Zisserman.2014</code>.
우리는 후속 장에서 더 엄격한 주장을 다룰 것입니다.</p>
<h2 id="활성화-함수-activation-functions"><a class="header" href="#활성화-함수-activation-functions">활성화 함수 (Activation Functions)</a></h2>
<p>:label:<code>subsec_activation-functions</code></p>
<p>활성화 함수는 가중 합을 계산하고 여기에 편향을 더함으로써 뉴런이 활성화되어야 하는지 여부를 결정합니다.
이들은 입력 신호를 출력으로 변환하기 위한 미분 가능한 연산자이며, 그들 대부분은 비선형성을 추가합니다.
활성화 함수는 딥러닝의 기본이므로, (<strong>몇 가지 일반적인 함수를 간략하게 살펴봅시다</strong>).</p>
<h3 id="relu-함수"><a class="header" href="#relu-함수">ReLU 함수</a></h3>
<p>구현의 단순성과 다양한 예측 작업에서의 우수한 성능으로 인해 가장 인기 있는 선택은 *ReLU(rectified linear unit)*입니다 :cite:<code>Nair.Hinton.2010</code>.
[<strong>ReLU는 매우 간단한 비선형 변환을 제공합니다</strong>].
요소 $x$가 주어지면 함수는 해당 요소와 $0$ 중 최댓값으로 정의됩니다:</p>
<p>$$\operatorname{ReLU}(x) = \max(x, 0).$$</p>
<p>비공식적으로 ReLU 함수는 양의 요소만 유지하고 해당 활성화를 0으로 설정하여 모든 음의 요소를 버립니다.
직관을 얻기 위해 함수를 플롯할 수 있습니다.
보시다시피 활성화 함수는 부분적으로 선형입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
    y = npx.relu(x)
d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)
y = tf.nn.relu(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'relu(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(-8.0, 8.0, 0.1)
y = jax.nn.relu(x)
d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))
</code></pre>
<p>입력이 음수일 때 ReLU 함수의 도함수는 0이고, 입력이 양수일 때 ReLU 함수의 도함수는 1입니다.
입력이 정확히 0인 값을 취할 때 ReLU 함수는 미분 불가능하다는 점에 유의하십시오.
이러한 경우, 우리는 기본적으로 좌측 도함수를 사용하여 입력이 0일 때 도함수가 0이라고 말합니다.
입력이 실제로 0이 되는 경우는 없을 수 있기 때문에(수학자들은 측도가 0인 집합에서 미분 불가능하다고 말할 것입니다) 우리는 이를 넘어갈 수 있습니다.
미묘한 경계 조건이 중요하다면 우리는 아마도 공학이 아니라 (<em>진짜</em>) 수학을 하고 있다는 옛 격언이 있습니다.
그 통념이 여기에 적용될 수 있거나, 적어도 우리가 제약 최적화 :cite:<code>Mangasarian.1965,Rockafellar.1970</code>를 수행하지 않는다는 사실이 적용될 수 있습니다.
아래에 ReLU 함수의 도함수를 플롯합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.relu(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of relu',
         figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
grad_relu = vmap(grad(jax.nn.relu))
d2l.plot(x, grad_relu(x), 'x', 'grad of relu', figsize=(5, 2.5))
</code></pre>
<p>ReLU를 사용하는 이유는 도함수가 특히 잘 작동하기 때문입니다: 사라지거나 인수를 그대로 통과시킵니다.
이는 최적화가 더 잘 작동하게 만들고 신경망의 이전 버전을 괴롭혔던 잘 문서화된 기울기 소실(vanishing gradients) 문제를 완화했습니다(나중에 자세히 설명).</p>
<p><em>파라미터화된 ReLU</em> (<em>pReLU</em>) 함수 :cite:<code>He.Zhang.Ren.ea.2015</code>를 포함하여 ReLU 함수에는 많은 변형이 있습니다.
이 변형은 ReLU에 선형 항을 추가하여 인수가 음수일 때도 일부 정보가 여전히 통과되도록 합니다:</p>
<p>$$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$$</p>
<h3 id="시그모이드-함수-sigmoid-function"><a class="header" href="#시그모이드-함수-sigmoid-function">시그모이드 함수 (Sigmoid Function)</a></h3>
<p>[** <em>시그모이드 함수(sigmoid function)<em>는 값이 도메인 $\mathbb{R}$에 있는 입력을</em></em>] (<strong>구간 (0, 1)에 있는 출력으로 변환합니다.</strong>)
그렇기 때문에 시그모이드는 종종 *스쿼싱 함수(squashing function)*라고 불립니다: (-inf, inf) 범위의 모든 입력을 (0, 1) 범위의 어떤 값으로 찌그러뜨립니다:</p>
<p>$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$</p>
<p>초기 신경망에서 과학자들은 <em>발화하거나</em> <em>발화하지 않는</em> 생물학적 뉴런을 모델링하는 데 관심이 있었습니다.
따라서 인공 뉴런의 발명가인 맥컬록과 피츠로 거슬러 올라가는 이 분야의 개척자들은 임계값 유닛에 집중했습니다 :cite:<code>McCulloch.Pitts.1943</code>.
임계값 활성화는 입력이 어떤 임계값보다 낮을 때 0 값을 갖고 입력이 임계값을 초과할 때 1 값을 갖습니다.</p>
<p>관심이 경사 기반 학습으로 옮겨갔을 때, 시그모이드 함수는 임계값 유닛에 대한 부드럽고 미분 가능한 근사치이기 때문에 자연스러운 선택이었습니다.
시그모이드는 이진 분류 문제에 대해 출력을 확률로 해석하고 싶을 때 출력 유닛의 활성화 함수로 여전히 널리 사용됩니다: 시그모이드를 소프트맥스의 특수한 경우로 생각할 수 있습니다.
그러나 시그모이드는 은닉층의 대부분의 용도에서 더 간단하고 훈련하기 쉬운 ReLU로 대체되었습니다.
이는 시그모이드가 큰 양수 <em>및</em> 음수 인수에 대해 기울기가 사라지기 때문에 최적화에 어려움을 준다는 사실과 관련이 있습니다 :cite:<code>LeCun.Bottou.Orr.ea.1998</code>.
이로 인해 탈출하기 어려운 고원(plateaus)이 생길 수 있습니다.
그럼에도 불구하고 시그모이드는 중요합니다. 순환 신경망에 대한 나중 장(예: :numref:<code>sec_lstm</code>)에서 우리는 시그모이드 유닛을 활용하여 시간에 따른 정보의 흐름을 제어하는 아키텍처를 설명할 것입니다.</p>
<p>아래에 시그모이드 함수를 플롯합니다.
입력이 0에 가까울 때 시그모이드 함수는 선형 변환에 접근한다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = npx.sigmoid(x)
d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = jax.nn.sigmoid(x)
d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))
</code></pre>
<p>시그모이드 함수의 도함수는 다음 방정식으로 주어집니다:</p>
<p>$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)(1-\operatorname{sigmoid}(x)).$$</p>
<p>시그모이드 함수의 도함수는 아래에 플롯되어 있습니다.
입력이 0일 때 시그모이드 함수의 도함수는 최대 0.25에 도달합니다.
입력이 어느 방향으로든 0에서 멀어질수록 도함수는 0에 접근합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 이전 기울기 지우기
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of sigmoid',
         figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
grad_sigmoid = vmap(grad(jax.nn.sigmoid))
d2l.plot(x, grad_sigmoid(x), 'x', 'grad of sigmoid', figsize=(5, 2.5))
</code></pre>
<h3 id="tanh-함수-tanh-function"><a class="header" href="#tanh-함수-tanh-function">Tanh 함수 (Tanh Function)</a></h3>
<p>:label:<code>subsec_tanh</code></p>
<p>시그모이드 함수처럼, [<strong>tanh(하이퍼볼릭 탄젠트) 함수도 입력을 찌그러뜨려</strong>] (<strong>$-1$과 $1$ 사이의</strong>) 구간의 요소로 변환합니다:</p>
<p>$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$</p>
<p>아래에 tanh 함수를 플롯합니다. 입력이 0에 가까워지면 tanh 함수는 선형 변환에 접근합니다. 함수의 모양은 시그모이드 함수의 모양과 유사하지만, tanh 함수는 좌표계 원점에 대해 점 대칭을 보입니다 :cite:<code>Kalman.Kwasny.1992</code>.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = np.tanh(x)
d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
y = tf.nn.tanh(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'tanh(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = jax.nn.tanh(x)
d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))
</code></pre>
<p>tanh 함수의 도함수는 다음과 같습니다:</p>
<p>$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$</p>
<p>아래에 플롯되어 있습니다.
입력이 0에 가까워지면 tanh 함수의 도함수는 최대 1에 접근합니다.
그리고 시그모이드 함수에서 보았듯이 입력이 어느 방향으로든 0에서 멀어지면 tanh 함수의 도함수는 0에 접근합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 이전 기울기 지우기
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.tanh(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of tanh',
         figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
grad_tanh = vmap(grad(jax.nn.tanh))
d2l.plot(x, grad_tanh(x), 'x', 'grad of tanh', figsize=(5, 2.5))
</code></pre>
<h2 id="요약-및-토론-summary-and-discussion"><a class="header" href="#요약-및-토론-summary-and-discussion">요약 및 토론 (Summary and Discussion)</a></h2>
<p>우리는 이제 비선형성을 통합하여 표현력 있는 다층 신경망 아키텍처를 구축하는 방법을 알게 되었습니다.
참고로 여러분의 지식은 이미 여러분을 1990년경의 실무자와 비슷한 도구 세트를 다룰 수 있는 위치에 올려놓았습니다.
어떤 면에서 여러분은 그 당시 일하던 누구보다 유리한 위치에 있습니다. 강력한 오픈 소스 딥러닝 프레임워크를 활용하여 단 몇 줄의 코드로 모델을 빠르게 구축할 수 있기 때문입니다.
이전에는 이러한 네트워크를 훈련하기 위해 연구자들은 레이어와 도함수를 C, Fortran, 심지어 Lisp(LeNet의 경우)로 명시적으로 코딩해야 했습니다.</p>
<p>부차적인 이점은 ReLU가 시그모이드나 tanh 함수보다 최적화에 훨씬 더 적합하다는 것입니다.
이것이 지난 10년 동안 딥러닝의 부활을 도운 주요 혁신 중 하나라고 주장할 수 있습니다.
하지만 활성화 함수에 대한 연구는 멈추지 않았다는 점에 유의하십시오.
예를 들어, :citet:<code>Hendrycks.Gimpel.2016</code>의 GELU(Gaussian error linear unit) 활성화 함수 $x \Phi(x)$ ($\Phi(x)$는 표준 정규 누적 분포 함수)와 :citet:<code>Ramachandran.Zoph.Le.2017</code>에서 제안한 Swish 활성화 함수 $\sigma(x) = x \operatorname{sigmoid}(\beta x)$는 많은 경우에 더 나은 정확도를 산출할 수 있습니다.</p>
<h2 id="연습-문제-exercises"><a class="header" href="#연습-문제-exercises">연습 문제 (Exercises)</a></h2>
<ol>
<li><em>선형</em> 심층 네트워크, 즉 비선형성 $\sigma$가 없는 네트워크에 레이어를 추가해도 네트워크의 표현력이 결코 증가하지 않음을 보이십시오.
적극적으로 줄이는 예를 드십시오.</li>
<li>pReLU 활성화 함수의 도함수를 계산하십시오.</li>
<li>Swish 활성화 함수 $x \operatorname{sigmoid}(\beta x)$의 도함수를 계산하십시오.</li>
<li>ReLU(또는 pReLU)만 사용하는 MLP가 연속적인 조각별 선형 함수를 구성함을 보이십시오.</li>
<li>시그모이드와 tanh는 매우 유사합니다.
<ol>
<li>$\operatorname{tanh}(x) + 1 = 2 \operatorname{sigmoid}(2x)$임을 보이십시오.</li>
<li>두 비선형성에 의해 파라미터화된 함수 클래스가 동일함을 증명하십시오. 힌트: 아핀 레이어에는 편향 항도 있습니다.</li>
</ol>
</li>
<li>배치 정규화 :cite:<code>Ioffe.Szegedy.2015</code>와 같이 한 번에 하나의 미니배치에 적용되는 비선형성이 있다고 가정해 봅시다. 이것이 어떤 종류의 문제를 일으킬 것으로 예상하십니까?</li>
<li>시그모이드 활성화 함수에 대해 기울기가 사라지는 예를 제시하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/90">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/91">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/226">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17984">토론</a>
:end_tab:</p>
<pre><code></code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_multilayer-perceptrons/index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_multilayer-perceptrons/mlp-implementation.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_multilayer-perceptrons/index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_multilayer-perceptrons/mlp-implementation.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
