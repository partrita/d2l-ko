<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>앵커 박스 (Anchor Boxes) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/anchor.html" class="active"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="앵커-박스-anchor-boxes"><a class="header" href="#앵커-박스-anchor-boxes">앵커 박스 (Anchor Boxes)</a></h1>
<p>:label:<code>sec_anchor</code></p>
<p>객체 감지 알고리즘은 일반적으로
입력 이미지에서 수많은 영역을 샘플링하고, 이 영역에 관심 있는 객체가 포함되어 있는지 확인하며,
객체의 *실제 바운딩 박스(ground-truth bounding boxes)*를
더 정확하게 예측하도록 영역의 경계를 조정합니다.
모델마다 다른 영역 샘플링 방식을 채택할 수 있습니다.
여기서는 그러한 방법 중 하나를 소개합니다.
각 픽셀을 중심으로 다양한 스케일과 가로세로 비율(aspect ratio)을 가진 여러 바운딩 박스를 생성하는 것입니다.
이러한 바운딩 박스를 *앵커 박스(anchor boxes)*라고 합니다.
:numref:<code>sec_ssd</code>에서 앵커 박스를 기반으로 한 객체 감지 모델을 설계할 것입니다.</p>
<p>먼저 더 간결한 출력을 위해 인쇄 정확도를 수정해 보겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, np, npx

np.set_printoptions(2)  # 인쇄 정확도 단순화
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

torch.set_printoptions(2)  # 인쇄 정확도 단순화
</code></pre>
<h2 id="여러-앵커-박스-생성-generating-multiple-anchor-boxes"><a class="header" href="#여러-앵커-박스-생성-generating-multiple-anchor-boxes">여러 앵커 박스 생성 (Generating Multiple Anchor Boxes)</a></h2>
<p>입력 이미지의 높이가 $h$, 너비가 $w$라고 가정해 보겠습니다.
우리는 이미지의 각 픽셀을 중심으로 다양한 모양의 앵커 박스를 생성합니다.
*스케일(scale)*을 $s\in (0, 1]$로,
<em>가로세로 비율(aspect ratio)</em> (높이 대비 너비의 비율)을 $r &gt; 0$이라고 합시다.
그러면 [<strong>앵커 박스의 너비와 높이는 각각 $ws\sqrt{r}$와 $hs/\sqrt{r}$입니다.</strong>]
중심 위치가 주어지면 너비와 높이가 알려진 앵커 박스가 결정됩니다.</p>
<p>다양한 모양의 여러 앵커 박스를 생성하기 위해,
일련의 스케일 $s_1,\ldots, s_n$과
일련의 가로세로 비율 $r_1,\ldots, r_m$을 설정해 봅시다.
각 픽셀을 중심으로 이러한 스케일과 가로세로 비율의 모든 조합을 사용할 때,
입력 이미지에는 총 $whnm$개의 앵커 박스가 생깁니다. 이러한 앵커 박스가 모든 실제 바운딩 박스를 커버할 수 있지만, 계산 복잡도가 너무 높아질 수 있습니다.
실제로,
우리는 (<strong>$s_1$ 또는 $r_1$을 포함하는 조합만 고려</strong>)할 수 있습니다:</p>
<p>(<strong>$$(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1).$$</strong>)</p>
<p>즉, 동일한 픽셀을 중심으로 하는 앵커 박스의 수는 $n+m-1$입니다. 전체 입력 이미지에 대해 총 $wh(n+m-1)$개의 앵커 박스를 생성하게 됩니다.</p>
<p>위의 앵커 박스 생성 방법은 다음 <code>multibox_prior</code> 함수에 구현되어 있습니다. 입력 이미지, 스케일 목록, 가로세로 비율 목록을 지정하면 이 함수가 모든 앵커 박스를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def multibox_prior(data, sizes, ratios):
    """서로 다른 모양의 앵커 박스를 픽셀 단위로 생성합니다."""
    in_height, in_width = data.shape[-2:]
    device, num_sizes, num_ratios = data.ctx, len(sizes), len(ratios)
    boxes_per_pixel = (num_sizes + num_ratios - 1)
    size_tensor = d2l.tensor(sizes, ctx=device)
    ratio_tensor = d2l.tensor(ratios, ctx=device)
    # 앵커를 픽셀 중심으로 이동하려면 오프셋이 필요합니다. 
    # 픽셀의 높이가 1이고 너비가 1이므로 중심을 0.5만큼 오프셋하기로 선택합니다.
    offset_h, offset_w = 0.5, 0.5
    steps_h = 1.0 / in_height  # y축의 스케일링된 단계
    steps_w = 1.0 / in_width  # x축의 스케일링된 단계

    # 앵커 박스의 모든 중심점 생성
    center_h = (d2l.arange(in_height, ctx=device) + offset_h) * steps_h
    center_w = (d2l.arange(in_width, ctx=device) + offset_w) * steps_w
    shift_x, shift_y = d2l.meshgrid(center_w, center_h)
    shift_x, shift_y = shift_x.reshape(-1), shift_y.reshape(-1)

    # 나중에 앵커 박스 모서리 좌표(xmin, xmax, ymin, ymax)를 만드는 데 사용되는
    # `boxes_per_pixel` 수의 높이와 너비를 생성합니다.
    w = np.concatenate((size_tensor * np.sqrt(ratio_tensor[0]),
                        sizes[0] * np.sqrt(ratio_tensor[1:]))) \
                        * in_height / in_width  # 직사각형 입력 처리
    h = np.concatenate((size_tensor / np.sqrt(ratio_tensor[0]),
                        sizes[0] / np.sqrt(ratio_tensor[1:])))
    # 반 높이와 반 너비를 얻으려면 2로 나눕니다.
    anchor_manipulations = np.tile(np.stack((-w, -h, w, h)).T,
                                   (in_height * in_width, 1)) / 2

    # 각 중심점에는 `boxes_per_pixel` 수의 앵커 박스가 있으므로
    # `boxes_per_pixel` 반복으로 모든 앵커 박스 중심의 그리드를 생성합니다.
    out_grid = d2l.stack([shift_x, shift_y, shift_x, shift_y],
                         axis=1).repeat(boxes_per_pixel, axis=0)
    output = out_grid + anchor_manipulations
    return np.expand_dims(output, axis=0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def multibox_prior(data, sizes, ratios):
    """서로 다른 모양의 앵커 박스를 픽셀 단위로 생성합니다."""
    in_height, in_width = data.shape[-2:]
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)
    boxes_per_pixel = (num_sizes + num_ratios - 1)
    size_tensor = d2l.tensor(sizes, device=device)
    ratio_tensor = d2l.tensor(ratios, device=device)
    # 앵커를 픽셀 중심으로 이동하려면 오프셋이 필요합니다. 
    # 픽셀의 높이가 1이고 너비가 1이므로 중심을 0.5만큼 오프셋하기로 선택합니다.
    offset_h, offset_w = 0.5, 0.5
    steps_h = 1.0 / in_height  # y축의 스케일링된 단계
    steps_w = 1.0 / in_width  # x축의 스케일링된 단계

    # 앵커 박스의 모든 중심점 생성
    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')
    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)

    # 나중에 앵커 박스 모서리 좌표(xmin, xmax, ymin, ymax)를 만드는 데 사용되는
    # `boxes_per_pixel` 수의 높이와 너비를 생성합니다.
    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),
                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\
                   * in_height / in_width  # 직사각형 입력 처리
    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
                   sizes[0] / torch.sqrt(ratio_tensor[1:])))
    # 반 높이와 반 너비를 얻으려면 2로 나눕니다.
    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(
                                        in_height * in_width, 1) / 2

    # 각 중심점에는 `boxes_per_pixel` 수의 앵커 박스가 있으므로
    # `boxes_per_pixel` 반복으로 모든 앵커 박스 중심의 그리드를 생성합니다.
    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
                dim=1).repeat_interleave(boxes_per_pixel, dim=0)
    output = out_grid + anchor_manipulations
    return output.unsqueeze(0)
</code></pre>
<p>[<strong>반환된 앵커 박스 변수 <code>Y</code>의 모양</strong>]은 (배치 크기, 앵커 박스 수, 4)임을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
img = image.imread('../img/catdog.jpg').asnumpy()
h, w = img.shape[:2]

print(h, w)
X = np.random.uniform(size=(1, 3, h, w))  # 입력 데이터 구성
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
Y.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
img = d2l.plt.imread('../img/catdog.jpg')
h, w = img.shape[:2]

print(h, w)
X = torch.rand(size=(1, 3, h, w))  # 입력 데이터 구성
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
Y.shape
</code></pre>
<p>앵커 박스 변수 <code>Y</code>의 모양을 (이미지 높이, 이미지 너비, 동일한 픽셀을 중심으로 하는 앵커 박스 수, 4)로 변경하면,
지정된 픽셀 위치를 중심으로 하는 모든 앵커 박스를 얻을 수 있습니다.
다음에서,
우리는 [<strong>(250, 250)을 중심으로 하는 첫 번째 앵커 박스에 액세스합니다</strong>]. 여기에는 앵커 박스의 왼쪽 상단 모서리의 $(x, y)$축 좌표와 오른쪽 하단 모서리의 $(x, y)$축 좌표인 네 가지 요소가 있습니다.
두 축의 좌표 값은 각각 이미지의 너비와 높이로 나누어집니다.</p>
<pre><code class="language-{.python .input}">#@tab all
boxes = Y.reshape(h, w, 5, 4)
boxes[250, 250, 0, :]
</code></pre>
<p>[<strong>이미지의 한 픽셀을 중심으로 하는 모든 앵커 박스를 표시</strong>]하기 위해,
이미지에 여러 바운딩 박스를 그리는 <code>show_bboxes</code> 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def show_bboxes(axes, bboxes, labels=None, colors=None):
    """바운딩 박스를 표시합니다."""

    def make_list(obj, default_values=None):
        if obj is None:
            obj = default_values
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj

    labels = make_list(labels)
    colors = make_list(colors, ['b', 'g', 'r', 'm', 'c'])
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        rect = d2l.bbox_to_rect(d2l.numpy(bbox), color)
        axes.add_patch(rect)
        if labels and len(labels) &gt; i:
            text_color = 'k' if color == 'w' else 'w'
            axes.text(rect.xy[0], rect.xy[1], labels[i],
                      va='center', ha='center', fontsize=9, color=text_color,
                      bbox=dict(facecolor=color, lw=0))
</code></pre>
<p>방금 본 것처럼 변수 <code>boxes</code>의 $x$ 및 $y$ 축 좌표 값은 각각 이미지의 너비와 높이로 나누어졌습니다.
앵커 박스를 그릴 때,
원래 좌표 값을 복원해야 하므로,
아래에서 <code>bbox_scale</code> 변수를 정의합니다.
이제 이미지의 (250, 250)을 중심으로 하는 모든 앵커 박스를 그릴 수 있습니다.
보시다시피 스케일이 0.75이고 가로세로 비율이 1인 파란색 앵커 박스가 이미지의 개를 잘 둘러싸고 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
d2l.set_figsize()
bbox_scale = d2l.tensor((w, h, w, h))
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',
             's=0.75, r=0.5'])
</code></pre>
<h2 id="iou-intersection-over-union"><a class="header" href="#iou-intersection-over-union">[<strong>IoU (Intersection over Union)</strong>]</a></h2>
<p>방금 앵커 박스가 이미지의 개를 "잘" 둘러싸고 있다고 언급했습니다.
객체의 실제 바운딩 박스가 알려진 경우, 여기서 "잘"을 어떻게 정량화할 수 있습니까?
직관적으로, 우리는 앵커 박스와 실제 바운딩 박스 사이의 유사성을 측정할 수 있습니다.
우리는 *자카드 지수(Jaccard index)*가 두 집합 간의 유사성을 측정할 수 있음을 알고 있습니다. 집합 $\mathcal{A}$와 $\mathcal{B}$가 주어지면, 자카드 지수는 교집합의 크기를 합집합의 크기로 나눈 것입니다:</p>
<p>$$J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}.$$</p>
<p>사실, 우리는 모든 바운딩 박스의 픽셀 영역을 픽셀 집합으로 간주할 수 있습니다.
이런 식으로 픽셀 집합의 자카드 지수를 통해 두 바운딩 박스의 유사성을 측정할 수 있습니다. 두 바운딩 박스의 경우, 우리는 일반적으로 이 자카드 지수를 *IoU(Intersection over Union)*라고 부르며, 이는 :numref:<code>fig_iou</code>와 같이 교차 영역 대 합집합 영역의 비율입니다.
IoU의 범위는 0에서 1 사이입니다:
0은 두 바운딩 박스가 전혀 겹치지 않음을 의미하고,
1은 두 바운딩 박스가 동일함을 나타냅니다.</p>
<p><img src="../img/iou.svg" alt="IoU는 두 바운딩 박스의 교차 영역 대 합집합 영역의 비율입니다." />
:label:<code>fig_iou</code></p>
<p>이 섹션의 나머지 부분에서는 IoU를 사용하여 앵커 박스와 실제 바운딩 박스 간의 유사성, 그리고 서로 다른 앵커 박스 간의 유사성을 측정합니다.
두 개의 앵커 또는 바운딩 박스 목록이 주어지면,
다음 <code>box_iou</code>는 이 두 목록에 걸쳐 쌍별 IoU를 계산합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def box_iou(boxes1, boxes2):
    """두 앵커 또는 바운딩 박스 목록에 걸쳐 쌍별 IoU를 계산합니다."""
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    # `boxes1`, `boxes2`, `areas1`, `areas2`의 모양: (boxes1 수, 4),
    # (boxes2 수, 4), (boxes1 수,), (boxes2 수,)
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    # `inter_upperlefts`, `inter_lowerrights`, `inters`의 모양: (boxes1 수,
    # boxes2 수, 2)
    inter_upperlefts = np.maximum(boxes1[:, None, :2], boxes2[:, :2])
    inter_lowerrights = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])
    inters = (inter_lowerrights - inter_upperlefts).clip(min=0)
    # `inter_areas` 및 `union_areas`의 모양: (boxes1 수, boxes2 수)
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    union_areas = areas1[:, None] + areas2 - inter_areas
    return inter_areas / union_areas
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def box_iou(boxes1, boxes2):
    """두 앵커 또는 바운딩 박스 목록에 걸쳐 쌍별 IoU를 계산합니다."""
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    # `boxes1`, `boxes2`, `areas1`, `areas2`의 모양: (boxes1 수, 4),
    # (boxes2 수, 4), (boxes1 수,), (boxes2 수,)
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    # `inter_upperlefts`, `inter_lowerrights`, `inters`의 모양: (boxes1 수,
    # boxes2 수, 2)
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)
    # `inter_areas` 및 `union_areas`의 모양: (boxes1 수, boxes2 수)
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    union_areas = areas1[:, None] + areas2 - inter_areas
    return inter_areas / union_areas
</code></pre>
<h2 id="훈련-데이터에서-앵커-박스-라벨링-labeling-anchor-boxes-in-training-data"><a class="header" href="#훈련-데이터에서-앵커-박스-라벨링-labeling-anchor-boxes-in-training-data">훈련 데이터에서 앵커 박스 라벨링 (Labeling Anchor Boxes in Training Data)</a></h2>
<p>:label:<code>subsec_labeling-anchor-boxes</code></p>
<p>훈련 데이터셋에서,
우리는 각 앵커 박스를 훈련 예제로 간주합니다.
객체 감지 모델을 훈련하려면,
각 앵커 박스에 대한 <em>클래스(class)</em> 및 <em>오프셋(offset)</em> 레이블이 필요합니다.
전자는 앵커 박스와 관련된 객체의 클래스이고,
후자는 앵커 박스에 대한 실제 바운딩 박스의 오프셋입니다.
예측 중에,
각 이미지에 대해
여러 앵커 박스를 생성하고,
모든 앵커 박스에 대한 클래스와 오프셋을 예측하고,
예측된 오프셋에 따라 위치를 조정하여 예측된 바운딩 박스를 얻고,
마지막으로 특정 기준을 충족하는 예측된 바운딩 박스만 출력합니다.</p>
<p>알다시피, 객체 감지 훈련 세트에는
<em>실제 바운딩 박스</em>의 위치와
둘러싸인 객체의 클래스에 대한 레이블이 함께 제공됩니다.
생성된 <em>앵커 박스</em>에 레이블을 지정하기 위해,
앵커 박스에 가장 가까운 <em>할당된</em> 실제 바운딩 박스의 레이블 위치와 클래스를 참조합니다.
다음에서,
가장 가까운 실제 바운딩 박스를 앵커 박스에 할당하는 알고리즘을 설명합니다.</p>
<h3 id="실제-바운딩-박스를-앵커-박스에-할당하기"><a class="header" href="#실제-바운딩-박스를-앵커-박스에-할당하기">[<strong>실제 바운딩 박스를 앵커 박스에 할당하기</strong>]</a></h3>
<p>이미지가 주어졌을 때,
앵커 박스가 $A_1, A_2, \ldots, A_{n_a}$이고 실제 바운딩 박스가 $B_1, B_2, \ldots, B_{n_b}$라고 가정합니다. 여기서 $n_a \geq n_b$입니다.
행렬 $\mathbf{X} \in \mathbb{R}^{n_a \times n_b}$를 정의합시다. 여기서 $i$번째 행과 $j$번째 열의 요소 $x_{ij}$는 앵커 박스 $A_i$와 실제 바운딩 박스 $B_j$의 IoU입니다. 알고리즘은 다음 단계로 구성됩니다:</p>
<ol>
<li>행렬 $\mathbf{X}$에서 가장 큰 요소를 찾아 행과 열 인덱스를 각각 $i_1$과 $j_1$로 표시합니다. 그러면 실제 바운딩 박스 $B_{j_1}$이 앵커 박스 $A_{i_1}$에 할당됩니다. 이것은 꽤 직관적입니다. $A_{i_1}$과 $B_{j_1}$이 모든 앵커 박스와 실제 바운딩 박스 쌍 중에서 가장 가깝기 때문입니다. 첫 번째 할당 후, 행렬 $\mathbf{X}$의 ${i_1}$번째 행과 ${j_1}$번째 열의 모든 요소를 버립니다.</li>
<li>행렬 $\mathbf{X}$의 나머지 요소 중에서 가장 큰 요소를 찾아 행과 열 인덱스를 각각 $i_2$와 $j_2$로 표시합니다. 실제 바운딩 박스 $B_{j_2}$를 앵커 박스 $A_{i_2}$에 할당하고 행렬 $\mathbf{X}$의 ${i_2}$번째 행과 ${j_2}$번째 열의 모든 요소를 버립니다.</li>
<li>이 시점에서 행렬 $\mathbf{X}$의 두 행과 두 열의 요소가 버려졌습니다. 행렬 $\mathbf{X}$의 $n_b$ 열에 있는 모든 요소가 버려질 때까지 진행합니다. 이때, 우리는 $n_b$개의 앵커 박스 각각에 실제 바운딩 박스를 할당했습니다.</li>
<li>나머지 $n_a - n_b$ 앵커 박스만 순회합니다. 예를 들어, 앵커 박스 $A_i$가 주어지면 행렬 $\mathbf{X}$의 $i$번째 행 전체에서 $A_i$와 가장 큰 IoU를 가진 실제 바운딩 박스 $B_j$를 찾고, 이 IoU가 미리 정의된 임계값보다 큰 경우에만 $B_j$를 $A_i$에 할당합니다.</li>
</ol>
<p>구체적인 예를 사용하여 위 알고리즘을 설명해 보겠습니다.
:numref:<code>fig_anchor_label</code> (왼쪽)과 같이, 행렬 $\mathbf{X}$의 최대값이 $x_{23}$이라고 가정하면 실제 바운딩 박스 $B_3$을 앵커 박스 $A_2$에 할당합니다.
그런 다음 행렬의 2행 3열의 모든 요소를 버리고, 나머지 요소(음영 처리된 영역)에서 가장 큰 $x_{71}$을 찾아 실제 바운딩 박스 $B_1$을 앵커 박스 $A_7$에 할당합니다.
다음으로, :numref:<code>fig_anchor_label</code> (가운데)와 같이 행렬의 7행 1열의 모든 요소를 버리고, 나머지 요소(음영 처리된 영역)에서 가장 큰 $x_{54}$를 찾아 실제 바운딩 박스 $B_4$를 앵커 박스 $A_5$에 할당합니다.
마지막으로, :numref:<code>fig_anchor_label</code> (오른쪽)과 같이 행렬의 5행 4열의 모든 요소를 버리고, 나머지 요소(음영 처리된 영역)에서 가장 큰 $x_{92}$를 찾아 실제 바운딩 박스 $B_2$를 앵커 박스 $A_9$에 할당합니다.
그 후에는 나머지 앵커 박스 $A_1, A_3, A_4, A_6, A_8$을 순회하고 임계값에 따라 실제 바운딩 박스를 할당할지 여부를 결정하기만 하면 됩니다.</p>
<p><img src="../img/anchor-label.svg" alt="실제 바운딩 박스를 앵커 박스에 할당하기." />
:label:<code>fig_anchor_label</code></p>
<p>이 알고리즘은 다음 <code>assign_anchor_to_bbox</code> 함수에 구현되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """가장 가까운 실제 바운딩 박스를 앵커 박스에 할당합니다."""
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
    # i번째 행과 j번째 열의 요소 x_ij는 앵커 박스 i와 실제 바운딩 박스 j의 IoU입니다.
    jaccard = box_iou(anchors, ground_truth)
    # 각 앵커에 대해 할당된 실제 바운딩 박스를 유지할 텐서를 초기화합니다.
    anchors_bbox_map = np.full((num_anchors,), -1, dtype=np.int32, ctx=device)
    # 임계값에 따라 실제 바운딩 박스를 할당합니다.
    max_ious, indices = np.max(jaccard, axis=1), np.argmax(jaccard, axis=1)
    anc_i = np.nonzero(max_ious &gt;= iou_threshold)[0]
    box_j = indices[max_ious &gt;= iou_threshold]
    anchors_bbox_map[anc_i] = box_j
    col_discard = np.full((num_anchors,), -1)
    row_discard = np.full((num_gt_boxes,), -1)
    for _ in range(num_gt_boxes):
        max_idx = np.argmax(jaccard)  # 가장 큰 IoU 찾기
        box_idx = (max_idx % num_gt_boxes).astype('int32')
        anc_idx = (max_idx / num_gt_boxes).astype('int32')
        anchors_bbox_map[anc_idx] = box_idx
        jaccard[:, box_idx] = col_discard
        jaccard[anc_idx, :] = row_discard
    return anchors_bbox_map
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """가장 가까운 실제 바운딩 박스를 앵커 박스에 할당합니다."""
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
    # i번째 행과 j번째 열의 요소 x_ij는 앵커 박스 i와 실제 바운딩 박스 j의 IoU입니다.
    jaccard = box_iou(anchors, ground_truth)
    # 각 앵커에 대해 할당된 실제 바운딩 박스를 유지할 텐서를 초기화합니다.
    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
                                  device=device)
    # 임계값에 따라 실제 바운딩 박스를 할당합니다.
    max_ious, indices = torch.max(jaccard, dim=1)
    anc_i = torch.nonzero(max_ious &gt;= iou_threshold).reshape(-1)
    box_j = indices[max_ious &gt;= iou_threshold]
    anchors_bbox_map[anc_i] = box_j
    col_discard = torch.full((num_anchors,), -1)
    row_discard = torch.full((num_gt_boxes,), -1)
    for _ in range(num_gt_boxes):
        max_idx = torch.argmax(jaccard)  # 가장 큰 IoU 찾기
        box_idx = (max_idx % num_gt_boxes).long()
        anc_idx = (max_idx / num_gt_boxes).long()
        anchors_bbox_map[anc_idx] = box_idx
        jaccard[:, box_idx] = col_discard
        jaccard[anc_idx, :] = row_discard
    return anchors_bbox_map
</code></pre>
<h3 id="클래스-및-오프셋-라벨링-labeling-classes-and-offsets"><a class="header" href="#클래스-및-오프셋-라벨링-labeling-classes-and-offsets">클래스 및 오프셋 라벨링 (Labeling Classes and Offsets)</a></h3>
<p>이제 각 앵커 박스에 대한 클래스와 오프셋을 라벨링할 수 있습니다. 앵커 박스 $A$가 실제 바운딩 박스 $B$에 할당되었다고 가정합니다.
한편으로,
앵커 박스 $A$의 클래스는 $B$의 클래스로 라벨링됩니다.
다른 한편으로, 앵커 박스 $A$의 오프셋은 $B$와 $A$의 중심 좌표 사이의 상대적 위치와
이 두 박스 사이의 상대적 크기에 따라 라벨링됩니다.
데이터셋에 있는 다양한 상자의 위치와 크기가 주어지면,
우리는 더 균일하게 분포된 오프셋으로 이어질 수 있는 변환을
해당 상대적 위치와 크기에 적용할 수 있습니다.
이러한 오프셋은 맞추기(fit) 더 쉽습니다.
여기서는 일반적인 변환을 설명합니다.
[**$A$와 $B$의 중심 좌표가 각각 $(x_a, y_a)$와 $(x_b, y_b)$이고,
너비가 $w_a$와 $w_b$,
높이가 $h_a$와 $h_b$라고 주어졌을 때. 우리는 $A$의 오프셋을 다음과 같이 라벨링할 수 있습니다.</p>
<p>$$\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},
\frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},
\frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},
\frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right),
**]
여기서 상수의 기본값은 $\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1$, $\sigma_w=\sigma_h=0.2$입니다.
이 변환은 아래 <code>offset_boxes</code> 함수에 구현되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """앵커 박스 오프셋을 위한 변환."""
    c_anc = d2l.box_corner_to_center(anchors)
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
    offset_wh = 5 * d2l.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    offset = d2l.concat([offset_xy, offset_wh], axis=1)
    return offset
</code></pre>
<p>앵커 박스에 할당된 실제 바운딩 박스가 없는 경우, 앵커 박스의 클래스를 "배경(background)"으로 라벨링합니다.
클래스가 배경인 앵커 박스를 종종 <em>음성(negative)</em> 앵커 박스라고 하고,
나머지를 <em>양성(positive)</em> 앵커 박스라고 합니다.
우리는 다음 <code>multibox_target</code> 함수를 구현하여
실제 바운딩 박스(<code>labels</code> 인수)를 사용하여 [<strong>앵커 박스(<code>anchors</code> 인수)에 대한 클래스와 오프셋을 라벨링</strong>]합니다.
이 함수는 배경 클래스를 0으로 설정하고 새 클래스의 정수 인덱스를 1씩 증가시킵니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def multibox_target(anchors, labels):
    """실제 바운딩 박스를 사용하여 앵커 박스에 라벨을 지정합니다."""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.ctx, anchors.shape[0]
    for i in range(batch_size):
        label = labels[i, :, :]
        anchors_bbox_map = assign_anchor_to_bbox(
            label[:, 1:], anchors, device)
        bbox_mask = np.tile((np.expand_dims((anchors_bbox_map &gt;= 0), 
                                            axis=-1)), (1, 4)).astype('int32')
        # 클래스 레이블 및 할당된 바운딩 박스 좌표를 0으로 초기화
        class_labels = d2l.zeros(num_anchors, dtype=np.int32, ctx=device)
        assigned_bb = d2l.zeros((num_anchors, 4), dtype=np.float32,
                                ctx=device)
        # 할당된 실제 바운딩 박스를 사용하여 앵커 박스의 클래스에 라벨을 지정합니다.
        # 앵커 박스에 할당된 것이 없으면 클래스를 배경으로 라벨링합니다(값은 0으로 유지됨).
        indices_true = np.nonzero(anchors_bbox_map &gt;= 0)[0]
        bb_idx = anchors_bbox_map[indices_true]
        class_labels[indices_true] = label[bb_idx, 0].astype('int32') + 1
        assigned_bb[indices_true] = label[bb_idx, 1:]
        # 오프셋 변환
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)
    bbox_offset = d2l.stack(batch_offset)
    bbox_mask = d2l.stack(batch_mask)
    class_labels = d2l.stack(batch_class_labels)
    return (bbox_offset, bbox_mask, class_labels)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def multibox_target(anchors, labels):
    """실제 바운딩 박스를 사용하여 앵커 박스에 라벨을 지정합니다."""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.device, anchors.shape[0]
    for i in range(batch_size):
        label = labels[i, :, :]
        anchors_bbox_map = assign_anchor_to_bbox(
            label[:, 1:], anchors, device)
        bbox_mask = ((anchors_bbox_map &gt;= 0).float().unsqueeze(-1)).repeat(
            1, 4)
        # 클래스 레이블 및 할당된 바운딩 박스 좌표를 0으로 초기화
        class_labels = torch.zeros(num_anchors, dtype=torch.long,
                                   device=device)
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,
                                  device=device)
        # 할당된 실제 바운딩 박스를 사용하여 앵커 박스의 클래스에 라벨을 지정합니다.
        # 앵커 박스에 할당된 것이 없으면 클래스를 배경으로 라벨링합니다(값은 0으로 유지됨).
        indices_true = torch.nonzero(anchors_bbox_map &gt;= 0)
        bb_idx = anchors_bbox_map[indices_true]
        class_labels[indices_true] = label[bb_idx, 0].long() + 1
        assigned_bb[indices_true] = label[bb_idx, 1:]
        # 오프셋 변환
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)
    bbox_offset = torch.stack(batch_offset)
    bbox_mask = torch.stack(batch_mask)
    class_labels = torch.stack(batch_class_labels)
    return (bbox_offset, bbox_mask, class_labels)
</code></pre>
<h3 id="예제-an-example"><a class="header" href="#예제-an-example">예제 (An Example)</a></h3>
<p>구체적인 예를 통해 앵커 박스 라벨링을 설명해 보겠습니다.
로드된 이미지의 개와 고양이에 대한 실제 바운딩 박스를 정의합니다.
첫 번째 요소는 클래스(개는 0, 고양이는 1)이고 나머지 4개 요소는
왼쪽 상단 모서리와 오른쪽 하단 모서리의 $(x, y)$축 좌표입니다(범위는 0과 1 사이).
또한 왼쪽 상단 모서리와 오른쪽 하단 모서리의 좌표를 사용하여
라벨링할 5개의 앵커 박스 $A_0, \ldots, A_4$를 구성합니다(인덱스는 0부터 시작).
그런 다음 [<strong>이 실제 바운딩 박스와 앵커 박스를 이미지에 그립니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab all
ground_truth = d2l.tensor([[0, 0.1, 0.08, 0.52, 0.92],
                         [1, 0.55, 0.2, 0.9, 0.88]])
anchors = d2l.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                    [0.57, 0.3, 0.92, 0.9]])

fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')
show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);
</code></pre>
<p>위에서 정의한 <code>multibox_target</code> 함수를 사용하여,
개와 고양이에 대한 [<strong>실제 바운딩 박스를 기반으로
이러한 앵커 박스의 클래스와 오프셋을 라벨링</strong>]할 수 있습니다.
이 예에서 배경, 개, 고양이 클래스의 인덱스는 각각 0, 1, 2입니다.
아래에서 앵커 박스 및 실제 바운딩 박스의 예제에 대한 차원을 추가합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
labels = multibox_target(np.expand_dims(anchors, axis=0),
                         np.expand_dims(ground_truth, axis=0))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
labels = multibox_target(anchors.unsqueeze(dim=0),
                         ground_truth.unsqueeze(dim=0))
</code></pre>
<p>반환된 결과에는 세 가지 항목이 있으며 모두 텐서 형식입니다.
세 번째 항목에는 입력 앵커 박스의 라벨링된 클래스가 포함됩니다.</p>
<p>이미지의 앵커 박스 및 실제 바운딩 박스 위치를 기반으로 반환된 클래스 레이블을 분석해 보겠습니다.
먼저, 모든 앵커 박스 및 실제 바운딩 박스 쌍 중에서
앵커 박스 $A_4$와 고양이의 실제 바운딩 박스의 IoU가 가장 큽니다.
따라서 $A_4$의 클래스는 고양이로 라벨링됩니다.
$A_4$ 또는 고양이의 실제 바운딩 박스를 포함하는 쌍을 제외하고, 나머지 중에서
앵커 박스 $A_1$과 개의 실제 바운딩 박스 쌍이 가장 큰 IoU를 가집니다.
따라서 $A_1$의 클래스는 개로 라벨링됩니다.
다음으로, 나머지 세 개의 라벨이 지정되지 않은 앵커 박스 $A_0, A_2, A_3$을 순회해야 합니다.
$A_0$의 경우,
IoU가 가장 큰 실제 바운딩 박스의 클래스는 개이지만,
IoU가 미리 정의된 임계값(0.5) 미만이므로 클래스는 배경으로 라벨링됩니다.
$A_2$의 경우,
IoU가 가장 큰 실제 바운딩 박스의 클래스는 고양이이고 IoU가 임계값을 초과하므로 클래스는 고양이로 라벨링됩니다.
$A_3$의 경우,
IoU가 가장 큰 실제 바운딩 박스의 클래스는 고양이이지만 값이 임계값 미만이므로 클래스는 배경으로 라벨링됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
labels[2]
</code></pre>
<p>두 번째 반환된 항목은 (배치 크기, 앵커 박스 수의 4배) 모양의 마스크 변수입니다.
마스크 변수의 4개 요소마다 각 앵커 박스의 4개 오프셋 값에 해당합니다.
배경 감지에는 신경 쓰지 않으므로,
이 음성 클래스의 오프셋은 목적 함수에 영향을 주지 않아야 합니다.
요소별 곱셈을 통해 마스크 변수의 0은 목적 함수를 계산하기 전에 음성 클래스 오프셋을 필터링합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
labels[1]
</code></pre>
<p>첫 번째 반환된 항목에는 각 앵커 박스에 대해 라벨링된 4개의 오프셋 값이 포함됩니다.
음성 클래스 앵커 박스의 오프셋은 0으로 라벨링된다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">#@tab all
labels[0]
</code></pre>
<h2 id="비최대-억제로-바운딩-박스-예측-predicting-bounding-boxes-with-non-maximum-suppression"><a class="header" href="#비최대-억제로-바운딩-박스-예측-predicting-bounding-boxes-with-non-maximum-suppression">비최대 억제로 바운딩 박스 예측 (Predicting Bounding Boxes with Non-Maximum Suppression)</a></h2>
<p>:label:<code>subsec_predicting-bounding-boxes-nms</code></p>
<p>예측 중에,
우리는 이미지에 대해 여러 앵커 박스를 생성하고 각각에 대한 클래스와 오프셋을 예측합니다.
따라서 <em>예측된 바운딩 박스</em>는 예측된 오프셋이 있는 앵커 박스에 따라 얻어집니다. 아래에서 앵커와 오프셋 예측을 입력으로 받아 [<strong>역 오프셋 변환을 적용하여 예측된 바운딩 박스 좌표를 반환</strong>]하는 <code>offset_inverse</code> 함수를 구현합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def offset_inverse(anchors, offset_preds):
    """예측된 오프셋이 있는 앵커 박스를 기반으로 바운딩 박스를 예측합니다."""
    anc = d2l.box_corner_to_center(anchors)
    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]
    pred_bbox_wh = d2l.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]
    pred_bbox = d2l.concat((pred_bbox_xy, pred_bbox_wh), axis=1)
    predicted_bbox = d2l.box_center_to_corner(pred_bbox)
    return predicted_bbox
</code></pre>
<p>앵커 박스가 많은 경우,
동일한 객체를 둘러싸기 위해 유사한(상당한 겹침이 있는) 예측된 바운딩 박스가 많이 출력될 수 있습니다.
출력을 단순화하기 위해, *비최대 억제(non-maximum suppression, NMS)*를 사용하여
동일한 객체에 속하는 유사한 예측된 바운딩 박스를 병합할 수 있습니다.</p>
<p>비최대 억제 작동 방식은 다음과 같습니다.
예측된 바운딩 박스 $B$에 대해,
객체 감지 모델은 각 클래스에 대한 예측 가능성을 계산합니다.
가장 큰 예측 가능성을 $p$라고 하면, 이 확률에 해당하는 클래스가 $B$의 예측 클래스입니다.
구체적으로, 우리는 $p$를 예측된 바운딩 박스 $B$의 <em>신뢰도(confidence)</em> (점수)라고 합니다.
동일한 이미지에서,
예측된 모든 비배경 바운딩 박스는 신뢰도에 따라 내림차순으로 정렬되어
목록 $L$을 생성합니다.
그런 다음 다음 단계에서 정렬된 목록 $L$을 조작합니다.</p>
<ol>
<li>$L$에서 가장 높은 신뢰도를 가진 예측된 바운딩 박스 $B_1$을 기준으로 선택하고, $B_1$과의 IoU가 미리 정의된 임계값 $\epsilon$을 초과하는 모든 비기준 예측된 바운딩 박스를 $L$에서 제거합니다. 이 시점에서 $L$은 가장 높은 신뢰도를 가진 예측된 바운딩 박스를 유지하지만 너무 유사한 다른 바운딩 박스는 삭제합니다. 한마디로, <em>비최대</em> 신뢰도 점수를 가진 것들은 <em>억제</em>됩니다.</li>
<li>$L$에서 두 번째로 높은 신뢰도를 가진 예측된 바운딩 박스 $B_2$를 다른 기준으로 선택하고, $B_2$과의 IoU가 $\epsilon$을 초과하는 모든 비기준 예측된 바운딩 박스를 $L$에서 제거합니다.</li>
<li>$L$의 모든 예측된 바운딩 박스가 기준으로 사용될 때까지 위의 과정을 반복합니다. 이때 $L$에 있는 예측된 바운딩 박스 쌍의 IoU는 임계값 $\epsilon$ 미만이므로 서로 너무 유사한 쌍은 없습니다.</li>
<li>목록 $L$에 있는 모든 예측된 바운딩 박스를 출력합니다.</li>
</ol>
<p>[<strong>다음 <code>nms</code> 함수는 신뢰도 점수를 내림차순으로 정렬하고 인덱스를 반환합니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def nms(boxes, scores, iou_threshold):
    """예측된 바운딩 박스의 신뢰도 점수를 정렬합니다."""
    B = scores.argsort()[::-1]
    keep = []  # 유지될 예측된 바운딩 박스의 인덱스
    while B.size &gt; 0:
        i = B[0]
        keep.append(i)
        if B.size == 1: break
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        inds = np.nonzero(iou &lt;= iou_threshold)[0]
        B = B[inds + 1]
    return np.array(keep, dtype=np.int32, ctx=boxes.ctx)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def nms(boxes, scores, iou_threshold):
    """예측된 바운딩 박스의 신뢰도 점수를 정렬합니다."""
    B = torch.argsort(scores, dim=-1, descending=True)
    keep = []  # 유지될 예측된 바운딩 박스의 인덱스
    while B.numel() &gt; 0:
        i = B[0]
        keep.append(i)
        if B.numel() == 1: break
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        inds = torch.nonzero(iou &lt;= iou_threshold).reshape(-1)
        B = B[inds + 1]
    return d2l.tensor(keep, device=boxes.device)
</code></pre>
<p>우리는 다음 <code>multibox_detection</code>을 정의하여
[<strong>비최대 억제를 적용하여
바운딩 박스를 예측</strong>]합니다.
구현이 조금 복잡하더라도 걱정하지 마십시오. 구현 직후 구체적인 예제를 통해 작동 방식을 보여드리겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """비최대 억제를 사용하여 바운딩 박스를 예측합니다."""
    device, batch_size = cls_probs.ctx, cls_probs.shape[0]
    anchors = np.squeeze(anchors, axis=0)
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    out = []
    for i in range(batch_size):
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        conf, class_id = np.max(cls_prob[1:], 0), np.argmax(cls_prob[1:], 0)
        predicted_bb = offset_inverse(anchors, offset_pred)
        keep = nms(predicted_bb, conf, nms_threshold)
        # 모든 비 `keep` 인덱스를 찾아 클래스를 배경으로 설정
        all_idx = np.arange(num_anchors, dtype=np.int32, ctx=device)
        combined = d2l.concat((keep, all_idx))
        unique, counts = np.unique(combined, return_counts=True)
        non_keep = unique[counts == 1]
        all_id_sorted = d2l.concat((keep, non_keep))
        class_id[non_keep] = -1
        class_id = class_id[all_id_sorted].astype('float32')
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # 여기서 `pos_threshold`는 양성(비배경) 예측을 위한 임계값입니다
        below_min_idx = (conf &lt; pos_threshold)
        class_id[below_min_idx] = -1
        conf[below_min_idx] = 1 - conf[below_min_idx]
        pred_info = d2l.concat((np.expand_dims(class_id, axis=1),
                                np.expand_dims(conf, axis=1),
                                predicted_bb), axis=1)
        out.append(pred_info)
    return d2l.stack(out)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """비최대 억제를 사용하여 바운딩 박스를 예측합니다."""
    device, batch_size = cls_probs.device, cls_probs.shape[0]
    anchors = anchors.squeeze(0)
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    out = []
    for i in range(batch_size):
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        conf, class_id = torch.max(cls_prob[1:], 0)
        predicted_bb = offset_inverse(anchors, offset_pred)
        keep = nms(predicted_bb, conf, nms_threshold)
        # 모든 비 `keep` 인덱스를 찾아 클래스를 배경으로 설정
        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)
        combined = torch.cat((keep, all_idx))
        uniques, counts = combined.unique(return_counts=True)
        non_keep = uniques[counts == 1]
        all_id_sorted = torch.cat((keep, non_keep))
        class_id[non_keep] = -1
        class_id = class_id[all_id_sorted]
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # 여기서 `pos_threshold`는 양성(비배경) 예측을 위한 임계값입니다
        below_min_idx = (conf &lt; pos_threshold)
        class_id[below_min_idx] = -1
        conf[below_min_idx] = 1 - conf[below_min_idx]
        pred_info = torch.cat((class_id.unsqueeze(1),
                               conf.unsqueeze(1),
                               predicted_bb), dim=1)
        out.append(pred_info)
    return d2l.stack(out)
</code></pre>
<p>이제 [<strong>위의 구현을 4개의 앵커 박스가 있는 구체적인 예제에 적용</strong>]해 보겠습니다.
간단하게 하기 위해, 예측된 오프셋이 모두 0이라고 가정합니다.
이는 예측된 바운딩 박스가 앵커 박스임을 의미합니다.
배경, 개, 고양이 중 각 클래스에 대해
예측된 가능성도 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
anchors = d2l.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],
                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])
offset_preds = d2l.tensor([0] * d2l.size(anchors))
cls_probs = d2l.tensor([[0] * 4,  # 예측된 배경 가능성 
                      [0.9, 0.8, 0.7, 0.1],  # 예측된 개 가능성 
                      [0.1, 0.2, 0.3, 0.9]])  # 예측된 고양이 가능성
</code></pre>
<p>우리는 [<strong>이미지에 신뢰도와 함께 예측된 바운딩 박스를 그릴 수 있습니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab all
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, anchors * bbox_scale,
            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])
</code></pre>
<p>이제 <code>multibox_detection</code> 함수를 호출하여
비최대 억제를 수행할 수 있습니다.
여기서 임계값은 0.5로 설정됩니다.
텐서 입력에 예제에 대한 차원을 추가한다는 점에 유의하십시오.</p>
<p>[<strong>반환된 결과의 모양</strong>]은
(배치 크기, 앵커 박스 수, 6)임을 알 수 있습니다.
가장 안쪽 차원의 6개 요소는
동일한 예측된 바운딩 박스에 대한 출력 정보를 제공합니다.
첫 번째 요소는 예측된 클래스 인덱스로, 0부터 시작합니다(0은 개, 1은 고양이). 값 -1은 비최대 억제에서의 배경 또는 제거를 나타냅니다.
두 번째 요소는 예측된 바운딩 박스의 신뢰도입니다.
나머지 4개 요소는 예측된 바운딩 박스의 왼쪽 상단 모서리와
오른쪽 하단 모서리의 $(x, y)$축 좌표입니다(범위는 0과 1 사이).</p>
<pre><code class="language-{.python .input}">#@tab mxnet
output = multibox_detection(np.expand_dims(cls_probs, axis=0),
                            np.expand_dims(offset_preds, axis=0),
                            np.expand_dims(anchors, axis=0),
                            nms_threshold=0.5)
output
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
output = multibox_detection(cls_probs.unsqueeze(dim=0),
                            offset_preds.unsqueeze(dim=0),
                            anchors.unsqueeze(dim=0),
                            nms_threshold=0.5)
output
</code></pre>
<p>클래스 -1인 예측된 바운딩 박스를 제거한 후,
[<strong>비최대 억제에 의해 유지된 최종 예측된 바운딩 박스를 출력</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
fig = d2l.plt.imshow(img)
for i in d2l.numpy(output[0]):
    if i[0] == -1:
        continue
    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])
    show_bboxes(fig.axes, [d2l.tensor(i[2:]) * bbox_scale], label)
</code></pre>
<p>실제로, 비최대 억제를 수행하기 전에도 낮은 신뢰도를 가진 예측된 바운딩 박스를 제거하여 이 알고리즘의 계산을 줄일 수 있습니다.
또한 비최대 억제의 출력을 후처리할 수도 있습니다. 예를 들어, 더 높은 신뢰도를 가진 결과만
최종 출력에 유지하는 것입니다.</p>
<h2 id="요약-summary"><a class="header" href="#요약-summary">요약 (Summary)</a></h2>
<ul>
<li>우리는 이미지의 각 픽셀을 중심으로 다양한 모양의 앵커 박스를 생성합니다.</li>
<li>자카드 지수라고도 하는 IoU(Intersection over Union)는 두 바운딩 박스의 유사성을 측정합니다. 교집합 영역 대 합집합 영역의 비율입니다.</li>
<li>훈련 세트에서, 각 앵커 박스에 대해 두 가지 유형의 레이블이 필요합니다. 하나는 앵커 박스와 관련된 객체의 클래스이고 다른 하나는 앵커 박스에 대한 실제 바운딩 박스의 오프셋입니다.</li>
<li>예측 중에, 비최대 억제(NMS)를 사용하여 유사한 예측된 바운딩 박스를 제거하여 출력을 단순화할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises"><a class="header" href="#연습-문제-exercises">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>multibox_prior</code> 함수에서 <code>sizes</code>와 <code>ratios</code> 값을 변경해 보십시오. 생성된 앵커 박스에 어떤 변화가 있습니까?</li>
<li>IoU가 0.5인 두 바운딩 박스를 구성하고 시각화해 보십시오. 서로 어떻게 겹칩니까?</li>
<li>:numref:<code>subsec_labeling-anchor-boxes</code> 및 :numref:<code>subsec_predicting-bounding-boxes-nms</code>에서 변수 <code>anchors</code>를 수정해 보십시오. 결과가 어떻게 변합니까?</li>
<li>비최대 억제는 <em>제거</em>함으로써 예측된 바운딩 박스를 억제하는 탐욕 알고리즘입니다. 제거된 것 중 일부가 실제로 유용할 수 있습니까? 이 알고리즘을 <em>부드럽게(softly)</em> 억제하도록 어떻게 수정할 수 있습니까? Soft-NMS :cite:<code>Bodla.Singh.Chellappa.ea.2017</code>를 참조할 수 있습니다.</li>
<li>수작업이 아닌, 비최대 억제를 학습할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/370">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1603">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_computer-vision/bounding-box.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_computer-vision/multiscale-object-detection.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_computer-vision/bounding-box.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_computer-vision/multiscale-object-detection.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
