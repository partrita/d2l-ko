<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>적분 (Integral Calculus) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html" class="active"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="적분학-integral-calculus"><a class="header" href="#적분학-integral-calculus">적분학 (Integral Calculus)</a></h1>
<p>:label:<code>sec_integral_calculus</code></p>
<p>미분은 전통적인 미적분 교육 내용의 절반만을 구성합니다. 다른 기둥인 적분은 "이 곡선 아래의 넓이는 얼마인가?"라는 다소 별개의 질문으로 시작합니다. 겉보기에는 관련이 없어 보이지만, 적분은 <em>미적분학의 기본 정리</em>라고 알려진 것을 통해 미분과 긴밀하게 얽혀 있습니다.</p>
<p>이 책에서 논의하는 머신러닝 수준에서 적분에 대한 깊은 이해가 필요하지는 않을 것입니다. 하지만 나중에 마주칠 추가 응용 분야를 위한 토대를 마련하기 위해 짧은 소개를 제공할 것입니다.</p>
<h2 id="기하학적-해석-geometric-interpretation"><a class="header" href="#기하학적-해석-geometric-interpretation">기하학적 해석 (Geometric Interpretation)</a></h2>
<p>함수 $f(x)$가 있다고 가정해 봅시다. 단순함을 위해 $f(x)$가 비음수(결코 0보다 작은 값을 취하지 않음)라고 가정합시다. 우리가 이해하고자 하는 것은: $f(x)$와 $x$축 사이에 포함된 넓이가 얼마인가 하는 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mpl_toolkits import mplot3d
from mxnet import np, npx
npx.set_np()

x = np.arange(-2, 2, 0.01)
f = np.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist(), f.tolist())
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
from mpl_toolkits import mplot3d
import torch

x = torch.arange(-2, 2, 0.01)
f = torch.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist(), f.tolist())
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
from mpl_toolkits import mplot3d
import tensorflow as tf

x = tf.range(-2, 2, 0.01)
f = tf.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.numpy(), f.numpy())
d2l.plt.show()
</code></pre>
<p>대부분의 경우 이 넓이는 무한하거나 정의되지 않을 것이므로($f(x) = x^{2}$ 아래의 넓이를 고려해 보십시오), 사람들은 종종 한 쌍의 끝점, 가령 $a$와 $b$ 사이의 넓이에 대해 이야기할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.arange(-2, 2, 0.01)
f = np.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250])
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.arange(-2, 2, 0.01)
f = torch.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250])
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.range(-2, 2, 0.01)
f = tf.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.numpy()[50:250], f.numpy()[50:250])
d2l.plt.show()
</code></pre>
<p>우리는 이 넓이를 아래의 적분 기호로 나타낼 것입니다.</p>
<p>$$ 	extrm{Area}(\mathcal{A}) = \int_a^b f(x) ;dx. $$</p>
<p>내부 변수는 $\sum$에서의 합의 인덱스와 매우 흡사한 더미 변수(dummy variable)이므로, 우리가 원하는 어떤 내부 값으로도 동등하게 쓰일 수 있습니다.</p>
<p>$$ \int_a^b f(x) ;dx = \int_a^b f(z) ;dz. $$</p>
<p>우리가 그러한 적분을 어떻게 근사할 수 있는지 이해하려는 전통적인 방법이 있습니다: $a$와 $b$ 사이의 영역을 취해 $N$개의 수직 슬라이스로 자르는 것을 상상할 수 있습니다. $N$이 크면 각 슬라이스의 넓이를 직사각형으로 근사할 수 있으며, 그런 다음 넓이들을 더해 곡선 아래의 총 넓이를 얻을 수 있습니다. 코드에서 이를 수행하는 예제를 살펴봅시다. 나중에 실제 값을 얻는 방법을 볼 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
epsilon = 0.05
a = 0
b = 2

x = np.arange(a, b, epsilon)
f = x / (1 + x**2)

approx = np.sum(epsilon*f)
true = np.log(2) / 2

d2l.set_figsize()
d2l.plt.bar(x.asnumpy(), f.asnumpy(), width=epsilon, align='edge')
d2l.plt.plot(x, f, color='black')
d2l.plt.ylim([0, 1])
d2l.plt.show()

f'근사값: {approx}, 참값: {true}'
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
epsilon = 0.05
a = 0
b = 2

x = torch.arange(a, b, epsilon)
f = x / (1 + x**2)

approx = torch.sum(epsilon*f)
true = torch.log(torch.tensor([5.])) / 2

d2l.set_figsize()
d2l.plt.bar(x, f, width=epsilon, align='edge')
d2l.plt.plot(x, f, color='black')
d2l.plt.ylim([0, 1])
d2l.plt.show()

f'근사값: {approx}, 참값: {true}'
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
epsilon = 0.05
a = 0
b = 2

x = tf.range(a, b, epsilon)
f = x / (1 + x**2)

approx = tf.reduce_sum(epsilon*f)
true = tf.math.log(tf.constant([5.])) / 2

d2l.set_figsize()
d2l.plt.bar(x, f, width=epsilon, align='edge')
d2l.plt.plot(x, f, color='black')
d2l.plt.ylim([0, 1])
d2l.plt.show()

f'근사값: {approx}, 참값: {true}'
</code></pre>
<p>문제는 이것이 수치적으로는 수행될 수 있지만, 분석적으로는 다음과 같은 아주 간단한 함수에 대해서만 이 접근 방식을 사용할 수 있다는 점입니다.</p>
<p>$$ \int_a^b x ;dx. $$</p>
<p>위 코드 예제와 같이 다소 더 복잡한 것들은</p>
<p>$$ \int_a^b \frac{x}{1+x^{2}} ;dx. $$</p>
<p>그러한 직접적인 방법으로 해결할 수 있는 범위를 넘어섭니다.</p>
<p>우리는 대신 다른 접근 방식을 취할 것입니다. 넓이의 개념과 함께 직관적으로 작업하고, 적분을 찾는 데 사용되는 주요 계산 도구인 <em>미적분학의 기본 정리</em>를 배울 것입니다. 이것이 우리 적분 연구의 기초가 될 것입니다.</p>
<h2 id="미적분학의-기본-정리-the-fundamental-theorem-of-calculus"><a class="header" href="#미적분학의-기본-정리-the-fundamental-theorem-of-calculus">미적분학의 기본 정리 (The Fundamental Theorem of Calculus)</a></h2>
<p>적분 이론을 깊이 파고들기 위해 함수를 하나 도입합시다.</p>
<p>$$ F(x) = \int_0^x f(y) dy. $$</p>
<p>이 함수는 $x$를 어떻게 바꾸느냐에 따라 $0$과 $x$ 사이의 넓이를 측정합니다. 다음이 성립하므로 이것이 우리가 필요한 전부임에 유의하십시오.</p>
<p>$$ \int_a^b f(x) ;dx = F(b) - F(a). $$</p>
<p>이는 그림 :numref:<code>fig_area-subtract</code>에 표시된 것처럼 우리가 먼 끝점까지의 넓이를 측정하고 가까운 끝점까지의 넓이를 뺄 수 있다는 사실을 수학적으로 인코딩한 것입니다.</p>
<p><img src="../img/sub-area.svg" alt="곡선 아래 두 점 사이의 넓이를 계산하는 문제를 특정 점의 왼쪽 넓이를 계산하는 것으로 왜 축소할 수 있는지 시각화한 그림." />
:label:<code>fig_area-subtract</code></p>
<p>따라서 우리는 $F(x)$가 무엇인지 알아냄으로써 임의의 구간에 대한 적분이 무엇인지 알아낼 수 있습니다.</p>
<p>그렇게 하기 위해 실험을 하나 고려해 봅시다. 미적분에서 자주 하듯이, 값을 아주 조금 옮겼을 때 어떤 일이 일어나는지 상상해 봅시다. 위의 언급으로부터 다음을 압니다.</p>
<p>$$ F(x+\epsilon) - F(x) = \int_x^{x+\epsilon} f(y) ; dy. $$</p>
<p>이는 함수가 함수의 아주 작은 조각 아래의 넓이만큼 변한다는 것을 알려줍니다.</p>
<p>이것이 우리가 근사를 하는 지점입니다. 만약 우리가 이와 같이 아주 작은 넓이의 조각을 본다면, 이 넓이는 높이가 $f(x)$이고 밑변의 너비가 $\epsilon$인 직사각형의 넓이에 가깝게 보입니다. 실제로 $\epsilon \rightarrow 0$에 따라 이 근사가 점점 더 좋아진다는 것을 보일 수 있습니다. 따라서 다음과 같이 결론지을 수 있습니다.</p>
<p>$$ F(x+\epsilon) - F(x) \approx \epsilon f(x). $$</p>
<p>하지만 이제 주목할 수 있습니다: 이것은 우리가 $F$의 도함수를 계산하고 있을 때 기대하는 바로 그 패턴입니다! 따라서 우리는 다음과 같은 다소 놀라운 사실을 봅니다.</p>
<p>$$ \frac{dF}{dx}(x) = f(x). $$</p>
<p>이것이 <em>미적분학의 기본 정리</em>입니다. 우리는 이를 확장된 형태로 다음과 같이 쓸 수 있습니다.
$$\frac{d}{dx}\int_0^x  f(y) ; dy = f(x).$$
:eqlabel:<code>eq_ftc</code></p>
<p>이것은 넓이를 찾는 개념(선험적으로 다소 어려움)을 취하여 도함수(훨씬 더 완전히 이해된 것)에 대한 진술로 축소합니다. 우리가 반드시 해야 할 마지막 한 가지 코멘트는 이것이 $F(x)$가 정확히 무엇인지 알려주지는 않는다는 것입니다. 실제로 임의의 $C$에 대해 $F(x) + C$는 동일한 도함수를 갖습니다. 이는 적분 이론에서의 삶의 단면(fact-of-life)입니다. 고맙게도 정적분으로 작업할 때 상수들은 상쇄되어 결과와 무관하게 됨에 유의하십시오.</p>
<p>$$ \int_a^b f(x) ; dx = (F(b) + C) - (F(a) + C) = F(b) - F(a). $$</p>
<p>이것이 추상적인 헛소리처럼 보일 수 있지만, 적분 계산에 대한 완전히 새로운 관점을 우리에게 제공했다는 것을 잠시 감상해 봅시다. 우리의 목표는 더 이상 넓이를 복구하기 위해 어떤 종류의 자르기 및 합산 과정을 수행하는 것이 아니라, 단순히 도함수가 우리가 가진 함수인 함수를 찾는 것입니다! 이는 이제 :numref:<code>sec_derivative_table</code>의 표를 반대로 함으로써 많은 다소 어려운 적분들을 나열할 수 있기 때문에 놀라운 일입니다. 예를 들어, 우리는 $x^{n}$의 도함수가 $nx^{n-1}$임을 압니다. 따라서 기본 정리 :eqref:<code>eq_ftc</code>를 사용하여 다음과 같이 말할 수 있습니다.</p>
<p>$$ \int_0^{x} ny^{n-1} ; dy = x^n - 0^n = x^n. $$</p>
<p>마찬가지로, 우리는 $e^{x}$의 도함수가 자기 자신임을 압니다. 이는 다음을 의미합니다.</p>
<p>$$ \int_0^{x} e^x ; dx = e^x - e^0 = e^x - 1. $$</p>
<p>이런 식으로 우리는 미분학의 아이디어들을 자유롭게 활용하여 적분학 전체 이론을 개발할 수 있습니다. 모든 적분 규칙은 이 한 가지 사실에서 파생됩니다.</p>
<h2 id="변수-변환-change-of-variables"><a class="header" href="#변수-변환-change-of-variables">변수 변환 (Change of Variables)</a></h2>
<p>:label:<code>subsec_integral_example</code></p>
<p>미분과 마찬가지로 적분 계산을 더 다루기 쉽게 만드는 여러 규칙이 있습니다. 실제로 미분학의 모든 규칙(곱의 미분법, 합의 법칙, 연쇄 법칙과 같은)은 각각 적분학의 대응하는 규칙(부분 적분법, 적분의 선형성, 변수 변환 공식)을 갖습니다. 이 섹션에서는 목록 중에서 가장 중요하다고 할 수 있는 변수 변환 공식을 깊이 파고들 것입니다.</p>
<p>먼저, 함수 자체가 적분인 함수가 있다고 가정합시다.</p>
<p>$$ F(x) = \int_0^x f(y) ; dy. $$</p>
<p>이 함수를 다른 함수와 합성하여 $F(u(x))$를 얻었을 때 어떻게 보이는지 알고 싶다고 가정해 봅시다. 연쇄 법칙에 의해 다음을 압니다.</p>
<p>$$ \frac{d}{dx}F(u(x)) = \frac{dF}{du}(u(x))\cdot \frac{du}{dx}. $$</p>
<p>우리는 위에서와 같이 기본 정리 :eqref:<code>eq_ftc</code>를 사용하여 이를 적분에 대한 진술로 바꿀 수 있습니다. 이는 다음을 제공합니다.</p>
<p>$$ F(u(x)) - F(u(0)) = \int_0^x \frac{dF}{du}(u(y))\cdot \frac{du}{dy} ;dy. $$</p>
<p>$F$ 자체가 적분임을 상기하면 좌변은 다음과 같이 다시 쓰일 수 있습니다.</p>
<p>$$ \int_{u(0)}^{u(x)} f(y) ; dy = \int_0^x \frac{dF}{du}(u(y))\cdot \frac{du}{dy} ;dy. $$</p>
<p>마찬가지로 $F$가 적분임을 상기하면 기본 정리 :eqref:<code>eq_ftc</code>를 사용하여 $\frac{dF}{dx} = f$임을 인식할 수 있으며, 따라서 다음과 같이 결론지을 수 있습니다.</p>
<p>$$\int_{u(0)}^{u(x)} f(y) ; dy = \int_0^x f(u(y))\cdot \frac{du}{dy} ;dy.$$
:eqlabel:<code>eq_change_var</code></p>
<p>이것이 <em>변수 변환(change of variables)</em> 공식입니다.</p>
<p>더 직관적인 유도를 위해, $x$와 $x+\epsilon$ 사이의 $f(u(x))$ 적분을 취할 때 어떤 일이 일어나는지 고려해 보십시오. 작은 $\epsilon$에 대해 이 적분은 대략 연관된 직사각형의 넓이인 $\epsilon f(u(x))$입니다. 이제 이것을 $u(x)$에서 $u(x+\epsilon)$까지의 $f(y)$ 적분과 비교해 봅시다. 우리는 $u(x+\epsilon) \approx u(x) + \epsilon \frac{du}{dx}(x)$임을 알고 있으므로, 이 직사각형의 넓이는 대략 $\epsilon \frac{du}{dx}(x)f(u(x))$입니다. 따라서 이러한 두 직사각형의 넓이가 일치하게 하려면, 그림 :numref:<code>fig_rect-transform</code>에 설명된 것처럼 첫 번째 것에 $\frac{du}{dx}(x)$를 곱해야 합니다.</p>
<p><img src="../img/rect-trans.svg" alt="변수 변환 하에서 단일 얇은 직사각형의 변환을 시각화한 그림." />
:label:<code>fig_rect-transform</code></p>
<p>이는 다음을 알려줍니다.</p>
<p>$$ \int_x^{x+\epsilon} f(u(y))\frac{du}{dy}(y);dy = \int_{u(x)}^{u(x+\epsilon)} f(y) ; dy. $$</p>
<p>이것이 단일 작은 직사각형에 대해 표현된 변수 변환 공식입니다.</p>
<p>만약 $u(x)$와 $f(x)$가 적절하게 선택된다면, 이는 믿을 수 없을 정도로 복잡한 적분의 계산을 가능하게 할 수 있습니다. 예를 들어, 우리가 $f(y) = 1$ 및 $u(x) = e^{-x^{2}}$($\frac{du}{dx}(x) = -2xe^{-x^{2}}$를 의미함)를 선택하더라도, 이는 예를 들어 다음을 보여줄 수 있습니다.</p>
<p>$$ e^{-1} - 1 = \int_{e^{-0}}^{e^{-1}} 1 ; dy = -2\int_0^{1} ye^{-y^2};dy, $$</p>
<p>따라서 재배열하면 다음과 같습니다.</p>
<p>$$ \int_0^{1} ye^{-y^2}; dy = \frac{1-e^{-1}}{2}. $$</p>
<h2 id="부호-관례에-대한-코멘트-a-comment-on-sign-conventions"><a class="header" href="#부호-관례에-대한-코멘트-a-comment-on-sign-conventions">부호 관례에 대한 코멘트 (A Comment on Sign Conventions)</a></h2>
<p>눈치 빠른 독자들은 위의 계산에서 이상한 점을 발견할 것입니다. 즉, 다음과 같은 계산입니다.</p>
<p>$$ \int_{e^{-0}}^{e^{-1}} 1 ; dy = e^{-1} -1 &lt; 0, $$</p>
<p>음수를 생성할 수 있습니다. 넓이에 대해 생각할 때 음수 값을 보는 것이 이상할 수 있으므로 관례가 무엇인지 파헤쳐 볼 가치가 있습니다.</p>
<p>수학자들은 부호가 있는 넓이(signed areas)의 개념을 취합니다. 이는 두 가지 방식으로 나타납니다. 첫째, 때때로 0보다 작은 함수 $f(x)$를 고려하면 넓이 또한 음수가 될 것입니다. 예를 들어 다음과 같습니다.</p>
<p>$$ \int_0^{1} (-1);dx = -1. $$</p>
<p>마찬가지로, 왼쪽에서 오른쪽이 아니라 오른쪽에서 왼쪽으로 진행되는 적분 또한 음수 넓이로 취해집니다.</p>
<p>$$ \int_0^{-1} 1; dx = -1. $$</p>
<p>표준 넓이(양수 함수의 왼쪽에서 오른쪽까지)는 항상 양수입니다. 그것을 뒤집어서 얻은 것(가령 $x$축을 뒤집어 음수 함수의 적분을 얻거나, $y$축을 뒤집어 잘못된 순서의 적분을 얻는 경우)은 음수 넓이를 생성할 것입니다. 실제로 두 번 뒤집으면 상쇄되는 한 쌍의 음수 부호를 주어 양수 넓이를 갖게 될 것입니다.</p>
<p>$$ \int_0^{-1} (-1);dx =  1. $$</p>
<p>이 논의가 익숙하게 들린다면 그렇습니다! :numref:<code>sec_geometry-linear-algebraic-ops</code>에서 우리는 행렬식이 거의 동일한 방식으로 부호가 있는 넓이를 어떻게 나타내는지 논의했습니다.</p>
<h2 id="다중-적분-multiple-integrals"><a class="header" href="#다중-적분-multiple-integrals">다중 적분 (Multiple Integrals)</a></h2>
<p>어떤 경우에는 고차원에서 작업해야 할 것입니다. 예를 들어 $f(x, y)$와 같은 두 변수 함수가 있고 $x$가 $[a, b]$를 범위로 하고 $y$가 $[c, d]$를 범위로 할 때 $f$ 아래의 부피를 알고 싶다고 가정해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 그리드 구성 및 함수 계산
x, y = np.meshgrid(np.linspace(-2, 2, 101), np.linspace(-2, 2, 101),
                   indexing='ij')
z = np.exp(- x**2 - y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x.asnumpy(), y.asnumpy(), z.asnumpy())
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.plt.xticks([-2, -1, 0, 1, 2])
d2l.plt.yticks([-2, -1, 0, 1, 2])
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(0, 1)
ax.dist = 12
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 그리드 구성 및 함수 계산
x, y = torch.meshgrid(torch.linspace(-2, 2, 101), torch.linspace(-2, 2, 101))
z = torch.exp(- x**2 - y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.plt.xticks([-2, -1, 0, 1, 2])
d2l.plt.yticks([-2, -1, 0, 1, 2])
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(0, 1)
ax.dist = 12
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 그리드 구성 및 함수 계산
x, y = tf.meshgrid(tf.linspace(-2., 2., 101), tf.linspace(-2., 2., 101))
z = tf.exp(- x**2 - y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.plt.xticks([-2, -1, 0, 1, 2])
d2l.plt.yticks([-2, -1, 0, 1, 2])
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(0, 1)
ax.dist = 12
</code></pre>
<p>우리는 이를 다음과 같이 씁니다.</p>
<p>$$ \int_{[a, b]\times[c, d]} f(x, y);dx;dy. $$</p>
<p>이 적분을 계산하고 싶다고 가정합시다. 저의 주장은 우리가 먼저 $x$에 대해 반복적으로 적분한 다음 $y$에 대한 적분으로 전환함으로써 이를 수행할 수 있다는 것입니다. 즉, 다음과 같습니다.</p>
<p>$$ \int_{[a, b]\times[c, d]} f(x, y);dx;dy = \int_c^{d} \left(\int_a^{b} f(x, y) ;dx\right) ; dy. $$</p>
<p>왜 그런지 봅시다.</p>
<p>우리가 함수를 정수 좌표 $i, j$로 인덱싱할 $\epsilon \times \epsilon$ 사각형들로 나눈 위 그림을 고려하십시오. 이 경우 우리의 적분은 대략 다음과 같습니다.</p>
<p>$$ \sum_{i, j} \epsilon^{2} f(\epsilon i, \epsilon j). $$</p>
<p>문제를 이산화하고 나면, 우리는 이러한 사각형의 값들을 원하는 순서대로 더할 수 있으며 값을 바꾸는 것에 대해 걱정하지 않아도 됩니다. 이는 그림 :numref:<code>fig_sum-order</code>에 설명되어 있습니다. 특히 다음과 같이 말할 수 있습니다.</p>
<p>$$  \sum _ {j} \epsilon \left(\sum_{i} \epsilon f(\epsilon i, \epsilon j)\right). $$</p>
<p><img src="../img/sum-order.svg" alt="많은 사각형에 대한 합을 먼저 열에 대한 합(1)으로 분해한 다음 열 합계들을 함께 더하는(2) 방법의 그림." />
:label:<code>fig_sum-order</code></p>
<p>안쪽의 합은 정확히 다음 적분의 이산화입니다.</p>
<p>$$ G(\epsilon j) = \int _a^{b} f(x, \epsilon j) ; dx. $$</p>
<p>마지막으로 이러한 두 식을 결합하면 다음을 얻는다는 점에 유의하십시오.</p>
<p>$$ \sum _ {j} \epsilon G(\epsilon j) \approx \int _ {c}^{d} G(y) ; dy = \int _ {[a, b]\times[c, d]} f(x, y);dx;dy. $$</p>
<p>따라서 이 모든 것을 종합하면 다음을 갖게 됩니다.</p>
<p>$$ \int _ {[a, b]\times[c, d]} f(x, y);dx;dy = \int _ c^{d} \left(\int _ a^{b} f(x, y) ;dx\right) ; dy. $$</p>
<p>이산화하고 나면 우리가 한 모든 것은 숫자 목록을 더하는 순서를 재배열한 것뿐임에 유의하십시오. 이것이 아무것도 아닌 것처럼 보일 수 있지만, 이 결과(*푸비니의 정리(Fubini's Theorem)*라고 불림)가 항상 참인 것은 아닙니다! 머신러닝을 할 때 접하는 수학 유형(연속 함수)의 경우 걱정할 필요가 없지만, 그것이 실패하는 예제를 만드는 것은 가능합니다(예를 들어 직사각형 $[0,2]\times[0,1]$에서 함수 $f(x, y) = xy(x^2-y^2)/(x^2+y^2)^3$).</p>
<p>먼저 $x$에 대해 적분한 다음 $y$에 대해 적분하기로 한 선택은 임의적이었음에 유의하십시오. 우리는 똑같이 $y$를 먼저 하고 그다음 $x$를 하도록 선택하여 다음을 볼 수도 있었습니다.</p>
<p>$$ \int _ {[a, b]\times[c, d]} f(x, y);dx;dy = \int _ a^{b} \left(\int _ c^{d} f(x, y) ;dy\right) ; dx. $$</p>
<p>종종 우리는 벡터 표기법으로 응축하여, $U = [a, b]\times [c, d]$에 대해 다음과 같이 말할 것입니다.</p>
<p>$$ \int _ U f(\mathbf{x});d\mathbf{x}. $$</p>
<h2 id="다중-적분에서의-변수-변환-change-of-variables-in-multiple-integrals"><a class="header" href="#다중-적분에서의-변수-변환-change-of-variables-in-multiple-integrals">다중 적분에서의 변수 변환 (Change of Variables in Multiple Integrals)</a></h2>
<p>:eqref:<code>eq_change_var</code>의 단일 변수와 마찬가지로, 고차원 적분 내에서 변수를 변환하는 능력은 핵심 도구입니다. 유도 없이 결과를 요약해 봅시다.</p>
<p>우리는 적분 도메인을 재파라미터화하는 함수가 필요합니다. 우리는 이것을 $\phi : \mathbb{R}^n \rightarrow \mathbb{R}^n$로 취할 수 있는데, 이는 $n$개의 실수 변수를 받아 다른 $n$개를 반환하는 임의의 함수입니다. 식을 깔끔하게 유지하기 위해 $\phi$가 *단사(injective)*라고 가정하겠습니다. 즉, 결코 자기 자신 위로 접히지 않습니다($\phi(\mathbf{x}) = \phi(\mathbf{y}) \implies \mathbf{x} = \mathbf{y}$).</p>
<p>이 경우 다음과 같이 말할 수 있습니다.</p>
<p>$$ \int _ {\phi(U)} f(\mathbf{x});d\mathbf{x} = \int _ {U} f(\phi(\mathbf{x})) \left|\det(D\phi(\mathbf{x}))\right|;d\mathbf{x}. $$</p>
<p>여기서 $D\phi$는 $\phi$의 *야코비 행렬(Jacobian)*로, $\boldsymbol{\phi} = (\phi_1(x_1, \ldots, x_n), \ldots, \phi_n(x_1, \ldots, x_n))$의 편미분 행렬입니다.</p>
<p>$$ D\boldsymbol{\phi} = \begin{bmatrix}
\frac{\partial \phi _ 1}{\partial x _ 1} &amp; \cdots &amp; \frac{\partial \phi _ 1}{\partial x _ n} \
\vdots &amp; \ddots &amp; \vdots \
\frac{\partial \phi _ n}{\partial x _ 1} &amp; \cdots &amp; \frac{\partial \phi _ n}{\partial x _ n}
\end{bmatrix}. $$</p>
<p>자세히 살펴보면, 이것이 $\frac{du}{dx}(x)$ 항을 $\left|\det(D\phi(\mathbf{x}))\right|$로 대체한 것을 제외하고는 단일 변수 연쇄 법칙 :eqref:<code>eq_change_var</code>과 유사함을 알 수 있습니다. 이 항을 어떻게 해석할 수 있는지 살펴봅시다. $\frac{du}{dx}(x)$ 항이 $u$를 적용하여 $x$축을 얼마나 늘렸는지를 말하기 위해 존재했음을 상기하십시오. 고차원에서의 동일한 과정은 $\boldsymbol{\phi}$를 적용하여 작은 사각형(또는 작은 <em>하이퍼큐브</em>)의 넓이(또는 부피, 또는 하이퍼볼륨)를 얼마나 늘리는지 결정하는 것입니다. 만약 $\boldsymbol{\phi}$가 행렬에 의한 곱셈이었다면, 우리는 행렬식이 이미 답을 준다는 것을 압니다.</p>
<p>약간의 작업을 통해, 도함수와 기울기로 직선이나 평면으로 근사할 수 있었던 것과 동일한 방식으로 <em>야코비 행렬</em>이 한 점에서의 다변수 함수 $\boldsymbol{\phi}$에 대한 최선의 근사를 제공함을 보일 수 있습니다. 따라서 야코비 행렬의 행렬식은 우리가 1차원에서 식별한 스케일링 인자를 정확히 반영합니다.</p>
<p>이에 대한 세부 사항을 채우는 데는 약간의 작업이 필요하므로, 지금 당장 명확하지 않더라도 걱정하지 마십시오. 나중에 활용할 예제를 하나라도 살펴봅시다. 적분을 고려해 보십시오.</p>
<p>$$ \int _ {-\infty}^{\infty} \int _ {-\infty}^{\infty} e^{-x^{2}-y^{2}} ;dx;dy. $$</p>
<p>이 적분을 직접 다루는 것은 아무데도 도달하지 못할 것이지만, 변수를 변환하면 상당한 진전을 이룰 수 있습니다. 만약 $\boldsymbol{\phi}(r, \theta) = (r \cos(\theta),  r\sin(\theta))$라고 하면($x = r \cos(\theta), y = r \sin(\theta)$임을 의미함), 변수 변환 공식을 적용하여 이것이 다음과 같음을 볼 수 있습니다.</p>
<p>$$ \int _ 0^\infty \int_0 ^ {2\pi} e^{-r^{2}} \left|\det(D\mathbf{\phi}(\mathbf{x}))\right|;d\theta;dr, $$</p>
<p>여기서</p>
<p>$$ \left|\det(D\mathbf{\phi}(\mathbf{x}))\right| = \left|\det\begin{bmatrix}
\cos(\theta) &amp; -r\sin(\theta) \
\sin(\theta) &amp; r\cos(\theta)
\end{bmatrix}\right| = r(\cos^{2}(\theta) + \sin^{2}(\theta)) = r. $$</p>
<p>따라서 적분은 다음과 같습니다.</p>
<p>$$ \int _ 0^\infty \int _ 0 ^ {2\pi} re^{-r^{2}} ;d\theta;dr = 2\pi\int _ 0^\infty re^{-r^{2}} ;dr = \pi, $$</p>
<p>여기서 마지막 등식은 섹션 :numref:<code>subsec_integral_example</code>에서 사용한 것과 동일한 계산에 의해 따릅니다.</p>
<p>우리는 :numref:<code>sec_random_variables</code>에서 연속 확률 변수를 공부할 때 이 적분을 다시 만날 것입니다.</p>
<h2 id="요약-summary"><a class="header" href="#요약-summary">요약 (Summary)</a></h2>
<ul>
<li>적분 이론은 넓이나 부피에 대한 질문에 답할 수 있게 해 줍니다.</li>
<li>미적분학의 기본 정리는 어떤 점까지의 넓이의 도함수가 적분되는 함수의 값에 의해 주어진다는 관찰을 통해, 넓이를 계산하는 데 미분에 대한 지식을 활용할 수 있게 해 줍니다.</li>
<li>고차원 적분은 단일 변수 적분을 반복하여 계산할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises"><a class="header" href="#연습-문제-exercises">연습 문제 (Exercises)</a></h2>
<ol>
<li>$\int_1^2 \frac{1}{x} ;dx$는 얼마입니까?</li>
<li>변수 변환 공식을 사용하여 $\int_0^{\sqrt{\pi}}x\sin(x^2);dx$를 적분하십시오.</li>
<li>$\int_{[0,1]^2} xy ;dx;dy$는 얼마입니까?</li>
<li>변수 변환 공식을 사용하여 $\int_0^2\int_0^1xy(x^2-y^2)/(x^2+y^2)^3;dy;dx$와 $\int_0^1\int_0^2f(x, y) = xy(x^2-y^2)/(x^2+y^2)^3;dx;dy$를 계산하여 그들이 다름을 확인하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/414">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1092">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1093">Discussions</a>
:end_tab:</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
