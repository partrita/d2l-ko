<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>고유 분해 (Eigendecompositions) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html" class="active"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="고유-분해-eigendecompositions"><a class="header" href="#고유-분해-eigendecompositions">고유 분해 (Eigendecompositions)</a></h1>
<p>:label:<code>sec_eigendecompositions</code></p>
<p>고유값(eigenvalues)은 선형 대수를 공부할 때 접하게 되는 가장 유용한 개념 중 하나인 경우가 많지만, 초보자로서 그 중요성을 간과하기 쉽습니다.
아래에서는 고유 분해를 소개하고 그것이 왜 그렇게 중요한지에 대한 느낌을 전달하려고 노력합니다.</p>
<p>다음과 같은 항목을 가진 행렬 $A$가 있다고 가정해 봅시다.</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 0 \ 0 &amp; -1
\end{bmatrix}.
$$</p>
<p>임의의 벡터 $\mathbf{v} = [x, y]^\top$에 $A$를 적용하면,
벡터 $\mathbf{A}\mathbf{v} = [2x, -y]^\top$를 얻습니다.
이것은 직관적인 해석을 갖습니다:
벡터를 $x$ 방향으로 두 배 넓게 늘리고, $y$ 방향으로 뒤집는 것입니다.</p>
<p>그러나 무언가가 변경되지 않고 유지되는 <em>일부</em> 벡터가 있습니다.
즉, $[1, 0]^\top$은 $[2, 0]^\top$으로 보내지고,
$[0, 1]^\top$은 $[0, -1]^\top$으로 보내집니다.
이러한 벡터는 여전히 동일한 직선상에 있으며, 유일한 수정 사항은 행렬이 각각 $2$와 $-1$의 인수로 늘린다는 것입니다.
우리는 그러한 벡터를 *고유 벡터(eigenvectors)*라고 부르고, 그것들이 늘어나는 인수를 *고유값(eigenvalues)*이라고 부릅니다.</p>
<p>일반적으로 다음과 같은 숫자 $\lambda$와 벡터 $\mathbf{v}$를 찾을 수 있다면</p>
<p>$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}.
$$</p>
<p>우리는 $\mathbf{v}$를 $A$에 대한 고유 벡터라고 하고 $\lambda$를 고유값이라고 합니다.</p>
<h2 id="고유값-찾기-finding-eigenvalues"><a class="header" href="#고유값-찾기-finding-eigenvalues">고유값 찾기 (Finding Eigenvalues)</a></h2>
<p>그것들을 찾는 방법을 알아봅시다. 양변에서 $\lambda \mathbf{v}$를 빼고 벡터를 인수분해하면 위의 식이 다음과 동등함을 알 수 있습니다.</p>
<p>$$(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = 0.$$
:eqlabel:<code>eq_eigvalue_der</code></p>
<p>:eqref:<code>eq_eigvalue_der</code>가 발생하려면 $(\mathbf{A} - \lambda \mathbf{I})$가 어떤 방향을 0으로 압축해야 하므로 가역적이지 않으며, 따라서 행렬식이 0입니다.
따라서 우리는 $\det(\mathbf{A}-\lambda \mathbf{I}) = 0$이 되는 $\lambda$가 무엇인지 찾음으로써 <em>고유값</em>을 찾을 수 있습니다.
고유값을 찾으면 $\mathbf{A}\mathbf{v} = \lambda \mathbf{v}$를 풀어 연관된 <em>고유 벡터</em>를 찾을 수 있습니다.</p>
<h3 id="예제-an-example"><a class="header" href="#예제-an-example">예제 (An Example)</a></h3>
<p>더 어려운 행렬로 이것을 살펴봅시다.</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 1\ 2 &amp; 3
\end{bmatrix}.
$$</p>
<p>$\det(\mathbf{A}-\lambda \mathbf{I}) = 0$을 고려하면,
이것이 다항식 방정식 $0 = (2-\lambda)(3-\lambda)-2 = (4-\lambda)(1-\lambda)$와 동등함을 알 수 있습니다.
따라서 두 개의 고유값은 $4$와 $1$입니다.
연관된 벡터를 찾으려면 다음을 풀어야 합니다.</p>
<p>$$
\begin{bmatrix}
2 &amp; 1\ 2 &amp; 3
\end{bmatrix}\begin{bmatrix}x \ y\end{bmatrix} = \begin{bmatrix}x \ y\end{bmatrix}  ; \textrm{및} ;
\begin{bmatrix}
2 &amp; 1\ 2 &amp; 3
\end{bmatrix}\begin{bmatrix}x \ y\end{bmatrix}  = \begin{bmatrix}4x \ 4y\end{bmatrix} .
$$</p>
<p>우리는 이를 각각 벡터 $[1, -1]^\top$ 및 $[1, 2]^\top$로 풀 수 있습니다.</p>
<p>내장된 <code>numpy.linalg.eig</code> 루틴을 사용하여 코드에서 이를 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
import numpy as np

np.linalg.eig(np.array([[2, 1], [2, 3]]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
import torch

torch.linalg.eig(torch.tensor([[2, 1], [2, 3]], dtype=torch.float64))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
import tensorflow as tf

tf.linalg.eig(tf.constant([[2, 1], [2, 3]], dtype=tf.float64))
</code></pre>
<p><code>numpy</code>는 고유 벡터의 길이를 1로 정규화하는 반면, 우리는 임의의 길이를 취했다는 점에 유의하십시오.
또한 부호의 선택은 임의적입니다.
그러나 계산된 벡터는 동일한 고유값을 가진 우리가 손으로 찾은 벡터와 평행합니다.</p>
<h2 id="행렬-분해-decomposing-matrices"><a class="header" href="#행렬-분해-decomposing-matrices">행렬 분해 (Decomposing Matrices)</a></h2>
<p>이전 예제를 한 단계 더 진행해 봅시다.</p>
<p>$$
\mathbf{W} = \begin{bmatrix}
1 &amp; 1 \ -1 &amp; 2
\end{bmatrix},
$$</p>
<p>를 행렬 $\mathbf{A}$의 고유 벡터들이 열인 행렬이라고 합시다.</p>
<p>$$
\boldsymbol{\Sigma} = \begin{bmatrix}
1 &amp; 0 \ 0 &amp; 4
\end{bmatrix},
$$</p>
<p>를 대각선에 연관된 고유값이 있는 행렬이라고 합시다.
그러면 고유값과 고유 벡터의 정의는 우리에게 다음을 알려줍니다.</p>
<p>$$
\mathbf{A}\mathbf{W} =\mathbf{W} \boldsymbol{\Sigma} .
$$</p>
<p>행렬 $W$는 가역적이므로 양변의 오른쪽에 $W^{-1}$을 곱하면 다음과 같이 쓸 수 있음을 알 수 있습니다.</p>
<p>$$\mathbf{A} = \mathbf{W} \boldsymbol{\Sigma} \mathbf{W}^{-1}.$$
:eqlabel:<code>eq_eig_decomp</code></p>
<p>다음 섹션에서 이것의 몇 가지 좋은 결과를 보겠지만, 지금은 선형 독립인 고유 벡터의 전체 컬렉션을 찾을 수 있는 한(그래서 $W$가 가역적이 되도록) 그러한 분해가 존재한다는 것만 알면 됩니다.</p>
<h2 id="고유-분해에-대한-연산-operations-on-eigendecompositions"><a class="header" href="#고유-분해에-대한-연산-operations-on-eigendecompositions">고유 분해에 대한 연산 (Operations on Eigendecompositions)</a></h2>
<p>고유 분해 :eqref:<code>eq_eig_decomp</code>의 한 가지 좋은 점은 우리가 보통 접하는 많은 연산을 고유 분해의 관점에서 깔끔하게 쓸 수 있다는 것입니다. 첫 번째 예로 다음을 고려하십시오.</p>
<p>$$
\mathbf{A}^n = \overbrace{\mathbf{A}\cdots \mathbf{A}}^{\textrm{$n$ 번}} = \overbrace{(\mathbf{W}\boldsymbol{\Sigma} \mathbf{W}^{-1})\cdots(\mathbf{W}\boldsymbol{\Sigma} \mathbf{W}^{-1})}^{\textrm{$n$ 번}} =  \mathbf{W}\overbrace{\boldsymbol{\Sigma}\cdots\boldsymbol{\Sigma}}^{\textrm{$n$ 번}}\mathbf{W}^{-1} = \mathbf{W}\boldsymbol{\Sigma}^n \mathbf{W}^{-1}.
$$</p>
<p>이것은 행렬의 임의의 양수 거듭제곱에 대해, 고유값을 동일한 거듭제곱으로 올림으로써 고유 분해를 얻을 수 있음을 알려줍니다.
음수 거듭제곱에 대해서도 동일하게 나타낼 수 있으므로, 행렬을 반전시키고 싶다면 다음만 고려하면 됩니다.</p>
<p>$$
\mathbf{A}^{-1} = \mathbf{W}\boldsymbol{\Sigma}^{-1} \mathbf{W}^{-1},
$$</p>
<p>즉, 각 고유값만 반전시키면 됩니다.
이것은 각 고유값이 0이 아닌 한 작동하므로, 가역적이라는 것이 0인 고유값이 없는 것과 같음을 알 수 있습니다.</p>
<p>실제로 $\lambda_1, \ldots, \lambda_n$이 행렬의 고유값이라면 그 행렬의 행렬식은</p>
<p>$$
\det(\mathbf{A}) = \lambda_1 \cdots \lambda_n,
$$</p>
<p>또는 모든 고유값의 곱임을 추가 작업으로 보일 수 있습니다.
이는 $\mathbf{W}$가 어떤 늘림을 하든 $W^{-1}$이 이를 되돌리므로 결국 발생하는 유일한 늘림은 대각 원소들의 곱으로 부피를 늘리는 대각 행렬 $\boldsymbol{\Sigma}$에 의한 곱셈이기 때문에 직관적으로 말이 됩니다.</p>
<p>마지막으로, 랭크(rank)가 행렬의 선형 독립인 열의 최대 개수였음을 상기하십시오.
고유 분해를 자세히 검토함으로써, 랭크가 $\mathbf{A}$의 0이 아닌 고유값의 개수와 같음을 알 수 있습니다.</p>
<p>예제는 계속될 수 있지만 요점은 명확합니다:
고유 분해는 많은 선형 대수 계산을 단순화할 수 있으며 많은 수치 알고리즘과 우리가 선형 대수에서 수행하는 많은 분석의 기초가 되는 근본적인 연산입니다.</p>
<h2 id="대칭-행렬의-고유-분해-eigendecompositions-of-symmetric-matrices"><a class="header" href="#대칭-행렬의-고유-분해-eigendecompositions-of-symmetric-matrices">대칭 행렬의 고유 분해 (Eigendecompositions of Symmetric Matrices)</a></h2>
<p>위의 과정이 작동할 만큼 충분한 선형 독립 고유 벡터를 항상 찾을 수 있는 것은 아닙니다. 예를 들어 행렬</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
1 &amp; 1 \ 0 &amp; 1
\end{bmatrix},
$$</p>
<p>은 단 하나의 고유 벡터, 즉 $(1, 0)^\top$만 갖습니다.
그러한 행렬을 다루기 위해서는 우리가 다룰 수 있는 것보다 더 발전된 기술(예: 조르단 표준형 또는 특이값 분해)이 필요합니다.
우리는 종종 고유 벡터의 전체 집합의 존재를 보장할 수 있는 행렬들에 주의를 집중해야 할 것입니다.</p>
<p>가장 흔히 마주치는 가족은 <em>대칭 행렬</em>입니다. 이는 $\mathbf{A} = \mathbf{A}^\top$인 행렬들입니다.
이 경우 우리는 $W$를 <em>직교 행렬(orthogonal matrix)</em>—모든 열이 서로 직각인 길이 1의 벡터인 행렬, 즉 $\mathbf{W}^\top = \mathbf{W}^{-1}$—로 취할 수 있으며 모든 고유값은 실수가 됩니다.
따라서 이 특별한 경우에 우리는 :eqref:<code>eq_eig_decomp</code>를 다음과 같이 쓸 수 있습니다.</p>
<p>$$
\mathbf{A} = \mathbf{W}\boldsymbol{\Sigma}\mathbf{W}^\top .
$$</p>
<h2 id="거시고린-원판-정리-gershgorin-circle-theorem"><a class="header" href="#거시고린-원판-정리-gershgorin-circle-theorem">거시고린 원판 정리 (Gershgorin Circle Theorem)</a></h2>
<p>고유값은 종종 직관적으로 추론하기 어렵습니다.
임의의 행렬이 제시되면 그것을 계산하지 않고는 고유값이 무엇인지에 대해 말할 수 있는 것이 거의 없습니다.
하지만 가장 큰 값들이 대각선에 있다면 잘 근사하기 쉽게 만들 수 있는 정리가 하나 있습니다.</p>
<p>$\mathbf{A} = (a_{ij})$를 임의의 정사각 행렬($n\times n$)이라고 합시다.
우리는 $r_i = \sum_{j \neq i} |a_{ij}|$로 정의할 것입니다.
$\mathcal{D}<em>i$가 복소평면에서 중심이 $a</em>{ii}$이고 반지름이 $r_i$인 원판을 나타낸다고 합시다.
그러면 $\mathbf{A}$의 모든 고유값은 $\mathcal{D}_i$ 중 하나에 포함됩니다.</p>
<p>이것은 이해하기에 조금 많을 수 있으므로 예제를 살펴봅시다.
행렬을 고려해 보십시오:</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
1.0 &amp; 0.1 &amp; 0.1 &amp; 0.1 \ 0.1 &amp; 3.0 &amp; 0.2 &amp; 0.3 \ 0.1 &amp; 0.2 &amp; 5.0 &amp; 0.5 \ 0.1 &amp; 0.3 &amp; 0.5 &amp; 9.0
\end{bmatrix}.
$$</p>
<p>우리는 $r_1 = 0.3, r_2 = 0.6, r_3 = 0.8, r_4 = 0.9$를 갖습니다.
행렬은 대칭이므로 모든 고유값은 실수입니다.
이는 우리의 모든 고유값이 다음 범위 중 하나에 있게 됨을 의미합니다.</p>
<p>$$[a_{11}-r_1, a_{11}+r_1] = [0.7, 1.3], $$</p>
<p>$$[a_{22}-r_2, a_{22}+r_2] = [2.4, 3.6], $$</p>
<p>$$[a_{33}-r_3, a_{33}+r_3] = [4.2, 5.8], $$</p>
<p>$$[a_{44}-r_4, a_{44}+r_4] = [8.1, 9.9]. $$</p>
<p>수치 계산을 수행하면 고유값이 대략 $0.99, 2.97, 4.95, 9.08$임을 알 수 있으며,
모두 제공된 범위 내에 편안하게 들어옵니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
A = np.array([[1.0, 0.1, 0.1, 0.1],
              [0.1, 3.0, 0.2, 0.3],
              [0.1, 0.2, 5.0, 0.5],
              [0.1, 0.3, 0.5, 9.0]])

v, _ = np.linalg.eig(A)
v
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
A = torch.tensor([[1.0, 0.1, 0.1, 0.1],
              [0.1, 3.0, 0.2, 0.3],
              [0.1, 0.2, 5.0, 0.5],
              [0.1, 0.3, 0.5, 9.0]])

v, _ = torch.linalg.eig(A)
v
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
A = tf.constant([[1.0, 0.1, 0.1, 0.1],
                [0.1, 3.0, 0.2, 0.3],
                [0.1, 0.2, 5.0, 0.5],
                [0.1, 0.3, 0.5, 9.0]])

v, _ = tf.linalg.eigh(A)
v
</code></pre>
<p>이런 식으로 고유값을 근사할 수 있으며,
대각선이 다른 모든 원소보다 상당히 큰 경우 근사는 상당히 정확할 것입니다.</p>
<p>작은 일이지만, 고유 분해와 같이 복잡하고 미묘한 주제에 대해 우리가 할 수 있는 어떤 직관적인 파악이라도 얻는 것은 좋습니다.</p>
<h2 id="유용한-응용-반복-맵의-성장-a-useful-application-the-growth-of-iterated-maps"><a class="header" href="#유용한-응용-반복-맵의-성장-a-useful-application-the-growth-of-iterated-maps">유용한 응용: 반복 맵의 성장 (A Useful Application: The Growth of Iterated Maps)</a></h2>
<p>이제 고유 벡터가 원칙적으로 무엇인지 이해했으므로,
신경망 동작의 중심 문제인 적절한 가중치 초기화에 대한 깊은 이해를 제공하는 데 그것들이 어떻게 사용될 수 있는지 살펴봅시다.</p>
<h3 id="장기-행동으로서의-고유-벡터-eigenvectors-as-long-term-behavior"><a class="header" href="#장기-행동으로서의-고유-벡터-eigenvectors-as-long-term-behavior">장기 행동으로서의 고유 벡터 (Eigenvectors as Long Term Behavior)</a></h3>
<p>심층 신경망 초기화에 대한 완전한 수학적 조사는 텍스트의 범위를 벗어나지만,
고유값이 이러한 모델이 어떻게 작동하는지 이해하는 데 어떻게 도움이 되는지 여기에서 장난감 버전을 볼 수 있습니다.
우리가 알고 있듯이, 신경망은 선형 변환 레이어와 비선형 연산을 산재시켜 작동합니다.
여기서 단순함을 위해 비선형성이 없다고 가정하고 변환이 단일 반복 행렬 연산 $A$라고 가정합시다. 그러면 우리 모델의 출력은 다음과 같습니다.</p>
<p>$$
\mathbf{v}<em>{out} = \mathbf{A}\cdot \mathbf{A}\cdots \mathbf{A} \mathbf{v}</em>{in} = \mathbf{A}^N \mathbf{v}_{in}.
$$</p>
<p>이러한 모델이 초기화될 때 $A$는 가우스(Gaussian) 항목을 가진 무작위 행렬로 취해지므로, 그중 하나를 만들어 봅시다.
구체적으로 평균 0, 분산 1인 가우스 분포 $5 \times 5$ 행렬로 시작합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
np.random.seed(8675309)

k = 5
A = np.random.randn(k, k)
A
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
torch.manual_seed(42)

k = 5
A = torch.randn(k, k, dtype=torch.float64)
A
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
k = 5
A = tf.random.normal((k, k), dtype=tf.float64)
A
</code></pre>
<h3 id="무작위-데이터에서의-행동-behavior-on-random-data"><a class="header" href="#무작위-데이터에서의-행동-behavior-on-random-data">무작위 데이터에서의 행동 (Behavior on Random Data)</a></h3>
<p>우리의 장난감 모델에서 단순함을 위해,
우리가 공급하는 데이터 벡터 $\mathbf{v}_{in}$이 무작위 5차원 가우스 벡터라고 가정합시다.
어떤 일이 일어나기를 원하는지 생각해 봅시다.
문맥을 위해 일반적인 머신러닝 문제를 생각해 봅시다.
우리는 이미지와 같은 입력 데이터를 이미지가 고양이 사진일 확률과 같은 예측으로 바꾸려고 노력하고 있습니다.
만약 $\mathbf{A}$를 반복적으로 적용하여 무작위 벡터를 매우 길게 늘린다면,
입력의 작은 변화가 출력의 큰 변화로 증폭될 것입니다 - 입력 이미지의 아주 작은 수정이 매우 다른 예측으로 이어질 것입니다.
이것은 옳지 않은 것 같습니다!</p>
<p>반대로 $\mathbf{A}$가 무작위 벡터를 더 짧게 수축시킨다면,
많은 레이어를 거친 후 벡터는 본질적으로 아무것도 아닌 것으로 수축할 것이고,
출력은 입력에 의존하지 않을 것입니다. 이것 또한 분명히 옳지 않습니다!</p>
<p>우리는 출력이 입력에 따라 변하되 너무 많이 변하지 않도록 하기 위해
성장과 쇠퇴 사이의 좁은 길을 걸어야 합니다!</p>
<p>무작위 입력 벡터에 대해 행렬 $\mathbf{A}$를 반복적으로 곱할 때 어떤 일이 일어나는지 살펴보고 노름(norm)을 추적해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# `A`를 반복적으로 적용한 후 노름의 시퀀스 계산
v_in = np.random.randn(k, 1)

norm_list = [np.linalg.norm(v_in)]
for i in range(1, 100):
    v_in = A.dot(v_in)
    norm_list.append(np.linalg.norm(v_in))

d2l.plot(np.arange(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# `A`를 반복적으로 적용한 후 노름의 시퀀스 계산
v_in = torch.randn(k, 1, dtype=torch.float64)

norm_list = [torch.norm(v_in).item()]
for i in range(1, 100):
    v_in = A @ v_in
    norm_list.append(torch.norm(v_in).item())

d2l.plot(torch.arange(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# `A`를 반복적으로 적용한 후 노름의 시퀀스 계산
v_in = tf.random.normal((k, 1), dtype=tf.float64)

norm_list = [tf.norm(v_in).numpy()]
for i in range(1, 100):
    v_in = tf.matmul(A, v_in)
    norm_list.append(tf.norm(v_in).numpy())

d2l.plot(tf.range(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<p>노름이 통제할 수 없이 커지고 있습니다!
실제로 몫의 리스트를 취하면 패턴을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 노름의 스케일링 인자 계산
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])

d2l.plot(np.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 노름의 스케일링 인자 계산
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])

d2l.plot(torch.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 노름의 스케일링 인자 계산
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])

d2l.plot(tf.range(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<p>위의 계산 마지막 부분을 보면, 무작위 벡터가 <code>1.974459321485[...]</code>라는 인수로 늘어나는 것을 볼 수 있습니다.
끝부분이 약간씩 바뀌기는 하지만 늘어나는 인수는 안정적입니다.</p>
<h3 id="고유-벡터와-다시-연결하기-relating-back-to-eigenvectors"><a class="header" href="#고유-벡터와-다시-연결하기-relating-back-to-eigenvectors">고유 벡터와 다시 연결하기 (Relating Back to Eigenvectors)</a></h3>
<p>우리는 고유 벡터와 고유값이 무언가가 늘어나는 양에 대응한다는 것을 보았지만, 그것은 특정 벡터와 특정 늘림에 대한 것이었습니다.
$\mathbf{A}$에 대해 그것들이 무엇인지 살펴봅시다.
여기서 약간의 주의사항이 있습니다: 그것들을 모두 보려면 복소수로 가야 한다는 것이 밝혀졌습니다.
이것들을 늘림과 회전으로 생각할 수 있습니다.
복소수의 노름(실수부와 허수부의 제곱 합의 제곱근)을 취함으로써 그 늘림 인수를 측정할 수 있습니다. 그것들을 정렬해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 고유값 계산
eigs = np.linalg.eigvals(A).tolist()
norm_eigs = [np.absolute(x) for x in eigs]
norm_eigs.sort()
print(f'norms of eigenvalues: {norm_eigs}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 고유값 계산
eigs = torch.linalg.eig(A).eigenvalues.tolist()
norm_eigs = [torch.abs(torch.tensor(x)) for x in eigs]
norm_eigs.sort()
print(f'norms of eigenvalues: {norm_eigs}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 고유값 계산
eigs = tf.linalg.eigh(A)[0].numpy().tolist()
norm_eigs = [tf.abs(tf.constant(x, dtype=tf.float64)) for x in eigs]
norm_eigs.sort()
print(f'norms of eigenvalues: {norm_eigs}')
</code></pre>
<h3 id="관찰-an-observation"><a class="header" href="#관찰-an-observation">관찰 (An Observation)</a></h3>
<p>우리는 여기서 약간 예상치 못한 일이 일어나는 것을 봅니다:
무작위 벡터에 적용된 우리 행렬 $\mathbf{A}$의 장기적인 늘림에 대해 우리가 이전에 식별한 그 숫자가 <em>정확히</em>
(소수점 아래 13자리까지 정확하게!)
$\mathbf{A}$의 가장 큰 고유값입니다.
이것은 분명히 우연이 아닙니다!</p>
<p>하지만 이제 기하학적으로 무슨 일이 일어나고 있는지 생각해보면 이해가 되기 시작합니다.
무작위 벡터를 고려해 보십시오. 이 무작위 벡터는 모든 방향을 조금씩 가리키고 있으므로,
특히 $\mathbf{A}$의 가장 큰 고유값과 연관된 고유 벡터와 동일한 방향을 적어도 조금은 가리킵니다.
이것은 너무 중요해서 <em>주 고유값(principal eigenvalue)</em> 및 *주 고유 벡터(principal eigenvector)*라고 불립니다.
$\mathbf{A}$를 적용한 후, 우리의 무작위 벡터는 가능한 모든 고유 벡터와 연관되어 모든 가능한 방향으로 늘어나지만,
무엇보다도 이 주 고유 벡터와 연관된 방향으로 가장 많이 늘어납니다.
이것이 의미하는 바는 $A$를 적용한 후 우리의 무작위 벡터가 더 길어지고 주 고유 벡터와 더 일치하는 방향을 가리킨다는 것입니다.
행렬을 여러 번 적용한 후 주 고유 벡터와의 일치는 점점 더 가까워져서,
모든 실질적인 목적을 위해 우리의 무작위 벡터는 주 고유 벡터로 변환됩니다!
실제로 이 알고리즘은 행렬의 가장 큰 고유값과 고유 벡터를 찾기 위한 *거듭제곱 반복(power iteration)*으로 알려진 것의 기초입니다. 자세한 내용은 예를 들어 :cite:<code>Golub.Van-Loan.1996</code>을 참조하십시오.</p>
<h3 id="정규화-수정-fixing-the-normalization"><a class="header" href="#정규화-수정-fixing-the-normalization">정규화 수정 (Fixing the Normalization)</a></h3>
<p>이제 위의 논의로부터 우리는 무작위 벡터가 전혀 늘어나거나 줄어들지 않기를 원한다는 결론을 내렸습니다.
우리는 전체 과정 동안 무작위 벡터가 거의 동일한 크기를 유지하기를 바랍니다.
그렇게 하기 위해 이제 우리는 가장 큰 고유값이 대신 1이 되도록 주 고유값으로 우리 행렬의 스케일을 조정합니다.
이 경우 어떤 일이 일어나는지 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 행렬 `A`의 스케일 조정
A /= norm_eigs[-1]

# 동일한 실험 다시 수행
v_in = np.random.randn(k, 1)

norm_list = [np.linalg.norm(v_in)]
for i in range(1, 100):
    v_in = A.dot(v_in)
    norm_list.append(np.linalg.norm(v_in))

d2l.plot(np.arange(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 행렬 `A`의 스케일 조정
A /= norm_eigs[-1]

# 동일한 실험 다시 수행
v_in = torch.randn(k, 1, dtype=torch.float64)

norm_list = [torch.norm(v_in).item()]
for i in range(1, 100):
    v_in = A @ v_in
    norm_list.append(torch.norm(v_in).item())

d2l.plot(torch.arange(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 행렬 `A`의 스케일 조정
A /= norm_eigs[-1]

# 동일한 실험 다시 수행
v_in = tf.random.normal((k, 1), dtype=tf.float64)

norm_list = [tf.norm(v_in).numpy()]
for i in range(1, 100):
    v_in = tf.matmul(A, v_in)
    norm_list.append(tf.norm(v_in).numpy())

d2l.plot(tf.range(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<p>이전처럼 연속된 노름 사이의 비율을 플롯할 수도 있으며 실제로 안정화되는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 비율도 플롯
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i-1])

d2l.plot(np.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 비율도 플롯
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i-1])

d2l.plot(torch.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 비율도 플롯
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i-1])

d2l.plot(tf.range(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<h2 id="토론-discussion"><a class="header" href="#토론-discussion">토론 (Discussion)</a></h2>
<p>우리는 이제 우리가 원했던 것을 정확히 봅니다!
행렬을 주 고유값으로 정규화한 후,
무작위 데이터가 이전처럼 폭발하지 않고 오히려 결국 특정 값으로 평형을 이룹니다.
이런 일들을 제1원리부터 할 수 있으면 좋겠는데,
수학을 깊이 파고들면 독립적인 평균 0, 분산 1인 가우스 항목을 가진 큰 무작위 행렬의 가장 큰 고유값은
*원형 법칙(circular law)*이라는 매혹적인 사실 덕분에 평균적으로 대략 $\sqrt{n}$,
또는 우리의 경우 $\sqrt{5} \approx 2.2$가 된다는 것을 알 수 있습니다 :cite:<code>Ginibre.1965</code>.
무작위 행렬의 고유값(및 특이값이라고 불리는 관련 객체) 사이의 관계는 :citet:<code>Pennington.Schoenholz.Ganguli.2017</code> 및 후속 작업에서 논의된 것처럼 신경망의 적절한 초기화와 깊은 관계가 있음이 밝혀졌습니다.</p>
<h2 id="요약-summary"><a class="header" href="#요약-summary">요약 (Summary)</a></h2>
<ul>
<li>고유 벡터는 방향을 바꾸지 않고 행렬에 의해 늘어나는 벡터입니다.</li>
<li>고유값은 행렬의 적용에 의해 고유 벡터가 늘어나는 양입니다.</li>
<li>행렬의 고유 분해는 많은 연산을 고유값에 대한 연산으로 축소할 수 있게 해줍니다.</li>
<li>거시고린 원판 정리는 행렬의 고유값에 대한 근사치를 제공할 수 있습니다.</li>
<li>반복되는 행렬 거듭제곱의 행동은 주로 가장 큰 고유값의 크기에 달려 있습니다. 이 이해는 신경망 초기화 이론에서 많은 응용 분야를 갖습니다.</li>
</ul>
<h2 id="연습-문제-exercises"><a class="header" href="#연습-문제-exercises">연습 문제 (Exercises)</a></h2>
<ol>
<li>다음 행렬의 고유값과 고유 벡터는 무엇입니까?
$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 1 \ 1 &amp; 2
\end{bmatrix}?
$$</li>
<li>다음 행렬의 고유값과 고유 벡터는 무엇이며, 이 예제는 이전 예제와 비교하여 무엇이 이상합니까?
$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 1 \ 0 &amp; 2
\end{bmatrix}.
$$</li>
<li>고유값을 계산하지 않고, 다음 행렬의 가장 작은 고유값이 $0.5$보다 작을 수 있습니까? <em>참고</em>: 이 문제는 머릿속으로 풀 수 있습니다.
$$
\mathbf{A} = \begin{bmatrix}
3.0 &amp; 0.1 &amp; 0.3 &amp; 1.0 \ 0.1 &amp; 1.0 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.1 &amp; 5.0 &amp; 0.0 \ 1.0 &amp; 0.2 &amp; 0.0 &amp; 1.8
\end{bmatrix}.
$$</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/411">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1086">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1087">Discussions</a>
:end_tab:</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
