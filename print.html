<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Dive into Deep Learning</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="dive-into-deep-learning"><a class="header" href="#dive-into-deep-learning">Dive into Deep Learning</a></h1>
<pre><code class="language-eval_rst">.. raw:: html
   :file: frontpage.html
</code></pre>
<pre><code class="language-toc">:maxdepth: 1

chapter_preface/index
chapter_installation/index
chapter_notation/index
</code></pre>
<pre><code class="language-toc">:maxdepth: 2
:numbered:

chapter_introduction/index
chapter_preliminaries/index
chapter_linear-regression/index
chapter_linear-classification/index
chapter_multilayer-perceptrons/index
chapter_builders-guide/index
chapter_convolutional-neural-networks/index
chapter_convolutional-modern/index
chapter_recurrent-neural-networks/index
chapter_recurrent-modern/index
chapter_attention-mechanisms-and-transformers/index
chapter_optimization/index
chapter_computational-performance/index
chapter_computer-vision/index
chapter_natural-language-processing-pretraining/index
chapter_natural-language-processing-applications/index
chapter_reinforcement-learning/index
chapter_gaussian-processes/index
chapter_hyperparameter-optimization/index
chapter_generative-adversarial-networks/index
chapter_recommender-systems/index
chapter_appendix-mathematics-for-deep-learning/index
chapter_appendix-tools-for-deep-learning/index

</code></pre>
<pre><code class="language-toc">:maxdepth: 1

chapter_references/zreferences
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="서문-preface"><a class="header" href="#서문-preface">서문 (Preface)</a></h1>
<p>불과 몇 년 전만 해도, 주요 기업과 스타트업에서
지능형 제품과 서비스를 개발하는 딥러닝 과학자 군단은 없었습니다.
우리가 이 분야에 들어왔을 때, 머신러닝은
일간지 헤드라인을 장식하지 않았습니다.
우리 부모님은 머신러닝이 무엇인지 전혀 몰랐고,
의학이나 법학 경력보다 우리가 왜 이것을 선호하는지는 말할 것도 없었습니다.
머신러닝은 산업적 중요성이 음성 인식 및 컴퓨터 비전을 포함한
좁은 범위의 실제 응용 분야에 국한된
비현실적인 학문 분야였습니다.
게다가 이러한 응용 분야 중 다수는
너무 많은 도메인 지식을 필요로 하여
종종 머신러닝이 하나의 작은 구성 요소인 완전히 별개의 영역으로 간주되었습니다.
당시에는 신경망(이 책에서 우리가 초점을 맞추는 딥러닝 방법의 전신)이
일반적으로 구식으로 간주되었습니다.</p>
<p>그러나 불과 몇 년 만에 딥러닝은 세상을 놀라게 했으며,
컴퓨터 비전, 자연어 처리, 자동 음성 인식, 강화 학습, 생물의학 정보학 등
다양한 분야에서 빠른 발전을 주도했습니다.
게다가 실용적인 관심이 있는 수많은 작업에서 딥러닝의 성공은
이론적 머신러닝과 통계학의 발전까지 촉매했습니다.
이러한 발전을 바탕으로, 이제 우리는 그 어느 때보다 더 높은 자율성으로 스스로 운전하는 자동차
(일부 회사에서 믿게 만드는 것보다는 덜 자율적이지만),
명확한 질문을 함으로써 코드를 디버깅하는 대화 시스템,
그리고 수십 년은 걸릴 것이라고 생각했던 바둑과 같은 보드게임에서 세계 최고의 인간 플레이어를 이기는 소프트웨어 에이전트를 구축할 수 있습니다.
이미 이러한 도구들은 산업과 사회에 점점 더 광범위한 영향을 미치고 있으며,
영화 제작 방식, 질병 진단 방식을 바꾸고,
천체 물리학에서 기후 모델링, 기상 예측, 생물의학에 이르기까지 기초 과학에서 점점 더 큰 역할을 하고 있습니다.</p>
<h2 id="이-책에-대하여"><a class="header" href="#이-책에-대하여">이 책에 대하여</a></h2>
<p>이 책은 여러분에게 <em>개념(concepts)</em>, <em>맥락(context)</em>, *코드(code)*를 가르쳐
딥러닝에 쉽게 접근할 수 있도록 하기 위한 저희의 시도입니다.</p>
<h3 id="코드-수학-html을-결합한-하나의-매체"><a class="header" href="#코드-수학-html을-결합한-하나의-매체">코드, 수학, HTML을 결합한 하나의 매체</a></h3>
<p>어떤 컴퓨팅 기술이든 완전한 영향을 미치려면,
잘 이해되고, 잘 문서화되고,
성숙하고 잘 유지 관리되는 도구의 지원을 받아야 합니다.
핵심 아이디어는 명확하게 증류되어야 하며,
새로운 실무자를 최신 상태로 만드는 데 필요한 온보딩 시간을 최소화해야 합니다.
성숙한 라이브러리는 일반적인 작업을 자동화해야 하며,
모범 코드는 실무자가 자신의 필요에 맞게
공통 애플리케이션을 쉽게 수정, 적용 및 확장할 수 있도록 해야 합니다.</p>
<p>예를 들어 동적 웹 애플리케이션을 생각해 봅시다.
아마존과 같은 많은 기업들이 1990년대에 성공적인 데이터베이스 기반 웹 애플리케이션을 개발했음에도 불구하고,
이 기술이 창의적인 기업가를 도울 수 있는 잠재력은
강력하고 잘 문서화된 프레임워크의 개발 덕분에
지난 10년 동안 훨씬 더 큰 정도로 실현되었습니다.</p>
<p>딥러닝의 잠재력을 테스트하는 것은 독특한 도전 과제를 제시합니다.
단일 애플리케이션이 다양한 분야를 하나로 모으기 때문입니다.
딥러닝을 적용하려면 동시에 다음을 이해해야 합니다:
(i) 문제를 특정 방식으로 캐스팅하는 동기;
(ii) 주어진 모델의 수학적 형태;
(iii) 모델을 데이터에 적합시키는 최적화 알고리즘;
(iv) 모델이 보지 못한 데이터로 일반화될 것으로 예상해야 하는 시기를 알려주는 통계적 원리와
실제로 일반화되었음을 증명하는 실용적인 방법;
그리고 (v) 모델을 효율적으로 훈련하고,
수치 컴퓨팅의 함정을 탐색하고,
사용 가능한 하드웨어를 최대한 활용하는 데 필요한 엔지니어링 기술.
문제를 공식화하는 데 필요한 비판적 사고 기술,
문제를 해결하기 위한 수학,
그리고 솔루션을 구현하기 위한 소프트웨어 도구를
한곳에서 가르치는 것은 엄청난 도전 과제를 제시합니다.
이 책에서 우리의 목표는 예비 실무자들을 빠르게 적응시키기 위한
통합된 리소스를 제공하는 것입니다.</p>
<p>우리가 이 책 프로젝트를 시작했을 때,
동시에 다음을 만족하는 리소스는 없었습니다:
(i) 최신 상태를 유지함;
(ii) 충분한 기술적 깊이로 현대 머신러닝 관행의 폭을 다룸;
(iii) 교과서에서 기대하는 품질의 설명과
실습 튜토리얼에서 기대하는 깔끔한 실행 가능한 코드를 교차시킴.
우리는 주어진 딥러닝 프레임워크를 사용하는 방법(예: TensorFlow에서 행렬로 기본 수치 계산을 수행하는 방법)이나
특정 기술을 구현하는 방법(예: LeNet, AlexNet, ResNet 등의 코드 조각)을 보여주는
많은 코드 예제가 다양한 블로그 게시물과 GitHub 저장소에 흩어져 있는 것을 발견했습니다.
그러나 이러한 예제는 일반적으로 주어진 접근 방식을 <em>어떻게</em> 구현하는지에 초점을 맞추었지만,
<em>왜</em> 특정 알고리즘 결정이 내려졌는지에 대한 논의는 빠져 있었습니다.
일부 대화형 리소스가 특정 주제를 다루기 위해 간헐적으로 나타났지만,
예를 들어 웹사이트 <a href="http://distill.pub">Distill</a>이나 개인 블로그에 게시된 매력적인 블로그 게시물,
그들은 딥러닝의 선택된 주제만 다루었으며 종종 관련 코드가 부족했습니다.
반면, 딥러닝 기초에 대한 포괄적인 조사를 제공하는
:citet:<code>Goodfellow.Bengio.Courville.2016</code>와 같은 여러 딥러닝 교과서가 등장했지만,
이러한 리소스는 설명을 코드의 개념 실현과 결합하지 않아,
때로는 독자가 구현 방법에 대해 전혀 알 수 없게 만듭니다.
게다가 너무 많은 리소스가 상업용 강의 제공업체의 유료 벽 뒤에 숨겨져 있습니다.</p>
<p>우리는 다음과 같은 리소스를 만들기 시작했습니다:
(i) 누구나 무료로 이용할 수 있을 것;
(ii) 실제로 응용 머신러닝 과학자가 되는 길의 출발점을 제공할 수 있을 만큼
충분한 기술적 깊이를 제공할 것;
(iii) 실행 가능한 코드를 포함하여 독자들에게 실제로 문제를 <em>어떻게</em> 해결하는지 보여줄 것;
(iv) 저희뿐만 아니라 커뮤니티 전체에 의해 빠르게 업데이트될 수 있을 것;
(v) 기술적인 세부 사항에 대한 대화형 토론과 질문 답변을 위한 <a href="https://discuss.d2l.ai/c/5">포럼</a>으로 보완될 것.</p>
<p>이러한 목표들은 종종 충돌했습니다.
방정식, 정리 및 인용은 LaTeX에서 가장 잘 관리되고 배치됩니다.
코드는 Python으로 가장 잘 설명됩니다.
그리고 웹페이지는 HTML과 JavaScript가 기본입니다.
또한 우리는 콘텐츠가 실행 가능한 코드, 실제 책, 다운로드 가능한 PDF,
그리고 인터넷상의 웹사이트로 모두 액세스 가능하기를 원했습니다.
이러한 요구 사항에 맞는 워크플로우가 없어 보여서,
우리는 자체적으로 조립하기로 결정했습니다 (:numref:<code>sec_how_to_contribute</code>).
우리는 소스를 공유하고 커뮤니티 기여를 촉진하기 위해 GitHub를;
코드, 방정식 및 텍스트를 혼합하기 위해 주피터 노트북을;
렌더링 엔진으로 Sphinx를;
토론 플랫폼으로 Discourse를 선택했습니다.
우리 시스템이 완벽하지는 않지만,
이러한 선택은 경쟁하는 우려 사항들 사이에서 타협점을 찾습니다.
우리는 <em>Dive into Deep Learning</em>이
이러한 통합 워크플로우를 사용하여 출판된 첫 번째 책일 수 있다고 믿습니다.</p>
<h3 id="실천을-통한-학습-learning-by-doing"><a class="header" href="#실천을-통한-학습-learning-by-doing">실천을 통한 학습 (Learning by Doing)</a></h3>
<p>많은 교과서가 개념을 연달아 제시하며,
각각을 철저하게 상세히 다룹니다.
예를 들어, :citet:<code>Bishop.2006</code>의 훌륭한 교과서는
각 주제를 너무 철저하게 가르쳐서
선형 회귀 챕터에 도달하기까지
상당한 양의 작업이 필요합니다.
전문가들은 바로 그 철저함 때문에 이 책을 좋아하지만,
진정한 초보자에게는 이 속성이 입문서로서의 유용성을 제한합니다.</p>
<p>이 책에서 우리는 대부분의 개념을 <em>적시에(just in time)</em> 가르칩니다.
즉, 어떤 실용적인 목적을 달성하는 데 필요한 바로 그 순간에 개념을 배우게 됩니다.
처음에 선형 대수와 확률 같은 기본적인 예비 지식을 가르치는 데 시간을 할애하지만,
우리는 여러분이 더 난해한 개념에 대해 걱정하기 전에
첫 번째 모델을 훈련하는 만족감을 맛보기를 원합니다.</p>
<p>기본적인 수학적 배경에 대한 집중 코스를 제공하는 몇 개의 예비 노트북을 제외하고,
각 후속 챕터는 합리적인 수의 새로운 개념을 소개하고
실제 데이터셋을 사용하는 여러 독립적인 작업 예제를 제공합니다.
이것은 조직적인 도전 과제를 제시했습니다.
일부 모델은 논리적으로 단일 노트북에 함께 그룹화될 수 있습니다.
그리고 일부 아이디어는 여러 모델을 연속으로 실행하여 가장 잘 가르칠 수 있습니다.
반면에, <em>하나의 작업 예제, 하나의 노트북</em> 정책을 고수하는 것에는 큰 장점이 있습니다:
이것은 여러분이 우리 코드를 활용하여 자신의 연구 프로젝트를 시작하는 것을
가능한 한 쉽게 만듭니다.
노트북을 복사하고 수정하기만 하면 됩니다.</p>
<p>전체적으로 우리는 실행 가능한 코드를 필요에 따라 배경 자료와 교차시킵니다.
일반적으로 우리는 도구를 완전히 설명하기 전에
도구를 사용할 수 있게 하는 쪽을 택합니다(종종 나중에 배경을 채웁니다).
예를 들어, 우리는 <em>확률적 경사 하강법</em>이 왜 유용한지 설명하거나
왜 작동하는지에 대한 직관을 제공하기 전에 사용할 수 있습니다.
이는 실무자에게 문제를 빠르게 해결하는 데 필요한 탄약을 제공하는 데 도움이 되지만,
독자가 우리의 일부 큐레이터 결정을 신뢰해야 한다는 대가가 따릅니다.</p>
<p>이 책은 딥러닝 개념을 처음부터(from scratch) 가르칩니다.
때로는 최신 딥러닝 프레임워크에 의해 사용자에게 숨겨져 있는
모델에 대한 세부 사항을 깊이 파고듭니다.
이는 특히 기본 튜토리얼에서 나타나는데,
우리는 여러분이 주어진 레이어(layer)나 최적화기(optimizer)에서 일어나는
모든 일을 이해하기를 원하기 때문입니다.
이러한 경우, 우리는 종종 예제의 두 가지 버전을 제시합니다:
하나는 NumPy와 유사한 기능과 자동 미분에만 의존하여 모든 것을 처음부터 구현하는 것이고,
다른 하나는 딥러닝 프레임워크의 고수준 API를 사용하여
간결한 코드를 작성하는 더 실용적인 예제입니다.
일부 구성 요소가 어떻게 작동하는지 설명한 후,
후속 튜토리얼에서는 고수준 API에 의존합니다.</p>
<h3 id="콘텐츠-및-구조"><a class="header" href="#콘텐츠-및-구조">콘텐츠 및 구조</a></h3>
<p>이 책은 대략 세 부분으로 나눌 수 있으며,
예비 지식,
딥러닝 기술,
그리고 실제 시스템과 응용에 초점을 맞춘
고급 주제를 다룹니다 (:numref:<code>fig_book_org</code>).</p>
<p><img src="chapter_preface/../img/book-org.svg" alt="책 구조." />
:label:<code>fig_book_org</code></p>
<ul>
<li>
<p><strong>1부: 기초 및 예비 지식</strong>.
:numref:<code>chap_introduction</code>은 딥러닝에 대한 소개입니다.
그 다음 :numref:<code>chap_preliminaries</code>에서는
데이터 저장 및 조작 방법,
선형 대수, 미적분, 확률의 기본 개념을 기반으로
다양한 수치 연산을 적용하는 방법 등
실습 딥러닝에 필요한 필수 조건을 빠르게 알려드립니다.
:numref:<code>chap_regression</code> 및 :numref:<code>chap_perceptrons</code>는
회귀 및 분류; 선형 모델; 다층 퍼셉트론;
과대적합 및 정규화를 포함한
딥러닝의 가장 기본적인 개념과 기술을 다룹니다.</p>
</li>
<li>
<p><strong>2부: 현대 딥러닝 기술</strong>.
:numref:<code>chap_computation</code>은 딥러닝 시스템의 핵심 계산 구성 요소를 설명하고
더 복잡한 모델의 후속 구현을 위한 토대를 마련합니다.
다음으로 :numref:<code>chap_cnn</code> 및 :numref:<code>chap_modern_cnn</code>은
대부분의 현대 컴퓨터 비전 시스템의 중추를 형성하는 강력한 도구인
합성곱 신경망(CNN)을 제시합니다.
마찬가지로 :numref:<code>chap_rnn</code> 및 :numref:<code>chap_modern_rnn</code>은
데이터의 순차적(예: 시간적) 구조를 활용하고
자연어 처리 및 시계열 예측에 일반적으로 사용되는 모델인
순환 신경망(RNN)을 소개합니다.
:numref:<code>chap_attention-and-transformers</code>에서는
대부분의 자연어 처리 작업에서 지배적인 아키텍처로 RNN을 대체한
소위 <em>어텐션 메커니즘</em>에 기반한 비교적 새로운 종류의 모델을 설명합니다.
이 섹션들은 딥러닝 실무자들이 널리 사용하는
가장 강력하고 일반적인 도구에 대해 빠르게 알려드릴 것입니다.</p>
</li>
<li>
<p><strong>3부: 확장성, 효율성 및 응용</strong> (<a href="https://d2l.ai">온라인</a>에서 이용 가능).
12장에서는 딥러닝 모델을 훈련하는 데 사용되는
몇 가지 일반적인 최적화 알고리즘에 대해 논의합니다.
다음으로 13장에서는 딥러닝 코드의 계산 성능에 영향을 미치는
몇 가지 주요 요소를 살펴봅니다.
그런 다음 14장에서는 컴퓨터 비전에서 딥러닝의 주요 응용 사례를 보여줍니다.
마지막으로 15장과 16장에서는 언어 표현 모델을 사전 훈련하고
자연어 처리 작업에 적용하는 방법을 시연합니다.</p>
</li>
</ul>
<h3 id="코드"><a class="header" href="#코드">코드</a></h3>
<p>:label:<code>sec_code</code></p>
<p>이 책의 대부분의 섹션에는 실행 가능한 코드가 있습니다.
우리는 일부 직관이 시행착오를 통해,
코드를 조금씩 조정하고 결과를 관찰함으로써 가장 잘 개발된다고 믿습니다.
이상적으로는 우아한 수학적 이론이 원하는 결과를 얻기 위해
코드를 어떻게 조정해야 하는지 정확하게 알려줄 수 있습니다.
그러나 오늘날 딥러닝 실무자들은 종종 확실한 이론적 지침이 없는 곳을 밟아야 합니다.
우리의 최선의 노력에도 불구하고, 다양한 기술의 효능에 대한 공식적인 설명은
여러 가지 이유로 여전히 부족합니다: 이러한 모델을 특징짓는 수학은 매우 어려울 수 있습니다;
설명은 현재 명확한 정의가 부족한 데이터의 속성에 따라 달라질 가능성이 높습니다;
그리고 이러한 주제에 대한 진지한 탐구는 최근에야 본격화되었습니다.
우리는 딥러닝 이론이 발전함에 따라,
이 책의 미래 판이 현재 이용 가능한 것보다 더 뛰어난 통찰력을 제공하기를 희망합니다.</p>
<p>불필요한 반복을 피하기 위해, 우리는 가장 자주 가져오고 사용하는
함수와 클래스 중 일부를 <code>d2l</code> 패키지에 캡처합니다.
전체적으로 우리는 코드 블록(함수, 클래스,
또는 import 문 모음 등)을 <code>#@save</code>로 표시하여
나중에 <code>d2l</code> 패키지를 통해 액세스될 것임을 나타냅니다.
우리는 :numref:<code>sec_d2l</code>에서 이러한 클래스와 함수에 대한 자세한 개요를 제공합니다.
<code>d2l</code> 패키지는 가볍고 다음 종속성만 필요합니다:</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
import inspect
import collections
from collections import defaultdict
from IPython import display
import math
from matplotlib import pyplot as plt
from matplotlib_inline import backend_inline
import os
import pandas as pd
import random
import re
import shutil
import sys
import tarfile
import time
import requests
import zipfile
import hashlib
d2l = sys.modules[__name__]
</code></pre>
<p>:begin_tab:<code>mxnet</code>
이 책의 대부분의 코드는 AWS(Amazon Web Services)뿐만 아니라
많은 대학과 회사에서 선호하는 오픈 소스 딥러닝 프레임워크인
Apache MXNet을 기반으로 합니다.
이 책의 모든 코드는 최신 MXNet 버전에서 테스트를 통과했습니다.
그러나 딥러닝의 빠른 발전으로 인해 <em>인쇄판</em>의 일부 코드는
향후 버전의 MXNet에서 제대로 작동하지 않을 수 있습니다.
우리는 온라인 버전을 최신 상태로 유지할 계획입니다.
문제가 발생하면 :ref:<code>chap_installation</code>을 참조하여
코드와 런타임 환경을 업데이트하십시오.
아래는 MXNet 구현의 종속성을 나열합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
이 책의 대부분의 코드는 딥러닝 연구 커뮤니티에서
열광적으로 받아들여진 인기 있는 오픈 소스 프레임워크인
PyTorch를 기반으로 합니다.
이 책의 모든 코드는 최신 안정 버전의 PyTorch에서 테스트를 통과했습니다.
그러나 딥러닝의 빠른 발전으로 인해 <em>인쇄판</em>의 일부 코드는
향후 버전의 PyTorch에서 제대로 작동하지 않을 수 있습니다.
우리는 온라인 버전을 최신 상태로 유지할 계획입니다.
문제가 발생하면 :ref:<code>chap_installation</code>을 참조하여
코드와 런타임 환경을 업데이트하십시오.
아래는 PyTorch 구현의 종속성을 나열합니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
이 책의 대부분의 코드는 업계에서 널리 채택되고
연구자들 사이에서 인기 있는 오픈 소스 딥러닝 프레임워크인
TensorFlow를 기반으로 합니다.
이 책의 모든 코드는 최신 안정 버전의 TensorFlow에서 테스트를 통과했습니다.
그러나 딥러닝의 빠른 발전으로 인해 <em>인쇄판</em>의 일부 코드는
향후 버전의 TensorFlow에서 제대로 작동하지 않을 수 있습니다.
우리는 온라인 버전을 최신 상태로 유지할 계획입니다.
문제가 발생하면 :ref:<code>chap_installation</code>을 참조하여
코드와 런타임 환경을 업데이트하십시오.
아래는 TensorFlow 구현의 종속성을 나열합니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
이 책의 대부분의 코드는 임의의 Python 및 NumPy 함수의 미분,
JIT 컴파일, 벡터화 등과 같은 구성 가능한 함수 변환을 가능하게 하는
오픈 소스 프레임워크인 Jax를 기반으로 합니다!
머신러닝 연구 공간에서 인기를 얻고 있으며
배우기 쉬운 NumPy와 유사한 API를 가지고 있습니다.
실제로 JAX는 NumPy와 1:1 동등성을 달성하려고 노력하므로,
코드를 전환하는 것은 단일 import 문을 변경하는 것만큼 간단할 수 있습니다!
그러나 딥러닝의 빠른 발전으로 인해 <em>인쇄판</em>의 일부 코드는
향후 버전의 Jax에서 제대로 작동하지 않을 수 있습니다.
우리는 온라인 버전을 최신 상태로 유지할 계획입니다.
문제가 발생하면 :ref:<code>chap_installation</code>을 참조하여
코드와 런타임 환경을 업데이트하십시오.
아래는 JAX 구현의 종속성을 나열합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
from mxnet import autograd, context, gluon, image, init, np, npx
from mxnet.gluon import nn, rnn
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
import numpy as np
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torchvision import transforms
from PIL import Image
from scipy.spatial import distance_matrix
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
#@save
import numpy as np
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">#@tab jax
#@save
from dataclasses import field
from functools import partial
import flax
from flax import linen as nn
from flax.training import train_state
import jax
from jax import numpy as jnp
from jax import grad, vmap
import numpy as np
import optax
import tensorflow as tf
import tensorflow_datasets as tfds
from types import FunctionType
from typing import Any
</code></pre>
<h3 id="대상-독자"><a class="header" href="#대상-독자">대상 독자</a></h3>
<p>이 책은 딥러닝의 실용적인 기술을 확실하게 파악하고자 하는
학생(학부 또는 대학원), 엔지니어 및 연구자를 위한 것입니다.
우리는 모든 개념을 처음부터 설명하므로
딥러닝이나 머신러닝에 대한 이전 배경 지식은 필요하지 않습니다.
딥러닝의 방법을 완전히 설명하려면 약간의 수학과 프로그래밍이 필요하지만,
우리는 여러분이 약간의 선형 대수, 미적분, 확률 및 Python 프로그래밍을 포함한
몇 가지 기본 지식을 가지고 들어온다고 가정할 것입니다.
혹시 잊어버렸을 경우를 대비하여,
<a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/index.html">온라인 부록</a>은
이 책에서 찾을 수 있는 대부분의 수학에 대한 복습을 제공합니다.
일반적으로 우리는 수학적 엄격함보다 직관과 아이디어를 우선시할 것입니다.
이 책을 이해하기 위한 필수 조건을 넘어 이러한 기초를 확장하고 싶다면,
몇 가지 다른 훌륭한 리소스를 기쁘게 추천합니다:
:citet:<code>Bollobas.1999</code>의 <em>Linear Analysis</em>는
선형 대수와 함수 해석학을 깊이 있게 다룹니다.
<em>All of Statistics</em> :cite:<code>Wasserman.2013</code>는
통계에 대한 놀라운 소개를 제공합니다.
확률과 추론에 대한 Joe Blitzstein의 <a href="https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1138369918">책</a>과
<a href="https://projects.iq.harvard.edu/stat110/home">강의</a>는 교육적 보석입니다.
그리고 Python을 사용해 본 적이 없다면,
이 <a href="http://learnpython.org/">Python 튜토리얼</a>을 정독하고 싶을 수도 있습니다.</p>
<h3 id="노트북-웹사이트-github-및-포럼"><a class="header" href="#노트북-웹사이트-github-및-포럼">노트북, 웹사이트, GitHub 및 포럼</a></h3>
<p>모든 노트북은 <a href="https://d2l.ai">D2L.ai 웹사이트</a>와
<a href="https://github.com/d2l-ai/d2l-en">GitHub</a>에서 다운로드할 수 있습니다.
이 책과 관련하여 우리는 <a href="https://discuss.d2l.ai/c/5">discuss.d2l.ai</a>에서 토론 포럼을 시작했습니다.
책의 어느 섹션에 대해서든 질문이 있을 때마다,
각 노트북 끝에서 관련 토론 페이지로 연결되는 링크를 찾을 수 있습니다.</p>
<h2 id="감사의-말"><a class="header" href="#감사의-말">감사의 말</a></h2>
<p>우리는 영어와 중국어 초안 모두에 대해 수백 명의 기여자들에게 빚을 지고 있습니다.
그들은 콘텐츠를 개선하는 데 도움을 주었고 귀중한 피드백을 제공했습니다.
이 책은 원래 기본 프레임워크로 MXNet을 사용하여 구현되었습니다.
이전 MXNet 코드의 대다수 부분을 각각 PyTorch와 TensorFlow 구현으로 수정해 준 Anirudh Dagar와 Yuan Tang에게 감사드립니다.
2021년 7월부터 우리는 이 책을 PyTorch, MXNet, TensorFlow로 재설계하고 재구현했으며, PyTorch를 기본 프레임워크로 선택했습니다.
최신 PyTorch 코드의 대다수 부분을 JAX 구현으로 수정해 준 Anirudh Dagar에게 감사드립니다.
중국어 초안에서 최신 PyTorch 코드의 대다수 부분을 PaddlePaddle 구현으로 수정해 준 Baidu의 Gaosheng Wu, Liujun Hu, Ge Zhang, Jiehang Xie에게 감사드립니다.
언론사의 LaTeX 스타일을 PDF 빌드에 통합해 준 Shuai Zhang에게 감사드립니다.</p>
<p>GitHub에서 모두를 위해 이 영어 초안을 더 좋게 만들어 준 모든 기여자에게 감사드립니다.
그들의 GitHub ID 또는 이름은 다음과 같습니다(순서 없음):
alxnorden, avinashingit, bowen0701, brettkoonce, Chaitanya Prakash Bapat,
cryptonaut, Davide Fiocco, edgarroman, gkutiel, John Mitro, Liang Pu,
Rahul Agarwal, Mohamed Ali Jamaoui, Michael (Stu) Stewart, Mike Müller,
NRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sundeepteki,
topecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh Ravi Shrimali, YaYaB, Yuhong Chen,
Evgeniy Smirnov, lgov, Simon Corston-Oliver, Igor Dzreyev, Ha Nguyen, pmuens,
Andrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Abhishek Gupta,
uwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, Prasanth Buddareddygari, brianhendee,
mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete Lüer, Surbhi Vijayvargeeya,
Muhyun Kim, dennismalmgren, adursun, Anirudh Dagar, liqingnz, Pedro Larroy,
lgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gardner,
Maximilian Böther, Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong,
Steve Sedlmeyer, Ruslan Baratov, Rafael Schlatter, liusy182, Giannis Pappas,
ati-ozgur, qbaza, dchoi77, Adam Gerson, Phuc Le, Mark Atwood, christabella, vn09,
Haibin Lin, jjangga0214, RichyChen, noelo, hansent, Giel Dops, dvincent1337, WhiteD3vil,
Peter Kulits, codypenta, joseppinilla, ahmaurya, karolszk, heytitle, Peter Goetz, rigtorp,
Tiep Vu, sfilip, mlxd, Kale-ab Tessera, Sanjar Adilov, MatteoFerrara, hsneto,
Katarzyna Biesialska, Gregory Bruss, Duy–Thanh Doan, paulaurel, graytowne, Duc Pham,
sl7423, Jaedong Hwang, Yida Wang, cys4, clhm, Jean Kaddour, austinmw, trebeljahr, tbaums,
Cuong V. Nguyen, pavelkomarov, vzlamal, NotAnotherSystem, J-Arun-Mani, jancio, eldarkurtic,
the-great-shazbot, doctorcolossus, gducharme, cclauss, Daniel-Mietchen, hoonose, biagiom,
abhinavsp0730, jonathanhrandall, ysraell, Nodar Okroshiashvili, UgurKap, Jiyang Kang,
StevenJokes, Tomer Kaftan, liweiwp, netyster, ypandya, NishantTharani, heiligerl, SportsTHU,
Hoa Nguyen, manuel-arno-korfmann-webentwicklung, aterzis-personal, nxby, Xiaoting He, Josiah Yoder,
mathresearch, mzz2017, jroberayalas, iluu, ghejc, BSharmi, vkramdev, simonwardjones, LakshKD,
TalNeoran, djliden, Nikhil95, Oren Barkan, guoweis, haozhu233, pratikhack, Yue Ying, tayfununal,
steinsag, charleybeller, Andrew Lumsdaine, Jiekui Zhang, Deepak Pathak, Florian Donhauser, Tim Gates,
Adriaan Tijsseling, Ron Medina, Gaurav Saha, Murat Semerci, Lei Mao, Levi McClenny, Joshua Broyde,
jake221, jonbally, zyhazwraith, Brian Pulfer, Nick Tomasino, Lefan Zhang, Hongshen Yang, Vinney Cavallo,
yuntai, Yuanxiang Zhu, amarazov, pasricha, Ben Greenawald, Shivam Upadhyay, Quanshangze Du, Biswajit Sahoo,
Parthe Pandit, Ishan Kumar, HomunculusK, Lane Schwartz, varadgunjal, Jason Wiener, Armin Gholampoor,
Shreshtha13, eigen-arnav, Hyeonggyu Kim, EmilyOng, Bálint Mucsányi, Chase DuBois, Juntian Tao,
Wenxiang Xu, Lifu Huang, filevich, quake2005, nils-werner, Yiming Li, Marsel Khisamutdinov,
Francesco "Fuma" Fumagalli, Peilin Sun, Vincent Gurgul, qingfengtommy, Janmey Shukla, Mo Shan,
Kaan Sancak, regob, AlexSauer, Gopalakrishna Ramachandra, Tobias Uelwer, Chao Wang, Tian Cao,
Nicolas Corthorn, akash5474, kxxt, zxydi1992, Jacob Britton, Shuangchi He, zhmou, krahets, Jie-Han Chen,
Atishay Garg, Marcel Flygare, adtygan, Nik Vaessen, bolded, Louis Schlessinger, Balaji Varatharajan,
atgctg, Kaixin Li, Victor Barbaros, Riccardo Musto, Elizabeth Ho, azimjonn, Guilherme Miotto, Alessandro Finamore,
Joji Joseph, Anthony Biel, Zeming Zhao, shjustinbaek, gab-chen, nantekoto, Yutaro Nishiyama, Oren Amsalem,
Tian-MaoMao, Amin Allahyar, Gijs van Tulder, Mikhail Berkov, iamorphen, Matthew Caseres, Andrew Walsh,
pggPL, RohanKarthikeyan, Ryan Choi, and Likun Lei.</p>
<p>이 책을 집필하는 데 아낌없는 지원을 해준 Amazon Web Services, 특히 Wen-Ming Ye, George Karypis, Swami Sivasubramanian, Peter DeSantis, Adam Selipsky, Andrew Jassy에게 감사드립니다.
사용 가능한 시간, 자원, 동료와의 토론, 지속적인 격려가 없었다면 이 책은 탄생하지 못했을 것입니다.
출판을 위해 책을 준비하는 동안 Cambridge University Press는 훌륭한 지원을 제공했습니다.
도움과 전문성을 보여준 커미셔닝 편집자 David Tranah에게 감사드립니다.</p>
<h2 id="요약"><a class="header" href="#요약">요약</a></h2>
<p>딥러닝은 패턴 인식을 혁신하여
컴퓨터 비전, 자연어 처리, 자동 음성 인식과 같은 다양한 분야에서
현재 광범위한 기술을 구동하는 기술을 도입했습니다.
딥러닝을 성공적으로 적용하려면,
문제를 캐스팅하는 방법, 모델링의 기본 수학,
모델을 데이터에 적합시키는 알고리즘,
그리고 이 모든 것을 구현하는 엔지니어링 기술을 이해해야 합니다.
이 책은 산문, 그림, 수학, 코드를 모두 한곳에 포함하는 포괄적인 리소스를 제공합니다.</p>
<h2 id="연습-문제"><a class="header" href="#연습-문제">연습 문제</a></h2>
<ol>
<li>이 책의 토론 포럼 <a href="https://discuss.d2l.ai/">discuss.d2l.ai</a>에 계정을 등록하십시오.</li>
<li>컴퓨터에 Python을 설치하십시오.</li>
<li>섹션 하단에 있는 링크를 따라 포럼으로 이동하여 도움을 구하고 책에 대해 토론하며 저자 및 광범위한 커뮤니티와 교류하여 질문에 대한 답변을 찾을 수 있습니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/18">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/20">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/186">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17963">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="설치-installation"><a class="header" href="#설치-installation">설치 (Installation)</a></h1>
<p>:label:<code>chap_installation</code></p>
<p>시작하고 실행하기 위해,
우리는 Python, Jupyter Notebook, 관련 라이브러리,
그리고 책 자체를 실행하는 데 필요한 코드를 실행할 수 있는 환경이 필요합니다.</p>
<h2 id="miniconda-설치"><a class="header" href="#miniconda-설치">Miniconda 설치</a></h2>
<p>가장 간단한 옵션은 <a href="https://conda.io/en/latest/miniconda.html">Miniconda</a>를 설치하는 것입니다.
Python 3.x 버전이 필요하다는 점에 유의하십시오.
머신에 이미 conda가 설치되어 있다면 다음 단계를 건너뛸 수 있습니다.</p>
<p>Miniconda 웹사이트를 방문하여 Python 3.x 버전과 머신 아키텍처에 따라
시스템에 적합한 버전을 확인하십시오.
Python 버전이 3.9라고 가정해 봅시다
(저희가 테스트한 버전입니다).
macOS를 사용하는 경우,
이름에 "MacOSX" 문자열이 포함된 bash 스크립트를 다운로드하고
다운로드 위치로 이동하여 다음과 같이 설치를 실행합니다
(Intel Mac을 예로 듦):</p>
<pre><code class="language-bash"># 파일 이름은 변경될 수 있습니다
sh Miniconda3-py39_4.12.0-MacOSX-x86_64.sh -b
</code></pre>
<p>Linux 사용자는
이름에 "Linux" 문자열이 포함된 파일을 다운로드하고
다운로드 위치에서 다음을 실행합니다:</p>
<pre><code class="language-bash"># 파일 이름은 변경될 수 있습니다
sh Miniconda3-py39_4.12.0-Linux-x86_64.sh -b
</code></pre>
<p>Windows 사용자는 <a href="https://conda.io/en/latest/miniconda.html">온라인 지침</a>에 따라 Miniconda를 다운로드하고 설치합니다.
Windows에서는 <code>cmd</code>를 검색하여 명령을 실행하기 위한 명령 프롬프트(명령줄 인터프리터)를 열 수 있습니다.</p>
<p>다음으로, <code>conda</code>를 직접 실행할 수 있도록 쉘을 초기화합니다.</p>
<pre><code class="language-bash">~/miniconda3/bin/conda init
</code></pre>
<p>그런 다음 현재 쉘을 닫았다가 다시 엽니다.
다음과 같이 새 환경을 만들 수 있어야 합니다:</p>
<pre><code class="language-bash">conda create --name d2l python=3.9 -y
</code></pre>
<p>이제 <code>d2l</code> 환경을 활성화할 수 있습니다:</p>
<pre><code class="language-bash">conda activate d2l
</code></pre>
<h2 id="딥러닝-프레임워크-및-d2l-패키지-설치"><a class="header" href="#딥러닝-프레임워크-및-d2l-패키지-설치">딥러닝 프레임워크 및 <code>d2l</code> 패키지 설치</a></h2>
<p>딥러닝 프레임워크를 설치하기 전에,
먼저 머신에 적절한 GPU가 있는지 확인하십시오
(표준 노트북의 디스플레이를 구동하는 GPU는 우리의 목적과 관련이 없습니다).
예를 들어,
컴퓨터에 NVIDIA GPU가 있고 <a href="https://developer.nvidia.com/cuda-downloads">CUDA</a>를 설치했다면,
모든 준비가 된 것입니다.
머신에 GPU가 없더라도 아직 걱정할 필요는 없습니다.
CPU는 처음 몇 챕터를 진행하기에 충분한 마력을 제공합니다.
더 큰 모델을 실행하기 전에 GPU에 액세스하고 싶을 것이라는 점만 기억하십시오.</p>
<p>:begin_tab:<code>mxnet</code></p>
<p>MXNet의 GPU 지원 버전을 설치하려면, 설치된 CUDA 버전을 알아야 합니다.
<code>nvcc --version</code> 또는 <code>cat /usr/local/cuda/version.txt</code>를 실행하여 확인할 수 있습니다.
CUDA 11.2를 설치했다고 가정하고 다음 명령을 실행하십시오:</p>
<pre><code class="language-bash"># macOS 및 Linux 사용자용
pip install mxnet-cu112==1.9.1

# Windows 사용자용
pip install mxnet-cu112==1.9.1 -f https://dist.mxnet.io/python
</code></pre>
<p>CUDA 버전에 따라 마지막 숫자를 변경할 수 있습니다. 예: CUDA 10.1의 경우 <code>cu101</code>,
CUDA 9.0의 경우 <code>cu90</code>.</p>
<p>머신에 NVIDIA GPU나 CUDA가 없는 경우,
다음과 같이 CPU 버전을 설치할 수 있습니다:</p>
<pre><code class="language-bash">pip install mxnet==1.9.1
</code></pre>
<p>:end_tab:</p>
<p>:begin_tab:<code>pytorch</code></p>
<p>다음과 같이 CPU 또는 GPU 지원으로 PyTorch(지정된 버전은 작성 시점에 테스트됨)를 설치할 수 있습니다:</p>
<pre><code class="language-bash">pip install torch==2.0.0 torchvision==0.15.1
</code></pre>
<p>:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
다음과 같이 CPU 또는 GPU 지원으로 TensorFlow를 설치할 수 있습니다:</p>
<pre><code class="language-bash">pip install tensorflow==2.12.0 tensorflow-probability==0.20.0
</code></pre>
<p>:end_tab:</p>
<p>:begin_tab:<code>jax</code>
다음과 같이 CPU 또는 GPU 지원으로 JAX와 Flax를 설치할 수 있습니다:</p>
<pre><code class="language-bash"># GPU
pip install "jax[cuda11_pip]==0.4.13" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html flax==0.7.0
</code></pre>
<p>머신에 NVIDIA GPU나 CUDA가 없는 경우,
다음과 같이 CPU 버전을 설치할 수 있습니다:</p>
<pre><code class="language-bash"># CPU
pip install "jax[cpu]==0.4.13" flax==0.7.0
</code></pre>
<p>:end_tab:</p>
<p>다음 단계는 이 책 전체에서 발견되는 자주 사용되는 함수와 클래스를 캡슐화하기 위해
우리가 개발한 <code>d2l</code> 패키지를 설치하는 것입니다:</p>
<pre><code class="language-bash">pip install d2l==1.0.3
</code></pre>
<h2 id="코드-다운로드-및-실행"><a class="header" href="#코드-다운로드-및-실행">코드 다운로드 및 실행</a></h2>
<p>다음으로, 책의 각 코드 블록을 실행할 수 있도록
노트북을 다운로드하고 싶을 것입니다.
<a href="https://d2l.ai/">D2L.ai 웹사이트</a>의 모든 HTML 페이지 상단에 있는
"Notebooks" 탭을 클릭하여 코드를 다운로드하고 압축을 풀면 됩니다.
대안으로, 다음과 같이 명령줄에서 노트북을 가져올 수 있습니다:</p>
<p>:begin_tab:<code>mxnet</code></p>
<pre><code class="language-bash">mkdir d2l-en &amp;&amp; cd d2l-en
curl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip
unzip d2l-en.zip &amp;&amp; rm d2l-en.zip
cd mxnet
</code></pre>
<p>:end_tab:</p>
<p>:begin_tab:<code>pytorch</code></p>
<pre><code class="language-bash">mkdir d2l-en &amp;&amp; cd d2l-en
curl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip
unzip d2l-en.zip &amp;&amp; rm d2l-en.zip
cd pytorch
</code></pre>
<p>:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code></p>
<pre><code class="language-bash">mkdir d2l-en &amp;&amp; cd d2l-en
curl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip
unzip d2l-en.zip &amp;&amp; rm d2l-en.zip
cd tensorflow
</code></pre>
<p>:end_tab:</p>
<p>:begin_tab:<code>jax</code></p>
<pre><code class="language-bash">mkdir d2l-en &amp;&amp; cd d2l-en
curl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip
unzip d2l-en.zip &amp;&amp; rm d2l-en.zip
cd jax
</code></pre>
<p>:end_tab:</p>
<p>아직 <code>unzip</code>이 설치되어 있지 않다면, 먼저 <code>sudo apt-get install unzip</code>을 실행하십시오.
이제 다음을 실행하여 Jupyter Notebook 서버를 시작할 수 있습니다:</p>
<pre><code class="language-bash">jupyter notebook
</code></pre>
<p>이 시점에서 웹 브라우저에서 http://localhost:8888을 열 수 있습니다
(이미 자동으로 열렸을 수 있습니다).
그런 다음 책의 각 섹션에 대한 코드를 실행할 수 있습니다.
새 명령줄 창을 열 때마다,
D2L 노트북을 실행하거나 패키지(딥러닝 프레임워크 또는 <code>d2l</code> 패키지)를 업데이트하기 전에
<code>conda activate d2l</code>을 실행하여 런타임 환경을 활성화해야 합니다.
환경을 종료하려면 <code>conda deactivate</code>를 실행하십시오.</p>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/23">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/24">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/436">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17964">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="표기법-notation"><a class="header" href="#표기법-notation">표기법 (Notation)</a></h1>
<p>:label:<code>chap_notation</code></p>
<p>이 책 전체에서 우리는 다음의 표기법 관례를 따릅니다.
이러한 기호 중 일부는 플레이스홀더(placeholder)이고,
다른 기호는 특정 객체를 나타냅니다.
일반적인 경험 법칙으로, 부정관사 "a"는 종종
기호가 플레이스홀더이며 유사하게 포맷된 기호가
동일한 유형의 다른 객체를 나타낼 수 있음을 나타냅니다.
예를 들어, "$x$: 스칼라(a scalar)"는 소문자가 일반적으로
스칼라 값을 나타냄을 의미하지만,
"$\mathbb{Z}$: 정수 집합(the set of integers)"은
구체적으로 기호 $\mathbb{Z}$를 나타냅니다.</p>
<h2 id="수치-객체-numerical-objects"><a class="header" href="#수치-객체-numerical-objects">수치 객체 (Numerical Objects)</a></h2>
<ul>
<li>$x$: 스칼라 (scalar)</li>
<li>$\mathbf{x}$: 벡터 (vector)</li>
<li>$\mathbf{X}$: 행렬 (matrix)</li>
<li>$\mathsf{X}$: 일반 텐서 (tensor)</li>
<li>$\mathbf{I}$: 단위 행렬 (identity matrix) (주어진 차원의), 즉 모든 대각선 항목이 $1$이고 모든 비대각선 항목이 $0$인 정사각 행렬</li>
<li>$x_i$, $[\mathbf{x}]_i$: 벡터 $\mathbf{x}$의 $i^\textrm{th}$ 요소</li>
<li>$x_{ij}$, $x_{i,j}$,$[\mathbf{X}]<em>{ij}$, $[\mathbf{X}]</em>{i,j}$: 행 $i$와 열 $j$에 있는 행렬 $\mathbf{X}$의 요소</li>
</ul>
<h2 id="집합론-set-theory"><a class="header" href="#집합론-set-theory">집합론 (Set Theory)</a></h2>
<ul>
<li>$\mathcal{X}$: 집합 (set)</li>
<li>$\mathbb{Z}$: 정수 집합</li>
<li>$\mathbb{Z}^+$: 양의 정수 집합</li>
<li>$\mathbb{R}$: 실수 집합</li>
<li>$\mathbb{R}^n$: $n$-차원 실수 벡터의 집합</li>
<li>$\mathbb{R}^{a\times b}$: $a$개의 행과 $b$개의 열을 가진 실수 행렬의 집합</li>
<li>$|\mathcal{X}|$: 집합 $\mathcal{X}$의 기수 (원소의 수)</li>
<li>$\mathcal{A}\cup\mathcal{B}$: 집합 $\mathcal{A}$와 $\mathcal{B}$의 합집합</li>
<li>$\mathcal{A}\cap\mathcal{B}$: 집합 $\mathcal{A}$와 $\mathcal{B}$의 교집합</li>
<li>$\mathcal{A}\setminus\mathcal{B}$: $\mathcal{A}$에서 $\mathcal{B}$의 차집합 ($\mathcal{B}$에 속하지 않는 $\mathcal{A}$의 원소만 포함)</li>
</ul>
<h2 id="함수-및-연산자-functions-and-operators"><a class="header" href="#함수-및-연산자-functions-and-operators">함수 및 연산자 (Functions and Operators)</a></h2>
<ul>
<li>$f(\cdot)$: 함수</li>
<li>$\log(\cdot)$: 자연 로그 (밑 $e$)</li>
<li>$\log_2(\cdot)$: 밑이 $2$인 로그</li>
<li>$\exp(\cdot)$: 지수 함수</li>
<li>$\mathbf{1}(\cdot)$: 지시 함수 (indicator function); 불리언 인수가 참이면 $1$, 그렇지 않으면 $0$으로 평가</li>
<li>$\mathbf{1}_{\mathcal{X}}(z)$: 집합 멤버십 지시 함수; 요소 $z$가 집합 $\mathcal{X}$에 속하면 $1$, 그렇지 않으면 $0$으로 평가</li>
<li>$\mathbf{(\cdot)}^\top$: 벡터 또는 행렬의 전치 (transpose)</li>
<li>$\mathbf{X}^{-1}$: 행렬 $\mathbf{X}$의 역행렬 (inverse)</li>
<li>$\odot$: 하다마드 (요소별) 곱 (Hadamard product)</li>
<li>$[\cdot, \cdot]$: 연결 (concatenation)</li>
<li><code>$\\cdot$</code>_p: $\ell_p$ 노름 (norm)</li>
<li><code>$\\cdot$</code>$: $\ell_2$ 노름</li>
<li>$\langle \mathbf{x}, \mathbf{y} \rangle$: 벡터 $\mathbf{x}$와 $\mathbf{y}$의 내적 (dot product)</li>
<li>$\sum$: 요소 모음에 대한 합계</li>
<li>$\prod$: 요소 모음에 대한 곱</li>
<li>$\stackrel{\textrm{def}}}{=}$: 왼쪽 기호의 정의로 주장되는 등식</li>
</ul>
<h2 id="미적분-calculus"><a class="header" href="#미적분-calculus">미적분 (Calculus)</a></h2>
<ul>
<li>$\frac{dy}{dx}$: $x$에 대한 $y$의 미분</li>
<li>$\frac{\partial y}{\partial x}$: $x$에 대한 $y$의 편미분</li>
<li>$\nabla_{\mathbf{x}} y$: $\mathbf{x}$에 대한 $y$의 기울기 (gradient)</li>
<li>$\int_a^b f(x) \;dx$: $x$에 대한 $a$에서 $b$까지 $f$의 정적분</li>
<li>$\int f(x) \;dx$: $x$에 대한 $f$의 부정적분</li>
</ul>
<h2 id="확률-및-정보-이론-probability-and-information-theory"><a class="header" href="#확률-및-정보-이론-probability-and-information-theory">확률 및 정보 이론 (Probability and Information Theory)</a></h2>
<ul>
<li>$X$: 확률 변수 (random variable)</li>
<li>$P$: 확률 분포 (probability distribution)</li>
<li>$X \sim P$: 확률 변수 $X$가 분포 $P$를 따름</li>
<li>$P(X=x)$: 확률 변수 $X$가 값 $x$를 취하는 사건에 할당된 확률</li>
<li>$P(X \mid Y)$: $Y$가 주어졌을 때 $X$의 조건부 확률 분포</li>
<li>$p(\cdot)$: 분포 $P$와 관련된 확률 밀도 함수 (PDF)</li>
<li>${E}[X]$: 확률 변수 $X$의 기댓값 (expectation)</li>
<li>$X \perp Y$: 확률 변수 $X$와 $Y$는 독립임</li>
<li>$X \perp Y \mid Z$: 확률 변수 $X$와 $Y$는 $Z$가 주어졌을 때 조건부 독립임</li>
<li>$\sigma_X$: 확률 변수 $X$의 표준 편차 (standard deviation)</li>
<li>$\textrm{Var}(X)$: 확률 변수 $X$의 분산 (variance), $\sigma^2_X$와 같음</li>
<li>$\textrm{Cov}(X, Y)$: 확률 변수 $X$와 $Y$의 공분산 (covariance)</li>
<li>$\rho(X, Y)$: $X$와 $Y$ 사이의 피어슨 상관 계수, $\</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="서론-introduction"><a class="header" href="#서론-introduction">서론 (Introduction)</a></h1>
<p>:label:<code>chap_introduction</code></p>
<p>최근까지도 우리가 매일 상호작용하는 대부분의 컴퓨터 프로그램은 소프트웨어 개발자가 처음부터 끝까지 코딩한 것이었습니다.
예를 들어, 우리가 이메일 애플리케이션을 작성하고 싶다고 가정해 봅시다.
우리는 사용자가 어떻게 상호작용해야 하는지, 모든 수신 이메일을 표시하는 방법, 보낸 이메일을 저장하는 방법 등을 알고 있습니다.
우리는 이메일을 검색 가능하게 만들고, 보낸 사람별로 그룹화하는 등의 작업을 수행할 수 있습니다.
여기서 모든 논리는 작성자가 이해하고 코드로 번역할 수 있는 것입니다.</p>
<p>하지만 이메일 앱에 스팸 필터를 추가하고 싶다면 어떻게 해야 할까요?
어떤 이메일이 스팸인지 아닌지를 결정하는 로직을 어떻게 작성해야 할까요?
"당첨되었습니다" 또는 "나이지리아의 왕자"와 같은 특정 단어를 찾도록 스팸 필터를 코딩할 수도 있습니다.
하지만 스팸 발송자들은 필터를 우회하기 위해 단어를 "w1nn3r" 또는 "prince of n1geria"로 바꾸는 등 수법을 계속 바꿀 것입니다.
따라서 우리는 필터를 계속 업데이트해야 할 것입니다.
더 나은 방법은 없을까요?</p>
<p>이것이 바로 머신러닝(machine learning)이 필요한 지점입니다.
우리는 스팸인지 아닌지를 결정하는 명시적인 규칙을 작성하는 대신,
수많은 이메일 예제(스팸과 정상 이메일 모두)를 컴퓨터에 보여주고 컴퓨터가 스스로 규칙을 배우게 합니다.
이러한 데이터로부터 학습하는 능력이 머신러닝의 핵심입니다.</p>
<p>딥러닝(deep learning)은 머신러닝의 한 분야로,
인간 뇌의 구조와 기능에서 영감을 받은 인공 신경망(artificial neural networks)을 사용합니다.
지난 10년 동안 딥러닝은 이미지 인식, 음성 인식, 자연어 처리 등 다양한 분야에서 획기적인 발전을 이루었습니다.</p>
<p>이 책은 딥러닝의 기초부터 최첨단 아키텍처까지 포괄적으로 다룹니다.
우리는 이론적인 설명뿐만 아니라 실제 작동하는 코드를 통해 여러분이 직접 딥러닝 모델을 구축하고 훈련할 수 있도록 도울 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

# 여기에 도입부 세부 섹션들이 나열될 수 있습니다.
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="예비-지식-preliminaries"><a class="header" href="#예비-지식-preliminaries">예비 지식 (Preliminaries)</a></h1>
<p>:label:<code>chap_preliminaries</code></p>
<p>딥러닝에 뛰어들 준비를 하기 위해,
몇 가지 생존 기술이 필요합니다:
(i) 데이터를 저장하고 조작하는 기술;
(ii) 다양한 소스에서 데이터를 수집하고 전처리하기 위한 라이브러리;
(iii) 고차원 데이터 요소에 적용하는 기본적인 선형 대수 연산에 대한 지식;
(iv) 손실 함수를 줄이기 위해 각 파라미터를 어떤 방향으로 조정해야 하는지 결정하기에 충분한 미적분;
(v) 방금 배운 미적분의 많은 부분을 잊어버릴 수 있도록 미분 값을 자동으로 계산하는 능력;
(vi) 불확실성 하에서 추론하기 위한 우리의 기본 언어인 확률에 대한 기본적인 유창함;
(vii) 막혔을 때 공식 문서에서 답을 찾는 적성.</p>
<p>간단히 말해서, 이 챕터는 이 책의 기술적인 내용의 <em>대부분</em>을 따라가는 데 필요한 기초에 대한 빠른 소개를 제공합니다.</p>
<pre><code class="language-toc">:maxdepth: 2

ndarray
pandas
linear-algebra
calculus
autograd
probability
lookup-api
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="데이터-조작-data-manipulation"><a class="header" href="#데이터-조작-data-manipulation">데이터 조작 (Data Manipulation)</a></h1>
<p>:label:<code>sec_ndarray</code></p>
<p>어떤 일을 하려면,
데이터를 저장하고 조작할 방법이 필요합니다.
일반적으로 데이터로 해야 할 두 가지 중요한 일이 있습니다:
(i) 데이터를 획득하는 것;
그리고 (ii) 컴퓨터 내부에 들어오면 처리하는 것.
데이터를 저장할 방법 없이 데이터를 획득하는 것은 의미가 없으므로,
시작하기 위해 *텐서(tensor)*라고도 부르는 $n$-차원 배열로
손을 더럽혀 봅시다.
이미 NumPy 과학 컴퓨팅 패키지를 알고 있다면,
이것은 식은 죽 먹기일 것입니다.
모든 최신 딥러닝 프레임워크의 <em>텐서 클래스</em>
(MXNet의 <code>ndarray</code>, PyTorch와 TensorFlow의 <code>Tensor</code>)는
몇 가지 킬러 기능이 추가된 NumPy의 <code>ndarray</code>와 유사합니다.
첫째, 텐서 클래스는 자동 미분을 지원합니다.
둘째, NumPy는 CPU에서만 실행되는 반면,
텐서 클래스는 GPU를 활용하여 수치 계산을 가속화합니다.
이러한 속성 덕분에 신경망은 코딩하기 쉽고 실행 속도도 빠릅니다.</p>
<h2 id="시작하기-getting-started"><a class="header" href="#시작하기-getting-started">시작하기 (Getting Started)</a></h2>
<p>:begin_tab:<code>mxnet</code>
시작하기 위해, MXNet에서 <code>np</code> (<code>numpy</code>)와 <code>npx</code> (<code>numpy_extension</code>) 모듈을 가져옵니다.
여기서 <code>np</code> 모듈은 NumPy가 지원하는 함수를 포함하고,
<code>npx</code> 모듈은 NumPy와 유사한 환경 내에서 딥러닝을 지원하기 위해 개발된 확장 세트를 포함합니다.
텐서를 사용할 때, 우리는 거의 항상 <code>set_np</code> 함수를 호출합니다:
이것은 MXNet의 다른 구성 요소에 의한 텐서 처리의 호환성을 위한 것입니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
(<strong>시작하기 위해, PyTorch 라이브러리를 가져옵니다.
패키지 이름은 <code>torch</code>입니다.</strong>)
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
시작하기 위해, <code>tensorflow</code>를 가져옵니다.
간결함을 위해 실무자들은 종종 별칭 <code>tf</code>를 할당합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
import jax
from jax import numpy as jnp
</code></pre>
<p>[<strong>텐서는 수치 값의 (아마도 다차원) 배열을 나타냅니다.</strong>]
1차원인 경우, 즉 데이터에 하나의 축만 필요한 경우,
텐서를 *벡터(vector)*라고 합니다.
두 개의 축을 가진 텐서를 *행렬(matrix)*이라고 합니다.
축이 $k &gt; 2$인 경우, 우리는 전문적인 이름을 버리고
그냥 $k$차 *텐서($k^	extrm{th}$-order tensor)*라고 부릅니다.</p>
<p>:begin_tab:<code>mxnet</code>
MXNet은 값으로 미리 채워진 새 텐서를 생성하기 위한
다양한 함수를 제공합니다.
예를 들어 <code>arange(n)</code>을 호출하여,
0(포함)에서 시작하여 <code>n</code>(포함되지 않음)으로 끝나는
균등하게 간격을 둔 값의 벡터를 만들 수 있습니다.
기본적으로 간격 크기는 $1$입니다.
달리 명시되지 않는 한, 새 텐서는 메인 메모리에 저장되고
CPU 기반 계산을 위해 지정됩니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
PyTorch는 값으로 미리 채워진 새 텐서를 생성하기 위한
다양한 함수를 제공합니다.
예를 들어 <code>arange(n)</code>을 호출하여,
0(포함)에서 시작하여 <code>n</code>(포함되지 않음)으로 끝나는
균등하게 간격을 둔 값의 벡터를 만들 수 있습니다.
기본적으로 간격 크기는 $1$입니다.
달리 명시되지 않는 한, 새 텐서는 메인 메모리에 저장되고
CPU 기반 계산을 위해 지정됩니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
TensorFlow는 값으로 미리 채워진 새 텐서를 생성하기 위한
다양한 함수를 제공합니다.
예를 들어 <code>range(n)</code>을 호출하여,
0(포함)에서 시작하여 <code>n</code>(포함되지 않음)으로 끝나는
균등하게 간격을 둔 값의 벡터를 만들 수 있습니다.
기본적으로 간격 크기는 $1$입니다.
달리 명시되지 않는 한, 새 텐서는 메인 메모리에 저장되고
CPU 기반 계산을 위해 지정됩니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(12)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(12, dtype=torch.float32)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.range(12, dtype=tf.float32)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(12)
x
</code></pre>
<p>:begin_tab:<code>mxnet</code>
이러한 각 값을 텐서의 *요소(element)*라고 합니다.
텐서 <code>x</code>에는 12개의 요소가 포함되어 있습니다.
<code>size</code> 속성을 통해 텐서의 총 요소 수를 검사할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
이러한 각 값을 텐서의 *요소(element)*라고 합니다.
텐서 <code>x</code>에는 12개의 요소가 포함되어 있습니다.
<code>numel</code> 메서드를 통해 텐서의 총 요소 수를 검사할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
이러한 각 값을 텐서의 *요소(element)*라고 합니다.
텐서 <code>x</code>에는 12개의 요소가 포함되어 있습니다.
<code>size</code> 함수를 통해 텐서의 총 요소 수를 검사할 수 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, jax
x.size
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x.numel()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.size(x)
</code></pre>
<p>(<strong>텐서의 *모양(shape)*에 액세스할 수 있습니다.</strong>)
(각 축을 따른 길이)
<code>shape</code> 속성을 검사하여 수행합니다.
여기서는 벡터를 다루고 있으므로,
<code>shape</code>에는 단일 요소만 포함되며 크기와 동일합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
x.shape
</code></pre>
<p>우리는 <code>reshape</code>를 호출하여
[<strong>크기나 값을 변경하지 않고 텐서의 모양을 변경할 수 있습니다.</strong>]
예를 들어, 모양이 (12,)인 벡터 <code>x</code>를
모양이 (3, 4)인 행렬 <code>X</code>로 변환할 수 있습니다.
이 새 텐서는 모든 요소를 유지하지만 행렬로 재구성합니다.
벡터의 요소는 한 번에 한 행씩 배치되므로
<code>x[3] == X[0, 3]</code>입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
X = x.reshape(3, 4)
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X = tf.reshape(x, (3, 4))
X
</code></pre>
<p><code>reshape</code>에 모든 모양 구성 요소를 지정하는 것은 중복입니다.
이미 텐서의 크기를 알고 있으므로, 나머지가 주어지면 모양의 한 구성 요소를 알아낼 수 있습니다.
예를 들어, 크기가 $n$인 텐서와 목표 모양 ($h$, $w$)가 주어지면,
$w = n/h$임을 알 수 있습니다.
모양의 한 구성 요소를 자동으로 추론하려면,
자동으로 추론해야 하는 모양 구성 요소에 <code>-1</code>을 넣을 수 있습니다.
우리의 경우 <code>x.reshape(3, 4)</code>를 호출하는 대신,
동등하게 <code>x.reshape(-1, 4)</code> 또는 <code>x.reshape(3, -1)</code>을 호출할 수 있었습니다.</p>
<p>실무자들은 종종 모두 0 또는 1을 포함하도록 초기화된 텐서로 작업해야 합니다.
<code>zeros</code> 함수를 통해 [<strong>모든 요소가 0으로 설정된 텐서를 구성할 수 있습니다.</strong>]
그리고 모양은 (2, 3, 4)입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.zeros((2, 3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.zeros((2, 3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.zeros((2, 3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.zeros((2, 3, 4))
</code></pre>
<p>마찬가지로 <code>ones</code>를 호출하여
모두 1인 텐서를 생성할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.ones((2, 3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.ones((2, 3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.ones((2, 3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.ones((2, 3, 4))
</code></pre>
<p>우리는 종종 주어진 확률 분포에서
[<strong>각 요소를 무작위로 (그리고 독립적으로) 샘플링</strong>]하고 싶어 합니다.
예를 들어, 신경망의 파라미터는 종종 무작위로 초기화됩니다.
다음 스니펫은 평균이 0이고 표준 편차가 1인
표준 가우스(정규) 분포에서 추출한 요소로 텐서를 생성합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.random.normal(0, 1, size=(3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.randn(3, 4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.random.normal(shape=[3, 4])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# JAX에서 무작위 함수를 호출하려면 키를 지정해야 합니다.
# 무작위 함수에 동일한 키를 제공하면 항상 동일한 샘플이 생성됩니다.
jax.random.normal(jax.random.PRNGKey(0), (3, 4))
</code></pre>
<p>마지막으로, 수치 리터럴을 포함하는 (아마도 중첩된) Python 리스트를 제공하여
[<strong>각 요소에 대한 정확한 값을 제공</strong>]함으로써 텐서를 구성할 수 있습니다.
여기서 우리는 리스트의 리스트로 행렬을 구성하는데,
가장 바깥쪽 리스트는 축 0에 해당하고 안쪽 리스트는 축 1에 해당합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
</code></pre>
<h2 id="인덱싱-및-슬라이싱-indexing-and-slicing"><a class="header" href="#인덱싱-및-슬라이싱-indexing-and-slicing">인덱싱 및 슬라이싱 (Indexing and Slicing)</a></h2>
<p>Python 리스트와 마찬가지로,
인덱싱(0부터 시작)을 통해 텐서 요소에 액세스할 수 있습니다.
리스트 끝을 기준으로 한 위치를 기준으로 요소에 액세스하려면
음수 인덱싱을 사용할 수 있습니다.
마지막으로, 슬라이싱(예: <code>X[start:stop]</code>)을 통해 전체 인덱스 범위에 액세스할 수 있으며,
반환된 값에는 첫 번째 인덱스(<code>start</code>)는 포함되지만 <em>마지막 인덱스</em>(<code>stop</code>)는 포함되지 않습니다.
마지막으로, $k$차 텐서에 대해 하나의 인덱스(또는 슬라이스)만 지정되면,
축 0을 따라 적용됩니다.
따라서 다음 코드에서
[<strong><code>[-1]</code>은 마지막 행을 선택하고 <code>[1:3]</code>은 두 번째와 세 번째 행을 선택합니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab all
X[-1], X[1:3]
</code></pre>
<p>:begin_tab:<code>mxnet, pytorch</code>
읽는 것 외에도, (<strong>인덱스를 지정하여 행렬의 요소를 <em>쓸(write)</em> 수도 있습니다.</strong>)
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
TensorFlow의 <code>Tensor</code>는 불변(immutable)이므로 할당할 수 없습니다.
TensorFlow의 <code>Variable</code>은 할당을 지원하는 변경 가능한(mutable) 상태 컨테이너입니다.
TensorFlow의 기울기는 <code>Variable</code> 할당을 통해 역방향으로 흐르지 않는다는 점을 명심하십시오.</p>
<p>전체 <code>Variable</code>에 값을 할당하는 것 외에도 인덱스를 지정하여 <code>Variable</code>의 요소를 쓸 수 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
X[1, 2] = 17
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X_var = tf.Variable(X)
X_var[1, 2].assign(9)
X_var
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# JAX 배열은 불변입니다. jax.numpy.ndarray.at 인덱스 업데이트 연산자는
# 해당 수정 사항이 적용된 새 배열을 만듭니다.
X_new_1 = X.at[1, 2].set(17)
X_new_1
</code></pre>
<p>[<strong>여러 요소에 동일한 값을 할당하려면,
할당 연산의 왼쪽에 인덱싱을 적용합니다.</strong>]
예를 들어 <code>[:2, :]</code>는 첫 번째와 두 번째 행에 액세스하며,
여기서 <code>:</code>는 축 1(열)을 따라 모든 요소를 가져옵니다.
행렬에 대한 인덱싱을 논의했지만,
이는 벡터와 2차원 이상의 텐서에서도 작동합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
X[:2, :] = 12
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X_var = tf.Variable(X)
X_var[:2, :].assign(tf.ones(X_var[:2,:].shape, dtype=tf.float32) * 12)
X_var
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X_new_2 = X_new_1.at[:2, :].set(12)
X_new_2
</code></pre>
<h2 id="연산-operations"><a class="header" href="#연산-operations">연산 (Operations)</a></h2>
<p>이제 텐서를 구성하는 방법과
요소를 읽고 쓰는 방법을 알았으므로,
다양한 수학적 연산으로 텐서를 조작할 수 있습니다.
이 중 가장 유용한 것은 <em>요소별(elementwise)</em> 연산입니다.
이것은 텐서의 각 요소에 표준 스칼라 연산을 적용합니다.
두 개의 텐서를 입력으로 받는 함수의 경우,
요소별 연산은 해당 요소의 각 쌍에 표준 이진 연산자를 적용합니다.
스칼라에서 스칼라로 매핑하는 모든 함수에서
요소별 함수를 만들 수 있습니다.</p>
<p>수학적 표기법에서, 우리는 이러한
<em>단항(unary)</em> 스칼라 연산자(하나의 입력을 받음)를
서명 $f: \mathbb{R} \rightarrow \mathbb{R}$로 나타냅니다.
이것은 함수가 임의의 실수를 다른 실수로 매핑한다는 것을 의미합니다.
$e^x$와 같은 단항 연산자를 포함한 대부분의 표준 연산자는 요소별로 적용될 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.exp(x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.exp(x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.exp(x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.exp(x)
</code></pre>
<p>마찬가지로, 우리는 실수의 쌍을 (단일) 실수로 매핑하는
<em>이진(binary)</em> 스칼라 연산자를
서명 $f: \mathbb{R}, \mathbb{R} \rightarrow \mathbb{R}$로 나타냅니다.
<em>모양이 같은</em> 임의의 두 벡터 $\mathbf{u}$와 $\mathbf{v}$와
이진 연산자 $f$가 주어지면, 모든 $i$에 대해 $c_i \gets f(u_i, v_i)$를 설정하여
벡터 $\mathbf{c} = F(\mathbf{u},\mathbf{v})$를 생성할 수 있습니다.
여기서 $c_i, u_i, v_i$는 벡터 $\mathbf{c}, \mathbf{u}, \mathbf{v}$의 $i^\textrm{th}$ 요소입니다.
여기서 우리는 스칼라 함수를 요소별 벡터 연산으로 *리프팅(lifting)*하여
벡터 값 함수 $F: \mathbb{R}^d, \mathbb{R}^d \rightarrow \mathbb{R}^d$를 생성했습니다.
덧셈(<code>+</code>), 뺄셈(<code>-</code>), 곱셈(<code>*</code>), 나눗셈(<code>/</code>), 거듭제곱(<code>**</code>)에 대한
일반적인 표준 산술 연산자는
임의의 모양의 동일한 모양을 가진 텐서에 대해 모두 요소별 연산으로 <em>리프팅</em>되었습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.array([1, 2, 4, 8])
y = np.array([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.constant([1.0, 2, 4, 8])
y = tf.constant([2.0, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.array([1.0, 2, 4, 8])
y = jnp.array([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y
</code></pre>
<p>요소별 계산 외에도,
내적 및 행렬 곱셈과 같은 선형 대수 연산을 수행할 수도 있습니다.
이에 대해서는 :numref:<code>sec_linear-algebra</code>에서 자세히 설명하겠습니다.</p>
<p>우리는 또한 [**<em>여러 텐서를 연결(concatenate)<em>하여</em></em>]
더 큰 텐서를 형성하기 위해 끝과 끝을 쌓을 수 있습니다.
텐서 리스트를 제공하고 시스템에 어떤 축을 따라 연결할지 알려주기만 하면 됩니다.
아래 예제는 열(축 1) 대신 행(축 0)을 따라 두 행렬을 연결할 때 어떤 일이 발생하는지 보여줍니다.
첫 번째 출력의 축 0 길이($6$)는 두 입력 텐서의 축 0 길이의 합($3 + 3$)이고,
두 번째 출력의 축 1 길이($8$)는 두 입력 텐서의 축 1 길이의 합($4 + 4$)임을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
X = np.arange(12).reshape(3, 4)
Y = np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
np.concatenate([X, Y], axis=0), np.concatenate([X, Y], axis=1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4))
Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
tf.concat([X, Y], axis=0), tf.concat([X, Y], axis=1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X = jnp.arange(12, dtype=jnp.float32).reshape((3, 4))
Y = jnp.array([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
jnp.concatenate((X, Y), axis=0), jnp.concatenate((X, Y), axis=1)
</code></pre>
<p>때로는 [<em>논리문</em>을 통해 이진 텐서를 구성]하고 싶을 때가 있습니다.
<code>X == Y</code>를 예로 들어보겠습니다.
각 위치 <code>i, j</code>에 대해 <code>X[i, j]</code>와 <code>Y[i, j]</code>가 같으면,
결과의 해당 항목은 값 <code>1</code>을 취하고,
그렇지 않으면 값 <code>0</code>을 취합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
X == Y
</code></pre>
<p>[<strong>텐서의 모든 요소를 합하면</strong>] 요소가 하나만 있는 텐서가 생성됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
X.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_sum(X)
</code></pre>
<h2 id="브로드캐스팅-broadcasting"><a class="header" href="#브로드캐스팅-broadcasting">브로드캐스팅 (Broadcasting)</a></h2>
<p>:label:<code>subsec_broadcasting</code></p>
<p>지금까지 여러분은 동일한 모양의 두 텐서에 대해
요소별 이진 연산을 수행하는 방법을 알고 있습니다.
특정 조건 하에서,
모양이 다르더라도
우리는 여전히 [*<strong>브로드캐스팅 메커니즘(broadcasting mechanism)*을 호출하여
요소별 이진 연산을 수행할 수 있습니다.</strong>]
브로드캐스팅은 다음 두 단계 절차에 따라 작동합니다:
(i) 길이가 1인 축을 따라 요소를 복사하여 하나 또는 두 배열을 모두 확장하여
이 변환 후 두 텐서가 동일한 모양을 갖도록 합니다;
(ii) 결과 배열에 대해 요소별 연산을 수행합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
a = np.arange(3).reshape(3, 1)
b = np.arange(2).reshape(1, 2)
a, b
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
a = tf.reshape(tf.range(3), (3, 1))
b = tf.reshape(tf.range(2), (1, 2))
a, b
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
a = jnp.arange(3).reshape((3, 1))
b = jnp.arange(2).reshape((1, 2))
a, b
</code></pre>
<p><code>a</code>와 <code>b</code>는 각각 $3\times1$ 및 $1\times2$ 행렬이므로
모양이 일치하지 않습니다.
브로드캐스팅은 요소별로 더하기 전에
행렬 <code>a</code>를 열을 따라 복제하고
행렬 <code>b</code>를 행을 따라 복제하여
더 큰 $3\times2$ 행렬을 생성합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
a + b
</code></pre>
<h2 id="메모리-절약-saving-memory"><a class="header" href="#메모리-절약-saving-memory">메모리 절약 (Saving Memory)</a></h2>
<p>[<strong>연산을 실행하면 결과를 호스팅하기 위해 새 메모리가 할당될 수 있습니다.</strong>]
예를 들어 <code>Y = X + Y</code>라고 쓰면,
<code>Y</code>가 가리키던 텐서의 참조를 해제하고
대신 <code>Y</code>가 새로 할당된 메모리를 가리키게 합니다.
우리는 참조된 객체의 정확한 메모리 주소를 제공하는
Python의 <code>id()</code> 함수로 이 문제를 시연할 수 있습니다.
<code>Y = Y + X</code>를 실행한 후,
<code>id(Y)</code>가 다른 위치를 가리킨다는 점에 유의하십시오.
이는 Python이 먼저 <code>Y + X</code>를 평가하여
결과를 위한 새 메모리를 할당한 다음
<code>Y</code>가 이 새 메모리 위치를 가리키게 하기 때문입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
before = id(Y)
Y = Y + X
id(Y) == before
</code></pre>
<p>이것은 두 가지 이유로 바람직하지 않을 수 있습니다.
첫째, 우리는 항상 불필요하게 메모리를 할당하며 돌아다니고 싶지 않습니다.
머신러닝에서는 종종 수백 메가바이트의 파라미터를 가지고 있으며
초당 여러 번 업데이트합니다.
가능할 때마다 이러한 업데이트를 <em>제자리에서(in place)</em> 수행하기를 원합니다.
둘째, 여러 변수에서 동일한 파라미터를 가리킬 수 있습니다.
제자리에서 업데이트하지 않으면,
메모리 누수가 발생하거나 실수로 오래된 파라미터를 참조하지 않도록
이러한 모든 참조를 신중하게 업데이트해야 합니다.</p>
<p>:begin_tab:<code>mxnet, pytorch</code>
다행히도 (<strong>제자리 연산을 수행하는 것</strong>)은 쉽습니다.
슬라이스 표기법 <code>Y[:] = &lt;expression&gt;</code>을 사용하여
이전에 할당된 배열 <code>Y</code>에 연산 결과를 할당할 수 있습니다.
이 개념을 설명하기 위해,
<code>zeros_like</code>를 사용하여 <code>Y</code>와 같은 모양을 갖도록 초기화한 후
텐서 <code>Z</code>의 값을 덮어씁니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<code>Variables</code>는 TensorFlow에서 변경 가능한 상태 컨테이너입니다.
모델 파라미터를 저장하는 방법을 제공합니다.
<code>assign</code>을 사용하여 연산 결과를 <code>Variable</code>에 할당할 수 있습니다.
이 개념을 설명하기 위해,
<code>zeros_like</code>를 사용하여 <code>Y</code>와 같은 모양을 갖도록 초기화한 후
<code>Variable</code> <code>Z</code>의 값을 덮어씁니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
Z = np.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
Z = tf.Variable(tf.zeros_like(Y))
print('id(Z):', id(Z))
Z.assign(X + Y)
print('id(Z):', id(Z))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# JAX 배열은 제자리 연산을 허용하지 않습니다
</code></pre>
<p>:begin_tab:<code>mxnet, pytorch</code>
[<strong>후속 계산에서 <code>X</code>의 값이 재사용되지 않는 경우,
<code>X[:] = X + Y</code> 또는 <code>X += Y</code>를 사용하여
연산의 메모리 오버헤드를 줄일 수도 있습니다.</strong>]
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<code>Variable</code>에 상태를 영구적으로 저장하더라도,
모델 파라미터가 아닌 텐서에 대한 초과 할당을 피하여
메모리 사용량을 더 줄이고 싶을 수 있습니다.
TensorFlow <code>Tensors</code>는 불변이고
기울기가 <code>Variable</code> 할당을 통해 흐르지 않기 때문에,
TensorFlow는 개별 연산을 제자리에서 실행하는 명시적인 방법을 제공하지 않습니다.</p>
<p>그러나 TensorFlow는 <code>tf.function</code> 데코레이터를 제공하여
실행하기 전에 컴파일되고 최적화되는 TensorFlow 그래프 내부의 계산을 래핑합니다.
이를 통해 TensorFlow는 사용되지 않는 값을 정리하고,
더 이상 필요하지 않은 이전 할당을 재사용할 수 있습니다.
이는 TensorFlow 계산의 메모리 오버헤드를 최소화합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
before = id(X)
X += Y
id(X) == before
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
@tf.function
def computation(X, Y):
    Z = tf.zeros_like(Y)  # 이 사용되지 않는 값은 제거됩니다
    A = X + Y  # 더 이상 필요하지 않을 때 할당이 재사용됩니다
    B = A + Y
    C = B + Y
    return C + Y

computation(X, Y)
</code></pre>
<h2 id="다른-python-객체로의-변환"><a class="header" href="#다른-python-객체로의-변환">다른 Python 객체로의 변환</a></h2>
<p>:begin_tab:<code>mxnet, tensorflow</code>
[<strong>NumPy 텐서(<code>ndarray</code>)로 변환하기</strong>] 또는 그 반대로 변환하는 것은 쉽습니다.
변환된 결과는 메모리를 공유하지 않습니다.
이 사소한 불편함은 실제로 꽤 중요합니다:
CPU나 GPU에서 연산을 수행할 때,
Python의 NumPy 패키지가 동일한 메모리 청크로
다른 작업을 하고 싶어 할지 확인하기 위해
계산을 중단하고 기다리고 싶지 않기 때문입니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
[<strong>NumPy 텐서(<code>ndarray</code>)로 변환하기</strong>] 또는 그 반대로 변환하는 것은 쉽습니다.
토치 텐서와 NumPy 배열은 기본 메모리를 공유하며,
제자리 연산을 통해 하나를 변경하면 다른 하나도 변경됩니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A = X.asnumpy()
B = np.array(A)
type(A), type(B)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A = X.numpy()
B = torch.from_numpy(A)
type(A), type(B)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A = X.numpy()
B = tf.constant(A)
type(A), type(B)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A = jax.device_get(X)
B = jax.device_put(A)
type(A), type(B)
</code></pre>
<p>(<strong>크기가 1인 텐서를 Python 스칼라로 변환</strong>)하려면,
<code>item</code> 함수나 Python의 내장 함수를 호출할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
a = np.array([3.5])
a, a.item(), float(a), int(a)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
a = tf.constant([3.5]).numpy()
a, a.item(), float(a), int(a)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
a = jnp.array([3.5])
a, a.item(), float(a), int(a)
</code></pre>
<h2 id="요약-1"><a class="header" href="#요약-1">요약</a></h2>
<p>텐서 클래스는 딥러닝 라이브러리에서 데이터를 저장하고 조작하기 위한 주요 인터페이스입니다.
텐서는 생성 루틴, 인덱싱 및 슬라이싱, 기본 수학 연산, 브로드캐스팅, 메모리 효율적인 할당, 다른 Python 객체와의 상호 변환을 포함한 다양한 기능을 제공합니다.</p>
<h2 id="연습-문제-1"><a class="header" href="#연습-문제-1">연습 문제</a></h2>
<ol>
<li>이 섹션의 코드를 실행하십시오. 조건문 <code>X == Y</code>를 <code>X &lt; Y</code> 또는 <code>X &gt; Y</code>로 변경하고 어떤 종류의 텐서를 얻을 수 있는지 확인하십시오.</li>
<li>브로드캐스팅 메커니즘에서 요소별로 작동하는 두 텐서를 다른 모양(예: 3차원 텐서)으로 대체하십시오. 결과가 예상과 같습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/26">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/27">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/187">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17966">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="데이터-전처리-data-preprocessing"><a class="header" href="#데이터-전처리-data-preprocessing">데이터 전처리 (Data Preprocessing)</a></h1>
<p>:label:<code>sec_pandas</code></p>
<p>지금까지 우리는 기성 텐서에 도착한 합성 데이터로 작업했습니다.
그러나 실제 환경에서 딥러닝을 적용하려면
임의의 형식으로 저장된 지저분한 데이터를 추출하고
필요에 맞게 전처리해야 합니다.
다행히도 <em>pandas</em> <a href="https://pandas.pydata.org/">라이브러리</a>는
무거운 작업의 대부분을 수행할 수 있습니다.
이 섹션은 적절한 <em>pandas</em> <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html">튜토리얼</a>을
대체할 수는 없지만, 가장 일반적인 루틴에 대한 집중 코스를 제공할 것입니다.</p>
<h2 id="데이터셋-읽기-reading-the-dataset"><a class="header" href="#데이터셋-읽기-reading-the-dataset">데이터셋 읽기 (Reading the Dataset)</a></h2>
<p>쉼표로 구분된 값(CSV) 파일은 표 형식(스프레드시트 같은) 데이터를 저장하는 데 어디에나 있습니다.
여기서 각 줄은 하나의 레코드에 해당하며
여러(쉼표로 구분된) 필드로 구성됩니다. 예:
"알버트 아인슈타인,1879년 3월 14일,울름,연방 공과 대학,중력 물리학 분야".
<code>pandas</code>로 CSV 파일을 로드하는 방법을 보여주기 위해,
우리는 <code>../data/house_tiny.csv</code> (<strong>아래에 CSV 파일을 생성합니다</strong>).
이 파일은 주택 데이터셋을 나타내며,
각 행은 별개의 집에 해당하고
열은 방 수(<code>NumRooms</code>), 지붕 유형(<code>RoofType</code>), 가격(<code>Price</code>)에 해당합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
import os

os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
    f.write('''NumRooms,RoofType,Price
NA,NA,127500
2,NA,106000
4,Slate,178100
NA,NA,140000''')
</code></pre>
<p>이제 <code>pandas</code>를 가져와서 <code>read_csv</code>로 데이터셋을 로드해 보겠습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
import pandas as pd

data = pd.read_csv(data_file)
print(data)
</code></pre>
<h2 id="데이터-준비-data-preparation"><a class="header" href="#데이터-준비-data-preparation">데이터 준비 (Data Preparation)</a></h2>
<p>지도 학습에서는 입력 값 세트가 주어졌을 때
지정된 <em>타겟</em> 값을 예측하도록 모델을 훈련합니다.
데이터셋을 처리하는 첫 번째 단계는
입력 값과 타겟 값에 해당하는 열을 분리하는 것입니다.
이름이나 정수 위치 기반 인덱싱(<code>iloc</code>)을 통해 열을 선택할 수 있습니다.</p>
<p><code>pandas</code>가 <code>NA</code> 값을 가진 모든 CSV 항목을
특별한 <code>NaN</code> (<em>숫자 아님</em>) 값으로 대체한 것을 눈치챘을 것입니다.
이것은 항목이 비어 있을 때도 발생할 수 있습니다. 예: "3,,,270000".
이것들을 *결측값(missing values)*이라고 하며
데이터 과학의 "빈대"와 같아서, 경력 내내 직면하게 될 지속적인 위협입니다.
맥락에 따라 결측값은 <em>대체(imputation)</em> 또는 *삭제(deletion)*를 통해 처리될 수 있습니다.
대체는 결측값을 값의 추정치로 바꾸는 반면,
삭제는 단순히 결측값이 포함된 행이나 열을 버립니다.</p>
<p>다음은 몇 가지 일반적인 대체 휴리스틱입니다.
[<strong>범주형 입력 필드의 경우, <code>NaN</code>을 하나의 범주로 취급할 수 있습니다.</strong>]
<code>RoofType</code> 열은 <code>Slate</code>와 <code>NaN</code> 값을 취하므로,
<code>pandas</code>는 이 열을 두 개의 열 <code>RoofType_Slate</code>와 <code>RoofType_nan</code>으로 변환할 수 있습니다.
지붕 유형이 <code>Slate</code>인 행은 <code>RoofType_Slate</code> 및 <code>RoofType_nan</code>의 값을 각각 1과 0으로 설정합니다.
<code>RoofType</code> 값이 누락된 행의 경우에는 그 반대가 성립합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
</code></pre>
<p>누락된 수치 값의 경우, 일반적인 휴리스틱 중 하나는
[<strong><code>NaN</code> 항목을 해당 열의 평균값으로 대체하는 것입니다</strong>].</p>
<pre><code class="language-{.python .input}">%%tab all
inputs = inputs.fillna(inputs.mean())
print(inputs)
</code></pre>
<h2 id="텐서-형식으로-변환-conversion-to-the-tensor-format"><a class="header" href="#텐서-형식으로-변환-conversion-to-the-tensor-format">텐서 형식으로 변환 (Conversion to the Tensor Format)</a></h2>
<p>이제 [<strong><code>inputs</code>와 <code>targets</code>의 모든 항목이 수치이므로,
텐서로 로드할 수 있습니다</strong>] (:numref:<code>sec_ndarray</code> 상기).</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np

X, y = np.array(inputs.to_numpy(dtype=float)), np.array(targets.to_numpy(dtype=float))
X, y
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch

X = torch.tensor(inputs.to_numpy(dtype=float))
y = torch.tensor(targets.to_numpy(dtype=float))
X, y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf

X = tf.constant(inputs.to_numpy(dtype=float))
y = tf.constant(targets.to_numpy(dtype=float))
X, y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import numpy as jnp

X = jnp.array(inputs.to_numpy(dtype=float))
y = jnp.array(targets.to_numpy(dtype=float))
X, y
</code></pre>
<h2 id="토론"><a class="header" href="#토론">토론</a></h2>
<p>이제 데이터 열을 분할하고, 누락된 변수를 대체하고,
<code>pandas</code> 데이터를 텐서로 로드하는 방법을 알게 되었습니다.
:numref:<code>sec_kaggle_house</code>에서 더 많은 데이터 처리 기술을 배우게 될 것입니다.
이 집중 코스는 상황을 단순하게 유지했지만, 데이터 처리는 까다로울 수 있습니다.
예를 들어, 단일 CSV 파일에 도착하는 대신,
데이터셋이 관계형 데이터베이스에서 추출된 여러 파일에 분산되어 있을 수 있습니다.
예를 들어 전자 상거래 애플리케이션에서 고객 주소는 한 테이블에 있고
구매 데이터는 다른 테이블에 있을 수 있습니다.
게다가 실무자들은 범주형 및 수치형을 넘어
텍스트 문자열, 이미지, 오디오 데이터, 포인트 클라우드 등 수많은 데이터 유형에 직면합니다.
종종 데이터 처리가 머신러닝 파이프라인의 가장 큰 병목 현상이 되는 것을 방지하기 위해
고급 도구와 효율적인 알고리즘이 필요합니다.
이러한 문제는 컴퓨터 비전과 자연어 처리에 도달했을 때 발생할 것입니다.
마지막으로 데이터 품질에 주의를 기울여야 합니다.
실제 데이터셋은 종종 이상값, 센서의 잘못된 측정, 기록 오류로 인해 골칫거리가 되며,
데이터를 모델에 공급하기 전에 해결해야 합니다.
<a href="https://seaborn.pydata.org/">seaborn</a>,
<a href="https://docs.bokeh.org/">Bokeh</a>, 또는 <a href="https://matplotlib.org/">matplotlib</a>과 같은
데이터 시각화 도구는 데이터를 수동으로 검사하고
해결해야 할 문제 유형에 대한 직관을 개발하는 데 도움이 될 수 있습니다.</p>
<h2 id="연습-문제-2"><a class="header" href="#연습-문제-2">연습 문제</a></h2>
<ol>
<li><a href="https://archive.ics.uci.edu/ml/datasets">UCI 머신러닝 저장소</a>의 Abalone과 같은 데이터셋을 로드하고 속성을 검사해 보십시오. 결측값이 있는 비율은 얼마입니까? 변수의 몇 퍼센트가 수치형, 범주형 또는 텍스트입니까?</li>
<li>열 번호 대신 이름으로 데이터 열을 인덱싱하고 선택해 보십시오. <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html">인덱싱</a>에 대한 pandas 문서에 이를 수행하는 방법에 대한 자세한 내용이 나와 있습니다.</li>
<li>이 방식으로 얼마나 큰 데이터셋을 로드할 수 있다고 생각하십니까? 한계는 무엇일까요? 힌트: 데이터 읽기 시간, 표현, 처리 및 메모리 사용량을 고려하십시오. 노트북에서 이것을 시도해 보십시오. 서버에서 시도하면 어떻게 됩니까?</li>
<li>매우 많은 수의 범주가 있는 데이터를 어떻게 처리하시겠습니까? 범주 레이블이 모두 고유하다면 어떻게 합니까? 후자를 포함해야 합니까?</li>
<li>pandas의 대안으로 무엇을 생각할 수 있습니까? <a href="https://numpy.org/doc/stable/reference/generated/numpy.load.html">파일에서 NumPy 텐서 로드하기</a>는 어떻습니까? Python 이미징 라이브러리인 <a href="https://python-pillow.org/">Pillow</a>를 확인해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/28">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/29">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/195">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17967">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="선형-대수-linear-algebra"><a class="header" href="#선형-대수-linear-algebra">선형 대수 (Linear Algebra)</a></h1>
<p>:label:<code>sec_linear-algebra</code></p>
<p>지금까지 우리는 데이터셋을 텐서로 로드하고
기본적인 수학 연산으로 텐서를 조작할 수 있었습니다.
정교한 모델을 구축하기 시작하려면,
선형 대수의 몇 가지 도구도 필요합니다.
이 섹션은 스칼라 산술에서 시작하여 행렬 곱셈까지
가장 필수적인 개념에 대한 부드러운 소개를 제공합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import numpy as jnp
</code></pre>
<h2 id="스칼라-scalars"><a class="header" href="#스칼라-scalars">스칼라 (Scalars)</a></h2>
<p>대부분의 일상적인 수학은
숫자를 한 번에 하나씩 조작하는 것으로 구성됩니다.
공식적으로 우리는 이러한 값을 *스칼라(scalars)*라고 부릅니다.
예를 들어 팰로앨토의 온도는
화씨 $72$도의 온화한 날씨입니다.
온도를 섭씨로 변환하려면
$f$를 $72$로 설정하여 표현식 $c = \frac{5}{9}(f - 32)$를 평가합니다.
이 방정식에서 값 $5$, $9$, $32$는 상수 스칼라입니다.
변수 $c$와 $f$는 일반적으로 알려지지 않은 스칼라를 나타냅니다.</p>
<p>우리는 일반적인 소문자(예: $x$, $y$, $z$)로 스칼라를 나타내고,
모든 (연속적인) <em>실수 값</em> 스칼라의 공간을 $\mathbb{R}$로 나타냅니다.
편의를 위해 *공간(spaces)*에 대한 엄격한 정의는 건너뛰겠습니다:
표현식 $x \in \mathbb{R}$은 $x$가 실수 값 스칼라라는 것을 말하는
공식적인 방법이라는 것만 기억하십시오.
기호 $\in$ ("in"으로 발음)은 집합의 멤버십을 나타냅니다.
예를 들어 $x, y \in {0, 1}$은
$x$와 $y$가 $0$ 또는 $1$ 값만 취할 수 있는 변수임을 나타냅니다.</p>
<p>(<strong>스칼라는 하나의 요소만 포함하는 텐서로 구현됩니다.</strong>) 아래에서는 두 개의 스칼라를 할당하고
친숙한 덧셈, 곱셈, 나눗셈, 거듭제곱 연산을 수행합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.array(3.0)
y = np.array(2.0)

x + y, x * y, x / y, x ** y
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.tensor(3.0)
y = torch.tensor(2.0)

x + y, x * y, x / y, x**y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.constant(3.0)
y = tf.constant(2.0)

x + y, x * y, x / y, x**y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.array(3.0)
y = jnp.array(2.0)

x + y, x * y, x / y, x**y
</code></pre>
<h2 id="벡터-vectors"><a class="header" href="#벡터-vectors">벡터 (Vectors)</a></h2>
<p>현재 목적을 위해, [<strong>벡터를 고정 길이의 스칼라 배열로 생각할 수 있습니다.</strong>]
코드 대응물과 마찬가지로,
우리는 이러한 스칼라를 벡터의 *요소(elements)*라고 부릅니다
(<em>항목(entries)</em> 및 *성분(components)*과 동의어). 벡터가 실제 데이터셋의 예제를 나타낼 때,
그 값은 실제적인 중요성을 갖습니다.
예를 들어, 대출 채무 불이행 위험을 예측하는 모델을 훈련하는 경우,
각 신청자를 소득, 고용 기간, 이전 채무 불이행 횟수와 같은
수량에 해당하는 성분을 가진 벡터와 연관시킬 수 있습니다.
심장마비 위험을 연구하는 경우,
각 벡터는 환자를 나타낼 수 있으며
그 성분은 가장 최근의 활력 징후, 콜레스테롤 수치,
하루 운동 시간 등에 해당할 수 있습니다. 우리는 굵은 소문자(예: $\mathbf{x}$, $\mathbf{y}$, $\mathbf{z}$)로 벡터를 나타냅니다.</p>
<p>벡터는 1차 텐서로 구현됩니다. 일반적으로 이러한 텐서는 메모리 제한에 따라 임의의 길이를 가질 수 있습니다. 주의: 대부분의 프로그래밍 언어와 마찬가지로 Python에서 벡터 인덱스는 $0$에서 시작하며(<em>0 기반 인덱싱</em>이라고도 함), 선형 대수에서는 아래첨자가 $1$에서 시작합니다(<em>1 기반 인덱싱</em>).</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(3)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(3)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.range(3)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(3)
x
</code></pre>
<p>아래첨자를 사용하여 벡터의 요소를 참조할 수 있습니다.
예를 들어 $x_2$는 $\mathbf{x}$의 두 번째 요소를 나타냅니다.
$x_2$는 스칼라이므로 굵게 표시하지 않습니다.
기본적으로 우리는 요소를 수직으로 쌓아 벡터를 시각화합니다:</p>
<p>$$\mathbf{x} =\begin{bmatrix}x_{1}  \ \vdots  \x_{n}\end{bmatrix}.$$
:eqlabel:<code>eq_vec_def</code></p>
<p>여기서 $x_1, \ldots, x_n$은 벡터의 요소입니다. 나중에 우리는 이러한 *열 벡터(column vectors)*와
요소가 수평으로 쌓인 *행 벡터(row vectors)*를 구별할 것입니다. [<strong>인덱싱을 통해 텐서의 요소에 액세스한다</strong>]는 것을 상기하십시오.</p>
<pre><code class="language-{.python .input}">%%tab all
x[2]
</code></pre>
<p>벡터에 $n$개의 요소가 포함되어 있음을 나타내기 위해
$\mathbf{x} \in \mathbb{R}^n$이라고 씁니다. 공식적으로 우리는 $n$을 벡터의 *차원(dimensionality)*이라고 부릅니다.
[<strong>코드에서 이는 텐서의 길이에 해당하며</strong>], Python의 내장 <code>len</code> 함수를 통해 액세스할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
len(x)
</code></pre>
<p><code>shape</code> 속성을 통해서도 길이에 액세스할 수 있습니다.
모양은 각 축을 따른 텐서의 길이를 나타내는 튜플입니다.
(<strong>축이 하나만 있는 텐서는 하나의 요소만 있는 모양을 갖습니다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab all
x.shape
</code></pre>
<p>종종 "차원(dimension)"이라는 단어는
축의 수와 특정 축을 따른 길이를 모두 의미하도록 과부하됩니다.
이러한 혼란을 피하기 위해,
우리는 축의 수를 나타낼 때는 *차수(order)*를 사용하고,
구성 요소의 수를 나타낼 때는 *차원(dimensionality)*을 독점적으로 사용합니다.</p>
<h2 id="행렬-matrices"><a class="header" href="#행렬-matrices">행렬 (Matrices)</a></h2>
<p>스칼라가 0차 텐서이고
벡터가 1차 텐서인 것처럼,
행렬은 2차 텐서입니다. 우리는 굵은 대문자(예: $\mathbf{X}$, $\mathbf{Y}$, $\mathbf{Z}$)로 행렬을 나타내고,
코드에서는 두 개의 축을 가진 텐서로 나타냅니다. 표현식 $\mathbf{A} \in \mathbb{R}^{m \times n}$은
행렬 $\mathbf{A}$가 $m$개의 행과 $n$개의 열로 배열된
$m \times n$개의 실수 값 스칼라를 포함함을 나타냅니다. $m = n$일 때, 우리는 행렬이 *정사각(square)*이라고 말합니다. 시각적으로 우리는 모든 행렬을 표로 설명할 수 있습니다. 개별 요소를 참조하려면 행과 열 인덱스를 모두 아래첨자로 사용합니다. 예:
$a_{ij}$는 $\mathbf{A}$의 $i$번째 행과 $j$번째 열에 속하는 값입니다:</p>
<p>$$\mathbf{A}=\begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{bmatrix}.$$
:eqlabel:<code>eq_matrix_def</code></p>
<p>코드에서 우리는 행렬 $\mathbf{A} \in \mathbb{R}^{m \times n}$을
모양 ($m$, $n$)을 가진 2차 텐서로 나타냅니다. 원하는 모양을 <code>reshape</code>에 전달하여
[<strong>적절한 크기의 $m \times n$ 텐서를 $m \times n$ 행렬로 변환할 수 있습니다</strong>]:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A = np.arange(6).reshape(3, 2)
A
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A = torch.arange(6).reshape(3, 2)
A
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A = tf.reshape(tf.range(6), (3, 2))
A
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A = jnp.arange(6).reshape(3, 2)
A
</code></pre>
<p>때때로 우리는 축을 뒤집고 싶을 때가 있습니다. 행렬의 행과 열을 교환하면,
그 결과를 *전치(transpose)*라고 합니다. 공식적으로 행렬 $\mathbf{A}$의 전치를 $\mathbf{A}^\top$로 표시하고,
$\mathbf{B} = \mathbf{A}^\top$이면 모든 $i$와 $j$에 대해 $b_{ij} = a_{ji}$입니다. 따라서 $m \times n$ 행렬의 전치는 $n \times m$ 행렬입니다:</p>
<p>$$ \mathbf{A}^\top = \begin{bmatrix} a_{11} &amp; a_{21} &amp; \dots  &amp; a_{m1} \ a_{12} &amp; a_{22} &amp; \dots  &amp; a_{m2} \ \vdots &amp; \vdots &amp; \ddots  &amp; \vdots \ a_{1n} &amp; a_{2n} &amp; \dots  &amp; a_{mn} \end{bmatrix}. $$</p>
<p>코드에서는 다음과 같이 모든 (<strong>행렬의 전치</strong>)에 액세스할 수 있습니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.T
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.transpose(A)
</code></pre>
<p>[<strong>대칭 행렬(Symmetric matrices)은 자신의 전치와 동일한 정사각 행렬의 하위 집합입니다:
$\mathbf{A} = \mathbf{A}^\top$.</strong>] 다음 행렬은 대칭입니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == A.T
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == A.T
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A = tf.constant([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == tf.transpose(A)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A = jnp.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == A.T
</code></pre>
<p>행렬은 데이터셋을 나타내는 데 유용합니다. 일반적으로 행은 개별 레코드에 해당하고
열은 별개의 속성에 해당합니다.</p>
<h2 id="텐서-tensors"><a class="header" href="#텐서-tensors">텐서 (Tensors)</a></h2>
<p>스칼라, 벡터, 행렬만으로도 머신러닝 여정을 멀리 갈 수 있지만,
결국에는 고차 [<strong>텐서</strong>]를 다뤄야 할 수도 있습니다. 텐서는 (<strong>$n$차 배열로의 확장을 설명하는 일반적인 방법을 제공합니다.</strong>) 우리는 텐서 클래스의 소프트웨어 객체도 임의의 수의 축을 가질 수 있기 때문에
정확히 "텐서"라고 부릅니다. 수학적 객체와 코드에서의 구현 모두에 <em>텐서</em>라는 단어를 사용하는 것이 혼란스러울 수 있지만,
우리의 의미는 일반적으로 문맥상 명확해야 합니다. 우리는 일반적인 텐서를 특수 글꼴(예: $\mathsf{X}$, $\mathsf{Y}$, $\mathsf{Z}$)을 사용한 대문자로 나타내며,
인덱싱 메커니즘(예: $x_{ijk}$ 및 $[\mathsf{X}]_{1, 2i-1, 3}$)은
행렬의 메커니즘을 자연스럽게 따릅니다.</p>
<p>이미지로 작업하기 시작하면 텐서가 더 중요해질 것입니다. 각 이미지는 높이, 너비, <em>채널</em>에 해당하는 축을 가진 3차 텐서로 도착합니다. 각 공간 위치에서 각 색상(빨강, 초록, 파랑)의 강도는 채널을 따라 쌓입니다. 또한 이미지 모음은 코드에서 4차 텐서로 표현되며,
여기서 별개의 이미지는 첫 번째 축을 따라 인덱싱됩니다. 고차 텐서는 벡터 및 행렬과 마찬가지로
모양 구성 요소의 수를 늘려 구성됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.arange(24).reshape(2, 3, 4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.arange(24).reshape(2, 3, 4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reshape(tf.range(24), (2, 3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.arange(24).reshape(2, 3, 4)
</code></pre>
<h2 id="텐서-산술의-기본-속성"><a class="header" href="#텐서-산술의-기본-속성">텐서 산술의 기본 속성</a></h2>
<p>스칼라, 벡터, 행렬, 그리고 고차 텐서는
모두 몇 가지 편리한 속성을 가지고 있습니다. 예를 들어, 요소별 연산은
피연산자와 동일한 모양을 가진 출력을 생성합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A = np.arange(6).reshape(2, 3)
B = A.copy()  # 새 메모리를 할당하여 A의 사본을 B에 할당
A, A + B
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A = torch.arange(6, dtype=torch.float32).reshape(2, 3)
B = A.clone()  # 새 메모리를 할당하여 A의 사본을 B에 할당
A, A + B
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A = tf.reshape(tf.range(6, dtype=tf.float32), (2, 3))
B = A  # 새 메모리를 할당하여 A를 B에 복제하지 않음
A, A + B
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A = jnp.arange(6, dtype=jnp.float32).reshape(2, 3)
B = A
A, A + B
</code></pre>
<p>[**두 행렬의 요소별 곱을 <em>하다마드 곱(Hadamard product)<em>이라고 합니다</em></em>] ($\odot$로 표시). 두 행렬 $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}$의 하다마드 곱의 항목은 다음과 같습니다:</p>
<p>$$ \mathbf{A} \odot \mathbf{B} = \begin{bmatrix} a_{11}  b_{11} &amp; a_{12}  b_{12} &amp; \dots  &amp; a_{1n}  b_{1n} \ a_{21}  b_{21} &amp; a_{22}  b_{22} &amp; \dots  &amp; a_{2n}  b_{2n} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ a_{m1}  b_{m1} &amp; a_{m2}  b_{m2} &amp; \dots  &amp; a_{mn}  b_{mn} \end{bmatrix}. $$</p>
<pre><code class="language-{.python .input}">%%tab all
A * B
</code></pre>
<p>[<strong>스칼라와 텐서를 더하거나 곱하면</strong>] 원래 텐서와 동일한 모양을 가진 결과가 생성됩니다. 여기서 텐서의 각 요소는 스칼라에 더해지거나 곱해집니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
a = 2
X = np.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
a = 2
X = tf.reshape(tf.range(24), (2, 3, 4))
a + X, (a * X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
a = 2
X = jnp.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
</code></pre>
<h2 id="축소-reduction"><a class="header" href="#축소-reduction">축소 (Reduction)</a></h2>
<p>:label:<code>subsec_lin-alg-reduction</code></p>
<p>종종 우리는 [<strong>텐서 요소의 합을 계산하고 싶어 합니다.</strong>] 길이가 $n$인 벡터 $\mathbf{x}$의 요소 합을 표현하기 위해
$\sum_{i=1}^n x_i$라고 씁니다. 이를 위한 간단한 함수가 있습니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(3)
x, x.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(3, dtype=torch.float32)
x, x.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.range(3, dtype=tf.float32)
x, tf.reduce_sum(x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(3, dtype=jnp.float32)
x, x.sum()
</code></pre>
<p>[<strong>임의의 모양을 가진 텐서 요소의 합</strong>]을 표현하기 위해,
우리는 단순히 모든 축에 대해 합계를 구합니다. 예를 들어, $m \times n$ 행렬 $\mathbf{A}$의 요소 합은
$\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$로 쓸 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.shape, A.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A.shape, tf.reduce_sum(A)
</code></pre>
<p>기본적으로 합계 함수를 호출하면
텐서를 모든 축을 따라 *축소(reduce)*하여
결국 스칼라를 생성합니다. 우리의 라이브러리는 또한 [<strong>텐서가 축소되어야 할 축을 지정</strong>]할 수 있게 해줍니다. 행(축 0)을 따라 모든 요소를 합산하려면,
<code>sum</code>에서 <code>axis=0</code>을 지정합니다. 입력 행렬이 축 0을 따라 축소되어 출력 벡터를 생성하므로,
이 축은 출력 모양에서 사라집니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.shape, A.sum(axis=0).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A.shape, tf.reduce_sum(A, axis=0).shape
</code></pre>
<p><code>axis=1</code>을 지정하면 모든 열의 요소를 합산하여 열 차원(축 1)을 축소합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.shape, A.sum(axis=1).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A.shape, tf.reduce_sum(A, axis=1).shape
</code></pre>
<p>합계를 통해 행과 열 모두를 따라 행렬을 축소하는 것은
행렬의 모든 요소를 합산하는 것과 같습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.sum(axis=[0, 1]) == A.sum()  # A.sum()과 동일
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_sum(A, axis=[0, 1]), tf.reduce_sum(A)  # tf.reduce_sum(A)와 동일
</code></pre>
<p>[<strong>관련된 양은 *평균(mean)*이며 *평균(average)*이라고도 합니다.</strong>] 우리는 합계를 총 요소 수로 나누어 평균을 계산합니다. 평균을 계산하는 것은 매우 일반적이기 때문에,
<code>sum</code>과 유사하게 작동하는 전용 라이브러리 함수가 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, jax
A.mean(), A.sum() / A.size
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A.mean(), A.sum() / A.numel()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_mean(A), tf.reduce_sum(A) / tf.size(A).numpy()
</code></pre>
<p>마찬가지로 평균을 계산하는 함수도
특정 축을 따라 텐서를 축소할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_mean(A, axis=0), tf.reduce_sum(A, axis=0) / A.shape[0]
</code></pre>
<h2 id="비축소-합계-non-reduction-sum"><a class="header" href="#비축소-합계-non-reduction-sum">비축소 합계 (Non-Reduction Sum)</a></h2>
<p>:label:<code>subsec_lin-alg-non-reduction</code></p>
<p>합계나 평균을 계산하는 함수를 호출할 때
[<strong>축의 수를 변경하지 않고 유지</strong>]하는 것이 유용할 때가 있습니다. 이것은 브로드캐스트 메커니즘을 사용하고 싶을 때 중요합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
sum_A = A.sum(axis=1, keepdims=True)
sum_A, sum_A.shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
sum_A = tf.reduce_sum(A, axis=1, keepdims=True)
sum_A, sum_A.shape
</code></pre>
<p>예를 들어, <code>sum_A</code>는 각 행을 합산한 후에도 두 축을 유지하므로,
우리는 (<strong>브로드캐스팅을 사용하여 <code>A</code>를 <code>sum_A</code>로 나누어</strong>) 각 행의 합이 $1$이 되는 행렬을 만들 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
A / sum_A
</code></pre>
<p>[<strong>어떤 축을 따라 <code>A</code> 요소의 누적 합을 계산하고 싶다면</strong>],
예를 들어 <code>axis=0</code> (행별로), <code>cumsum</code> 함수를 호출할 수 있습니다. 설계상, 이 함수는 어떤 축을 따라 입력 텐서를 축소하지 않습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.cumsum(axis=0)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.cumsum(A, axis=0)
</code></pre>
<h2 id="내적-dot-products"><a class="header" href="#내적-dot-products">내적 (Dot Products)</a></h2>
<p>지금까지 우리는 요소별 연산, 합계, 평균만 수행했습니다. 이것이 우리가 할 수 있는 전부라면 선형 대수는 별도의 섹션을 가질 자격이 없을 것입니다. 다행히도 여기서부터 상황이 더 흥미로워집니다. 가장 기본적인 연산 중 하나는 내적입니다. 두 벡터 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^d$가 주어졌을 때, 그들의 <em>내적(dot product)</em> $\mathbf{x}^\top \mathbf{y}$ (<em>내적(inner product)</em> $\langle \mathbf{x}, \mathbf{y}  \rangle$라고도 함)는 동일한 위치에 있는 요소들의 곱에 대한 합입니다:
$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$.</p>
<p>[<del>두 벡터의 <em>내적</em>은 같은 위치에 있는 요소들의 곱에 대한 합입니다</del>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y = np.ones(3)
x, y, np.dot(x, y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y = torch.ones(3, dtype = torch.float32)
x, y, torch.dot(x, y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
y = tf.ones(3, dtype=tf.float32)
x, y, tf.tensordot(x, y, axes=1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = jnp.ones(3, dtype = jnp.float32)
x, y, jnp.dot(x, y)
</code></pre>
<p>동등하게, (<strong>요소별 곱셈을 수행한 다음 합계를 구하여 두 벡터의 내적을 계산할 수 있습니다:</strong>)</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.sum(x * y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.sum(x * y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_sum(x * y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.sum(x * y)
</code></pre>
<p>내적은 광범위한 맥락에서 유용합니다. 예를 들어, 벡터 $\mathbf{x}  \in \mathbb{R}^n$으로 표시되는 일련의 값과
$\mathbf{w} \in \mathbb{R}^n$으로 표시되는 일련의 가중치가 주어졌을 때, 가중치 $\mathbf{w}$에 따른 $\mathbf{x}$ 값의 가중 합은
내적 $\mathbf{x}^\top \mathbf{w}$로 표현될 수 있습니다. 가중치가 음수가 아니고
합이 $1$일 때, 즉 $\left(\sum_{i=1}^{n} {w_i} = 1\right)$,
내적은 *가중 평균(weighted average)*을 나타냅니다. 단위 길이를 갖도록 두 벡터를 정규화한 후,
내적은 두 벡터 사이 각도의 코사인을 나타냅니다. 이 섹션의 뒷부분에서 이 <em>길이</em> 개념을 공식적으로 소개할 것입니다.</p>
<h2 id="행렬-벡터-곱-matrix--vector-products"><a class="header" href="#행렬-벡터-곱-matrix--vector-products">행렬-벡터 곱 (Matrix--Vector Products)</a></h2>
<p>이제 내적을 계산하는 방법을 알았으므로,
$m \times n$ 행렬 $\mathbf{A}$와 $n$차원 벡터 $\mathbf{x}$ 사이의 <em>곱</em>을 이해할 수 있습니다. 시작하기 위해, 우리는 행렬을 행 벡터로 시각화합니다</p>
<p>$$\mathbf{A}= \begin{bmatrix} \mathbf{a}^\top_{1} \ \mathbf{a}^\top_{2} \ \vdots \ \mathbf{a}^\top_m \ \end{bmatrix},$$</p>
<p>여기서 각 $\mathbf{a}^\top_{i} \in \mathbb{R}^n$은 행렬 $\mathbf{A}$의 $i$번째 행을 나타내는 행 벡터입니다.</p>
<p>[<strong>행렬-벡터 곱 $\mathbf{A}\mathbf{x}$는 단순히 길이가 $m$인 열 벡터이며, 그 $i$번째 요소는 내적 $\mathbf{a}^\top_i \mathbf{x}$입니다:</strong>]</p>
<p>$$ \mathbf{A}\mathbf{x} = \begin{bmatrix} \mathbf{a}^\top_{1} \ \mathbf{a}^\top_{2} \ \vdots \ \mathbf{a}^\top_m \ \end{bmatrix}\mathbf{x} = \begin{bmatrix}  \mathbf{a}^\top_{1} \mathbf{x}  \ \mathbf{a}^\top_{2} \mathbf{x} \ \vdots\ \mathbf{a}^\top_{m} \mathbf{x}\end{bmatrix}. $$</p>
<p>우리는 행렬 $\mathbf{A}\in \mathbb{R}^{m \times n}$과의 곱셈을 벡터를 $\mathbb{R}^{n}$에서 $\mathbb{R}^{m}$으로 투영하는 변환으로 생각할 수 있습니다. 이러한 변환은 놀라울 정도로 유용합니다. 예를 들어, 우리는 회전을 특정 정사각 행렬에 의한 곱셈으로 나타낼 수 있습니다. 행렬-벡터 곱은 또한 이전 레이어의 출력이 주어졌을 때 신경망의 각 레이어의 출력을 계산하는 데 관련된 핵심 계산을 설명합니다.</p>
<p>:begin_tab:<code>mxnet</code>
코드에서 행렬-벡터 곱을 표현하기 위해,
우리는 동일한 <code>dot</code> 함수를 사용합니다. 연산은 인수의 유형에 따라 추론됩니다. <code>A</code>의 열 차원(축 1을 따른 길이)은
<code>x</code>의 차원(길이)과 같아야 합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
코드에서 행렬-벡터 곱을 표현하기 위해,
우리는 <code>mv</code> 함수를 사용합니다. <code>A</code>의 열 차원(축 1을 따른 길이)은
<code>x</code>의 차원(길이)과 같아야 합니다. Python에는 행렬-벡터 및 행렬-행렬 곱을 모두 실행할 수 있는
편의 연산자 <code>@</code>가 있습니다(인수에 따라 다름). 따라서 <code>A@x</code>라고 쓸 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
코드에서 행렬-벡터 곱을 표현하기 위해,
우리는 <code>matvec</code> 함수를 사용합니다. <code>A</code>의 열 차원(축 1을 따른 길이)은
<code>x</code>의 차원(길이)과 같아야 합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A.shape, x.shape, np.dot(A, x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A.shape, x.shape, torch.mv(A, x), A@x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A.shape, x.shape, tf.linalg.matvec(A, x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A.shape, x.shape, jnp.matmul(A, x)
</code></pre>
<h2 id="행렬-행렬-곱셈-matrix--matrix-multiplication"><a class="header" href="#행렬-행렬-곱셈-matrix--matrix-multiplication">행렬-행렬 곱셈 (Matrix--Matrix Multiplication)</a></h2>
<p>내적과 행렬-벡터 곱에 익숙해졌다면,
<em>행렬-행렬 곱셈</em>은 간단할 것입니다.</p>
<p>두 개의 행렬 $\mathbf{A} \in \mathbb{R}^{n \times k}$와
$\mathbf{B} \in \mathbb{R}^{k \times m}$이 있다고 가정해 봅시다:</p>
<p>$$\mathbf{A}=\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1k} \
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2k} \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nk} \
\end{bmatrix},
\quad
\mathbf{B}=\begin{bmatrix}
b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1m} \
b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2m} \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
b_{k1} &amp; b_{k2} &amp; \cdots &amp; b_{km} \
\end{bmatrix}.$$</p>
<p>$\mathbf{a}^\top_{i} \in \mathbb{R}^k$는
행렬 $\mathbf{A}$의 $i$번째 행을 나타내는 행 벡터를 나타내고,
$\mathbf{b}_{j} \in \mathbb{R}^k$는
행렬 $\mathbf{B}$의 $j$번째 열에서 나온 열 벡터를 나타냅니다:</p>
<p>$$\mathbf{A}= \begin{bmatrix}
\mathbf{a}^\top_{1} \
\mathbf{a}^\top_{2} \
\vdots \
\mathbf{a}^\top_n \
\end{bmatrix},
\quad
\mathbf{B}=\begin{bmatrix}
\mathbf{b}<em>{1} &amp; \mathbf{b}</em>{2} &amp; \cdots &amp; \mathbf{b}_{m} \
\end{bmatrix}. $$</p>
<p>행렬 곱 $\mathbf{C} \in \mathbb{R}^{n \times m}$을 형성하기 위해,
우리는 각 요소 $c_{ij}$를
$\mathbf{A}$의 $i$번째 행과
$\mathbf{B}$의 $j$번째 열 사이의 내적,
즉 $\mathbf{a}^\top_i \mathbf{b}_j$로 간단히 계산합니다:</p>
<p>$$\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \
\mathbf{a}^\top_{2} \
\vdots \
\mathbf{a}^\top_n \
\end{bmatrix}
\begin{bmatrix}
\mathbf{b}<em>{1} &amp; \mathbf{b}</em>{2} &amp; \cdots &amp; \mathbf{b}<em>{m} \
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top</em>{1} \mathbf{b}<em>1 &amp; \mathbf{a}^\top</em>{1}\mathbf{b}<em>2&amp; \cdots &amp; \mathbf{a}^\top</em>{1} \mathbf{b}<em>m \
\mathbf{a}^\top</em>{2}\mathbf{b}<em>1 &amp; \mathbf{a}^\top</em>{2} \mathbf{b}<em>2 &amp; \cdots &amp; \mathbf{a}^\top</em>{2} \mathbf{b}<em>m \
\vdots &amp; \vdots &amp; \ddots &amp;\vdots\
\mathbf{a}^\top</em>{n} \mathbf{b}<em>1 &amp; \mathbf{a}^\top</em>{n}\mathbf{b}<em>2&amp; \cdots&amp; \mathbf{a}^\top</em>{n} \mathbf{b}_m
\end{bmatrix}. $$</p>
<p>[<strong>행렬-행렬 곱셈 $\mathbf{AB}$를
$m$개의 행렬-벡터 곱을 수행하거나
$m \times n$개의 내적을 수행하고
결과를 연결하여 $n \times m$ 행렬을 형성하는 것으로
생각할 수 있습니다.</strong>] 다음 스니펫에서,
우리는 <code>A</code>와 <code>B</code>에 대해 행렬 곱셈을 수행합니다. 여기서 <code>A</code>는 2개의 행과 3개의 열을 가진 행렬이고,
<code>B</code>는 3개의 행과 4개의 열을 가진 행렬입니다. 곱셈 후, 우리는 2개의 행과 4개의 열을 가진 행렬을 얻습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
B = np.ones(shape=(3, 4))
np.dot(A, B)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
B = torch.ones(3, 4)
torch.mm(A, B), A@B
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
B = tf.ones((3, 4), tf.float32)
tf.matmul(A, B)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
B = jnp.ones((3, 4))
jnp.matmul(A, B)
</code></pre>
<p><em>행렬-행렬 곱셈</em>이라는 용어는 종종
<em>행렬 곱셈</em>으로 단순화되며, 하다마드 곱과 혼동해서는 안 됩니다.</p>
<h2 id="노름-norms"><a class="header" href="#노름-norms">노름 (Norms)</a></h2>
<p>:label:<code>subsec_lin-algebra-norms</code></p>
<p>선형 대수에서 가장 유용한 연산자 중 일부는 *노름(norms)*입니다. 비공식적으로, 벡터의 노름은 벡터가 얼마나 <em>큰지</em> 알려줍니다. 예를 들어, $\ell_2$ 노름은 벡터의 (유클리드) 길이를 측정합니다. 여기서 우리는 벡터의 차원이 아니라 벡터 성분의 크기와 관련된 <em>크기</em> 개념을 사용하고 있습니다.</p>
<p>노름은 벡터를 스칼라로 매핑하고 다음 세 가지 속성을 만족하는 함수 $\ | \cdot \ |$입니다:</p>
<ol>
<li>어떤 벡터 $\mathbf{x}$가 주어졌을 때, 벡터의 (모든 요소)를 스칼라 $\alpha \in \mathbb{R}$로 스케일링하면, 노름도 그에 따라 스케일링됩니다:
$\|\alpha \mathbf{x}\| = |\alpha| \|\mathbf{x}\|$.</li>
<li>어떤 벡터 $\mathbf{x}$와 $\mathbf{y}$에 대해서도:
노름은 삼각 부등식을 만족합니다:
$\|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\$.</li>
<li>벡터의 노름은 음이 아니며 벡터가 0일 때만 사라집니다:
모든 $\mathbf{x} \neq 0$에 대해 $\|\mathbf{x}\| &gt; 0$.</li>
</ol>
<p>많은 함수가 유효한 노름이며 서로 다른 노름은 서로 다른 크기 개념을 인코딩합니다. 초등학교 기하학에서 직각 삼각형의 빗변을 계산할 때 배웠던 유클리드 노름은 벡터 요소의 제곱 합의 제곱근입니다. 공식적으로 이것은 [<strong>$\ell_2$ <em>노름</em></strong>]이라고 하며 다음과 같이 표현됩니다.</p>
<p>(<strong>$$\|\mathbf{x}\|<em>2 = \sqrt{\sum</em>{i=1}^n x_i^2}.$$</strong>)</p>
<p><code>norm</code> 메서드는 $\ell_2$ 노름을 계산합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
u = np.array([3, -4])
np.linalg.norm(u)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
u = torch.tensor([3.0, -4.0])
torch.norm(u)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
u = tf.constant([3.0, -4.0])
tf.norm(u)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
u = jnp.array([3.0, -4.0])
jnp.linalg.norm(u)
</code></pre>
<p>[<strong>$\ell_1$ 노름</strong>]도 일반적이며 관련된 측정값을 맨해튼 거리라고 합니다. 정의에 따라 $\ell_1$ 노름은 벡터 요소의 절댓값을 합산합니다:</p>
<p>(<strong>$$\|\mathbf{x}\|<em>1 = \sum</em>{i=1}^n \left|x_i \right|.$$</strong>)</p>
<p>$\ell_2$ 노름에 비해 이상값에 덜 민감합니다. $\ell_1$ 노름을 계산하기 위해,
우리는 절댓값 연산과 합계 연산을 구성합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.abs(u).sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.abs(u).sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_sum(tf.abs(u))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.linalg.norm(u, ord=1) # jnp.abs(u).sum()과 동일
</code></pre>
<p>$\ell_2$ 및 $\ell_1$ 노름은 모두 더 일반적인 $\ell_p$ <em>노름</em>의 특수한 경우입니다:</p>
<p>$$\|\mathbf{x}\|<em>p = \left(\sum</em>{i=1}^n \left|x_i \right|^p \right)^{1/p}.$$</p>
<p>행렬의 경우 문제는 더 복잡합니다. 결국 행렬은 개별 항목의 모음이자
벡터에 작용하여 다른 벡터로 변환하는 객체로 볼 수 있습니다. 예를 들어, 우리는 행렬-벡터 곱 $\mathbf{X} \mathbf{v}$가
$\mathbf{v}$에 비해 얼마나 더 길어질 수 있는지 물을 수 있습니다. 이러한 생각은 <em>스펙트럼(spectral)</em> 노름이라고 불리는 것으로 이어집니다. 지금은 [<strong>계산하기 훨씬 쉬운 <em>프로베니우스(Frobenius) 노름</em></strong>]을 소개합니다. 이것은 행렬 요소의 제곱 합의 제곱근으로 정의됩니다:</p>
<p>[<strong>$$\|\mathbf{X}\|<em>\textrm{F} = \sqrt{\sum</em>{i=1}^m \sum_{j=1}^n x_{ij}^2}.$$</strong>]</p>
<p>프로베니우스 노름은 마치 행렬 모양 벡터의 $\ell_2$ 노름인 것처럼 동작합니다. 다음 함수를 호출하면 행렬의 프로베니우스 노름이 계산됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.linalg.norm(np.ones((4, 9)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.norm(torch.ones((4, 9)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.norm(tf.ones((4, 9)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.linalg.norm(jnp.ones((4, 9)))
</code></pre>
<p>너무 앞서 나가고 싶지는 않지만, 우리는 이미 이러한 개념이 유용한 이유에 대한 직관을 심을 수 있습니다. 딥러닝에서 우리는 종종 최적화 문제를 해결하려고 합니다:
관찰된 데이터에 할당된 확률을 <em>최대화</em>합니다; 추천 모델과 관련된 수익을 <em>최대화</em>합니다; 예측과 실제 관찰 사이의 거리를 <em>최소화</em>합니다; 동일한 사람의 사진 표현 간의 거리를 <em>최소화</em>하는 동시에
다른 사람의 사진 표현 간의 거리를 <em>최대화</em>합니다. 딥러닝 알고리즘의 목표를 구성하는 이러한 거리는
종종 노름으로 표현됩니다.</p>
<h2 id="토론-1"><a class="header" href="#토론-1">토론</a></h2>
<p>이 섹션에서 우리는 현대 딥러닝의 상당 부분을 이해하는 데 필요한
모든 선형 대수를 검토했습니다. 하지만 선형 대수에는 훨씬 더 많은 내용이 있으며,
그 중 많은 부분이 머신러닝에 유용합니다. 예를 들어 행렬을 인수로 분해할 수 있으며,
이러한 분해는 실제 데이터셋의 저차원 구조를 드러낼 수 있습니다. 데이터셋의 구조를 발견하고 예측 문제를 해결하기 위해
행렬 분해와 고차 텐서로의 일반화를 사용하는 데 초점을 맞춘
머신러닝의 전체 하위 분야가 있습니다. 하지만 이 책은 딥러닝에 초점을 맞춥니다. 그리고 우리는 여러분이 실제 데이터셋에 머신러닝을 적용하며
손을 더럽히고 나면 더 많은 수학을 배우고 싶어질 것이라고 믿습니다. 따라서 나중에 더 많은 수학을 소개할 권리는 보유하지만,
이 섹션은 여기서 마무리합니다.</p>
<p>선형 대수를 더 배우고 싶다면,
훌륭한 책과 온라인 리소스가 많이 있습니다. 더 고급 집중 코스를 원한다면
:citet:<code>Strang.1993</code>, :citet:<code>Kolter.2008</code>, 및 :citet:<code>Petersen.Pedersen.ea.2008</code>을 확인해 보십시오.</p>
<p>요약하자면:</p>
<ul>
<li>스칼라, 벡터, 행렬, 텐서는
선형 대수에서 사용되는 기본 수학적 객체이며
각각 0개, 1개, 2개, 그리고 임의의 수의 축을 가지고 있습니다.</li>
<li>텐서는 인덱싱을 통해 특정 축을 따라 슬라이스하거나,
<code>sum</code> 및 <code>mean</code>과 같은 연산을 통해 축소할 수 있습니다.</li>
<li>요소별 곱을 하다마드 곱이라고 합니다.
반면 내적, 행렬-벡터 곱, 행렬-행렬 곱은
요소별 연산이 아니며 일반적으로 피연산자와 다른 모양을 가진 객체를 반환합니다.</li>
<li>하다마드 곱에 비해 행렬-행렬 곱은
계산하는 데 상당히 더 오래 걸립니다(2차 시간이 아닌 3차 시간).</li>
<li>노름은 벡터(또는 행렬)의 크기에 대한 다양한 개념을 포착하며,
일반적으로 두 벡터 사이의 거리를 측정하기 위해 두 벡터의 차이에 적용됩니다.</li>
<li>일반적인 벡터 노름에는 $\ell_1$ 및 $\ell_2$ 노름이 포함되며,
일반적인 행렬 노름에는 <em>스펙트럼</em> 및 <em>프로베니우스</em> 노름이 포함됩니다.</li>
</ul>
<h2 id="연습-문제-3"><a class="header" href="#연습-문제-3">연습 문제</a></h2>
<ol>
<li>행렬 전치의 전치는 행렬 자체임을 증명하십시오: $(\mathbf{A}^\top)^\top = \mathbf{A}$.</li>
<li>두 행렬 $\mathbf{A}$와 $\mathbf{B}$가 주어졌을 때, 합과 전치가 교환 가능함을 보이십시오: $\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top$.</li>
<li>임의의 정사각 행렬 $\mathbf{A}$에 대해, $\mathbf{A} + \mathbf{A}^\top$는 항상 대칭입니까? 이전 두 연습 문제의 결과만 사용하여 결과를 증명할 수 있습니까?</li>
<li>우리는 이 섹션에서 모양 (2, 3, 4)의 텐서 <code>X</code>를 정의했습니다. <code>len(X)</code>의 출력은 무엇입니까? 코드를 구현하지 말고 답을 쓴 다음 코드를 사용하여 답을 확인하십시오.</li>
<li>임의의 모양을 가진 텐서 <code>X</code>의 경우, <code>len(X)</code>는 항상 <code>X</code>의 특정 축의 길이에 해당합니까? 그 축은 무엇입니까?</li>
<li><code>A / A.sum(axis=1)</code>을 실행하고 무슨 일이 일어나는지 확인하십시오. 결과를 분석할 수 있습니까?</li>
<li>맨해튼 시내의 두 지점 사이를 이동할 때, 좌표 측면에서, 즉 거리와 거리(avenues and streets) 측면에서 커버해야 하는 거리는 얼마입니까? 대각선으로 이동할 수 있습니까?</li>
<li>모양 (2, 3, 4)의 텐서를 고려하십시오. 축 0, 1, 2를 따른 합계 출력의 모양은 무엇입니까?</li>
<li>3개 이상의 축을 가진 텐서를 <code>linalg.norm</code> 함수에 입력하고 출력을 관찰하십시오. 이 함수는 임의의 모양을 가진 텐서에 대해 무엇을 계산합니까?</li>
<li>가우스 무작위 변수로 초기화된 $\mathbf{A} \in \mathbb{R}^{2^{10} \times 2^{16}}$, $\mathbf{B} \in \mathbb{R}^{2^{16} \times 2^{5}}$, $\mathbf{C} \in \mathbb{R}^{2^{5} \times 2^{14}}$의 세 개의 큰 행렬을 고려하십시오. 곱 $\mathbf{A} \mathbf{B} \mathbf{C}$를 계산하려고 합니다. $(\mathbf{A} \mathbf{B}) \mathbf{C}$를 계산하는지 아니면 $\mathbf{A} (\mathbf{B} \mathbf{C})$를 계산하는지에 따라 메모리 사용량과 속도에 차이가 있습니까? 왜 그렇습니까?</li>
<li>세 개의 큰 행렬 $\mathbf{A} \in \mathbb{R}^{2^{10} \times 2^{16}}$, $\mathbf{B} \in \mathbb{R}^{2^{16} \times 2^{5}}$, $\mathbf{C} \in \mathbb{R}^{2^{5} \times 2^{16}}$을 고려하십시오. $\mathbf{A} \mathbf{B}$를 계산하는지 아니면 $\mathbf{A} \mathbf{C}^\top$를 계산하는지에 따라 속도에 차이가 있습니까? 왜 그렇습니까? 메모리를 복제하지 않고 $\mathbf{C} = \mathbf{B}^\top$를 초기화하면 어떻게 변합니까? 왜 그렇습니까?</li>
<li>세 행렬 $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{100 \times 200}$을 고려하십시오. $[\mathbf{A}, \mathbf{B}, \mathbf{C}]$를 쌓아서 3개의 축을 가진 텐서를 만듭니다. 차원(dimensionality)은 무엇입니까? 세 번째 축의 두 번째 좌표를 슬라이스하여 $\mathbf{B}$를 복구하십시오. 답이 맞는지 확인하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/30">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/31">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/196">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17968">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="미적분-calculus-1"><a class="header" href="#미적분-calculus-1">미적분 (Calculus)</a></h1>
<p>:label:<code>sec_calculus</code></p>
<p>오랫동안 원의 넓이를 계산하는 방법은 미스터리로 남아 있었습니다.
그러다 고대 그리스에서 수학자 아르키메데스가
원 내부에 꼭짓점 수가 증가하는 일련의 다각형을 새기는
영리한 아이디어를 생각해 냈습니다
(:numref:<code>fig_circle_area</code>).
$n$개의 꼭짓점이 있는 다각형의 경우,
우리는 $n$개의 삼각형을 얻습니다.
원을 더 미세하게 분할할수록 각 삼각형의 높이는 반지름 $r$에 가까워집니다.
동시에, 많은 수의 꼭짓점에 대해 호와 할선 사이의 비율이 1에 가까워지므로,
밑변은 $2 \pi r/n$에 가까워집니다.
따라서 다각형의 넓이는
$n \cdot r \cdot \frac{1}{2} (2 \pi r/n) = \pi r^2$에 가까워집니다.</p>
<p><img src="chapter_preliminaries/../img/polygon-circle.svg" alt="극한 절차로서 원의 넓이 찾기." />
:label:<code>fig_circle_area</code></p>
<p>이 극한 절차는 <em>미분 미적분</em>과 <em>적분 미적분</em>의 뿌리에 있습니다.
전자는 인수를 조작하여 함수의 값을 증가시키거나 감소시키는 방법을 알려줄 수 있습니다.
이것은 우리가 딥러닝에서 직면하는 <em>최적화 문제</em>에 유용합니다.
여기서 우리는 손실 함수를 줄이기 위해 파라미터를 반복적으로 업데이트합니다.
최적화는 모델을 훈련 데이터에 맞추는 방법을 다루며,
미적분은 그 핵심 전제 조건입니다.
그러나 우리의 궁극적인 목표는 <em>이전에 본 적 없는</em> 데이터에서 잘 수행하는 것임을 잊지 마십시오.
그 문제를 <em>일반화</em>라고 하며 다른 장의 핵심 초점이 될 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from matplotlib_inline import backend_inline
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
from matplotlib_inline import backend_inline
import numpy as np
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from matplotlib_inline import backend_inline
import numpy as np
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
from matplotlib_inline import backend_inline
import numpy as np
</code></pre>
<h2 id="도함수와-미분-derivatives-and-differentiation"><a class="header" href="#도함수와-미분-derivatives-and-differentiation">도함수와 미분 (Derivatives and Differentiation)</a></h2>
<p>간단히 말해서, *도함수(derivative)*는 인수의 변화에 대한 함수의 변화율입니다.
도함수는 각 파라미터를 무한히 작은 양만큼 <em>증가</em>시키거나 <em>감소</em>시킬 경우
손실 함수가 얼마나 빠르게 증가하거나 감소하는지 알려줄 수 있습니다.
공식적으로, 스칼라에서 스칼라로 매핑하는 함수 $f: \mathbb{R} \rightarrow \mathbb{R}$의 경우,
[<strong>점 $x$에서 $f$의 <em>도함수</em>는 다음과 같이 정의됩니다</strong>]</p>
<p>(<strong>$$f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.$$</strong>)
:eqlabel:<code>eq_derivative</code></p>
<p>오른쪽에 있는 이 항을 *극한(limit)*이라고 하며,
지정된 변수가 특정 값에 접근할 때
표현식의 값에 어떤 일이 일어나는지 알려줍니다.
이 극한은 섭동(perturbation) $h$와
함수 값의 변화 $f(x + h) - f(x)$ 사이의 비율이
크기를 0으로 줄일 때 무엇으로 수렴하는지 알려줍니다.</p>
<p>$f'(x)$가 존재할 때, $f$는 $x$에서 *미분 가능(differentiable)*하다고 합니다;
그리고 $f'(x)$가 집합(예: 구간 $[a,b]$)의 모든 $x$에 대해 존재할 때,
우리는 $f$가 이 집합에서 미분 가능하다고 말합니다.
정확도나 수신자 조작 특성 곡선 아래 면적(AUC)과 같이 우리가 최적화하고자 하는 많은 함수를 포함하여,
모든 함수가 미분 가능한 것은 아닙니다.
그러나 손실의 도함수를 계산하는 것은 심층 신경망을 훈련하기 위한
거의 모든 알고리즘에서 중요한 단계이므로,
우리는 종종 대신 미분 가능한 *대리(surrogate)*를 최적화합니다.</p>
<p>우리는 도함수 $f'(x)$를
$x$에 대한 $f(x)$의 <em>순간</em> 변화율로 해석할 수 있습니다.
예제를 통해 직관을 키워봅시다.
(<strong>$u = f(x) = 3x^2-4x$를 정의합니다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def f(x):
    return 3 * x ** 2 - 4 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def f(x):
    return 3 * x ** 2 - 4 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def f(x):
    return 3 * x ** 2 - 4 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def f(x):
    return 3 * x ** 2 - 4 * x
</code></pre>
<p>[<strong>$x=1$로 설정하면, $\frac{f(x+h) - f(x)}{h}$가</strong>] (<strong>$h$가 $0$에 접근함에 따라 $2$에 접근하는 것을 볼 수 있습니다.</strong>)
이 실험은 수학적 증명의 엄격함이 부족하지만,
실제로 $f'(1) = 2$임을 빠르게 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
for h in 10.0**np.arange(-1, -6, -1):
    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')
</code></pre>
<p>도함수에 대한 몇 가지 동등한 표기법 관례가 있습니다.
$y = f(x)$가 주어지면 다음 표현식은 동등합니다:</p>
<p>$$f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),$$</p>
<p>여기서 기호 $\frac{d}{dx}$와 $D$는 <em>미분 연산자</em>입니다.
아래에 몇 가지 일반적인 함수의 도함수를 제시합니다:</p>
<p>$$\begin{aligned} \frac{d}{dx} C &amp; = 0 &amp;&amp; \textrm{어떤 상수 $C$에 대해} \ \frac{d}{dx} x^n &amp; = n x^{n-1} &amp;&amp; \textrm{단, } n \neq 0 \ \frac{d}{dx} e^x &amp; = e^x \ \frac{d}{dx} \ln x &amp; = x^{-1}. \end{aligned}$$</p>
<p>미분 가능한 함수들로 구성된 함수는 종종 그 자체로 미분 가능합니다.
다음 규칙들은 미분 가능한 함수 $f$와 $g$, 그리고 상수 $C$의
합성을 다룰 때 유용합니다.</p>
<p>$$\begin{aligned} \frac{d}{dx} [C f(x)] &amp; = C \frac{d}{dx} f(x) &amp;&amp; \textrm{상수 배수 규칙} \ \frac{d}{dx} [f(x) + g(x)] &amp; = \frac{d}{dx} f(x) + \frac{d}{dx} g(x) &amp;&amp; \textrm{합의 규칙} \ \frac{d}{dx} [f(x) g(x)] &amp; = f(x) \frac{d}{dx} g(x) + g(x) \frac{d}{dx} f(x) &amp;&amp; \textrm{곱의 규칙} \ \frac{d}{dx} \frac{f(x)}{g(x)} &amp; = \frac{g(x) \frac{d}{dx} f(x) - f(x) \frac{d}{dx} g(x)}{g^2(x)} &amp;&amp; \textrm{몫의 규칙} \end{aligned}$$</p>
<p>이것을 사용하여, 우리는 다음을 통해 $3 x^2 - 4x$의 도함수를 찾기 위해 규칙을 적용할 수 있습니다.</p>
<p>$$\frac{d}{dx} [3 x^2 - 4x] = 3 \frac{d}{dx} x^2 - 4 \frac{d}{dx} x = 6x - 4.$$</p>
<p>$x = 1$을 대입하면 실제로 이 위치에서 도함수가 $2$와 같음을 보여줍니다.
도함수는 특정 위치에서 함수의 *기울기(slope)*를 알려줍니다.</p>
<h2 id="시각화-유틸리티-visualization-utilities"><a class="header" href="#시각화-유틸리티-visualization-utilities">시각화 유틸리티 (Visualization Utilities)</a></h2>
<p>[<strong><code>matplotlib</code> 라이브러리를 사용하여 함수의 기울기를 시각화할 수 있습니다</strong>].
몇 가지 함수를 정의해야 합니다.
이름에서 알 수 있듯이 <code>use_svg_display</code>는
더 선명한 이미지를 위해 <code>matplotlib</code>에 SVG 형식으로 그래픽을 출력하도록 지시합니다.
주석 <code>#@save</code>는 함수, 클래스 또는 기타 코드 블록을 <code>d2l</code> 패키지에 저장하여
나중에 코드를 반복하지 않고 호출할 수 있게 해주는 특수 수정자입니다.
예: <code>d2l.use_svg_display()</code>.</p>
<pre><code class="language-{.python .input}">%%tab all
def use_svg_display():  #@save
    """Jupyter에서 플롯을 표시하기 위해 svg 형식을 사용합니다."""
    backend_inline.set_matplotlib_formats('svg')
</code></pre>
<p>편리하게도 <code>set_figsize</code>로 그림 크기를 설정할 수 있습니다.
import 문 <code>from matplotlib import pyplot as plt</code>가
<code>d2l</code> 패키지에서 <code>#@save</code>를 통해 표시되었으므로 <code>d2l.plt</code>를 호출할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def set_figsize(figsize=(3.5, 2.5)):  #@save
    """matplotlib의 그림 크기를 설정합니다."""
    use_svg_display()
    d2l.plt.rcParams['figure.figsize'] = figsize
</code></pre>
<p><code>set_axes</code> 함수는 축을 레이블, 범위, 스케일을 포함한 속성과 연결할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
#@save
def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """matplotlib의 축을 설정합니다."""
    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)
    axes.set_xscale(xscale), axes.set_yscale(yscale)
    axes.set_xlim(xlim),     axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()
</code></pre>
<p>이 세 가지 함수를 사용하여 여러 곡선을 겹쳐 그리는 <code>plot</code> 함수를 정의할 수 있습니다.
여기에 있는 대부분의 코드는 입력의 크기와 모양이 일치하는지 확인하는 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
#@save
def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    """데이터 포인트를 플롯합니다."""

    def has_one_axis(X):  # X(텐서 또는 리스트)가 1개의 축을 가지면 True
        return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], "__len__"))
    
    if has_one_axis(X): X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y):
        X = X * len(Y)
        
    set_figsize(figsize)
    if axes is None:
        axes = d2l.plt.gca()
    axes.cla()
    for x, y, fmt in zip(X, Y, fmts):
        axes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
</code></pre>
<p>이제 [<strong>함수 $u = f(x)$와 $x=1$에서의 접선 $y = 2x - 3$을 플롯할 수 있습니다</strong>],
여기서 계수 $2$는 접선의 기울기입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
x = np.arange(0, 3, 0.1)
plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])
</code></pre>
<h2 id="편도함수와-기울기-partial-derivatives-and-gradients"><a class="header" href="#편도함수와-기울기-partial-derivatives-and-gradients">편도함수와 기울기 (Partial Derivatives and Gradients)</a></h2>
<p>:label:<code>subsec_calculus-grad</code></p>
<p>지금까지 우리는 변수가 하나뿐인 함수를 미분해 왔습니다.
딥러닝에서는 <em>많은</em> 변수를 가진 함수도 다뤄야 합니다.
이러한 <em>다변수</em> 함수에 적용되는 도함수의 개념을 간략하게 소개합니다.</p>
<p>$y = f(x_1, x_2, \ldots, x_n)$을 $n$개의 변수를 가진 함수라고 합시다.
$y$의 $i$번째 파라미터 $x_i$에 대한 *편도함수(partial derivative)*는 다음과 같습니다.</p>
<p>$$ \frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.$$</p>
<p>$rac{\partial y}{\partial x_i}$를 계산하기 위해,
우리는 $x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n$을 상수로 취급하고
$x_i$에 대한 $y$의 도함수를 계산할 수 있습니다.
편도함수에 대한 다음 표기법 관례는 모두 일반적이며 모두 같은 의미입니다:</p>
<p>$$\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = \partial_{x_i} f = \partial_i f = f_{x_i} = f_i = D_i f = D_{x_i} f.$$</p>
<p>우리는 다변수 함수의 모든 변수에 대한 편도함수를 연결하여
함수의 *기울기(gradient)*라고 불리는 벡터를 얻을 수 있습니다.
함수 $f: \mathbb{R}^n \rightarrow \mathbb{R}$의 입력이
$n$차원 벡터 $\mathbf{x} = [x_1, x_2, \ldots, x_n]^\top$이고
출력이 스칼라라고 가정해 봅시다.
$\mathbf{x}$에 대한 함수 $f$의 기울기는
$n$개의 편도함수로 구성된 벡터입니다:</p>
<p>$$\nabla_{\mathbf{x}} f(\mathbf{x}) = \left[\partial_{x_1} f(\mathbf{x}), \partial_{x_2} f(\mathbf{x}), \ldots
\partial_{x_n} f(\mathbf{x})\right]^\top.$$</p>
<p>모호함이 없을 때,
$\nabla_{\mathbf{x}} f(\mathbf{x})$는 일반적으로
$\nabla f(\mathbf{x})$로 대체됩니다.
다음 규칙은 다변수 함수를 미분할 때 유용합니다:</p>
<ul>
<li>모든 $\mathbf{A} \in \mathbb{R}^{m \times n}$에 대해 $\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top$이고 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A}  = \mathbf{A}$입니다.</li>
<li>정사각 행렬 $\mathbf{A} \in \mathbb{R}^{n \times n}$에 대해 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x}  = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$이고 특히
$\nabla_{\mathbf{x}} |\mathbf{x} |^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$입니다.</li>
</ul>
<p>마찬가지로, 어떤 행렬 $\mathbf{X}$에 대해서도,
우리는 $\nabla_{\mathbf{X}} |\mathbf{X} |_\textrm{F}^2 = 2\mathbf{X}$를 갖습니다.</p>
<h2 id="연쇄-법칙-chain-rule"><a class="header" href="#연쇄-법칙-chain-rule">연쇄 법칙 (Chain Rule)</a></h2>
<p>딥러닝에서 관심 있는 기울기는 종종 계산하기 어렵습니다.
우리가 깊게 중첩된 함수(함수의 (함수의...))를 다루고 있기 때문입니다.
다행히도 *연쇄 법칙(chain rule)*이 이것을 처리합니다.
단일 변수 함수로 돌아가서, $y = f(g(x))$이고
기본 함수 $y=f(u)$와 $u=g(x)$가 모두 미분 가능하다고 가정해 봅시다.
연쇄 법칙은 다음을 명시합니다.</p>
<p>$$\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}.$$</p>
<p>다변수 함수로 돌아가서,
$y = f(\mathbf{u})$가 변수 $u_1, u_2, \ldots, u_m$을 가지고 있고,
각 $u_i = g_i(\mathbf{x})$가 변수 $x_1, x_2, \ldots, x_n$을 가지고 있다고 가정해 봅시다.
즉, $\mathbf{u} = g(\mathbf{x})$입니다.
그러면 연쇄 법칙은 다음을 명시합니다.</p>
<p>$$\frac{\partial y}{\partial x_{i}} = \frac{\partial y}{\partial u_{1}} \frac{\partial u_{1}}{\partial x_{i}} + \frac{\partial y}{\partial u_{2}} \frac{\partial u_{2}}{\partial x_{i}} + \ldots + \frac{\partial y}{\partial u_{m}} \frac{\partial u_{m}}{\partial x_{i}} \ \textrm{ 따라서 } \ \nabla_{\mathbf{x}} y =  \mathbf{A} \nabla_{\mathbf{u}} y,$$</p>
<p>여기서 $\mathbf{A} \in \mathbb{R}^{n \times m}$은
벡터 $\mathbf{x}$에 대한 벡터 $\mathbf{u}$의 도함수를 포함하는 <em>행렬</em>입니다.
따라서 기울기를 평가하려면 벡터-행렬 곱을 계산해야 합니다.
이것이 선형 대수가 딥러닝 시스템을 구축하는 데 있어
그토록 필수적인 구성 요소인 주요 이유 중 하나입니다.</p>
<h2 id="토론-2"><a class="header" href="#토론-2">토론</a></h2>
<p>우리는 깊은 주제의 겉핥기만 했지만,
이미 많은 개념이 초점에 들어왔습니다:
첫째, 미분을 위한 합성 규칙을 일상적으로 적용할 수 있어,
기울기를 <em>자동으로</em> 계산할 수 있습니다.
이 작업은 창의성이 필요하지 않으므로 우리는 인지 능력을 다른 곳에 집중할 수 있습니다.
둘째, 벡터 값 함수의 도함수를 계산하려면 출력에서 입력으로
변수의 종속성 그래프를 추적하면서 행렬을 곱해야 합니다.
특히, 이 그래프는 함수를 평가할 때 *순방향(forward)*으로 순회하고
기울기를 계산할 때 *역방향(backwards)*으로 순회합니다.
나중 챕터에서는 연쇄 법칙을 적용하기 위한 계산 절차인 역전파를 공식적으로 소개할 것입니다.</p>
<p>최적화의 관점에서, 기울기는 손실을 낮추기 위해
모델의 파라미터를 어떻게 이동해야 하는지 결정할 수 있게 해주며,
이 책 전체에서 사용되는 최적화 알고리즘의 각 단계는 기울기 계산을 필요로 합니다.</p>
<h2 id="연습-문제-4"><a class="header" href="#연습-문제-4">연습 문제</a></h2>
<ol>
<li>지금까지 우리는 도함수 규칙을 당연하게 여겼습니다.
정의와 극한을 사용하여 (i) $f(x) = c$, (ii) $f(x) = x^n$, (iii) $f(x) = e^x$, (iv) $f(x) = \log x$에 대한 속성을 증명하십시오.</li>
<li>같은 맥락에서, 첫 번째 원칙에서 곱, 합, 몫의 규칙을 증명하십시오.</li>
<li>상수 배수 규칙이 곱의 규칙의 특수한 경우로 뒤따른다는 것을 증명하십시오.</li>
<li>$f(x) = x^x$의 도함수를 계산하십시오.</li>
<li>어떤 $x$에 대해 $f'(x) = 0$이라는 것은 무엇을 의미합니까?
이것이 성립할 수 있는 함수 $f$와 위치 $x$의 예를 드십시오.</li>
<li>함수 $y = f(x) = x^3 - \frac{1}{x}$의 그래프와 $x = 1$에서의 접선을 플롯하십시오.</li>
<li>함수 $f(\mathbf{x}) = 3x_1^2 + 5e^{x_2}$의 기울기를 구하십시오.</li>
<li>함수 $f(\mathbf{x}) = |\mathbf{x}|_2$의 기울기는 무엇입니까? $\mathbf{x} = \mathbf{0}$일 때 어떻게 됩니까?</li>
<li>$u = f(x, y, z)$이고 $x = x(a, b)$, $y = y(a, b)$, $z = z(a, b)$인 경우에 대해 연쇄 법칙을 작성할 수 있습니까?</li>
<li>역함수가 존재하는 함수 $f(x)$가 주어졌을 때, 역함수 $f^{-1}(x)$의 도함수를 계산하십시오.
여기서 우리는 $f^{-1}(f(x)) = x$이고 반대로 $f(f^{-1}(y)) = y$를 갖습니다.
힌트: 유도 과정에서 이 속성을 사용하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/32">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/33">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/197">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17969">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="자동-미분-automatic-differentiation"><a class="header" href="#자동-미분-automatic-differentiation">자동 미분 (Automatic Differentiation)</a></h1>
<p>:label:<code>sec_autograd</code></p>
<p>:numref:<code>sec_calculus</code>에서
도함수 계산이 심층 신경망을 훈련하는 데 사용할
모든 최적화 알고리즘의 중요한 단계라는 것을 상기해 보십시오.
계산은 간단하지만, 손으로 계산하는 것은 지루하고 오류가 발생하기 쉬우며,
모델이 더 복잡해짐에 따라 이러한 문제는 커질 뿐입니다.</p>
<p>다행히도 모든 최신 딥러닝 프레임워크는
<em>자동 미분(automatic differentiation)</em> (종종 <em>autograd</em>로 줄임)을 제공하여
이 작업을 대신해 줍니다.
각 연속적인 함수를 통해 데이터를 전달할 때,
프레임워크는 각 값이 다른 값에 어떻게 의존하는지 추적하는
*계산 그래프(computational graph)*를 구축합니다.
도함수를 계산하기 위해, 자동 미분은
연쇄 법칙을 적용하여 이 그래프를 역방향으로 진행합니다.
이런 방식으로 연쇄 법칙을 적용하는 계산 알고리즘을 *역전파(backpropagation)*라고 합니다.</p>
<p>autograd 라이브러리는 지난 10년 동안 뜨거운 관심사가 되었지만,
오랜 역사를 가지고 있습니다.
사실 autograd에 대한 최초의 언급은 반세기 전으로 거슬러 올라갑니다 :cite:<code>Wengert.1964</code>.
현대 역전파의 핵심 아이디어는 1980년 박사 학위 논문 :cite:<code>Speelpenning.1980</code>으로 거슬러 올라가며
1980년대 후반에 더욱 발전되었습니다 :cite:<code>Griewank.1989</code>.
역전파는 기울기를 계산하는 기본 방법이 되었지만 유일한 옵션은 아닙니다.
예를 들어 Julia 프로그래밍 언어는 순방향 전파를 사용합니다 :cite:<code>Revels.Lubin.Papamarkou.2016</code>.
방법을 탐구하기 전에, 먼저 autograd 패키지를 마스터해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import autograd, np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import numpy as jnp
</code></pre>
<h2 id="간단한-함수-a-simple-function"><a class="header" href="#간단한-함수-a-simple-function">간단한 함수 (A Simple Function)</a></h2>
<p>우리가 열 벡터 $\mathbf{x}$에 대해
(<strong>함수 $y = 2\mathbf{x}^{\top}\mathbf{x}$를 미분하는 데</strong>) 관심이 있다고 가정해 봅시다.
시작하기 위해 <code>x</code>에 초기 값을 할당합니다.</p>
<pre><code class="language-{.python .input  n=1}">%%tab mxnet
x = np.arange(4.0)
x
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab pytorch
x = torch.arange(4.0)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.range(4, dtype=tf.float32)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(4.0)
x
</code></pre>
<p>:begin_tab:<code>mxnet, pytorch, tensorflow</code>
[<strong>$\mathbf{x}$에 대한 $y$의 기울기를 계산하기 전에,
그것을 저장할 장소가 필요합니다.</strong>] 일반적으로 우리는 도함수를 취할 때마다 새 메모리를 할당하는 것을 피합니다.
왜냐하면 딥러닝은 동일한 파라미터에 대해
도함수를 계속해서 매우 여러 번 계산해야 하기 때문이며,
메모리가 부족할 위험이 있습니다. 벡터 $\mathbf{x}$에 대한 스칼라 값 함수의 기울기는
$\mathbf{x}$와 동일한 모양을 가진 벡터 값입니다.
:end_tab:</p>
<pre><code class="language-{.python .input  n=8}">%%tab mxnet
# `attach_grad`를 호출하여 텐서의 기울기를 위한 메모리를 할당합니다
x.attach_grad()
# `x`에 대해 취한 기울기를 계산한 후, 0으로 초기화된 값을 가진
# `grad` 속성을 통해 액세스할 수 있습니다
x.grad
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab pytorch
# x = torch.arange(4.0, requires_grad=True)를 생성할 수도 있습니다
x.requires_grad_(True)
x.grad  # 기울기는 기본적으로 None입니다
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.Variable(x)
</code></pre>
<p>(<strong>이제 <code>x</code>의 함수를 계산하고 결과를 <code>y</code>에 할당합니다.</strong>)</p>
<pre><code class="language-{.python .input  n=10}">%%tab mxnet
# 코드는 계산 그래프를 구축하기 위해 `autograd.record` 범위 안에 있습니다
with autograd.record():
    y = 2 * np.dot(x, x)
y
</code></pre>
<pre><code class="language-{.python .input  n=11}">%%tab pytorch
y = 2 * torch.dot(x, x)
y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# 모든 계산을 테이프에 기록합니다
with tf.GradientTape() as t:
    y = 2 * tf.tensordot(x, x, axes=1)
y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = lambda x: 2 * jnp.dot(x, x)
y(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
<code>backward</code> 메서드를 호출하여
[<strong>이제 <code>x</code>에 대한 <code>y</code>의 기울기를 취할 수 있습니다</strong>].
다음으로, <code>x</code>의 <code>grad</code> 속성을 통해 기울기에 액세스할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<code>backward</code> 메서드를 호출하여
[<strong>이제 <code>x</code>에 대한 <code>y</code>의 기울기를 취할 수 있습니다</strong>].
다음으로, <code>x</code>의 <code>grad</code> 속성을 통해 기울기에 액세스할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<code>gradient</code> 메서드를 호출하여
[<strong>이제 <code>x</code>에 대한 <code>y</code>의 기울기를 계산할 수 있습니다</strong>].
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<code>grad</code> 변환을 통과시켜
[<strong>이제 <code>x</code>에 대한 <code>y</code>의 기울기를 취할 수 있습니다</strong>].
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
x.grad
</code></pre>
<pre><code class="language-{.python .input  n=12}">%%tab pytorch
y.backward()
x.grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x_grad = t.gradient(y, x)
x_grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import grad
# `grad` 변환은 원래 함수의 기울기를 계산하는 Python 함수를 반환합니다
x_grad = grad(y)(x)
x_grad
</code></pre>
<p>(<strong>우리는 이미 $\mathbf{x}$에 대한 함수 $y = 2\mathbf{x}^{\top}\mathbf{x}$의 기울기가
$4\mathbf{x}$여야 한다는 것을 알고 있습니다.</strong>) 이제 자동 기울기 계산과 예상 결과가 동일한지 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=13}">%%tab mxnet
x.grad == 4 * x
</code></pre>
<pre><code class="language-{.python .input  n=14}">%%tab pytorch
x.grad == 4 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x_grad == 4 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x_grad == 4 * x
</code></pre>
<p>:begin_tab:<code>mxnet</code>
[<strong>이제 <code>x</code>의 다른 함수를 계산하고
기울기를 취해 봅시다.</strong>] MXNet은 우리가 새 기울기를 기록할 때마다
기울기 버퍼를 재설정한다는 점에 유의하십시오.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
[<strong>이제 <code>x</code>의 다른 함수를 계산하고
기울기를 취해 봅시다.</strong>] PyTorch는 우리가 새 기울기를 기록할 때
자동으로 기울기 버퍼를 재설정하지 않는다는 점에 유의하십시오.
대신, 새 기울기가 이미 저장된 기울기에 추가됩니다.
이 동작은 여러 목적 함수의 합을 최적화하고 싶을 때 유용합니다.
기울기 버퍼를 재설정하려면 다음과 같이 <code>x.grad.zero_()</code>를 호출할 수 있습니다:
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
[<strong>이제 <code>x</code>의 다른 함수를 계산하고
기울기를 취해 봅시다.</strong>] TensorFlow는 우리가 새 기울기를 기록할 때마다
기울기 버퍼를 재설정한다는 점에 유의하십시오.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = x.sum()
y.backward()
x.grad  # 새로 계산된 기울기로 덮어쓰여짐
</code></pre>
<pre><code class="language-{.python .input  n=20}">%%tab pytorch
x.grad.zero_()  # 기울기 재설정
y = x.sum()
y.backward()
x.grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.reduce_sum(x)
t.gradient(y, x)  # 새로 계산된 기울기로 덮어쓰여짐
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = lambda x: x.sum()
grad(y)(x)
</code></pre>
<h2 id="비스칼라-변수의-역전파-backward-for-non-scalar-variables"><a class="header" href="#비스칼라-변수의-역전파-backward-for-non-scalar-variables">비스칼라 변수의 역전파 (Backward for Non-Scalar Variables)</a></h2>
<p><code>y</code>가 벡터일 때,
벡터 <code>x</code>에 대한 <code>y</code>의 도함수의 가장 자연스러운 표현은
<code>y</code>의 각 성분의 <code>x</code>의 각 성분에 대한 편도함수를 포함하는
*자코비안(Jacobian)*이라는 행렬입니다.
마찬가지로 고차 <code>y</code>와 <code>x</code>의 경우 미분 결과는 더 높은 차수의 텐서가 될 수 있습니다.</p>
<p>자코비안은 일부 고급 머신러닝 기술에 등장하지만,
더 일반적으로 우리는 전체 벡터 <code>x</code>에 대한
<code>y</code>의 각 성분의 기울기를 합산하여
<code>x</code>와 동일한 모양의 벡터를 생성하기를 원합니다.
예를 들어, 우리는 종종 훈련 예제의 <em>배치</em> 중 각 예제에 대해
개별적으로 계산된 손실 함수의 값을 나타내는 벡터를 가지고 있습니다.
여기서 우리는 단지 (<strong>각 예제에 대해 개별적으로 계산된 기울기를 합산</strong>)하기를 원합니다.</p>
<p>:begin_tab:<code>mxnet</code>
MXNet은 기울기를 계산하기 전에 합계를 통해 모든 텐서를 스칼라로 축소하여 이 문제를 처리합니다.
즉, 자코비안 $\partial_{\mathbf{x}} \mathbf{y}$를 반환하는 대신,
합의 기울기 $\partial_{\mathbf{x}} \sum_i y_i$를 반환합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
딥러닝 프레임워크마다 비스칼라 텐서의 기울기를 해석하는 방식이 다르기 때문에,
PyTorch는 혼란을 피하기 위해 몇 가지 조치를 취합니다.
비스칼라에서 <code>backward</code>를 호출하면 PyTorch에 객체를 스칼라로 축소하는 방법을 알려주지 않는 한 오류가 발생합니다.
더 공식적으로, 우리는 <code>backward</code>가 $\partial_{\mathbf{x}} \mathbf{y}$ 대신
$\mathbf{v}^\top \partial_{\mathbf{x}} \mathbf{y}$를 계산하도록 하는 어떤 벡터 $\mathbf{v}$를 제공해야 합니다.
이 다음 부분은 혼란스러울 수 있지만, 나중에 명확해질 이유들로 인해
이 인수($\mathbf{v}$를 나타냄)의 이름은 <code>gradient</code>입니다. 자세한 설명은 Yang Zhang의 <a href="https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29">Medium 게시물</a>을 참조하십시오.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
기본적으로 TensorFlow는 합의 기울기를 반환합니다.
즉, 자코비안 $\partial_{\mathbf{x}} \mathbf{y}$를 반환하는 대신,
합의 기울기 $\partial_{\mathbf{x}} \sum_i y_i$를 반환합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = x * x  
y.backward()
x.grad  # y = sum(x * x)의 기울기와 동일
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x.grad.zero_()
y = x * x
y.backward(gradient=torch.ones(len(y)))  # 더 빠름: y.sum().backward()
x.grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = x * x
t.gradient(y, x)  # y = tf.reduce_sum(x * x)와 동일
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = lambda x: x * x
# grad는 스칼라 출력 함수에 대해서만 정의됨
grad(lambda x: y(x).sum())(x)
</code></pre>
<h2 id="계산-분리-detaching-computation"><a class="header" href="#계산-분리-detaching-computation">계산 분리 (Detaching Computation)</a></h2>
<p>때때로 우리는 [<strong>일부 계산을 기록된 계산 그래프 외부로 이동</strong>]하고 싶을 때가 있습니다.
예를 들어, 입력을 사용하여 기울기를 계산하고 싶지 않은
일부 보조 중간 항을 만든다고 가정해 봅시다.
이 경우, 최종 결과에서 해당 계산 그래프를 *분리(detach)*해야 합니다.
다음의 장난감 예제는 이것을 더 명확하게 만듭니다:
<code>z = x * y</code>이고 <code>y = x * x</code>이지만
<code>y</code>를 통해 전달되는 영향보다는 <code>z</code>에 대한 <code>x</code>의 <em>직접적인</em> 영향에 초점을 맞추고 싶다고 가정해 봅시다.
이 경우, <code>y</code>와 동일한 값을 갖지만
<em>출처(provenance)</em> (어떻게 생성되었는지)가 지워진
새 변수 <code>u</code>를 만들 수 있습니다.
따라서 <code>u</code>는 그래프에 조상이 없으며 기울기는 <code>u</code>를 통해 <code>x</code>로 흐르지 않습니다.
예를 들어 <code>z = x * u</code>의 기울기를 취하면 결과 <code>u</code>를 산출합니다
(<code>z = x * x * x</code>이기 때문에 예상할 수 있는 <code>3 * x * x</code>가 아님).</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = x * x
    u = y.detach()
    z = u * x
z.backward()
x.grad == u
</code></pre>
<pre><code class="language-{.python .input  n=21}">%%tab pytorch
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# persistent=True로 설정하여 계산 그래프를 보존합니다.
# 이를 통해 t.gradient를 두 번 이상 실행할 수 있습니다.
with tf.GradientTape(persistent=True) as t:
    y = x * x
    u = tf.stop_gradient(y)
    z = u * x

x_grad = t.gradient(z, x)
x_grad == u
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
import jax

y = lambda x: x * x
# jax.lax 기본 요소는 XLA 연산에 대한 Python 래퍼입니다
u = jax.lax.stop_gradient(y(x))
z = lambda x: u * x

grad(lambda x: z(x).sum())(x) == y(x)
</code></pre>
<p>이 절차가 <code>z</code>로 이어지는 그래프에서
<code>y</code>의 조상을 분리하지만, <code>y</code>로 이어지는 계산 그래프는 유지되므로
<code>x</code>에 대한 <code>y</code>의 기울기를 계산할 수 있다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
x.grad == 2 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
t.gradient(y, x) == 2 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
grad(lambda x: y(x).sum())(x) == 2 * x
</code></pre>
<h2 id="기울기와-python-제어-흐름-gradients-and-python-control-flow"><a class="header" href="#기울기와-python-제어-흐름-gradients-and-python-control-flow">기울기와 Python 제어 흐름 (Gradients and Python Control Flow)</a></h2>
<p>지금까지 우리는 <code>z = x * x * x</code>와 같은 함수를 통해
입력에서 출력까지의 경로가 잘 정의된 사례를 검토했습니다.
프로그래밍은 결과를 계산하는 방법에 있어 훨씬 더 많은 자유를 제공합니다.
예를 들어, 보조 변수에 의존하게 하거나 중간 결과에 대한 선택을 조건부로 만들 수 있습니다.
자동 미분을 사용하는 이점 중 하나는
(<strong>함수의 계산 그래프를 구축하는 데 Python 제어 흐름의 미로를 통과해야 하더라도</strong>)
(예: 조건문, 루프, 임의 함수 호출), [<strong>결과 변수의 기울기를 여전히 계산할 수 있다는 것입니다.</strong>]
이를 설명하기 위해 <code>while</code> 루프의 반복 횟수와 <code>if</code> 문의 평가가
모두 입력 <code>a</code>의 값에 의존하는 다음 코드 스니펫을 고려해 보십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def f(a):
    b = a * 2
    while np.linalg.norm(b) &lt; 1000:
        b = b * 2
    if b.sum() &gt; 0:
        c = b
    else:
        c = 100 * b
    return c
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def f(a):
    b = a * 2
    while b.norm() &lt; 1000:
        b = b * 2
    if b.sum() &gt; 0:
        c = b
    else:
        c = 100 * b
    return c
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def f(a):
    b = a * 2
    while tf.norm(b) &lt; 1000:
        b = b * 2
    if tf.reduce_sum(b) &gt; 0:
        c = b
    else:
        c = 100 * b
    return c
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def f(a):
    b = a * 2
    while jnp.linalg.norm(b) &lt; 1000:
        b = b * 2
    if b.sum() &gt; 0:
        c = b
    else:
        c = 100 * b
    return c
</code></pre>
<p>아래에서 우리는 무작위 값을 입력으로 전달하여 이 함수를 호출합니다.
입력이 확률 변수이므로, 우리는 계산 그래프가 어떤 형태를 취할지 모릅니다.
그러나 특정 입력에 대해 <code>f(a)</code>를 실행할 때마다
특정 계산 그래프를 실현하고 이후에 <code>backward</code>를 실행할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
a = np.random.normal()
a.attach_grad()
with autograd.record():
    d = f(a)
d.backward()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
a = tf.Variable(tf.random.normal(shape=()))
with tf.GradientTape() as t:
    d = f(a)
d_grad = t.gradient(d, a)
d_grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import random
a = random.normal(random.PRNGKey(1), ()) 
d = f(a)
d_grad = grad(f)(a)
</code></pre>
<p>비록 우리 함수 <code>f</code>가 데모 목적으로 약간 작위적이지만,
입력에 대한 의존성은 꽤 간단합니다: 그것은 부분적으로 정의된 스케일을 가진 <code>a</code>의 <em>선형</em> 함수입니다.
따라서 <code>f(a) / a</code>는 상수 항목의 벡터이며, 게다가 <code>f(a) / a</code>는 <code>a</code>에 대한 <code>f(a)</code>의 기울기와 일치해야 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
a.grad == d / a
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
a.grad == d / a
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
d_grad == d / a
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
d_grad == d / a
</code></pre>
<p>동적 제어 흐름은 딥러닝에서 매우 일반적입니다.
예를 들어 텍스트를 처리할 때 계산 그래프는 입력 길이에 따라 달라집니다.
이러한 경우, 기울기를 <em>선험적으로(a priori)</em> 계산하는 것이 불가능하기 때문에
자동 미분은 통계적 모델링에 필수적이 됩니다.</p>
<h2 id="토론-3"><a class="header" href="#토론-3">토론</a></h2>
<p>여러분은 이제 자동 미분의 힘을 맛보았습니다.
도함수를 자동으로 그리고 효율적으로 계산하는 라이브러리의 개발은
딥러닝 실무자들에게 엄청난 생산성 향상 요인이 되어, 그들이 덜 하찮은 일에 집중할 수 있게 해주었습니다.
게다가 autograd를 사용하면 펜과 종이로 기울기를 계산하는 것이
엄두도 못 낼 정도로 시간이 많이 걸리는 거대한 모델을 설계할 수 있습니다.
흥미롭게도, 우리는 모델을 (통계적 의미에서) <em>최적화</em>하기 위해 autograd를 사용하지만,
autograd 라이브러리 자체의 (계산적 의미에서의) <em>최적화</em>는
프레임워크 설계자들에게 매우 중요한 관심사인 풍부한 주제입니다.
여기서 가장 신속하고 메모리 효율적인 방식으로 결과를 계산하기 위해
컴파일러와 그래프 조작 도구가 활용됩니다.</p>
<p>지금은 다음 기본 사항을 기억해 두십시오: (i) 도함수를 원하는 변수에 기울기를 연결합니다; (ii) 목표 값의 계산을 기록합니다; (iii) 역전파 함수를 실행합니다; (iv) 결과 기울기에 액세스합니다.</p>
<h2 id="연습-문제-5"><a class="header" href="#연습-문제-5">연습 문제</a></h2>
<ol>
<li>2계 도함수가 1계 도함수보다 계산 비용이 훨씬 더 많이 드는 이유는 무엇입니까?</li>
<li>역전파 함수를 실행한 후 즉시 다시 실행하면 어떻게 됩니까? 조사해 보십시오.</li>
<li><code>a</code>에 대한 <code>d</code>의 도함수를 계산하는 제어 흐름 예제에서 변수 <code>a</code>를 무작위 벡터나 행렬로 변경하면 어떻게 됩니까? 이 시점에서 계산 <code>f(a)</code>의 결과는 더 이상 스칼라가 아닙니다. 결과에 어떤 일이 발생합니까? 이것을 어떻게 분석합니까?</li>
<li>$f(x) = \sin(x)$라고 합시다. $f$와 도함수 $f'$의 그래프를 플롯하십시오. $f'(x) = \cos(x)$라는 사실을 이용하지 말고 자동 미분을 사용하여 결과를 얻으십시오.</li>
<li>$f(x) = ((\log x^2) \cdot \sin x) + x^{-1}$이라고 합시다. $x$에서 $f(x)$까지 결과를 추적하는 종속성 그래프를 작성하십시오.</li>
<li>연쇄 법칙을 사용하여 앞서 언급한 함수의 도함수 $\frac{df}{dx}$를 계산하고, 이전에 구성한 종속성 그래프에 각 항을 배치하십시오.</li>
<li>그래프와 중간 도함수 결과가 주어졌을 때, 기울기를 계산할 때 여러 가지 옵션이 있습니다. $x$에서 $f$로 시작하여 한 번, $f$에서 $x$로 추적하여 한 번 결과를 평가하십시오. $x$에서 $f$로의 경로는 일반적으로 *순방향 미분(forward differentiation)*으로 알려져 있으며, $f$에서 $x$로의 경로는 *역방향 미분(backward differentiation)*으로 알려져 있습니다.</li>
<li>언제 순방향 미분을 사용하고 언제 역방향 미분을 사용하고 싶으십니까? 힌트: 필요한 중간 데이터의 양, 단계를 병렬화하는 능력, 관련된 행렬 및 벡터의 크기를 고려하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/34">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/35">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/200">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17970">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="확률과-통계-probability-and-statistics"><a class="header" href="#확률과-통계-probability-and-statistics">확률과 통계 (Probability and Statistics)</a></h1>
<p>:label:<code>sec_prob</code></p>
<p>어쨌든, 머신러닝은 불확실성에 관한 것입니다.
지도 학습에서 우리는 알려진 것(<em>특성</em>)이 주어졌을 때
알려지지 않은 것(<em>타겟</em>)을 예측하고 싶습니다.
목표에 따라, 우리는 타겟의 가장 가능성 있는 값을 예측하려고 시도할 수 있습니다.
또는 타겟과 예상 거리가 가장 작은 값을 예측할 수 있습니다.
그리고 때때로 우리는 특정 값을 예측할 뿐만 아니라
<em>불확실성을 정량화</em>하고 싶습니다.
예를 들어 환자를 설명하는 일부 특성이 주어졌을 때,
우리는 그들이 내년에 심장마비를 겪을 가능성이 <em>얼마나 되는지</em> 알고 싶을 수 있습니다.
비지도 학습에서 우리는 종종 불확실성에 관심을 갖습니다.
일련의 측정값이 비정상적인지 판단하려면,
관심 집단에서 값을 관찰할 가능성이 얼마나 되는지 아는 것이 도움이 됩니다.
또한 강화 학습에서 우리는 다양한 환경에서 지능적으로 행동하는 에이전트를 개발하기를 원합니다.
이를 위해서는 환경이 어떻게 변할 것으로 예상되는지,
그리고 사용 가능한 각 행동에 대한 응답으로 어떤 보상을 기대할 수 있는지에 대한 추론이 필요합니다.</p>
<p>*확률(Probability)*은 불확실성 하에서의 추론을 다루는 수학 분야입니다.
어떤 프로세스의 확률 모델이 주어지면, 우리는 다양한 사건의 가능성에 대해 추론할 수 있습니다.
반복 가능한 사건(동전 던지기 등)의 빈도를 설명하기 위해 확률을 사용하는 것은
상당히 논란의 여지가 없습니다.
사실, <em>빈도주의(frequentist)</em> 학자들은
그러한 반복 가능한 사건에 <em>만</em> 적용되는 확률 해석을 고수합니다.
반면 <em>베이지안(Bayesian)</em> 학자들은
불확실성 하에서의 추론을 공식화하기 위해 확률 언어를 더 광범위하게 사용합니다.
베이지안 확률은 두 가지 고유한 특징으로 특징지어집니다:
(i) 반복 불가능한 사건에 대한 믿음의 정도 할당,
예: 댐이 무너질 <em>확률</em>은 얼마인가?;
(ii) 주관성. 베이지안 확률은
새로운 증거에 비추어 믿음을 어떻게 업데이트해야 하는지에 대한 모호하지 않은 규칙을 제공하지만,
다른 개인이 다른 <em>사전(prior)</em> 믿음으로 시작할 수 있도록 허용합니다.
*통계(Statistics)*는 데이터 수집 및 구성으로 시작하여
데이터를 생성한 프로세스에 대해 어떤 추론을 도출할 수 있는지로 거슬러 올라가며
우리가 역으로 추론하도록 돕습니다.
데이터셋을 분석할 때마다, 더 넓은 집단을 특징지을 수 있는 패턴을 찾고 있다면,
우리는 통계적 사고를 사용하고 있는 것입니다.
많은 과정, 전공, 논문, 직업, 학과, 회사 및 기관이
확률과 통계 연구에 헌신했습니다.
이 섹션은 겉핥기에 불과하지만,
우리는 모델 구축을 시작하는 데 필요한 기초를 제공할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.numpy.random import multinomial
import random
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import random
import torch
from torch.distributions.multinomial import Multinomial
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import random
import tensorflow as tf
from tensorflow_probability import distributions as tfd
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import random
import jax
from jax import numpy as jnp
import numpy as np
</code></pre>
<h2 id="간단한-예제-동전-던지기-a-simple-example-tossing-coins"><a class="header" href="#간단한-예제-동전-던지기-a-simple-example-tossing-coins">간단한 예제: 동전 던지기 (A Simple Example: Tossing Coins)</a></h2>
<p>우리가 동전을 던질 계획이고 앞면(대 뒷면)이 나올 가능성이 얼마나 되는지
정량화하고 싶다고 상상해 보십시오.
동전이 *공정(fair)*하다면, 두 결과(앞면과 뒷면)가 나올 가능성은 동일합니다.
게다가 동전을 $n$번 던질 계획이라면, 우리가 볼 것으로 <em>예상</em>되는 앞면의 비율은
뒷면의 <em>예상</em> 비율과 정확히 일치해야 합니다.
이것을 보는 직관적인 방법 중 하나는 대칭입니다:
$n_\textrm{h}$개의 앞면과 $n_\textrm{t} = (n - n_\textrm{h})$개의 뒷면이 있는 모든 가능한 결과에 대해, $n_\textrm{t}$개의 앞면과 $n_\textrm{h}$개의 뒷면이 있는 똑같이 가능한 결과가 있습니다.
이것은 평균적으로 던지기의 $1/2$이 앞면이 나오고 $1/2$이 뒷면이 나올 것으로 예상하는 경우에만 가능하다는 점에 유의하십시오.
물론 각각 $n=1000000$번 던지기로 이 실험을 여러 번 수행하더라도,
$n_\textrm{h} = n_\textrm{t}$가 정확히 일치하는 시행은 결코 보지 못할 수도 있습니다.</p>
<p>공식적으로 양 $1/2$을 <em>확률</em>이라고 하며,
여기서는 주어진 던지기에서 앞면이 나올 확실성을 포착합니다.
확률은 관심 있는 결과, 즉 *사건(events)*에 $0$과 $1$ 사이의 점수를 할당합니다.
여기서 관심 있는 사건은 $\textrm{heads}$이며 해당 확률을 $P(\textrm{heads})$로 나타냅니다.
확률 $1$은 절대적인 확실성을 나타내고(양쪽이 모두 앞면인 속임수 동전을 상상해 보십시오)
확률 $0$은 불가능함을 나타냅니다(예: 양쪽이 모두 뒷면인 경우).
빈도 $n_\textrm{h}/n$과 $n_\textrm{t}/n$은 확률이 아니라 <em>통계</em>입니다.
확률은 데이터 생성 프로세스의 기초가 되는 <em>이론적</em> 양입니다.
여기서 확률 $1/2$은 동전 자체의 속성입니다.
반면 통계는 관찰된 데이터의 함수로 계산되는 <em>경험적</em> 양입니다.
확률적 및 통계적 양에 대한 우리의 관심은 불가분의 관계에 있습니다.
우리는 종종 데이터셋이 주어졌을 때 확률과 같은 모델 파라미터의 *추정치(estimates)*를 생성하는
*추정량(estimators)*이라는 특별한 통계를 설계합니다.
게다가 그 추정량이 *일관성(consistency)*이라는 좋은 속성을 만족할 때,
우리의 추정치는 해당 확률로 수렴할 것입니다.
결과적으로 이러한 추론된 확률은
우리가 미래에 마주칠 수 있는 동일한 모집단의 데이터에 대한
가능성 있는 통계적 속성에 대해 알려줍니다.</p>
<p>우리가 실제 $P(\textrm{heads})$를 모르는
진짜 동전을 우연히 발견했다고 가정해 봅시다.
이 양을 통계적 방법으로 조사하려면,
우리는 (i) 데이터를 수집하고; (ii) 추정량을 설계해야 합니다.
여기서 데이터 획득은 쉽습니다; 동전을 여러 번 던지고 모든 결과를 기록할 수 있습니다.
공식적으로 기본 확률 프로세스에서 실현(realizations)을 그리는 것을 *샘플링(sampling)*이라고 합니다.
짐작하셨겠지만, 하나의 자연스러운 추정량은
관찰된 <em>앞면</em>의 수를 총 던지기 횟수로 나눈 비율입니다.</p>
<p>이제 동전이 실제로 공정하다고 가정해 봅시다.
즉, $P(\textrm{heads}) = 0.5$.
공정한 동전 던지기를 시뮬레이션하기 위해 임의의 난수 생성기를 호출할 수 있습니다.
확률 $0.5$로 사건의 샘플을 그리는 쉬운 방법이 몇 가지 있습니다.
예를 들어 Python의 <code>random.random</code>은 구간 $[0,1]$의 숫자를 산출하며,
여기서 임의의 하위 구간 $[a, b] \subset [0,1]$에 있을 확률은 $b-a$와 같습니다.
따라서 반환된 부동 소수점 숫자가 <code>0.5</code>보다 큰지 테스트하여
각각 확률 <code>0.5</code>로 <code>0</code>과 <code>1</code>을 얻을 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
num_tosses = 100
heads = sum([random.random() &gt; 0.5 for _ in range(num_tosses)])
tails = num_tosses - heads
print("heads, tails: ", [heads, tails])
</code></pre>
<p>일반적으로 다항 함수(multinomial function)를 호출하여
가능한 결과의 수가 유한한 모든 변수(동전 던지기나 주사위 굴리기 등)에서
여러 번 그리기를 시뮬레이션할 수 있습니다.
첫 번째 인수를 그리기 횟수로 설정하고
두 번째 인수를 각 가능한 결과와 관련된 확률 리스트로 설정합니다.
공정한 동전 던지기를 10번 시뮬레이션하기 위해 확률 벡터 <code>[0.5, 0.5]</code>를 할당하고,
인덱스 0을 앞면으로, 인덱스 1을 뒷면으로 해석합니다.
함수는 가능한 결과의 수(여기서는 2)와 동일한 길이를 가진 벡터를 반환하며,
여기서 첫 번째 성분은 앞면의 발생 횟수를 알려주고
두 번째 성분은 뒷면의 발생 횟수를 알려줍니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
fair_probs = [0.5, 0.5]
multinomial(100, fair_probs)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
fair_probs = torch.tensor([0.5, 0.5])
Multinomial(100, fair_probs).sample()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
fair_probs = tf.ones(2) / 2
tfd.Multinomial(100, fair_probs).sample()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
fair_probs = [0.5, 0.5]
# jax.random에는 다항 분포가 구현되어 있지 않습니다
np.random.multinomial(100, fair_probs)
</code></pre>
<p>이 샘플링 과정을 실행할 때마다,
이전 결과와 다를 수 있는 새로운 무작위 값을 받게 됩니다.
던진 횟수로 나누면 데이터에서 각 결과의 <em>빈도</em>를 얻을 수 있습니다.
이러한 빈도는 추정하려는 확률과 마찬가지로 합이 $1$이 된다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
multinomial(100, fair_probs) / 100
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
Multinomial(100, fair_probs).sample() / 100
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tfd.Multinomial(100, fair_probs).sample() / 100
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
np.random.multinomial(100, fair_probs) / 100
</code></pre>
<p>여기서 시뮬레이션된 동전이 공정하더라도
(우리 스스로 확률을 <code>[0.5, 0.5]</code>로 설정했음), 앞면과 뒷면의 수가 동일하지 않을 수 있습니다.
비교적 적은 수의 샘플만 뽑았기 때문입니다.
시뮬레이션을 직접 구현하지 않고 결과만 보았다면,
동전이 약간 불공정한지 아니면 $1/2$에서 벗어난 가능성이
단지 작은 표본 크기의 인공물(artifact)인지 어떻게 알 수 있을까요?
10,000번 던지기를 시뮬레이션할 때 어떤 일이 일어나는지 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
counts = multinomial(10000, fair_probs).astype(np.float32)
counts / 10000
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
counts = Multinomial(10000, fair_probs).sample()
counts / 10000
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
counts = tfd.Multinomial(10000, fair_probs).sample()
counts / 10000
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
counts = np.random.multinomial(10000, fair_probs).astype(np.float32)
counts / 10000
</code></pre>
<p>일반적으로 반복되는 사건(동전 던지기 등)의 평균의 경우,
반복 횟수가 증가함에 따라 우리의 추정치는
실제 기본 확률로 수렴하도록 보장됩니다.
이 현상의 수학적 공식화를 *대수의 법칙(law of large numbers)*이라고 하며,
*중심 극한 정리(central limit theorem)*는 많은 상황에서
표본 크기 $n$이 증가함에 따라 이러한 오류가 $(1/\sqrt{n})$의 비율로 줄어들어야 함을 알려줍니다.
던지기 횟수를 1에서 10,000으로 늘림에 따라 추정치가 어떻게 진화하는지 연구하여
더 많은 직관을 얻어 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
counts = Multinomial(1, fair_probs).sample((10000,))
cum_counts = counts.cumsum(dim=0)
estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)
estimates = estimates.numpy()

d2l.set_figsize((4.5, 3.5))
d2l.plt.plot(estimates[:, 0], label=("P(coin=heads)"))
d2l.plt.plot(estimates[:, 1], label=("P(coin=tails)"))
d2l.plt.axhline(y=0.5, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('Samples')
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend();
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
counts = multinomial(1, fair_probs, size=10000)
cum_counts = counts.astype(np.float32).cumsum(axis=0)
estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
counts = tfd.Multinomial(1, fair_probs).sample(10000)
cum_counts = tf.cumsum(counts, axis=0)
estimates = cum_counts / tf.reduce_sum(cum_counts, axis=1, keepdims=True)
estimates = estimates.numpy()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
counts = np.random.multinomial(1, fair_probs, size=10000).astype(np.float32)
cum_counts = counts.cumsum(axis=0)
estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow, jax
d2l.set_figsize((4.5, 3.5))
d2l.plt.plot(estimates[:, 0], label=("P(coin=heads)"))
d2l.plt.plot(estimates[:, 1], label=("P(coin=tails)"))
d2l.plt.axhline(y=0.5, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('Samples')
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend();
</code></pre>
<p>각 실선 곡선은 동전의 두 값 중 하나에 해당하며
각 실험 그룹 후 동전이 해당 값을 낼 것으로 추정되는 확률을 제공합니다.
검은색 점선은 실제 기본 확률을 나타냅니다.
더 많은 실험을 수행하여 더 많은 데이터를 얻으면,
곡선은 실제 확률로 수렴합니다.
여러분은 이미 통계학자들을 몰두하게 하는
더 발전된 질문들의 윤곽을 보기 시작했을 것입니다:
이 수렴은 얼마나 빨리 일어나는가?
동일한 공장에서 제조된 많은 동전을 이미 테스트했다면,
이 정보를 어떻게 통합할 수 있을까?</p>
<h2 id="더-공식적인-처리-a-more-formal-treatment"><a class="header" href="#더-공식적인-처리-a-more-formal-treatment">더 공식적인 처리 (A More Formal Treatment)</a></h2>
<p>우리는 이미 꽤 멀리 왔습니다: 확률 모델을 제기하고,
합성 데이터를 생성하고, 통계적 추정량을 실행하고,
경험적으로 수렴을 평가하고, 오류 지표를 보고(편차 확인)했습니다.
그러나 훨씬 더 나아가려면 더 정확해야 합니다.</p>
<p>무작위성을 다룰 때, 우리는 가능한 결과의 집합을 $\mathcal{S}$로 나타내고
이를 <em>표본 공간(sample space)</em> 또는 *결과 공간(outcome space)*이라고 부릅니다.
여기서 각 요소는 별개의 가능한 *결과(outcome)*입니다.
단일 동전을 던지는 경우, $\mathcal{S} = {\textrm{heads}, \textrm{tails}}$입니다.
단일 주사위의 경우, $\mathcal{S} = {1, 2, 3, 4, 5, 6}$입니다.
동전 두 개를 던질 때 가능한 결과는
${(\textrm{heads}, \textrm{heads}), (\textrm{heads}, \textrm{tails}), (\textrm{tails}, \textrm{heads}),  (\textrm{tails}, \textrm{tails})}$입니다.
*사건(Events)*은 표본 공간의 부분 집합입니다.
예를 들어, "첫 번째 동전 던지기가 앞면이 나오는" 사건은
집합 ${(\textrm{heads}, \textrm{heads}), (\textrm{heads}, \textrm{tails})}$에 해당합니다.
무작위 실험의 결과 $z$가 $z \in \mathcal{A}$를 만족할 때마다 사건 $\mathcal{A}$가 발생했습니다.
주사위를 한 번 굴릴 때, 우리는 "$5$ 보기"($\mathcal{A} = {5}$)와
"홀수 보기"($\mathcal{B} = {1, 3, 5}$)라는 사건을 정의할 수 있습니다.
이 경우, 주사위가 $5$가 나오면 $\mathcal{A}$와 $\mathcal{B}$가 모두 발생했다고 말합니다.
반면 $z = 3$이면 $\mathcal{A}$는 발생하지 않았지만 $\mathcal{B}$는 발생했습니다.</p>
<p><em>확률</em> 함수는 사건을 실수 값 ${P: \mathcal{A} \subseteq \mathcal{S} \rightarrow [0,1]}$에 매핑합니다.
주어진 표본 공간 $\mathcal{S}$에서 사건 $\mathcal{A}$의 확률 $P(\mathcal{A})$는
다음 속성을 갖습니다:</p>
<ul>
<li>모든 사건 $\mathcal{A}$의 확률은 음이 아닌 실수입니다. 즉, $P(\mathcal{A}) \geq 0$;</li>
<li>전체 표본 공간의 확률은 $1$입니다. 즉, $P(\mathcal{S}) = 1$;</li>
<li>*상호 배타적(mutually exclusive)*인(즉, 모든 $i \neq j$에 대해 $\mathcal{A}_i \cap \mathcal{A}_j = \emptyset$) 가산 가능한 사건 시퀀스 $\mathcal{A}_1, \mathcal{A}<em>2, \ldots$에 대해, 그들 중 하나라도 발생할 확률은 개별 확률의 합과 같습니다. 즉, $P(\bigcup</em>{i=1}^{\infty} \mathcal{A}<em>i) = \sum</em>{i=1}^{\infty} P(\mathcal{A}_i)$.</li>
</ul>
<p>:citet:<code>Kolmogorov.1933</code>이 제안한 이러한 확률 이론의 공리들은
여러 중요한 결과를 빠르게 도출하는 데 적용될 수 있습니다.
예를 들어, 사건 $\mathcal{A}$ <em>또는</em> 그 여사건 $\mathcal{A}'$가 발생할 확률은 1입니다
($\mathcal{A} \cup \mathcal{A}' = \mathcal{S}$이기 때문).
우리는 또한 $P(\emptyset) = 0$임을 증명할 수 있습니다.
$1 = P(\mathcal{S} \cup \mathcal{S}') = P(\mathcal{S} \cup \emptyset) = P(\mathcal{S}) + P(\emptyset) = 1 + P(\emptyset)$이기 때문입니다.
결과적으로 사건 $\mathcal{A}$ <em>와</em> 그 여사건 $\mathcal{A}'$가 동시에 발생할 확률은
$P(\mathcal{A} \cap \mathcal{A}') = 0$입니다.
비공식적으로, 이것은 불가능한 사건이 발생할 확률이 0임을 알려줍니다.</p>
<h2 id="확률-변수-random-variables"><a class="header" href="#확률-변수-random-variables">확률 변수 (Random Variables)</a></h2>
<p>동전 던지기나 주사위 굴리기와 같은 사건에 대해 이야기할 때,
우리는 *확률 변수(random variable)*라는 아이디어를 불러일으키고 있었습니다.
공식적으로 확률 변수는 기본 표본 공간에서
(아마도 많은) 값의 집합으로의 매핑입니다.
확률 변수가 표본 공간과 어떻게 다른지 궁금할 수 있습니다.
둘 다 결과의 모음이기 때문입니다.
중요한 점은 확률 변수가 원시 표본 공간보다 훨씬 더 거칠 수(coarser) 있다는 것입니다.
기본 표본 공간이 무한하더라도(예: $0$과 $1$ 사이의 선분 위의 점), "0.5보다 큼"과 같은 이진 확률 변수를 정의할 수 있습니다.
또한 여러 확률 변수가 동일한 기본 표본 공간을 공유할 수 있습니다.
예를 들어 "내 집 경보가 울리는지 여부"와
"내 집에 도둑이 들었는지 여부"는
기본 표본 공간을 공유하는 이진 확률 변수입니다.
결과적으로, 한 확률 변수가 취한 값을 알면
다른 확률 변수의 가능한 값에 대해 무언가를 알 수 있습니다.
경보가 울렸다는 것을 알면, 집이 털렸을 가능성이 높다고 의심할 수 있습니다.</p>
<p>확률 변수가 취하는 모든 값은 기본 표본 공간의 부분 집합에 해당합니다.
따라서 확률 변수 $X$가 값 $v$를 취하는 발생($X=v$로 표시)은 <em>사건</em>이며,
$P(X=v)$는 그 확률을 나타냅니다.
때때로 이 표기법은 투박해질 수 있으며, 문맥이 명확할 때 표기법을 남용할 수 있습니다.
예를 들어, $P(X)$를 사용하여 $X$의 <em>분포(distribution)</em>, 즉 $X$가 주어진 값을 취할 확률을 알려주는 함수를 광범위하게 지칭할 수 있습니다.
다른 때에는 $P(X,Y) = P(X) P(Y)$와 같은 표현을 사용하여
확률 변수 $X$와 $Y$가 취할 수 있는 모든 값에 대해 참인 진술을 속기로 표현합니다.
즉, 모든 $i,j$에 대해 $P(X=i \textrm{ and } Y=j) = P(X=i)P(Y=j)$가 성립합니다.
다른 때에는 확률 변수가 문맥에서 명확할 때 $P(v)$라고 써서 표기법을 남용합니다.
확률 이론의 사건은 표본 공간의 결과 집합이므로, 확률 변수가 취할 값의 범위를 지정할 수 있습니다.
예를 들어, $P(1 \leq X \leq 3)$은 사건 ${1 \leq X \leq 3}$의 확률을 나타냅니다.</p>
<p>동전 던지기나 주사위 굴리기와 같은 <em>이산(discrete)</em> 확률 변수와
모집단에서 무작위로 추출한 사람의 체중이나 키와 같은 <em>연속(continuous)</em> 확률 변수 사이에는
미묘한 차이가 있음에 유의하십시오.
이 경우 우리는 누군가의 정확한 키에 대해 거의 신경 쓰지 않습니다.
게다가 우리가 충분히 정밀한 측정을 했다면,
지구상의 어떤 두 사람도 정확히 같은 키를 가지고 있지 않다는 것을 알게 될 것입니다.
사실 충분히 미세한 측정으로, 여러분은 일어날 때와 잠자리에 들 때
절대 같은 키를 가질 수 없을 것입니다.
누군가의 키가 정확히 1.801392782910287192 미터일 정확한 확률을 묻는 것은 의미가 없습니다.
대신, 우리는 일반적으로 누군가의 키가 주어진 구간, 예를 들어 1.79와 1.81 미터 사이에 속하는지 말할 수 있는 것에 더 관심이 있습니다.
이러한 경우 우리는 확률 *밀도(densities)*를 다룹니다.
정확히 1.80 미터의 키는 확률이 없지만 0이 아닌 밀도를 갖습니다.
구간에 할당된 확률을 알아내려면, 해당 구간에 대해 밀도의 *적분(integral)*을 취해야 합니다.</p>
<h2 id="다중-확률-변수-multiple-random-variables"><a class="header" href="#다중-확률-변수-multiple-random-variables">다중 확률 변수 (Multiple Random Variables)</a></h2>
<p>여러 확률 변수 간의 상호 작용과 관련된 진술을 하지 않고는
이전 섹션을 통과할 수도 없었다는 것을 눈치챘을 것입니다
($P(X,Y) = P(X) P(Y)$를 상기하십시오).
머신러닝의 대부분은 이러한 관계와 관련이 있습니다.
여기서 표본 공간은 관심 있는 모집단, 예를 들어 비즈니스와 거래하는 고객, 인터넷상의 사진, 또는 생물학자에게 알려진 단백질일 수 있습니다.
각 확률 변수는 서로 다른 속성의 (알려지지 않은) 값을 나타냅니다.
모집단에서 개인을 샘플링할 때마다, 우리는 각 확률 변수의 실현을 관찰합니다.
확률 변수가 취하는 값은 겹치거나, 부분적으로 겹치거나, 완전히 분리될 수 있는
표본 공간의 부분 집합에 해당하기 때문에, 한 확률 변수가 취한 값을 알면
다른 확률 변수의 어떤 값이 가능성이 있는지에 대한
믿음을 업데이트할 수 있습니다.
환자가 병원에 걸어 들어왔는데 그들이 숨 쉬는 데 어려움을 겪고 있고
후각을 상실했다는 것을 관찰했다면,
그들이 숨 쉬는 데 문제가 없고 완전히 평범한 후각을 가진 경우보다
COVID-19에 걸렸을 가능성이 더 높다고 믿습니다.</p>
<p>여러 확률 변수로 작업할 때, 우리는 변수들이 공동으로 취할 수 있는 모든 값의 조합에 해당하는
사건을 구성할 수 있습니다.
이러한 각 조합(예: $A=a$ 및 $B=b$)에 확률을 할당하는 확률 함수를
<em>결합 확률(joint probability)</em> 함수라고 하며,
단순히 표본 공간의 해당 부분 집합의 교집합에 할당된 확률을 반환합니다.
확률 변수 $A$와 $B$가 각각 값 $a$와 $b$를 취하는 사건에 할당된 <em>결합 확률</em>은
$P(A = a, B = b)$로 표시되며, 여기서 쉼표는 "그리고(and)"를 나타냅니다.
모든 값 $a$와 $b$에 대해 다음이 성립한다는 점에 유의하십시오.</p>
<p>$$P(A=a, B=b) \leq P(A=a) \textrm{ and } P(A=a, B=b) \leq P(B = b),$$</p>
<p>$A=a$와 $B=b$가 발생하려면 $A=a$가 발생해야 <em>하고</em> $B=b$도 발생해야 하기 때문입니다.
흥미롭게도 결합 확률은 이러한 확률 변수에 대해 우리가 알 수 있는 모든 것을
확률적 의미에서 알려주며, 개별 분포 $P(A)$와 $P(B)$를 복구하는 것을 포함하여
다른 많은 유용한 양을 도출하는 데 사용할 수 있습니다.
$P(A=a)$를 복구하려면 확률 변수 $B$가 취할 수 있는 모든 값 $v$에 대해
$P(A=a, B=v)$를 합산하면 됩니다:
$P(A=a) = \sum_v P(A=a, B=v)$.</p>
<p>비율 $\frac{P(A=a, B=b)}{P(A=a)} \leq 1$은 매우 중요한 것으로 밝혀졌습니다.
이것을 *조건부 확률(conditional probability)*이라고 하며 "$\mid$" 기호를 통해 표시됩니다:</p>
<p>$$P(B=b \mid A=a) = P(A=a,B=b)/P(A=a).$$</p>
<p>이것은 $A=a$가 발생했다는 사실을 조건으로 했을 때,
사건 $B=b$와 관련된 새로운 확률을 알려줍니다.
이 조건부 확률을 $A=a$와 관련된 표본 공간의 부분 집합에만 관심을 제한한 다음
모든 확률의 합이 1이 되도록 재정규화하는 것으로 생각할 수 있습니다.
조건부 확률은 사실 일반적인 확률일 뿐이므로
모든 항을 동일한 사건에 조건화하여 동일한 표본 공간에 주의를 제한하는 한
모든 공리를 존중합니다.
예를 들어, 분리된 사건 $\mathcal{B}$와 $\mathcal{B}'$에 대해
$P(\mathcal{B} \cup \mathcal{B}' \mid A = a) = P(\mathcal{B} \mid A = a) + P(\mathcal{B}' \mid A = a)$가 성립합니다.</p>
<p>조건부 확률의 정의를 사용하여 *베이즈 정리(Bayes' theorem)*라는 유명한 결과를 도출할 수 있습니다.
구성에 따라 $P(A, B) = P(B\mid A) P(A)$이고 $P(A, B) = P(A\mid B) P(B)$입니다.
두 방정식을 결합하면 $P(B\mid A) P(A) = P(A\mid B) P(B)$가 되고 따라서 다음을 얻습니다.</p>
<p>$$P(A \mid B) = \frac{P(B\mid A) P(A)}{P(B)}.$$</p>
<p>이 간단한 방정식은 조건화 순서를 뒤집을 수 있게 해주기 때문에 심오한 의미를 갖습니다.
$P(B\mid A)$, $P(A)$, $P(B)$를 추정하는 방법을 안다면,
$P(A\mid B)$를 추정할 수 있습니다.
우리는 종종 한 항을 직접 추정하는 것이 다른 항보다 쉽다는 것을 알게 되는데,
여기서 베이즈 정리가 구출해 줄 수 있습니다.
예를 들어, 특정 질병에 대한 증상의 유병률과 질병 및 증상의 전체 유병률을 각각 알고 있다면,
증상을 바탕으로 누군가가 질병에 걸렸을 가능성을 결정할 수 있습니다.
어떤 경우에는 증상의 유병률과 같은 $P(B)$에 직접 액세스하지 못할 수도 있습니다.
이 경우 베이즈 정리의 단순화된 버전이 유용합니다:</p>
<p>$$P(A \mid B) \propto P(B \mid A) P(A).$$</p>
<p>$P(A \mid B)$가 $1$로 정규화되어야 함을 알고 있으므로, 즉 $\sum_a P(A=a \mid B) = 1$,
다음을 계산하는 데 사용할 수 있습니다.</p>
<p>$$P(A \mid B) = \frac{P(B \mid A) P(A)}{\sum_a P(B \mid A=a) P(A = a)}.$$</p>
<p>베이지안 통계에서 우리는 관찰자가 <em>사전 확률(prior)</em> $P(H)$에 인코딩된
사용 가능한 가설의 타당성에 대한 일부 (주관적인) 사전 믿음과,
클래스 $P(E \mid H)$의 각 가설에 대해 수집된 증거의 값을 관찰할 가능성이 얼마나 되는지 말해주는
*우도 함수(likelihood function)*를 가지고 있다고 생각합니다.
베이즈 정리는 사용 가능한 증거 $E$에 비추어 초기 <em>사전 확률</em> $P(H)$를 업데이트하여
<em>사후 확률(posterior)</em> 믿음 $P(H \mid E) = \frac{P(E \mid H) P(H)}{P(E)}$를 생성하는 방법을
알려주는 것으로 해석됩니다.
비공식적으로 이것은 "사후 확률은 사전 확률 곱하기 우도를 증거로 나눈 것과 같다"라고 진술할 수 있습니다.
이제 증거 $P(E)$는 모든 가설에 대해 동일하므로,
가설에 대해 단순히 정규화하는 것으로 넘어갈 수 있습니다.</p>
<p>$\sum_a P(A=a \mid B) = 1$은 또한 확률 변수에 대해 *주변화(marginalize)*할 수 있게 해줍니다.
즉, $P(A, B)$와 같은 결합 분포에서 변수를 삭제할 수 있습니다.
결국 다음을 얻습니다.</p>
<p>$$\sum_a P(B \mid A=a) P(A=a) = \sum_a P(B, A=a) = P(B).$$</p>
<p>독립성(Independence)은 통계학의 많은 중요한 아이디어의 중추를 형성하는
또 다른 근본적으로 중요한 개념입니다.
간단히 말해서, $A$의 값에 대한 조건화가
$B$와 관련된 확률 분포에 어떠한 변화도 일으키지 않고 그 반대의 경우도 마찬가지라면,
두 변수는 <em>독립</em>입니다.
더 공식적으로, $A \perp B$로 표시되는 독립성은
$P(A \mid B) = P(A)$를 요구하며, 결과적으로
$P(A,B) = P(A \mid B) P(B) = P(A) P(B)$를 요구합니다.
독립성은 종종 적절한 가정입니다.
예를 들어, 확률 변수 $A$가 공정한 동전 하나를 던진 결과를 나타내고
확률 변수 $B$가 다른 동전을 던진 결과를 나타낸다면,
$A$가 앞면이 나왔는지 아는 것이 $B$가 앞면이 나올 확률에 영향을 주어서는 안 됩니다.</p>
<p>독립성은 기본 분포에서 데이터의 연속적인 추출 사이에 성립할 때(강력한 통계적 결론을 내릴 수 있음)
또는 데이터의 다양한 변수 사이에 성립할 때 특히 유용하며,
이 독립성 구조를 인코딩하는 더 간단한 모델로 작업할 수 있게 해줍니다.
반면에 확률 변수 간의 종속성을 추정하는 것은 종종 학습의 바로 그 목표입니다.
우리는 질병과 증상이 독립적이지 않다고 믿기 때문에
증상이 주어졌을 때 질병의 확률을 추정하는 데 관심을 갖습니다.</p>
<p>조건부 확률은 적절한 확률이기 때문에 독립성과 종속성의 개념도 적용된다는 점에 유의하십시오.
두 확률 변수 $A$와 $B$는 제3의 변수 $C$가 주어졌을 때
$P(A, B \mid C) = P(A \mid C)P(B \mid C)$인 경우에만 *조건부 독립(conditionally independent)*입니다.
흥미롭게도 두 변수는 일반적으로 독립적일 수 있지만
제3의 변수에 대해 조건화할 때 종속적이 될 수 있습니다.
이것은 두 확률 변수 $A$와 $B$가 제3의 변수 $C$의 원인에 해당할 때 종종 발생합니다.
예를 들어 골절과 폐암은 일반 모집단에서는 독립적일 수 있지만,
병원에 입원한 경우를 조건으로 하면 골절이 폐암과 음의 상관관계가 있음을 발견할 수 있습니다.
골절이 어떤 사람이 병원에 입원한 이유를 <em>설명해 버리기(explains away)</em> 때문에
폐암에 걸려 입원했을 확률을 낮추기 때문입니다.</p>
<p>반대로, 두 종속 확률 변수는 제3의 변수에 대해 조건화할 때 독립적이 될 수 있습니다.
이것은 그렇지 않으면 관련이 없는 두 사건이 공통 원인을 가질 때 종종 발생합니다.
신발 사이즈와 독해 수준은 초등학생들 사이에서 높은 상관관계가 있지만,
나이를 조건으로 하면 이 상관관계는 사라집니다.</p>
<h2 id="예제-an-example"><a class="header" href="#예제-an-example">예제 (An Example)</a></h2>
<p>:label:<code>subsec_probability_hiv_app</code></p>
<p>우리의 기술을 시험해 봅시다.
의사가 환자에게 HIV 검사를 시행한다고 가정합니다.
이 검사는 상당히 정확하며 환자가 건강하지만 질병이 있다고 보고되는 경우,
즉 건강한 환자가 1%의 경우 양성 반응을 보이는 경우에만 1% 확률로 실패합니다.
게다가 환자가 실제로 HIV에 걸린 경우 감지하는 데 결코 실패하지 않습니다.
우리는 $D_1 \in {0, 1}$을 사용하여 진단을 나타내고
($0$이면 음성, $1$이면 양성)
$H \in {0, 1}$을 사용하여 HIV 상태를 나타냅니다.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">조건부 확률</th><th style="text-align: right">$H=1$</th><th style="text-align: right">$H=0$</th></tr></thead><tbody>
<tr><td style="text-align: left">$P(D_1 = 1 \mid H)$</td><td style="text-align: right">1</td><td style="text-align: right">0.01</td></tr>
<tr><td style="text-align: left">$P(D_1 = 0 \mid H)$</td><td style="text-align: right">0</td><td style="text-align: right">0.99</td></tr>
</tbody></table>
</div>
<p>열의 합은 모두 1입니다(행의 합은 그렇지 않음).
이들은 조건부 확률이기 때문입니다.
검사 결과가 양성으로 나오면 환자가 HIV에 걸릴 확률, 즉 $P(H = 1 \mid D_1 = 1)$을 계산해 봅시다.
직관적으로 이것은 질병이 얼마나 흔한지에 달려 있습니다.
거짓 경보의 수에 영향을 미치기 때문입니다.
모집단에 질병이 거의 없다고 가정해 봅시다. 예: $P(H=1) = 0.0015$.
베이즈 정리를 적용하려면 주변화를 적용하여 다음을 결정해야 합니다.</p>
<p>$$\begin{aligned}
P(D_1 = 1)
=&amp; P(D_1=1, H=0) + P(D_1=1, H=1)  \
=&amp; P(D_1=1 \mid H=0) P(H=0) + P(D_1=1 \mid H=1) P(H=1) \
=&amp; 0.011485.
\end{aligned}
$$</p>
<p>이것은 다음으로 이어집니다.</p>
<p>$$P(H = 1 \mid D_1 = 1) = \frac{P(D_1=1 \mid H=1) P(H=1)}{P(D_1=1)} = 0.1306.$$</p>
<p>즉, 검사가 꽤 정확함에도 불구하고 환자가 실제로 HIV에 걸렸을 확률은 13.06%에 불과합니다.
보시다시피 확률은 직관에 반할 수 있습니다.
그런 무시무시한 소식을 접한 환자는 어떻게 해야 할까요?
아마도 환자는 의사에게 명확성을 얻기 위해 다른 검사를 시행해 달라고 요청할 것입니다.
두 번째 검사는 다른 특성을 가지고 있으며 첫 번째 검사만큼 좋지 않습니다.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">조건부 확률</th><th style="text-align: right">$H=1$</th><th style="text-align: right">$H=0$</th></tr></thead><tbody>
<tr><td style="text-align: left">$P(D_2 = 1 \mid H)$</td><td style="text-align: right">0.98</td><td style="text-align: right">0.03</td></tr>
<tr><td style="text-align: left">$P(D_2 = 0 \mid H)$</td><td style="text-align: right">0.02</td><td style="text-align: right">0.97</td></tr>
</tbody></table>
</div>
<p>불행히도 두 번째 검사도 양성으로 나옵니다.
조건부 독립성을 가정하여 베이즈 정리를 호출하는 데 필요한 확률을 계산해 봅시다.</p>
<p>$$\begin{aligned}
P(D_1 = 1, D_2 = 1 \mid H = 0)
&amp; = P(D_1 = 1 \mid H = 0) P(D_2 = 1 \mid H = 0)
=&amp; 0.0003, \
P(D_1 = 1, D_2 = 1 \mid H = 1)
&amp; = P(D_1 = 1 \mid H = 1) P(D_2 = 1 \mid H = 1)
=&amp; 0.98.
\end{aligned}
$$</p>
<p>이제 주변화를 적용하여 두 검사 모두 양성으로 나올 확률을 얻을 수 있습니다.</p>
<p>$$\begin{aligned}
&amp;P(D_1 = 1, D_2 = 1)\
&amp;= P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \
&amp;= P(D_1 = 1, D_2 = 1 \mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \mid H = 1)P(H=1)\
&amp;= 0.00176955.
\end{aligned}
$$</p>
<p>마지막으로, 두 검사가 모두 양성일 때 환자가 HIV에 걸렸을 확률은 다음과 같습니다.</p>
<p>$$P(H = 1 \mid D_1 = 1, D_2 = 1)
= \frac{P(D_1 = 1, D_2 = 1 \mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)}
= 0.8307.$$</p>
<p>즉, 두 번째 검사를 통해 모든 것이 좋지 않다는 훨씬 더 높은 확신을 얻을 수 있었습니다.
두 번째 검사가 첫 번째 검사보다 훨씬 덜 정확했음에도 불구하고,
여전히 우리의 추정치를 크게 개선했습니다.
두 검사가 서로 조건부 독립적이라는 가정은 더 정확한 추정치를 생성하는 능력에 결정적이었습니다.
동일한 검사를 두 번 실행하는 극단적인 경우를 생각해 보십시오.
이 상황에서는 두 번 모두 같은 결과를 기대하므로 동일한 검사를 다시 실행해도 추가적인 통찰력을 얻을 수 없습니다.
기민한 독자는 진단이 더 많은 특성(검사 결과)을 얻을수록
환자가 건강한지 여부를 결정하는 능력이 증가하는
눈에 잘 띄지 않는 분류기처럼 행동했다는 것을 눈치챘을 수 있습니다.</p>
<h2 id="기댓값-expectations"><a class="header" href="#기댓값-expectations">기댓값 (Expectations)</a></h2>
<p>종종 결정을 내리려면 개별 사건에 할당된 확률만 보는 것이 아니라
지침을 제공할 수 있는 유용한 집계로 구성해야 합니다.
예를 들어, 확률 변수가 연속적인 스칼라 값을 취할 때, 우리는 종종 <em>평균적으로</em> 어떤 값을 기대해야 하는지 아는 데 관심이 있습니다.
이 양을 공식적으로 *기댓값(expectation)*이라고 합니다.
투자를 하는 경우, 첫 번째 관심 수량은
모든 가능한 결과에 대해 평균을 낸(그리고 적절한 확률로 가중치를 둔) 기대 수익일 수 있습니다.
예를 들어, 50% 확률로 투자가 완전히 실패할 수 있고, 40% 확률로 2배 수익을 제공할 수 있으며, 10% 확률로 10배 수익을 제공할 수 있다고 가정해 봅시다.
기대 수익을 계산하기 위해, 모든 수익을 합산하고 각 수익이 발생할 확률을 곱합니다.
이것은 기댓값 $0.5 \cdot 0 + 0.4 \cdot 2 + 0.1 \cdot 10 = 1.8$을 산출합니다.
따라서 기대 수익은 1.8배입니다.</p>
<p>일반적으로 확률 변수 $X$의 <em>기댓값</em> (또는 평균)은 다음과 같이 정의됩니다.</p>
<p>$$E[X] = E_{x \sim P}[x] = \sum_{x} x P(X = x).$$</p>
<p>마찬가지로 밀도에 대해서는 $E[X] = \int x ;dp(x)$를 얻습니다.
때때로 우리는 $x$의 어떤 함수의 기댓값에 관심이 있습니다.
이러한 기댓값은 다음과 같이 계산할 수 있습니다.</p>
<p>$$E_{x \sim P}[f(x)] = \sum_x f(x) P(x) \textrm{ and } E_{x \sim P}[f(x)] = \int f(x) p(x) ;dx$$</p>
<p>이산 확률과 밀도에 대해 각각.
위의 투자 예제로 돌아가서, $f$는 수익과 관련된 <em>효용(utility)</em> (행복)일 수 있습니다.
행동 경제학자들은 오랫동안 사람들이
기준선 대비 1달러를 벌어서 얻는 효용보다
돈을 잃는 것에 더 큰 비효용을 연관시킨다는 점에 주목해 왔습니다.
게다가 돈의 가치는 하위 선형(sub-linear) 경향이 있습니다.
10만 달러를 소유하는 것과 0달러를 소유하는 것의 차이는
임대료를 내고, 잘 먹고, 양질의 의료 서비스를 즐기는 것과
노숙 생활을 겪는 것의 차이를 만들 수 있습니다.
반면 20만 달러 대 10만 달러 소유로 인한 이득은 덜 극적입니다.
이와 같은 추론은 "돈의 효용은 로그적이다"라는 진부한 표현의 동기가 됩니다.</p>
<p>만약 총 손실과 관련된 효용이 $-1$이고,
수익 $1$, $2$, $10$과 관련된 효용이 각각 $1$, $2$, $4$라면, 투자의 기대 행복은 $0.5 \cdot (-1) + 0.4 \cdot 2 + 0.1 \cdot 4 = 0.7$이 됩니다
(효용의 기대 손실 30%).
실제로 이것이 당신의 효용 함수라면, 돈을 은행에 보관하는 것이 가장 좋을 수 있습니다.</p>
<p>재무 결정의 경우, 투자가 얼마나 <em>위험한지</em> 측정하고 싶을 수도 있습니다.
여기서 우리는 기댓값뿐만 아니라 실제 값이 이 값에 비해 얼마나 *변동(vary)*하는 경향이 있는지에 관심을 갖습니다.
실제 값과 기댓값 차이의 기댓값을 취할 수는 없다는 점에 유의하십시오.
차이의 기댓값은 기댓값의 차이이기 때문입니다. 즉, $E[X - E[X]] = E[X] - E[E[X]] = 0$.
그러나 우리는 이 차이의 음이 아닌 함수의 기댓값을 볼 수 있습니다.
확률 변수의 *분산(variance)*은 <em>제곱</em> 차이의 기댓값을 보고 계산됩니다:</p>
<p>$$\textrm{Var}[X] = E\left[(X - E[X])^2\right] = E[X^2] - E[X]^2.$$</p>
<p>여기서 등식은 $(X - E[X])^2 = X^2 - 2 X E[X] + E[X]^2$를 확장하고
각 항에 대한 기댓값을 취함으로써 따릅니다.
분산의 제곱근은 *표준 편차(standard deviation)*라고 하는 또 다른 유용한 양입니다.
이것과 분산은 동일한 정보를 전달하지만(서로 계산 가능),
표준 편차는 확률 변수가 나타내는 원래 수량과 동일한 단위로 표현된다는
좋은 속성을 가지고 있습니다.</p>
<p>마지막으로, 확률 변수 함수의 분산은 유사하게 다음과 같이 정의됩니다.</p>
<p>$$\textrm{Var}<em>{x \sim P}[f(x)] = E</em>{x \sim P}[f^2(x)] - E_{x \sim P}[f(x)]^2.$$</p>
<p>투자 예제로 돌아가서, 이제 투자의 분산을 계산할 수 있습니다.
$0.5 \cdot 0 + 0.4 \cdot 2^2 + 0.1 \cdot 10^2 - 1.8^2 = 8.36$으로 주어집니다.
모든 의도와 목적에 있어 이것은 위험한 투자입니다.
수학적 관례에 따라 평균과 분산은 종종 $\mu$와 $\sigma^2$로 참조됩니다.
이것은 가우스 분포를 파라미터화하기 위해 사용할 때 특히 그렇습니다.</p>
<p><em>스칼라</em> 확률 변수에 대해 기댓값과 분산을 도입한 것과 같은 방식으로,
벡터 값 확률 변수에 대해서도 할 수 있습니다.
기댓값은 요소별로 적용할 수 있으므로 쉽습니다.
예를 들어, $\boldsymbol{\mu} \stackrel{\textrm{def}}{=} E_{\mathbf{x} \sim P}[\mathbf{x}]$는
좌표 $\mu_i = E_{\mathbf{x} \sim P}[x_i]$를 갖습니다.
*공분산(Covariances)*은 더 복잡합니다.
우리는 확률 변수와 그 평균의 차이의 *외적(outer product)*의 기댓값을 취하여 정의합니다:</p>
<p>$$\boldsymbol{\Sigma} \stackrel{\textrm{def}}{=} \textrm{Cov}<em>{\mathbf{x} \sim P}[\mathbf{x}] = E</em>{\mathbf{x} \sim P}\left[(\mathbf{x} - \boldsymbol{\mu}) (\mathbf{x} - \boldsymbol{\mu})^\top\right].$$</p>
<p>이 행렬 $\boldsymbol{\Sigma}$를 공분산 행렬이라고 합니다.
그 효과를 보는 쉬운 방법은 $\mathbf{x}$와 같은 크기의 어떤 벡터 $\mathbf{v}$를 고려하는 것입니다.
다음과 같습니다.</p>
<p>$$\mathbf{v}^\top \boldsymbol{\Sigma} \mathbf{v} = E_{\mathbf{x} \sim P}\left[\mathbf{v}^\top(\mathbf{x} - \boldsymbol{\mu}) (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{v}\right] = \textrm{Var}_{x \sim P}[\mathbf{v}^\top \mathbf{x}].$$</p>
<p>따라서 $\boldsymbol{\Sigma}$를 사용하면 간단한 행렬 곱셈으로 $\mathbf{x}$의 모든 선형 함수에 대한 분산을 계산할 수 있습니다.
비대각선 요소는 좌표가 얼마나 상관되어 있는지 알려줍니다: 0 값은 상관관계가 없음을 의미하며,
더 큰 양수 값은 더 강하게 상관되어 있음을 의미합니다.</p>
<h2 id="토론-4"><a class="header" href="#토론-4">토론</a></h2>
<p>머신러닝에는 불확실한 것들이 많습니다!
입력이 주어졌을 때 레이블의 값에 대해 불확실할 수 있습니다.
파라미터의 추정 값에 대해 불확실할 수 있습니다.
배포 시 도착하는 데이터가 훈련 데이터와 동일한 분포에서 온 것인지조차 불확실할 수 있습니다.</p>
<p>*우연적 불확실성(aleatoric uncertainty)*이란 문제에 내재되어 있고
관찰된 변수로 설명되지 않는 진정한 무작위성으로 인한 불확실성을 의미합니다.
*인식적 불확실성(epistemic uncertainty)*이란 모델 파라미터에 대한 불확실성, 즉 더 많은 데이터를 수집하여 줄일 수 있기를 바라는 종류의 불확실성을 의미합니다.
우리는 동전이 앞면이 나올 확률에 관해 인식적 불확실성을 가질 수 있지만,
이 확률을 알고 나더라도 미래의 던지기 결과에 대한 우연적 불확실성은 남습니다.
누군가가 공정한 동전을 던지는 것을 아무리 오래 지켜보더라도,
다음 던지기가 앞면이 될 것이라는 확신은 50% 이상도 이하도 아닐 것입니다.
이 용어들은 기계적 모델링에서 왔습니다
(<a href="https://en.wikipedia.org/wiki/Uncertainty_quantification">불확실성 정량화</a>의 이 측면에 대한 검토는 예: :citet:<code>Der-Kiureghian.Ditlevsen.2009</code> 참조).
그러나 이 용어들이 언어를 약간 남용하고 있다는 점에 주목할 가치가 있습니다.
<em>인식적</em>이라는 용어는 <em>지식</em>에 관한 모든 것을 지칭하므로, 철학적 의미에서 모든 불확실성은 인식적입니다.</p>
<p>알려지지 않은 확률 분포에서 데이터를 샘플링하면
데이터 생성 분포의 파라미터를 추정하는 데 사용할 수 있는 정보를 제공할 수 있음을 보았습니다.
그렇기는 하지만, 이것이 가능한 속도는 꽤 느릴 수 있습니다.
동전 던지기 예제(및 다른 많은 예제)에서
$1/\sqrt{n}$의 비율로 수렴하는 추정량을 설계하는 것보다 더 잘할 수는 없습니다.
여기서 $n$은 표본 크기(예: 던지기 횟수)입니다.
이는 10개에서 1000개의 관찰로 이동하면(보통 매우 달성 가능한 작업) 불확실성이 10배 감소하는 것을 볼 수 있는 반면,
다음 1000개의 관찰은 비교적 도움이 거의 안 되며 1.41배 감소만 제공한다는 것을 의미합니다.
이것은 머신러닝의 지속적인 특징입니다:
종종 쉬운 이득이 있지만, 추가적인 이득을 얻으려면 매우 많은 양의 데이터와
종종 그에 따른 엄청난 양의 계산이 필요합니다.
대규모 언어 모델에 대한 이 사실의 경험적 검토는 :citet:<code>Revels.Lubin.Papamarkou.2016</code>을 참조하십시오.</p>
<p>우리는 또한 통계적 모델링을 위한 언어와 도구를 연마했습니다.
그 과정에서 우리는 조건부 확률과 통계에서 가장 중요한 방정식 중 하나인 베이즈 정리에 대해 배웠습니다.
이것은 관찰 $B$가 파라미터 $A$의 선택과 얼마나 잘 일치하는지를 다루는 우도 항 $P(B \mid A)$와
애초에 $A$의 특정 선택이 얼마나 그럴듯한지를 지배하는 사전 확률 $P(A)$를 통해
데이터가 전달하는 정보를 분리하는 데 효과적인 도구입니다.
특히, 검사의 효능 <em>및</em> 질병 자체의 유병률(즉, 우리의 사전 확률)을 기반으로
진단에 확률을 할당하는 데 이 규칙을 어떻게 적용할 수 있는지 보았습니다.</p>
<p>마지막으로, 특정 확률 분포의 효과에 대한 첫 번째 비자명한 질문 세트,
즉 기댓값과 분산을 소개했습니다.
확률 분포에 대한 선형 및 2차 기댓값 외에도 더 많은 것이 있지만,
이 두 가지는 이미 분포의 가능한 동작에 대한 상당한 지식을 제공합니다.
예를 들어, <a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality">체비쇼프 부등식(Chebyshev's inequality)</a>은
$P(|X - \mu| \geq k \sigma) \leq 1/k^2$라고 명시합니다.
여기서 $\mu$는 기댓값, $\sigma^2$는 분포의 분산, $k &gt; 1$은 우리가 선택한 신뢰 파라미터입니다.
이것은 분포에서 추출한 값이 기댓값을 중심으로 한 $[-\sqrt{2} \sigma, \sqrt{2} \sigma]$ 구간 내에
적어도 50%의 확률로 존재함을 알려줍니다.</p>
<h2 id="연습-문제-6"><a class="header" href="#연습-문제-6">연습 문제</a></h2>
<ol>
<li>더 많은 데이터를 관찰하면 결과에 대한 불확실성의 양을 임의로 낮은 수준으로 줄일 수 있는 예를 제시하십시오.</li>
<li>더 많은 데이터를 관찰하면 불확실성의 양이 일정 지점까지만 줄어들고 그 이상은 줄어들지 않는 예를 제시하십시오. 왜 이런 경우가 발생하는지 그리고 이 지점이 어디에서 발생할 것으로 예상하는지 설명하십시오.</li>
<li>우리는 동전 던지기에 대한 평균으로의 수렴을 경험적으로 입증했습니다. $n$개의 샘플을 추출한 후 앞면을 볼 확률 추정치의 분산을 계산하십시오.
<ol>
<li>분산은 관찰 수에 따라 어떻게 확장됩니까?</li>
<li>체비쇼프 부등식을 사용하여 기댓값으로부터의 편차를 제한하십시오.</li>
<li>이것은 중심 극한 정리와 어떤 관련이 있습니까?</li>
</ol>
</li>
<li>평균이 0이고 분산이 1인 확률 분포에서 $m$개의 샘플 $x_i$를 추출한다고 가정합니다. 평균 $z_m \stackrel{\textrm{def}}{=} m^{-1} \sum_{i=1}^m x_i$를 계산합니다. 모든 $z_m$에 대해 독립적으로 체비쇼프 부등식을 적용할 수 있습니까? 왜 안 됩니까?</li>
<li>확률이 $P(\mathcal{A})$와 $P(\mathcal{B})$인 두 사건이 주어졌을 때, $P(\mathcal{A} \cup \mathcal{B})$와 $P(\mathcal{A} \cap \mathcal{B})$의 상한과 하한을 계산하십시오. 힌트: <a href="https://en.wikipedia.org/wiki/Venn_diagram">벤 다이어그램</a>을 사용하여 상황을 그래프로 나타내십시오.</li>
<li>일련의 확률 변수, 예를 들어 $A$, $B$, $C$가 있고, 여기서 $B$는 $A$에만 의존하고 $C$는 $B$에만 의존한다고 가정할 때, 결합 확률 $P(A, B, C)$를 단순화할 수 있습니까? 힌트: 이것은 <a href="https://en.wikipedia.org/wiki/Markov_chain">마르코프 체인</a>입니다.</li>
<li>:numref:<code>subsec_probability_hiv_app</code>에서 두 검사의 결과가 독립적이지 않다고 가정합니다. 특히 두 검사 중 하나라도 10%의 위양성률과 1%의 위음성률을 갖는다고 가정합니다. 즉, $P(D =1 \mid H=0) = 0.1$이고 $P(D = 0 \mid H=1) = 0.01$이라고 가정합니다. 또한 $H = 1$ (감염됨)인 경우 검사 결과는 조건부 독립적, 즉 $P(D_1, D_2 \mid H=1) = P(D_1 \mid H=1) P(D_2 \mid H=1)$이지만 건강한 환자의 경우 결과는 $P(D_1 = D_2 = 1 \mid H=0) = 0.02$를 통해 결합된다고 가정합니다.
<ol>
<li>지금까지 가진 정보를 바탕으로 $H=0$이 주어졌을 때 $D_1$과 $D_2$에 대한 결합 확률 표를 작성하십시오.</li>
<li>한 검사가 양성으로 나온 후 환자가 병에 걸렸을($H=1$) 확률을 도출하십시오. 이전과 동일한 기준 확률 $P(H=1) = 0.0015$를 가정할 수 있습니다.</li>
<li>두 검사 모두 양성으로 나온 후 환자가 병에 걸렸을($H=1$) 확률을 도출하십시오.</li>
</ol>
</li>
<li>당신이 투자 은행의 자산 관리자이고 투자할 주식 $s_i$를 선택할 수 있다고 가정합니다. 포트폴리오는 각 주식에 대한 가중치 $\alpha_i$의 합이 $1$이어야 합니다. 주식은 평균 수익 $\boldsymbol{\mu} = E_{\mathbf{s} \sim P}[\mathbf{s}]$과 공분산 $\boldsymbol{\Sigma} = \textrm{Cov}_{\mathbf{s} \sim P}[\mathbf{s}]$을 갖습니다.
<ol>
<li>주어진 포트폴리오 $\boldsymbol{\alpha}$에 대한 기대 수익을 계산하십시오.</li>
<li>포트폴리오의 수익을 극대화하고 싶다면 투자를 어떻게 선택해야 합니까?</li>
<li>포트폴리오의 <em>분산</em>을 계산하십시오.</li>
<li>분산을 상한으로 제한하면서 수익을 극대화하는 최적화 문제를 공식화하십시오. 이것은 노벨상을 수상한 <a href="https://en.wikipedia.org/wiki/Markowitz_model">마코비츠 포트폴리오</a> :cite:<code>Mangram.2013</code>입니다. 이를 해결하려면 2차 프로그래밍 솔버가 필요하며, 이는 이 책의 범위를 훨씬 벗어납니다.</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/36">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/37">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/198">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17971">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="문서화-documentation"><a class="header" href="#문서화-documentation">문서화 (Documentation)</a></h1>
<p>:begin_tab:<code>mxnet</code>
모든 MXNet 함수와 클래스를 일일이 소개할 수는 없지만(정보가 빠르게 구식이 될 수도 있음),
<a href="https://mxnet.apache.org/versions/1.8.0/api">API 문서</a>와
추가적인 <a href="https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/">튜토리얼</a> 및 예제들이
그러한 문서를 제공합니다.
이 섹션은 MXNet API를 탐색하는 방법에 대한 지침을 제공합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
모든 PyTorch 함수와 클래스를 일일이 소개할 수는 없지만(정보가 빠르게 구식이 될 수도 있음),
<a href="https://pytorch.org/docs/stable/index.html">API 문서</a>와
추가적인 <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">튜토리얼</a> 및 예제들이
그러한 문서를 제공합니다.
이 섹션은 PyTorch API를 탐색하는 방법에 대한 지침을 제공합니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
모든 TensorFlow 함수와 클래스를 일일이 소개할 수는 없지만(정보가 빠르게 구식이 될 수도 있음),
<a href="https://www.tensorflow.org/api_docs">API 문서</a>와
추가적인 <a href="https://www.tensorflow.org/tutorials">튜토리얼</a> 및 예제들이
그러한 문서를 제공합니다.
이 섹션은 TensorFlow API를 탐색하는 방법에 대한 지침을 제공합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
import jax
</code></pre>
<h2 id="모듈-내의-함수와-클래스-functions-and-classes-in-a-module"><a class="header" href="#모듈-내의-함수와-클래스-functions-and-classes-in-a-module">모듈 내의 함수와 클래스 (Functions and Classes in a Module)</a></h2>
<p>모듈에서 어떤 함수와 클래스를 호출할 수 있는지 알기 위해 <code>dir</code> 함수를 호출합니다. 예를 들어,
(<strong>난수를 생성하는 모듈의 모든 속성을 쿼리</strong>)할 수 있습니다:</p>
<pre><code class="language-{.python .input  n=1}">%%tab mxnet
print(dir(np.random))
</code></pre>
<pre><code class="language-{.python .input  n=1}">%%tab pytorch
print(dir(torch.distributions))
</code></pre>
<pre><code class="language-{.python .input  n=1}">%%tab tensorflow
print(dir(tf.random))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
print(dir(jax.random))
</code></pre>
<p>일반적으로 <code>__</code>(Python의 특수 객체)로 시작하고 끝나는 함수나 <code>_</code>(보통 내부 함수)로 시작하는 함수는 무시할 수 있습니다.
남은 함수나 속성 이름을 바탕으로, 이 모듈이 균등 분포(<code>uniform</code>), 정규 분포(<code>normal</code>), 다항 분포(<code>multinomial</code>)로부터의 샘플링을 포함하여
난수를 생성하는 다양한 방법을 제공한다는 것을 짐작할 수 있습니다.</p>
<h2 id="특정-함수와-클래스-specific-functions-and-classes"><a class="header" href="#특정-함수와-클래스-specific-functions-and-classes">특정 함수와 클래스 (Specific Functions and Classes)</a></h2>
<p>주어진 함수나 클래스를 사용하는 방법에 대한 구체적인 지침을 보려면 <code>help</code> 함수를 호출할 수 있습니다. 예시로,
[<strong>텐서의 <code>ones</code> 함수 사용법을 탐색</strong>]해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
help(np.ones)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
help(torch.ones)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
help(tf.ones)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
help(jax.numpy.ones)
</code></pre>
<p>문서에서 <code>ones</code> 함수가 지정된 모양으로 새 텐서를 생성하고
모든 요소를 값 1로 설정한다는 것을 볼 수 있습니다.
가능할 때마다 해석을 확인하기 위해 (<strong>빠른 테스트를 실행</strong>)해야 합니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.ones(4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.ones(4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.ones(4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jax.numpy.ones(4)
</code></pre>
<p>Jupyter 노트북에서는 <code>?</code>를 사용하여 문서를 다른 창에 표시할 수 있습니다. 예를 들어, <code>list?</code>는 <code>help(list)</code>와 거의 동일한 내용을 생성하여 새 브라우저 창에 표시합니다. 또한 <code>list??</code>와 같이 물음표를 두 개 사용하면 함수를 구현하는 Python 코드도 표시됩니다.</p>
<p>공식 문서는 이 책의 범위를 넘어서는 많은 설명과 예제를 제공합니다.
우리는 포괄적인 범위보다는 실실적인 문제로 빠르게 시작할 수 있게 해주는
중요한 사용 사례를 강조합니다.
또한 라이브러리의 소스 코드를 공부하여 고품질의 프로덕션 코드 구현 사례를 확인해 볼 것을 권장합니다.
이렇게 함으로써 여러분은 과학자일 뿐만 아니라 더 나은 엔지니어가 될 것입니다.</p>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/38">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/39">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/199">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17972">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="회귀를-위한-선형-신경망-linear-neural-networks-for-regression"><a class="header" href="#회귀를-위한-선형-신경망-linear-neural-networks-for-regression">회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a></h1>
<p>:label:<code>chap_regression</code></p>
<p>신경망을 깊게 만드는 것을 걱정하기 전에, 입력이 출력에 직접 연결되는 몇 가지 얕은 신경망을 구현해 보는 것이 도움이 될 것입니다. 이는 몇 가지 이유로 중요합니다.
첫째, 복잡한 아키텍처에 주의를 빼앗기기보다 출력 레이어 파라미터화, 데이터 처리, 손실 함수 지정, 모델 훈련을 포함한 신경망 훈련의 기초에 집중할 수 있습니다.
둘째, 이 클래스의 얕은 네트워크는 선형 및 소프트맥스 회귀를 포함하여 통계적 예측의 많은 고전적인 방법을 포괄하는 선형 모델 세트를 구성합니다.
이러한 고전적인 도구들을 이해하는 것은 많은 맥락에서 널리 사용되고, 더 화려한 아키텍처의 사용을 정당화할 때 베이스라인으로 자주 사용해야 하기 때문에 매우 중요합니다.
이 장에서는 선형 회귀에 좁게 초점을 맞출 것이며, 다음 장에서는 분류를 위한 선형 신경망을 개발하여 우리의 모델링 레퍼토리를 확장할 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

linear-regression
oo-design
synthetic-regression-data
linear-regression-scratch
linear-regression-concise
generalization
weight-decay
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="선형-회귀-linear-regression"><a class="header" href="#선형-회귀-linear-regression">선형 회귀 (Linear Regression)</a></h1>
<p>:label:<code>sec_linear_regression</code></p>
<p><em>회귀(Regression)</em> 문제는 수치 값을 예측하고 싶을 때마다 나타납니다.
일반적인 예로는 주택, 주식 등의 가격 예측, 병원 환자의 입원 기간 예측, 소매 판매 수요 예측 등이 있습니다.
모든 예측 문제가 고전적인 회귀 문제는 아닙니다.
나중에 목표가 여러 범주 중 멤버십을 예측하는 것인 분류 문제를 소개할 것입니다.</p>
<p>실행 예제로, 주택의 면적(제곱피트)과 연식(년)을 기반으로 주택 가격(달러)을 추정하고 싶다고 가정해 봅시다.
주택 가격을 예측하는 모델을 개발하려면 각 주택의 판매 가격, 면적, 연식을 포함한 데이터를 확보해야 합니다.
머신러닝 용어에서 이 데이터셋을 <em>훈련 데이터셋(training dataset)</em> 또는 *훈련 세트(training set)*라고 하며,
각 행(한 번의 판매에 해당하는 데이터를 포함)을 <em>예제(example)</em> (또는 <em>데이터 포인트(data point)</em>, <em>인스턴스(instance)</em>, <em>샘플(sample)</em>)라고 합니다.
우리가 예측하려는 것(가격)을 <em>레이블(label)</em> (또는 <em>타겟(target)</em>)이라고 합니다.
예측의 기반이 되는 변수(연식 및 면적)를 <em>특성(features)</em> (또는 <em>공변량(covariates)</em>)이라고 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np
import time
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
import numpy as np
import time
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
import numpy as np
import time
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
from jax import numpy as jnp
import math
import time
</code></pre>
<h2 id="기초-basics"><a class="header" href="#기초-basics">기초 (Basics)</a></h2>
<p>*선형 회귀(Linear regression)*는 회귀 문제를 다루기 위한 표준 도구 중 가장 단순하면서도 가장 인기가 있습니다.
19세기 초로 거슬러 올라가는 :cite:<code>Legendre.1805,Gauss.1809</code> 선형 회귀는 몇 가지 간단한 가정에서 출발합니다.
첫째, 우리는 특성 $\mathbf{x}$와 타겟 $y$ 사이의 관계가 대략 선형이라고 가정합니다.
즉, 조건부 평균 $E[Y \mid X=\mathbf{x}]$가 특성 $\mathbf{x}$의 가중 합으로 표현될 수 있다는 것입니다.
이 설정은 관찰 노이즈 때문에 타겟 값이 예상 값에서 여전히 벗어날 수 있음을 허용합니다.
다음으로, 우리는 그러한 노이즈가 가우스 분포를 따르며 잘 작동한다고 가정을 부여할 수 있습니다.
일반적으로 우리는 데이터셋의 예제 수를 나타내기 위해 $n$을 사용합니다.
샘플과 타겟을 열거하기 위해 위첨자를 사용하고, 좌표를 인덱싱하기 위해 아래첨자를 사용합니다.
더 구체적으로, $\mathbf{x}^{(i)}$는 $i^{\textrm{th}}$ 샘플을 나타내고 $x_j^{(i)}$는 그 $j^{\textrm{th}}$ 좌표를 나타냅니다.</p>
<h3 id="모델-model"><a class="header" href="#모델-model">모델 (Model)</a></h3>
<p>:label:<code>subsec_linear_model</code></p>
<p>모든 솔루션의 핵심은 특성을 타겟의 추정치로 변환하는 방법을 설명하는 모델입니다.
선형성 가정은 타겟(가격)의 예상 값이 특성(면적 및 연식)의 가중 합으로 표현될 수 있음을 의미합니다:</p>
<p>$$\textrm{가격} = w_{\textrm{area}} \cdot \textrm{면적} + w_{\textrm{age}} \cdot \textrm{연식} + b.$$
:eqlabel:<code>eq_price-area</code></p>
<p>여기서 $w_{\textrm{area}}$와 $w_{\textrm{age}}$는 *가중치(weights)*라고 불리고, $b$는 <em>편향(bias)</em> (또는 <em>오프셋</em> 또는 <em>절편</em>)이라고 불립니다.
가중치는 각 특성이 우리의 예측에 미치는 영향을 결정합니다.
편향은 모든 특성이 0일 때의 추정치 값을 결정합니다.
면적이 정확히 0인 신축 주택은 결코 볼 수 없겠지만, (원점을 지나는 직선으로 제한하기보다) 우리 특성의 모든 선형 함수를 표현할 수 있게 해주기 때문에 여전히 편향이 필요합니다.
엄밀히 말하면, :eqref:<code>eq_price-area</code>는 입력 특성의 *아핀 변환(affine transformation)*으로, 가중 합을 통한 특성의 <em>선형 변환</em>과 추가된 편향을 통한 *평행 이동(translation)*이 결합된 특징이 있습니다.
데이터셋이 주어졌을 때, 우리의 목표는 평균적으로 모델의 예측이 데이터에서 관찰된 실제 가격과 가능한 한 밀접하게 일치하도록 하는 가중치 $\mathbf{w}$와 편향 $b$를 선택하는 것입니다.</p>
<p>단 몇 가지 특성만 있는 데이터셋에 집중하는 것이 일반적인 학문 분야에서는 :eqref:<code>eq_price-area</code>와 같이 모델을 긴 형식으로 명시적으로 표현하는 것이 일반적입니다.
머신러닝에서는 보통 고차원 데이터셋으로 작업하며, 이때는 간결한 선형 대수 표기법을 사용하는 것이 더 편리합니다.
입력이 $d$개의 특성으로 구성될 때, 각 특성에 (1에서 $d$ 사이의) 인덱스를 할당하고 우리의 예측 $\hat{y}$ (일반적으로 "햇(hat)" 기호는 추정치를 나타냄)를 다음과 같이 표현할 수 있습니다.</p>
<p>$$\hat{y} = w_1  x_1 + \cdots + w_d  x_d + b.$$</p>
<p>모든 특성을 벡터 $\mathbf{x} \in \mathbb{R}^d$로, 모든 가중치를 벡터 $\mathbf{w} \in \mathbb{R}^d$로 모으면, $\mathbf{w}$와 $\mathbf{x}$ 사이의 내적을 통해 우리 모델을 간결하게 표현할 수 있습니다:</p>
<p>$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b.$$
:eqlabel:<code>eq_linreg-y</code></p>
<p>:eqref:<code>eq_linreg-y</code>에서 벡터 $\mathbf{x}$는 단일 예제의 특성에 해당합니다.
우리는 $n$개 예제로 구성된 전체 데이터셋의 특성을 <em>설계 행렬(design matrix)</em> $\mathbf{X} \in \mathbb{R}^{n \times d}$를 통해 참조하는 것이 편리할 때가 많습니다.
여기서 $\mathbf{X}$는 모든 예제에 대해 하나의 행을, 모든 특성에 대해 하나의 열을 포함합니다.
특성 모음 $\mathbf{X}$에 대해, 예측 $\hat{\mathbf{y}} \in \mathbb{R}^n$은 행렬-벡터 곱을 통해 표현될 수 있습니다:</p>
<p>$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,$$
:eqlabel:<code>eq_linreg-y-vec</code></p>
<p>여기서 합산 중에 브로드캐스팅(:numref:<code>subsec_broadcasting</code>)이 적용됩니다.
훈련 데이터셋 $\mathbf{X}$의 특성과 해당 (알려진) 레이블 $\mathbf{y}$가 주어졌을 때, 선형 회귀의 목표는 $\mathbf{X}$와 동일한 분포에서 샘플링된 새로운 데이터 예제의 특성이 주어졌을 때, 새로운 예제의 레이블이 (기대치에서) 가장 작은 오차로 예측되도록 가중치 벡터 $\mathbf{w}$와 편향 항 $b$를 찾는 것입니다.</p>
<p>$\mathbf{x}$가 주어졌을 때 $y$를 예측하기 위한 최선의 모델이 선형이라고 믿더라도, 모든 $1 \leq i \leq n$에 대해 $y^{(i)}$가 $\mathbf{w}^\top \mathbf{x}^{(i)}+b$와 정확히 일치하는 $n$개 예제의 실제 데이터셋을 찾을 것으로 기대하지는 않습니다.
예를 들어, 특성 $\mathbf{X}$와 레이블 $\mathbf{y}$를 관찰하기 위해 어떤 도구를 사용하든 소량의 측정 오차가 있을 수 있습니다.
따라서 기본 관계가 선형이라고 확신하더라도 그러한 오차를 설명하기 위해 노이즈 항을 통합할 것입니다.</p>
<p>최상의 <em>파라미터</em> (또는 <em>모델 파라미터</em>) $\mathbf{w}$와 $b$를 탐색하기 전에, 두 가지가 더 필요합니다:
(i) 주어진 모델의 품질에 대한 척도;
그리고 (ii) 품질을 개선하기 위해 모델을 업데이트하는 절차.</p>
<h3 id="손실-함수-loss-function"><a class="header" href="#손실-함수-loss-function">손실 함수 (Loss Function)</a></h3>
<p>:label:<code>subsec_linear-regression-loss-function</code></p>
<p>당연하게도 모델을 데이터에 맞추려면 <em>적합성(fitness)</em> (또는 동등하게 <em>부적합성</em>)에 대한 어떤 척도에 합의해야 합니다.
*손실 함수(Loss functions)*는 타겟의 <em>실제</em> 값과 <em>예측</em> 값 사이의 거리를 정량화합니다.
손실은 일반적으로 값이 작을수록 더 좋고 완벽한 예측은 0의 손실을 입는 음이 아닌 숫자입니다.
회귀 문제에서 가장 일반적인 손실 함수는 제곱 오차(squared error)입니다.
예제 $i$에 대한 우리의 예측이 $\hat{y}^{(i)}$이고 해당 실제 레이블이 $y^{(i)}$일 때, <em>제곱 오차</em>는 다음과 같이 주어집니다:</p>
<p>$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$
:eqlabel:<code>eq_mse</code></p>
<p>상수 $\frac{1}{2}$은 실제적인 차이를 만들지 않지만, 손실의 도함수를 취할 때 상쇄되기 때문에 표기법상 편리한 것으로 밝혀졌습니다.
훈련 데이터셋은 우리에게 주어지며 우리의 통제 밖이기 때문에, 경험적 오차는 모델 파라미터의 함수일 뿐입니다.
:numref:<code>fig_fit_linreg</code>에서는 1차원 입력 문제에서 선형 회귀 모델의 적합도를 시각화합니다.</p>
<p><img src="chapter_linear-regression/../img/fit-linreg.svg" alt="1차원 데이터에 선형 회귀 모델 맞추기." />
:label:<code>fig_fit_linreg</code></p>
<p>추정치 $\hat{y}^{(i)}$와 타겟 $y^{(i)}$ 사이의 큰 차이는 이차 형식(quadratic form)으로 인해 손실에 훨씬 더 큰 기여를 한다는 점에 유의하십시오.
(이 이차성은 양날의 검이 될 수 있습니다. 모델이 큰 오차를 피하도록 장려하는 반면, 비정상적인 데이터에 과도하게 민감해질 수도 있습니다.)
$n$개 예제로 구성된 전체 데이터셋에서 모델의 품질을 측정하기 위해, 우리는 단순히 훈련 세트에서의 손실을 평균(또는 동등하게 합산)합니다:</p>
<p>$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$</p>
<p>모델을 훈련할 때, 우리는 모든 훈련 예제에 걸쳐 총 손실을 최소화하는 파라미터($\mathbf{w}^<em>, b^</em>$)를 찾습니다:</p>
<p>$$\mathbf{w}^<em>, b^</em> = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$$$</p>
<h3 id="해석적-해-analytic-solution"><a class="header" href="#해석적-해-analytic-solution">해석적 해 (Analytic Solution)</a></h3>
<p>우리가 다룰 대부분의 모델과 달리, 선형 회귀는 놀라울 정도로 쉬운 최적화 문제를 제시합니다.
특히, 다음과 같이 간단한 공식을 적용하여 (훈련 데이터에서 평가된) 최적의 파라미터를 해석적으로 찾을 수 있습니다.
먼저, 모두 1로 구성된 열을 설계 행렬에 추가하여 편향 $b$를 파라미터 $\mathbf{w}$에 포함시킬 수 있습니다.
그러면 우리의 예측 문제는 $|\mathbf{y} - \mathbf{X}\mathbf{w}|^2$를 최소화하는 것이 됩니다.
설계 행렬 $\mathbf{X}$가 풀 랭크(full rank)인 한 (어떤 특성도 다른 특성에 선형적으로 종속되지 않음), 손실 표면에는 임계점이 단 하나만 존재하며 이는 전체 도메인에서의 손실 최소값에 해당합니다.
$\mathbf{w}$에 대해 손실의 도함수를 취하고 0으로 설정하면 다음을 얻습니다:</p>
<p>$$\begin{aligned}
\partial_{\mathbf{w}} |\mathbf{y} - \mathbf{X}\mathbf{w}|^2 =
2 \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}) = 0
\textrm{ 이고 따라서 }
\mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{X} \mathbf{w}.
\end{aligned}$$</p>
<p>$\mathbf{w}$에 대해 풀면 최적화 문제에 대한 최적의 해를 얻을 수 있습니다.
이 해</p>
<p>$$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}$$</p>
<p>는 행렬 $\mathbf X^\top \mathbf X$가 가역적일 때, 즉 설계 행렬의 열이 선형 독립일 때만 유일할 것입니다 :cite:<code>Golub.Van-Loan.1996</code>.</p>
<p>선형 회귀와 같은 단순한 문제는 해석적 해를 허용할 수 있지만, 그러한 행운에 익숙해져서는 안 됩니다.
해석적 해는 멋진 수학적 분석을 가능하게 하지만, 해석적 해의 요구 사항은 너무 제한적이어서 딥러닝의 거의 모든 흥미로운 측면을 제외하게 될 것입니다.</p>
<h3 id="미니배치-확률적-경사-하강법-minibatch-stochastic-gradient-descent"><a class="header" href="#미니배치-확률적-경사-하강법-minibatch-stochastic-gradient-descent">미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></h3>
<p>다행히도 모델을 해석적으로 풀 수 없는 경우에도 실제로는 모델을 효과적으로 훈련할 수 있는 경우가 많습니다.
게다가 많은 작업에서 최적화하기 어려운 모델들이 훨씬 더 뛰어나다는 것이 밝혀져서, 그것들을 어떻게 훈련할지 알아내는 것은 충분히 수고할 가치가 있게 됩니다.</p>
<p>거의 모든 딥러닝 모델을 최적화하는 핵심 기술이자 이 책 전체에서 계속해서 호출할 기술은, 손실 함수를 점진적으로 낮추는 방향으로 파라미터를 업데이트하여 오차를 반복적으로 줄이는 것입니다.
이 알고리즘을 *경사 하강법(gradient descent)*이라고 합니다.</p>
<p>경사 하강법의 가장 순진한 적용은 데이터셋의 모든 단일 예제에서 계산된 손실의 평균인 손실 함수의 도함수를 취하는 것입니다.
실제로 이는 매우 느릴 수 있습니다. 업데이트 단계가 매우 강력하더라도 단일 업데이트를 수행하기 전에 전체 데이터셋을 거쳐야 하기 때문입니다 :cite:<code>Liu.Nocedal.1989</code>.
설상가상으로 훈련 데이터에 중복이 많으면 전체 업데이트의 이점이 제한적입니다.</p>
<p>다른 극단은 한 번에 하나의 예제만 고려하고 한 번에 하나의 관찰을 기반으로 업데이트 단계를 밟는 것입니다.
결과 알고리즘인 <em>확률적 경사 하강법</em>(Stochastic Gradient Descent, SGD)은 대규모 데이터셋에 대해서도 효과적인 전략이 될 수 있습니다 :cite:<code>Bottou.2010</code>.
불행히도 SGD는 계산적 및 통계적으로 모두 단점이 있습니다.
한 가지 문제는 프로세서가 메인 메모리에서 프로세서 캐시로 데이터를 이동하는 것보다 숫자를 곱하고 더하는 속도가 훨씬 빠르다는 사실에서 발생합니다.
행렬-벡터 곱셈을 수행하는 것이 그에 상응하는 수의 벡터-벡터 연산을 수행하는 것보다 최대 한 자릿수 더 효율적입니다.
이는 전체 배치에 비해 한 번에 한 샘플을 처리하는 데 훨씬 더 오랜 시간이 걸릴 수 있음을 의미합니다.
두 번째 문제는 배치 정규화(batch normalization, :numref:<code>sec_batch_norm</code>에서 설명)와 같은 일부 레이어는 한 번에 두 개 이상의 관찰에 액세스할 수 있을 때만 잘 작동한다는 것입니다.</p>
<p>두 문제에 대한 해결책은 중간 전략을 선택하는 것입니다. 전체 배치나 단일 샘플만 취하는 대신, 관찰의 <em>미니배치</em>를 취하는 것입니다 :cite:<code>Li.Zhang.Chen.ea.2014</code>.
해당 미니배치 크기의 구체적인 선택은 메모리 양, 가속기 수, 레이어 선택, 총 데이터셋 크기 등 많은 요소에 달려 있습니다.
그럼에도 불구하고 32에서 256 사이, 가급적이면 2의 거듭제곱 배수가 좋은 시작입니다.
이것은 우리를 <em>미니배치 확률적 경사 하강법</em>으로 인도합니다.</p>
<p>가장 기본적인 형태에서, 각 반복 $t$마다 먼저 고정된 수 $|\mathcal{B}|$의 훈련 예제로 구성된 미니배치 $\mathcal{B}_t$를 무작위로 샘플링합니다.
그런 다음 모델 파라미터에 대한 미니배치의 평균 손실의 도함수(기울기)를 계산합니다.
마지막으로 기울기에 *학습률(learning rate)*이라 불리는 미리 정해진 작은 양수 값 $\eta$를 곱하고, 현재 파라미터 값에서 결과 항을 뺍니다.
업데이트를 다음과 같이 표현할 수 있습니다:</p>
<p>$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}<em>t} \partial</em>{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$$</p>
<p>요약하자면, 미니배치 SGD는 다음과 같이 진행됩니다:
(i) 모델 파라미터의 값을 초기화합니다(일반적으로 무작위로);
(ii) 데이터에서 무작위 미니배치를 반복적으로 샘플링하여 음의 기울기 방향으로 파라미터를 업데이트합니다.
이차 손실과 아핀 변환의 경우, 이는 닫힌 형식(closed-form) 확장을 갖습니다:</p>
<p>$$\begin{aligned} \mathbf{w} &amp; \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}<em>t} \partial</em>{\mathbf{w}} l^{(i)}(\mathbf{w}, b) &amp;&amp; = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}<em>t} \mathbf{x}^{(i)} (\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)})\ b &amp;\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum</em>{i \in \mathcal{B}<em>t} \partial_b l^{(i)}(\mathbf{w}, b) &amp;&amp;  = b - \frac{\eta}{|\mathcal{B}|} \sum</em>{i \in \mathcal{B}_t} (\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}). \end{aligned}$$
:eqlabel:<code>eq_linreg_batch_update</code></p>
<p>미니배치 $\mathcal{B}$를 선택하므로 그 크기 $|\mathcal{B}|$로 정규화해야 합니다.
종종 미니배치 크기와 학습률은 사용자가 정의합니다.
훈련 루프에서 업데이트되지 않는 이러한 조정 가능한 파라미터를 *하이퍼파라미터(hyperparameters)*라고 합니다.
베이지안 최적화 :cite:<code>Frazier.2018</code>와 같은 여러 기술을 통해 자동으로 튜닝될 수 있습니다.
결국 솔루션의 품질은 일반적으로 별도의 <em>검증 데이터셋(validation dataset)</em> (또는 <em>검증 세트</em>)에서 평가됩니다.</p>
<p>미리 정해진 반복 횟수 동안 (또는 다른 중지 기준이 충족될 때까지) 훈련한 후, $\hat{\mathbf{w}}, \hat{b}$로 표시되는 추정된 모델 파라미터를 기록합니다.
우리의 함수가 진정으로 선형이고 노이즈가 없더라도, 이러한 파라미터는 손실의 정확한 최소값이 아니며 심지어 결정론적이지도 않을 것임에 유의하십시오.
알고리즘이 최소값을 향해 천천히 수렴하지만 유한한 단계 내에서 정확하게 찾지는 못할 것이기 때문입니다.
게다가 파라미터를 업데이트하는 데 사용되는 미니배치 $\mathcal{B}$는 무작위로 선택됩니다.
이것은 결정론을 깨뜨립니다.</p>
<p>선형 회귀는 전역 최소값이 있는 학습 문제입니다
($\mathbf{X}$가 풀 랭크일 때마다, 또는 동등하게 $\mathbf{X}^\top \mathbf{X}$가 가역적일 때마다).
그러나 심층 네트워크의 손실 표면에는 많은 안장점과 최소값이 포함되어 있습니다.
다행히도 우리는 일반적으로 정확한 파라미터 세트를 찾는 데 관심이 있는 것이 아니라, 정확한 예측(따라서 낮은 손실)으로 이어지는 파라미터 세트만 찾으면 됩니다.
실제로 딥러닝 실무자들은 <em>훈련 세트에서</em> 손실을 최소화하는 파라미터를 찾는 데 거의 어려움을 겪지 않습니다 :cite:<code>Izmailov.Podoprikhin.Garipov.ea.2018,Frankle.Carbin.2018</code>.
더 어려운 과제는 이전에 본 적 없는 데이터에서 정확한 예측으로 이어지는 파라미터를 찾는 것이며, 이 도전을 *일반화(generalization)*라고 합니다.
우리는 책 전반에 걸쳐 이러한 주제로 돌아올 것입니다.</p>
<h3 id="예측-predictions"><a class="header" href="#예측-predictions">예측 (Predictions)</a></h3>
<p>모델 $\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}$가 주어지면, 이제 새로운 예제에 대해 <em>예측</em>을 할 수 있습니다.
예를 들어, 면적 $x_1$과 연식 $x_2$가 주어졌을 때 이전에 본 적 없는 주택의 판매 가격을 예측하는 것입니다.
딥러닝 실무자들은 예측 단계를 *추론(inference)*이라고 부르곤 하지만, 이는 약간 잘못된 명칭입니다.
<em>추론</em>은 파라미터의 값과 보지 못한 인스턴스에 대한 가능성 있는 레이블을 모두 포함하여 증거를 바탕으로 도달한 모든 결론을 광범위하게 지칭하기 때문입니다.
통계 문헌에서 <em>추론</em>은 파라미터 추론을 더 자주 의미하며, 이러한 용어의 중복 사용은 딥러닝 실무자들이 통계학자들과 대화할 때 불필요한 혼란을 야기합니다.
다음에서는 가능한 한 *예측(prediction)*을 고수할 것입니다.</p>
<h2 id="속도를-위한-벡터화-vectorization-for-speed"><a class="header" href="#속도를-위한-벡터화-vectorization-for-speed">속도를 위한 벡터화 (Vectorization for Speed)</a></h2>
<p>모델을 훈련할 때, 우리는 일반적으로 예제의 전체 미니배치를 동시에 처리하기를 원합니다.
이를 효율적으로 수행하려면 (<strong>Python에서 비용이 많이 드는 for-루프를 작성하기보다 계산을 벡터화하고 빠른 선형 대수 라이브러리를 활용해야 합니다.</strong>)</p>
<p>이것이 왜 그렇게 중요한지 알아보기 위해, (<strong>벡터를 더하는 두 가지 방법을 고려해 봅시다.</strong>)
시작하기 위해 모두 1을 포함하는 10,000차원 벡터 두 개를 인스턴스화합니다.
첫 번째 방법에서는 Python for-루프로 벡터를 반복합니다.
두 번째 방법에서는 <code>+</code>에 대한 단일 호출에 의존합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
n = 10000
a = d2l.ones(n)
b = d2l.ones(n)
</code></pre>
<p>이제 작업 부하를 벤치마킹할 수 있습니다.
먼저, [<strong>for-루프를 사용하여 한 번에 한 좌표씩 더합니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
c = d2l.zeros(n)
t = time.time()
for i in range(n):
    c[i] = a[i] + b[i]
f'{time.time() - t:.5f} sec'
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
c = tf.Variable(d2l.zeros(n))
t = time.time()
for i in range(n):
    c[i].assign(a[i] + b[i])
f'{time.time() - t:.5f} sec'
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# JAX 배열은 불변이므로 한 번 생성되면 내용을 변경할 수 없습니다. 
# 개별 요소를 업데이트하기 위해 JAX는 업데이트된 사본을 반환하는 
# 인덱싱된 업데이트 구문을 제공합니다.
c = d2l.zeros(n)
t = time.time()
for i in range(n):
    c = c.at[i].set(a[i] + b[i])
f'{time.time() - t:.5f} sec'
</code></pre>
<p>(<strong>대안으로, 재정의된 <code>+</code> 연산자에 의존하여 요소별 합계를 계산합니다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab all
t = time.time()
d = a + b
f'{time.time() - t:.5f} sec'
</code></pre>
<p>두 번째 방법이 첫 번째 방법보다 획기적으로 빠릅니다.
코드를 벡터화하면 종종 수십 배의 속도 향상을 얻을 수 있습니다.
게다가 우리는 수학의 더 많은 부분을 라이브러리에 밀어 넣어 스스로 많은 계산을 작성할 필요가 없게 함으로써, 오차 가능성을 줄이고 코드의 이식성을 높입니다.</p>
<h2 id="정규-분포와-제곱-손실-the-normal-distribution-and-squared-loss"><a class="header" href="#정규-분포와-제곱-손실-the-normal-distribution-and-squared-loss">정규 분포와 제곱 손실 (The Normal Distribution and Squared Loss)</a></h2>
<p>:label:<code>subsec_normal_distribution_and_squared_loss</code></p>
<p>지금까지 우리는 제곱 손실 목적 함수에 대해 상당히 기능적인 동기를 부여했습니다:
기본 패턴이 진정으로 선형일 때마다 최적의 파라미터는 조건부 기대치 $E[Y\mid X]$를 반환하며, 손실은 이상값에 대해 큰 페널티를 할당한다는 것입니다.
우리는 또한 노이즈 분포에 대한 확률적 가정을 함으로써 제곱 손실 목적 함수에 대해 더 공식적인 동기를 제공할 수 있습니다.</p>
<p>선형 회귀는 19세기 전환기에 발명되었습니다.
가우스(Gauss)나 르장드르(Legendre) 중 누가 먼저 아이디어를 냈는지에 대해서는 오랫동안 논쟁이 있어 왔지만, 정규 분포(<em>가우시안</em>이라고도 함)를 발견한 것도 가우스였습니다.
정규 분포와 제곱 손실을 사용한 선형 회귀는 공통 조상 이상의 깊은 연결을 공유하는 것으로 밝혀졌습니다.</p>
<p>시작하기 위해, 평균이 $\mu$이고 분산이 $\sigma^2$ (표준 편차 $\sigma$)인 정규 분포가 다음과 같이 주어진다는 것을 상기하십시오.</p>
<p>$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$$</p>
<p>아래에서 [<strong>정규 분포를 계산하는 함수를 정의합니다</strong>].</p>
<pre><code class="language-{.python .input}">%%tab all
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    if tab.selected('jax'):
        return p * jnp.exp(-0.5 * (x - mu)**2 / sigma**2)
    if tab.selected('pytorch', 'mxnet', 'tensorflow'):
        return p * np.exp(-0.5 * (x - mu)**2 / sigma**2)
</code></pre>
<p>이제 (<strong>정규 분포를 시각화</strong>)할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
# 시각화를 위해 NumPy 다시 사용
x = np.arange(-7, 7, 0.01)

# 평균 및 표준 편차 쌍
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x.asnumpy(), [normal(x, mu, sigma).asnumpy() for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
</code></pre>
<pre><code class="language-{.python .input}">
%%tab pytorch, tensorflow, jax
if tab.selected('jax'):
    # 시각화를 위해 JAX NumPy 사용
    x = jnp.arange(-7, 7, 0.01)
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    # 시각화를 위해 NumPy 다시 사용
    x = np.arange(-7, 7, 0.01)

# 평균 및 표준 편차 쌍
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
</code></pre>
<p>평균을 변경하면 $x$축을 따라 이동하고, 분산을 늘리면 분포가 퍼져서 정점이 낮아진다는 점에 유의하십시오.</p>
<p>제곱 손실을 사용한 선형 회귀에 동기를 부여하는 한 가지 방법은 관찰이 노이즈가 섞인 측정값에서 발생한다고 가정하는 것입니다. 여기서 노이즈 $\epsilon$은 정규 분포 $\mathcal{N}(0, \sigma^2)$를 따릅니다:</p>
<p>$$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \textrm{ 여기서 } \epsilon \sim \mathcal{N}(0, \sigma^2).$$$</p>
<p>따라서 주어진 $\mathbf{x}$에 대해 특정 $y$를 볼 *우도(likelihood)*를 다음과 같이 작성할 수 있습니다.</p>
<p>$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).$$$</p>
<p>이와 같이 우도는 인수 분해됩니다. *최대 우도 원칙(principle of maximum likelihood)*에 따라, 파라미터 $\mathbf{w}$와 $b$의 최적 값은 전체 데이터셋의 <em>우도</em>를 최대화하는 값입니다:</p>
<p>$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)}).$$$</p>
<p>모든 쌍 $(\mathbf{x}^{(i)}, y^{(i)})$이 서로 독립적으로 추출되었으므로 등식이 성립합니다.
최대 우도 원칙에 따라 선택된 추정량을 *최대 우도 추정량(maximum likelihood estimators)*이라고 합니다.
많은 지수 함수의 곱을 최대화하는 것이 어려워 보일 수 있지만, 목적 함수를 바꾸지 않고 대신 우도의 로그를 최대화함으로써 상황을 크게 단순화할 수 있습니다.
역사적인 이유로 최적화는 최대화보다 최소화로 더 자주 표현됩니다.
따라서 아무것도 바꾸지 않고 *음의 로그 우도(negative log-likelihood)*를 <em>최소화</em>할 수 있으며, 이를 다음과 같이 표현할 수 있습니다:</p>
<p>$$- \log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.$$</p>
<p>$\sigma$가 고정되어 있다고 가정하면 첫 번째 항은 $\mathbf{w}$나 $b$에 의존하지 않으므로 무시할 수 있습니다.
두 번째 항은 곱셈 상수 $\frac{1}{\sigma^2}$를 제외하고 앞서 소개한 제곱 오차 손실과 동일합니다.
다행히도 해는 $\sigma$에도 의존하지 않습니다.
따라서 평균 제곱 오차를 최소화하는 것은 가산 가우스 노이즈 가정 하에서 선형 모델의 최대 우도 추정과 동일하다는 결론이 나옵니다.</p>
<h2 id="신경망으로서의-선형-회귀-linear-regression-as-a-neural-network"><a class="header" href="#신경망으로서의-선형-회귀-linear-regression-as-a-neural-network">신경망으로서의 선형 회귀 (Linear Regression as a Neural Network)</a></h2>
<p>선형 모델은 이 책에서 소개할 많은 복잡한 네트워크를 표현하기에 충분히 풍부하지 않지만, (인공) 신경망은 모든 특성이 입력 뉴런으로 표현되고 모두 출력에 직접 연결되는 네트워크로서 선형 모델을 포괄할 만큼 충분히 풍부합니다.</p>
<p>:numref:<code>fig_single_neuron</code>은 선형 회귀를 신경망으로 묘사합니다.
다이어그램은 각 입력이 출력에 연결되는 방식과 같은 연결 패턴을 강조하지만, 가중치나 편향이 취하는 특정 값은 강조하지 않습니다.</p>
<p><img src="chapter_linear-regression/../img/singleneuron.svg" alt="선형 회귀는 단일 레이어 신경망입니다." />
:label:<code>fig_single_neuron</code></p>
<p>입력은 $x_1, \ldots, x_d$입니다.
우리는 $d$를 입력 레이어의 <em>입력 수</em> 또는 <em>특성 차원</em>이라고 부릅니다.
네트워크의 출력은 $o_1$입니다.
우리는 단일 수치 값을 예측하려고 하기 때문에 출력 뉴런이 하나만 있습니다.
입력 값은 모두 <em>주어진</em> 값이라는 점에 유의하십시오. <em>계산된</em> 뉴런은 단 하나뿐입니다.
요약하자면, 우리는 선형 회귀를 단일 레이어 완전 연결 신경망으로 생각할 수 있습니다.
우리는 나중 장에서 훨씬 더 많은 레이어를 가진 네트워크를 만나게 될 것입니다.</p>
<h3 id="생물학-biology"><a class="header" href="#생물학-biology">생물학 (Biology)</a></h3>
<p>선형 회귀는 계산 신경과학보다 앞서기 때문에 선형 회귀를 신경망 측면에서 설명하는 것이 시대착오적으로 보일 수 있습니다.
그럼에도 불구하고 사이버네틱스학자와 신경생리학자인 워런 맥컬록(Warren McCulloch)과 월터 피츠(Walter Pitts)가 인공 뉴런 모델을 개발하기 시작했을 때 선형 회귀는 자연스러운 출발점이었습니다.
:numref:<code>fig_Neuron</code>에서 <em>수상 돌기(dendrites)</em> (입력 단자), <em>핵(nucleus)</em> (CPU), <em>축삭(axon)</em> (출력 와이어), <em>축삭 말단(axon terminals)</em> (출력 단자)으로 구성되어 <em>시냅스</em>를 통해 다른 뉴런과의 연결을 가능하게 하는 생물학적 뉴런의 만화 같은 그림을 고려해 보십시오.</p>
<p><img src="chapter_linear-regression/../img/neuron.svg" alt="실제 뉴런 (출처: 미국 국립 암 연구소의 감시, 역학 및 최종 결과(SEER) 프로그램의 &quot;해부학 및 생리학&quot;)." />
:label:<code>fig_Neuron</code></p>
<p>다른 뉴런(또는 환경 센서)에서 도착하는 정보 $x_i$는 수상 돌기에서 수신됩니다.
특히 그 정보는 <em>시냅스 가중치</em> $w_i$에 의해 가중치가 부여되어, 곱 $x_i w_i$를 통한 활성화 또는 억제와 같은 입력의 효과를 결정합니다.
여러 소스에서 도착하는 가중치 입력은 핵에서 가중 합 $y = \sum_i x_i w_i + b$로 집계되며, 함수 $\sigma(y)$를 통한 일부 비선형 후처리를 거칠 수 있습니다.
이 정보는 축삭을 통해 축삭 말단으로 보내져 목적지(예: 근육과 같은 액추에이터)에 도달하거나 수상 돌기를 통해 다른 뉴런으로 공급됩니다.</p>
<p>분명히, 올바른 연결성과 학습 알고리즘이 주어지면 하나의 뉴런만으로는 표현할 수 없는 훨씬 더 흥미롭고 복잡한 행동을 생성하기 위해 많은 그러한 유닛이 결합될 수 있다는 높은 수준의 아이디어는 실제 생물학적 신경계 연구에서 비롯된 것입니다.
동시에 오늘날 딥러닝의 대부분의 연구는 훨씬 더 넓은 소스에서 영감을 얻습니다.
우리는 비행기가 새에서 <em>영감을 받았을</em> 수는 있지만, 조류학이 몇 세기 동안 항공 혁신의 주요 동력은 아니었다고 지적한 :citet:<code>Russell.Norvig.2016</code>를 인용합니다.
마찬가지로 요즘 딥러닝의 영감은 수학, 언어학, 심리학, 통계학, 컴퓨터 과학 및 기타 많은 분야에서 동일하거나 더 큰 비중으로 옵니다.</p>
<h2 id="요약-summary"><a class="header" href="#요약-summary">요약 (Summary)</a></h2>
<p>이 섹션에서는 훈련 세트에서 제곱 손실을 최소화하도록 선형 함수의 파라미터를 선택하는 전통적인 선형 회귀를 소개했습니다.
우리는 또한 몇 가지 실용적인 고려 사항과 선형성 및 가우스 노이즈 가정 하에서의 최대 우도 추정으로서의 선형 회귀 해석을 통해 이 목적 함수 선택에 동기를 부여했습니다.
계산적 고려 사항과 통계와의 연결을 모두 논의한 후, 이러한 선형 모델이 입력이 출력에 직접 연결되는 단순한 신경망으로 어떻게 표현될 수 있는지 보여주었습니다.
곧 선형 모델을 완전히 벗어나겠지만, 이들은 우리가 모델에 요구하는 대부분의 구성 요소를 도입하기에 충분합니다: 파라미터 형식, 미분 가능한 목적 함수, 미니배치 확률적 경사 하강법을 통한 최적화, 그리고 궁극적으로 이전에 본 적 없는 데이터에 대한 평가입니다.</p>
<h2 id="연습-문제-exercises"><a class="header" href="#연습-문제-exercises">연습 문제 (Exercises)</a></h2>
<ol>
<li>일부 데이터 $x_1, \ldots, x_n \in \mathbb{R}$가 있다고 가정합니다. 우리의 목표는 $\sum_i (x_i - b)^2$가 최소화되는 상수 $b$를 찾는 것입니다.
<ol>
<li>$b$의 최적 값에 대한 해석적 해를 찾으십시오.</li>
<li>이 문제와 그 해는 정규 분포와 어떤 관련이 있습니까?</li>
<li>손실을 $\sum_i (x_i - b)^2$에서 $\sum_i |x_i-b|$로 바꾸면 어떻게 됩니까? $b$에 대한 최적의 해를 찾을 수 있습니까?</li>
</ol>
</li>
<li>$\mathbf{x}^\top \mathbf{w} + b$로 표현될 수 있는 아핀 함수가 $(\mathbf{x}, 1)$에 대한 선형 함수와 동등함을 증명하십시오.</li>
<li>$\mathbf{x}$의 이차 함수, 즉 $f(\mathbf{x}) = b + \sum_i w_i x_i + \sum_{j \leq i} w_{ij} x_{i} x_{j}$를 찾고 싶다고 가정합니다. 이를 심층 네트워크에서 어떻게 공식화하겠습니까?</li>
<li>선형 회귀 문제가 해결 가능하기 위한 조건 중 하나는 설계 행렬 $\mathbf{X}^\top \mathbf{X}$가 풀 랭크(full rank)를 갖는 것이었습니다.
<ol>
<li>그렇지 않은 경우 어떻게 됩니까?</li>
<li>어떻게 고칠 수 있을까요? $\mathbf{X}$의 모든 항목에 좌표별로 독립적인 가우스 노이즈를 소량 추가하면 어떻게 됩니까?</li>
<li>이 경우 설계 행렬 $\mathbf{X}^\top \mathbf{X}$의 기댓값은 얼마입니까?</li>
<li>$\mathbf{X}^\top \mathbf{X}$가 풀 랭크가 아닐 때 확률적 경사 하강법은 어떻게 됩니까?</li>
</ol>
</li>
<li>가산 노이즈 $\epsilon$을 지배하는 노이즈 모델이 지수 분포라고 가정합니다. 즉, $p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$입니다.
<ol>
<li>모델 하에서 데이터의 음의 로그 우도 $-\log P(\mathbf y \mid \mathbf X)$를 작성하십시오.</li>
<li>닫힌 형식의 해를 찾을 수 있습니까?</li>
<li>이 문제를 해결하기 위해 미니배치 확률적 경사 하강법 알고리즘을 제안하십시오. 무엇이 잘못될 수 있을까요 (힌트: 파라미터를 계속 업데이트함에 따라 정지점 근처에서 무슨 일이 일어납니까)? 이를 고칠 수 있습니까?</li>
</ol>
</li>
<li>두 개의 선형 레이어를 합성하여 두 개의 레이어가 있는 신경망을 설계하고 싶다고 가정합니다. 즉, 첫 번째 레이어의 출력이 두 번째 레이어의 입력이 됩니다. 왜 그런 단순한 합성이 작동하지 않을까요?</li>
<li>식료품점에서 판매된 사과의 <em>수</em>를 추정하기 위해 회귀를 사용하고 싶다고 가정해 봅시다.
<ol>
<li>가우스 가산 노이즈 모델의 문제는 무엇입니까? 힌트: 당신은 기름이 아니라 사과를 팔고 있습니다.</li>
<li><a href="https://en.wikipedia.org/wiki/Poisson_distribution">포아송 분포(Poisson distribution)</a>는 카운트에 대한 분포를 캡처합니다. $p(k \mid \lambda) = \lambda^k e^{-\lambda}/k!$로 주어집니다. 여기서 $\lambda$는 비율 함수이고 $k$는 관찰되는 이벤트의 수입니다. $\lambda$가 카운트 $k$의 기댓값임을 증명하십시오.</li>
<li>포아송 분포와 관련된 손실 함수를 설계하십시오.</li>
<li>대신 $\log \lambda$를 추정하기 위한 손실 함수를 설계하십시오.</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/40">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/258">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/259">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="구현을-위한-객체-지향-설계-object-oriented-design-for-implementation"><a class="header" href="#구현을-위한-객체-지향-설계-object-oriented-design-for-implementation">구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></h1>
<p>:label:<code>sec_oo-design</code></p>
<p>선형 회귀 소개에서 우리는 데이터, 모델, 손실 함수, 최적화 알고리즘을 포함한 다양한 구성 요소를 살펴보았습니다.
사실 선형 회귀는 가장 단순한 머신러닝 모델 중 하나입니다.
하지만 이를 훈련하는 데는 이 책의 다른 모델들이 요구하는 것과 동일한 구성 요소가 많이 사용됩니다.
따라서 구현 세부 사항을 깊이 파고들기 전에, 우리가 전체적으로 사용하는 일부 API를 설계하는 것이 가치가 있습니다.
딥러닝의 구성 요소를 객체로 취급하여, 이러한 객체와 그 상호 작용을 위한 클래스를 정의하는 것부터 시작할 수 있습니다.
구현을 위한 이러한 객체 지향 설계는 설명을 크게 간소화할 것이며, 여러분의 프로젝트에서도 사용하고 싶어질 것입니다.</p>
<p><a href="https://www.pytorchlightning.ai/">PyTorch Lightning</a>과 같은 오픈 소스 라이브러리에서 영감을 받아, 높은 수준에서 세 가지 클래스를 갖고자 합니다:
(i) <code>Module</code>은 모델, 손실, 최적화 방법을 포함합니다;
(ii) <code>DataModule</code>은 훈련 및 검증을 위한 데이터 로더를 제공합니다;
(iii) 두 클래스는 다양한 하드웨어 플랫폼에서 모델을 훈련할 수 있게 해주는 <code>Trainer</code> 클래스를 사용하여 결합됩니다.
이 책의 대부분의 코드는 <code>Module</code>과 <code>DataModule</code>을 조정합니다. <code>Trainer</code> 클래스는 GPU, CPU, 병렬 훈련, 최적화 알고리즘을 논의할 때만 언급할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
import time
import numpy as np
from d2l import mxnet as d2l
from mxnet.gluon import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import time
import numpy as np
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import time
import numpy as np
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from dataclasses import field
from d2l import jax as d2l
from flax import linen as nn
from flax.training import train_state
from jax import numpy as jnp
import numpy as np
import jax
import time
from typing import Any
</code></pre>
<h2 id="유틸리티-utilities"><a class="header" href="#유틸리티-utilities">유틸리티 (Utilities)</a></h2>
<p>:label:<code>oo-design-utilities</code></p>
<p>Jupyter 노트북에서 객체 지향 프로그래밍을 단순화하기 위해 몇 가지 유틸리티가 필요합니다. 한 가지 어려운 점은 클래스 정의가 상당히 긴 코드 블록이 되는 경향이 있다는 것입니다. 노트북 가독성을 위해서는 설명이 섞인 짧은 코드 조각이 요구되는데, 이는 Python 라이브러리에서 흔히 볼 수 있는 프로그래밍 스타일과 호환되지 않습니다. 첫 번째 유틸리티 함수를 사용하면 클래스가 생성된 <em>후에</em> 함수를 클래스의 메서드로 등록할 수 있습니다. 사실, 클래스의 인스턴스를 생성한 <em>후에도</em> 그렇게 할 수 있습니다! 이를 통해 클래스의 구현을 여러 코드 블록으로 나눌 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def add_to_class(Class):  #@save
    """함수를 생성된 클래스의 메서드로 등록합니다."""
    def wrapper(obj):
        setattr(Class, obj.__name__, obj)
    return wrapper
</code></pre>
<p>사용법을 빠르게 살펴봅시다. <code>do</code> 메서드를 가진 클래스 <code>A</code>를 구현하려고 합니다. 동일한 코드 블록에 <code>A</code>와 <code>do</code>에 대한 코드를 모두 넣는 대신, 먼저 클래스 <code>A</code>를 선언하고 인스턴스 <code>a</code>를 생성할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
class A:
    def __init__(self):
        self.b = 1

a = A()
</code></pre>
<p>다음으로 평소처럼 <code>do</code> 메서드를 정의하되, 클래스 <code>A</code>의 범위 내에서 정의하지 않습니다. 대신, 이 메서드를 클래스 <code>A</code>를 인수로 하는 <code>add_to_class</code>로 데코레이션합니다. 그렇게 함으로써, 메서드는 마치 <code>A</code> 정의의 일부로 포함된 것처럼 <code>A</code>의 멤버 변수에 액세스할 수 있습니다. 인스턴스 <code>a</code>에 대해 호출할 때 어떤 일이 일어나는지 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
@add_to_class(A)
def do(self):
    print('클래스 속성 "b"는', self.b)

a.do()
</code></pre>
<p>두 번째는 클래스의 <code>__init__</code> 메서드에 있는 모든 인수를 클래스 속성으로 저장하는 유틸리티 클래스입니다. 이를 통해 추가 코드 없이 생성자 호출 서명을 암시적으로 확장할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
class HyperParameters:  #@save
    """하이퍼파라미터의 기본 클래스입니다."""
    def save_hyperparameters(self, ignore=[]):
        raise NotImplemented
</code></pre>
<p>이 구현은 :numref:<code>sec_utils</code>로 미룹니다. 이를 사용하기 위해 <code>HyperParameters</code>를 상속받고 <code>__init__</code> 메서드에서 <code>save_hyperparameters</code>를 호출하는 클래스를 정의합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
# d2l에 저장된 완전히 구현된 HyperParameters 클래스를 호출합니다.
class B(d2l.HyperParameters):
    def __init__(self, a, b, c):
        self.save_hyperparameters(ignore=['c'])
        print('self.a =', self.a, 'self.b =', self.b)
        print('self.c가 존재하지 않음 =', not hasattr(self, 'c'))

b = B(a=1, b=2, c=3)
</code></pre>
<p>마지막 유틸리티를 사용하면 실험이 진행되는 동안 대화식으로 실험 진행 상황을 플롯할 수 있습니다. 훨씬 더 강력하고 복잡한 <a href="https://www.tensorflow.org/tensorboard">TensorBoard</a>에 대한 경의를 표하며 이름을 <code>ProgressBoard</code>라고 명명했습니다. 구현은 :numref:<code>sec_utils</code>로 미룹니다. 지금은 단순히 작동하는 모습을 봅시다.</p>
<p><code>draw</code> 메서드는 그림에 점 <code>(x, y)</code>를 플롯하며, 범례에 <code>label</code>이 지정됩니다. 선택적 인수 <code>every_n</code>은 그림에 $1/n$개의 점만 표시하여 선을 부드럽게 만듭니다. 그 값은 원래 그림의 $n$개 이웃 점의 평균입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
class ProgressBoard(d2l.HyperParameters):  #@save
    """데이터 포인트를 애니메이션으로 플롯하는 보드입니다."""
    def __init__(self, xlabel=None, ylabel=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 ls=['-', '--', '-.', ':'], colors=['C0', 'C1', 'C2', 'C3'],
                 fig=None, axes=None, figsize=(3.5, 2.5), display=True):
        self.save_hyperparameters()

    def draw(self, x, y, label, every_n=1):
        raise NotImplemented
</code></pre>
<p>다음 예제에서는 서로 다른 부드러움으로 <code>sin</code>과 <code>cos</code>을 그립니다. 이 코드 블록을 실행하면 선이 애니메이션으로 자라나는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
board = d2l.ProgressBoard('x')
for x in np.arange(0, 10, 0.1):
    board.draw(x, np.sin(x), 'sin', every_n=2)
    board.draw(x, np.cos(x), 'cos', every_n=10)
</code></pre>
<h2 id="모델-models"><a class="header" href="#모델-models">모델 (Models)</a></h2>
<p>:label:<code>subsec_oo-design-models</code></p>
<p><code>Module</code> 클래스는 우리가 구현할 모든 모델의 기본 클래스입니다. 최소한 세 가지 메서드가 필요합니다. 첫 번째 <code>__init__</code>은 학습 가능한 파라미터를 저장하고, <code>training_step</code> 메서드는 데이터 배치를 받아 손실 값을 반환하며, 마지막으로 <code>configure_optimizers</code>는 학습 가능한 파라미터를 업데이트하는 데 사용되는 최적화 방법 또는 그 리스트를 반환합니다. 선택적으로 평가 측정치를 보고하기 위해 <code>validation_step</code>을 정의할 수 있습니다.
때때로 우리는 출력을 계산하는 코드를 더 재사용하기 쉽게 별도의 <code>forward</code> 메서드에 넣습니다.</p>
<p>:begin_tab:<code>jax</code>
Python 3.7에서 <a href="https://docs.python.org/3/library/dataclasses.html">dataclasses</a>가 도입되면서, <code>@dataclass</code>로 데코레이션된 클래스는 <code>__init__</code> 및 <code>__repr__</code>과 같은 매직 메서드를 자동으로 추가합니다. 멤버 변수는 유형 주석(type annotations)을 사용하여 정의됩니다. 모든 Flax 모듈은 Python 3.7 데이터 클래스입니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch
class Module(d2l.nn_Module, d2l.HyperParameters):  #@save
    """모델의 기본 클래스입니다."""
    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):
        super().__init__()
        self.save_hyperparameters()
        self.board = ProgressBoard()

    def loss(self, y_hat, y):
        raise NotImplementedError

    def forward(self, X):
        assert hasattr(self, 'net'), '신경망이 정의되었습니다.'
        return self.net(X)

    def plot(self, key, value, train):
        """점을 애니메이션으로 플롯합니다."""
        assert hasattr(self, 'trainer'), 'Trainer가 초기화되지 않았습니다.'
        self.board.xlabel = 'epoch'
        if train:
            x = self.trainer.train_batch_idx / \
                self.trainer.num_train_batches
            n = self.trainer.num_train_batches / \
                self.plot_train_per_epoch
        else:
            x = self.trainer.epoch + 1
            n = self.trainer.num_val_batches / \
                self.plot_valid_per_epoch
        self.board.draw(x, d2l.numpy(d2l.to(value, d2l.cpu())), 
                        ('train_' if train else 'val_') + key,
                        every_n=int(n))

    def training_step(self, batch):
        l = self.loss(self(*batch[:-1]), batch[-1])
        self.plot('loss', l, train=True)
        return l

    def validation_step(self, batch):
        l = self.loss(self(*batch[:-1]), batch[-1])
        self.plot('loss', l, train=False)

    def configure_optimizers(self):
        raise NotImplementedError
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow, jax
class Module(d2l.nn_Module, d2l.HyperParameters):  #@save
    """모델의 기본 클래스입니다."""
    if tab.selected('mxnet', 'tensorflow'):
        def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):
            super().__init__()
            self.save_hyperparameters()
            self.board = ProgressBoard()
        if tab.selected('tensorflow'):
            self.training = None

    if tab.selected('jax'):
        # Python 데이터 클래스를 사용할 때는 save_hyperparam이 필요하지 않습니다.
        plot_train_per_epoch: int = field(default=2, init=False)
        plot_valid_per_epoch: int = field(default=1, init=False)
        # 매 실행마다 새 플롯이 생성되도록 default_factory를 사용합니다.
        board: ProgressBoard = field(default_factory=lambda: ProgressBoard(),
                                     init=False)

    def loss(self, y_hat, y):
        raise NotImplementedError

    if tab.selected('mxnet', 'tensorflow'):
        def forward(self, X):
            assert hasattr(self, 'net'), '신경망이 정의되었습니다.'
            return self.net(X)

    if tab.selected('tensorflow'):
        def call(self, X, *args, **kwargs):
            if kwargs and "training" in kwargs:
                self.training = kwargs['training']
            return self.forward(X, *args)

    if tab.selected('jax'):
        # JAX 및 Flax에는 forward-method와 같은 구문이 없습니다. Flax는 순방향 패스를 위해 setup과
        # 내장 __call__ 매직 메서드를 사용합니다. 일관성을 위해 여기에 추가합니다.
        def forward(self, X, *args, **kwargs):
            assert hasattr(self, 'net'), '신경망이 정의되었습니다.'
            return self.net(X, *args, **kwargs)

        def __call__(self, X, *args, **kwargs):
            return self.forward(X, *args, **kwargs)

    def plot(self, key, value, train):
        """점을 애니메이션으로 플롯합니다."""
        assert hasattr(self, 'trainer'), 'Trainer가 초기화되지 않았습니다.'
        self.board.xlabel = 'epoch'
        if train:
            x = self.trainer.train_batch_idx / \
                self.trainer.num_train_batches
            n = self.trainer.num_train_batches / \
                self.plot_train_per_epoch
        else:
            x = self.trainer.epoch + 1
            n = self.trainer.num_val_batches / \
                self.plot_valid_per_epoch
        if tab.selected('mxnet', 'tensorflow'):
            self.board.draw(x, d2l.numpy(value), (
                'train_' if train else 'val_') + key, every_n=int(n))
        if tab.selected('jax'):
            self.board.draw(x, d2l.to(value, d2l.cpu()),
                            ('train_' if train else 'val_') + key,
                            every_n=int(n))

    if tab.selected('mxnet', 'tensorflow'):
        def training_step(self, batch):
            l = self.loss(self(*batch[:-1]), batch[-1])
            self.plot('loss', l, train=True)
            return l

        def validation_step(self, batch):
            l = self.loss(self(*batch[:-1]), batch[-1])
            self.plot('loss', l, train=False)

    if tab.selected('jax'):
        def training_step(self, params, batch, state):
            l, grads = jax.value_and_grad(self.loss)(params, batch[:-1],
                                                     batch[-1], state)
            self.plot("loss", l, train=True)
            return l, grads

        def validation_step(self, params, batch, state):
            l = self.loss(params, batch[:-1], batch[-1], state)
            self.plot('loss', l, train=False)
        
        def apply_init(self, dummy_input, key):
            """나중에 :numref:`sec_lazy_init`에서 정의될 예정입니다."""
            raise NotImplementedError

    def configure_optimizers(self):
        raise NotImplementedError
</code></pre>
<p>:begin_tab:<code>mxnet</code>
<code>Module</code>이 Gluon의 신경망 기본 클래스인 <code>nn.Block</code>의 서브클래스임을 알 수 있습니다.
이는 신경망을 처리하는 데 편리한 기능을 제공합니다. 예를 들어, <code>forward(self, X)</code>와 같은 <code>forward</code> 메서드를 정의하면 인스턴스 <code>a</code>에 대해 <code>a(X)</code>로 이 메서드를 호출할 수 있습니다. 이는 내장된 <code>__call__</code> 메서드에서 <code>forward</code> 메서드를 호출하기 때문입니다. <code>nn.Block</code>에 대한 자세한 내용과 예제는 :numref:<code>sec_model_construction</code>에서 확인할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<code>Module</code>이 PyTorch의 신경망 기본 클래스인 <code>nn.Module</code>의 서브클래스임을 알 수 있습니다.
이는 신경망을 처리하는 데 편리한 기능을 제공합니다. 예를 들어, <code>forward(self, X)</code>와 같은 <code>forward</code> 메서드를 정의하면 인스턴스 <code>a</code>에 대해 <code>a(X)</code>로 이 메서드를 호출할 수 있습니다. 이는 내장된 <code>__call__</code> 메서드에서 <code>forward</code> 메서드를 호출하기 때문입니다. <code>nn.Module</code>에 대한 자세한 내용과 예제는 :numref:<code>sec_model_construction</code>에서 확인할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<code>Module</code>이 TensorFlow의 신경망 기본 클래스인 <code>tf.keras.Model</code>의 서브클래스임을 알 수 있습니다.
이는 신경망을 처리하는 데 편리한 기능을 제공합니다. 예를 들어, 내장된 <code>__call__</code> 메서드에서 <code>call</code> 메서드를 호출합니다. 여기서는 <code>call</code>을 <code>forward</code> 메서드로 리디렉션하고 그 인수를 클래스 속성으로 저장합니다. 우리는 코드를 다른 프레임워크 구현과 더 유사하게 만들기 위해 이렇게 합니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<code>Module</code>이 Flax의 신경망 기본 클래스인 <code>linen.Module</code>의 서브클래스임을 알 수 있습니다.
이는 신경망을 처리하는 데 편리한 기능을 제공합니다. 예를 들어, 모델 파라미터를 처리하고, 코드를 단순화하기 위해 <code>nn.compact</code> 데코레이터를 제공하며, <code>__call__</code> 메서드 등을 호출합니다.
여기서도 <code>__call__</code>을 <code>forward</code> 메서드로 리디렉션합니다. 우리는 코드를 다른 프레임워크 구현과 더 유사하게 만들기 위해 이렇게 합니다.
:end_tab:</p>
<h2 id="데이터-data"><a class="header" href="#데이터-data">데이터 (Data)</a></h2>
<p>:label:<code>oo-design-data</code></p>
<p><code>DataModule</code> 클래스는 데이터의 기본 클래스입니다. 상당히 자주 <code>__init__</code> 메서드는 데이터를 준비하는 데 사용됩니다. 여기에는 필요한 경우 다운로드 및 전처리가 포함됩니다. <code>train_dataloader</code>는 훈련 데이터셋에 대한 데이터 로더를 반환합니다. 데이터 로더는 사용될 때마다 데이터 배치를 생성하는 (Python) 제너레이터입니다. 이 배치는 손실을 계산하기 위해 <code>Module</code>의 <code>training_step</code> 메서드에 공급됩니다. 검증 데이터셋 로더를 반환하는 선택적인 <code>val_dataloader</code>가 있습니다. 이는 <code>Module</code>의 <code>validation_step</code> 메서드에 대한 데이터 배치를 생성한다는 점을 제외하고는 동일한 방식으로 동작합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
class DataModule(d2l.HyperParameters):  #@save
    """데이터의 기본 클래스입니다."""
    if tab.selected('mxnet', 'pytorch'):
        def __init__(self, root='../data', num_workers=4):
            self.save_hyperparameters()

    if tab.selected('tensorflow', 'jax'):
        def __init__(self, root='../data'):
            self.save_hyperparameters()

    def get_dataloader(self, train):
        raise NotImplementedError

    def train_dataloader(self):
        return self.get_dataloader(train=True)

    def val_dataloader(self):
        return self.get_dataloader(train=False)
</code></pre>
<h2 id="훈련-training"><a class="header" href="#훈련-training">훈련 (Training)</a></h2>
<p>:label:<code>oo-design-training</code></p>
<p>:begin_tab:<code>pytorch, mxnet, tensorflow</code>
<code>Trainer</code> 클래스는 <code>DataModule</code>에 지정된 데이터를 사용하여 <code>Module</code> 클래스의 학습 가능한 파라미터를 훈련합니다. 핵심 메서드는 <code>fit</code>으로, <code>Module</code>의 인스턴스인 <code>model</code>과 <code>DataModule</code>의 인스턴스인 <code>data</code>라는 두 개의 인수를 받습니다. 그런 다음 모델을 훈련하기 위해 전체 데이터셋을 <code>max_epochs</code>번 반복합니다. 이전과 마찬가지로, 이 메서드의 구현은 나중 장으로 미룰 것입니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<code>Trainer</code> 클래스는 <code>DataModule</code>에 지정된 데이터를 사용하여 학습 가능한 파라미터 <code>params</code>를 훈련합니다. 핵심 메서드는 <code>fit</code>으로, <code>Module</code>의 인스턴스인 <code>model</code>, <code>DataModule</code>의 인스턴스인 <code>data</code>, 그리고 JAX <code>PRNGKeyArray</code>인 <code>key</code>라는 세 개의 인수를 받습니다. 여기서는 인터페이스를 단순화하기 위해 <code>key</code> 인수를 선택 사항으로 만들었지만, JAX와 Flax에서 모델 파라미터를 항상 루트 키로 전달하고 초기화하는 것이 좋습니다. 그런 다음 모델을 훈련하기 위해 전체 데이터셋을 <code>max_epochs</code>번 반복합니다. 이전과 마찬가지로, 이 메서드의 구현은 나중 장으로 미룰 것입니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab all
class Trainer(d2l.HyperParameters):  #@save
    """데이터로 모델을 훈련하기 위한 기본 클래스입니다."""
    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):
        self.save_hyperparameters()
        assert num_gpus == 0, '아직 GPU를 지원하지 않습니다.'

    def prepare_data(self, data):
        self.train_dataloader = data.train_dataloader()
        self.val_dataloader = data.val_dataloader()
        self.num_train_batches = len(self.train_dataloader)
        self.num_val_batches = (len(self.val_dataloader)
                                if self.val_dataloader is not None else 0)

    def prepare_model(self, model):
        model.trainer = self
        model.board.xlim = [0, self.max_epochs]
        self.model = model

    if tab.selected('pytorch', 'mxnet', 'tensorflow'):
        def fit(self, model, data):
            self.prepare_data(data)
            self.prepare_model(model)
            self.optim = model.configure_optimizers()
            self.epoch = 0
            self.train_batch_idx = 0
            self.val_batch_idx = 0
            for self.epoch in range(self.max_epochs):
                self.fit_epoch()

    if tab.selected('jax'):
        def fit(self, model, data, key=None):
            self.prepare_data(data)
            self.prepare_model(model)
            self.optim = model.configure_optimizers()

            if key is None:
                root_key = d2l.get_key()
            else:
                root_key = key
            params_key, dropout_key = jax.random.split(root_key)
            key = {'params': params_key, 'dropout': dropout_key}

            dummy_input = next(iter(self.train_dataloader))[:-1]
            variables = model.apply_init(dummy_input, key=key)
            params = variables['params']

            if 'batch_stats' in variables.keys():
                # 여기서 batch_stats는 나중에 사용됩니다(예: 배치 정규화)
                batch_stats = variables['batch_stats']
            else:
                batch_stats = {}

            # Flax는 내부적으로 단일 상태 객체 TrainState를 위해 optax를 사용합니다.
            # 드롭아웃 및 배치 정규화 섹션에서 자세히 논의될 것입니다.
            class TrainState(train_state.TrainState):
                batch_stats: Any
                dropout_rng: jax.random.PRNGKeyArray

            self.state = TrainState.create(apply_fn=model.apply,
                                           params=params,
                                           batch_stats=batch_stats,
                                           dropout_rng=dropout_key,
                                           tx=model.configure_optimizers())
            self.epoch = 0
            self.train_batch_idx = 0
            self.val_batch_idx = 0
            for self.epoch in range(self.max_epochs):
                self.fit_epoch()

    def fit_epoch(self):
        raise NotImplementedError
</code></pre>
<h2 id="요약-summary-1"><a class="header" href="#요약-summary-1">요약 (Summary)</a></h2>
<p>향후 딥러닝 구현을 위한 객체 지향 설계를 강조하기 위해, 위의 클래스들은 단순히 그 객체들이 어떻게 데이터를 저장하고 서로 상호 작용하는지를 보여줍니다. 우리는 책의 나머지 부분에서 <code>@add_to_class</code> 등을 통해 이러한 클래스들의 구현을 계속 풍부하게 만들 것입니다.
게다가, 완전히 구현된 이러한 클래스들은 <a href="https://github.com/d2l-ai/d2l-en/tree/master/d2l">D2L 라이브러리</a>에 저장되어 있으며, 이는 딥러닝을 위한 구조화된 모델링을 쉽게 만들어주는 <em>경량 툴킷</em>입니다. 특히, 프로젝트 간에 거의 아무것도 바꾸지 않고도 많은 구성 요소를 재사용할 수 있게 해줍니다. 예를 들어, 최적화기만, 모델만, 또는 데이터셋만 교체할 수 있습니다. 이러한 정도의 모듈성은 간결함과 단순함 측면에서 책 전체에 걸쳐 이점을 제공하며(이것이 우리가 이를 추가한 이유입니다), 여러분의 프로젝트에서도 동일한 역할을 할 수 있습니다.</p>
<h2 id="연습-문제-exercises-1"><a class="header" href="#연습-문제-exercises-1">연습 문제 (Exercises)</a></h2>
<ol>
<li><a href="https://github.com/d2l-ai/d2l-en/tree/master/d2l">D2L 라이브러리</a>에 저장된 위 클래스들의 전체 구현을 찾아보십시오. 딥러닝 모델링에 좀 더 익숙해지면 구현을 자세히 살펴볼 것을 강력히 권장합니다.</li>
<li><code>B</code> 클래스에서 <code>save_hyperparameters</code> 문을 제거하십시오. 여전히 <code>self.a</code>와 <code>self.b</code>를 출력할 수 있습니까? 선택 사항: <code>HyperParameters</code> 클래스의 전체 구현을 깊이 파고들었다면 이유를 설명할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/6645">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/6646">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/6647">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17974">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="합성-회귀-데이터-synthetic-regression-data"><a class="header" href="#합성-회귀-데이터-synthetic-regression-data">합성 회귀 데이터 (Synthetic Regression Data)</a></h1>
<p>:label:<code>sec_synthetic-regression-data</code></p>
<p>머신러닝은 데이터에서 정보를 추출하는 것이 전부입니다.
그렇다면 여러분은 합성 데이터에서 무엇을 배울 수 있을지 궁금할 것입니다.
우리가 직접 만든 인공 데이터 생성 모델에 포함된 패턴 자체에는 본질적으로 관심이 없을 수도 있지만,
그러한 데이터셋은 교육적인 목적으로는 여전히 유용하며,
학습 알고리즘의 속성을 평가하고 우리의 구현이 예상대로 작동하는지 확인하는 데 도움이 됩니다.
예를 들어, 정답 파라미터를 미리 알고 있는 데이터를 생성하면,
우리 모델이 실제로 그 파라미터를 복구할 수 있는지 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx, gluon
import random
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
import random
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
import random
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
import numpy as np
import random
import tensorflow as tf
import tensorflow_datasets as tfds
</code></pre>
<h2 id="데이터셋-생성하기-generating-the-dataset"><a class="header" href="#데이터셋-생성하기-generating-the-dataset">데이터셋 생성하기 (Generating the Dataset)</a></h2>
<p>이 예제에서는 간결함을 위해 저차원 데이터를 사용합니다.
다음 코드 스니펫은 표준 정규 분포에서 추출한 2차원 특성을 가진 1000개의 예제를 생성합니다.
결과로 생성된 설계 행렬 $\mathbf{X}$는 $\mathbb{R}^{1000 \times 2}$에 속합니다.
우리는 <em>실제(ground truth)</em> 선형 함수를 적용하여 각 레이블을 생성하고,
각 예제에 대해 독립적이고 동일하게 추출된 가산 노이즈 $\boldsymbol{\epsilon}$을 통해 오염시킵니다:</p>
<p>(<strong>$$\mathbf{y}= \mathbf{X} \mathbf{w} + b + \boldsymbol{\epsilon}.$$</strong>)</p>
<p>편의를 위해 $\boldsymbol{\epsilon}$은 평균 $\mu= 0$이고 표준 편차 $\sigma = 0.01$인 정규 분포에서 추출된 것으로 가정합니다.
객체 지향 설계를 위해, (:numref:<code>oo-design-data</code>에서 소개된) <code>d2l.DataModule</code>의 서브클래스의 <code>__init__</code> 메서드에 코드를 추가합니다.
추가적인 하이퍼파라미터를 설정할 수 있게 하는 것이 좋은 관행입니다.
우리는 <code>save_hyperparameters()</code>를 통해 이를 달성합니다.
<code>batch_size</code>는 나중에 결정될 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
class SyntheticRegressionData(d2l.DataModule):  #@save
    """선형 회귀를 위한 합성 데이터입니다."""
    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000, 
                 batch_size=32):
        super().__init__()
        self.save_hyperparameters()
        n = num_train + num_val
        if tab.selected('pytorch') or tab.selected('mxnet'):                
            self.X = d2l.randn(n, len(w))
            noise = d2l.randn(n, 1) * noise
        if tab.selected('tensorflow'):
            self.X = tf.random.normal((n, w.shape[0]))
            noise = tf.random.normal((n, 1)) * noise
        if tab.selected('jax'):
            key = jax.random.PRNGKey(0)
            key1, key2 = jax.random.split(key)
            self.X = jax.random.normal(key1, (n, w.shape[0]))
            noise = jax.random.normal(key2, (n, 1)) * noise
        self.y = d2l.matmul(self.X, d2l.reshape(w, (-1, 1))) + b + noise
</code></pre>
<p>아래에서는 실제 파라미터를 $\mathbf{w} = [2, -3.4]^	op$ 및 $b = 4.2$로 설정합니다.
나중에 추정된 파라미터를 이러한 <em>실제</em> 값과 비교하여 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = SyntheticRegressionData(w=d2l.tensor([2, -3.4]), b=4.2)
</code></pre>
<p>[<strong><code>features</code>의 각 행은 $\mathbb{R}^2$의 벡터로 구성되고 <code>labels</code>의 각 행은 스칼라입니다.</strong>] 첫 번째 항목을 살펴봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
print('features:', data.X[0],'_label:', data.y[0])
</code></pre>
<h2 id="데이터셋-읽기-reading-the-dataset-1"><a class="header" href="#데이터셋-읽기-reading-the-dataset-1">데이터셋 읽기 (Reading the Dataset)</a></h2>
<p>머신러닝 모델을 훈련할 때 종종 데이터셋을 여러 번 훑으며 한 번에 하나의 미니배치 예제를 가져와야 합니다.
이 데이터는 모델을 업데이트하는 데 사용됩니다.
이것이 어떻게 작동하는지 설명하기 위해, 우리는 (:numref:<code>oo-design-utilities</code>에서 소개된) <code>add_to_class</code>를 통해 <code>SyntheticRegressionData</code> 클래스에 등록되는 [<strong><code>get_dataloader</code> 메서드를 구현합니다.</strong>]
이 메서드는 (<strong>배치 크기, 특성 행렬, 레이블 벡터를 받아 <code>batch_size</code> 크기의 미니배치를 생성합니다.</strong>)
따라서 각 미니배치는 특성과 레이블의 튜플로 구성됩니다.
훈련 모드인지 검증 모드인지에 유의해야 합니다:
전자의 경우 데이터를 무작위 순서로 읽기를 원할 것이고, 후자의 경우 디버깅 목적으로 미리 정의된 순서대로 데이터를 읽을 수 있는 것이 중요할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(SyntheticRegressionData)
def get_dataloader(self, train):
    if train:
        indices = list(range(0, self.num_train))
        # 예제를 무작위 순서로 읽습니다.
        random.shuffle(indices)
    else:
        indices = list(range(self.num_train, self.num_train+self.num_val))
    for i in range(0, len(indices), self.batch_size):
        if tab.selected('mxnet', 'pytorch', 'jax'):
            batch_indices = d2l.tensor(indices[i: i+self.batch_size])
            yield self.X[batch_indices], self.y[batch_indices]
        if tab.selected('tensorflow'):
            j = tf.constant(indices[i : i+self.batch_size])
            yield tf.gather(self.X, j), tf.gather(self.y, j)
</code></pre>
<p>직관을 얻기 위해 데이터의 첫 번째 미니배치를 검사해 봅시다.
각 특성 미니배치는 그 크기와 입력 특성의 차원을 모두 제공합니다.
마찬가지로 레이블 미니배치는 <code>batch_size</code>에 의해 주어지는 일치하는 모양을 가질 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
X, y = next(iter(data.train_dataloader()))
print('X shape:', X.shape, '_y shape:', y.shape)
</code></pre>
<p>겉보기에는 무해해 보이지만, <code>iter(data.train_dataloader())</code>의 호출은 Python의 객체 지향 설계의 힘을 잘 보여줍니다.
<code>data</code> 객체를 생성한 <em>후에</em> <code>SyntheticRegressionData</code> 클래스에 메서드를 추가했다는 점에 유의하십시오.
그럼에도 불구하고, 객체는 클래스에 기능이 사후에 추가된 혜택을 받습니다.</p>
<p>반복을 통해 전체 데이터셋이 소진될 때까지 별개의 미니배치를 얻습니다 (직접 시도해 보십시오).
위에서 구현된 반복은 교육적 목적으로는 좋지만, 실제 문제에서는 곤란할 정도로 비효율적입니다.
예를 들어, 모든 데이터를 메모리에 로드하고 많은 무작위 메모리 액세스를 수행해야 하기 때문입니다.
딥러닝 프레임워크에 구현된 내장 반복기는 훨씬 더 효율적이며, 파일에 저장된 데이터, 스트림을 통해 수신된 데이터, 즉석에서 생성되거나 처리되는 데이터와 같은 소스를 처리할 수 있습니다.
다음으로 내장 반복기를 사용하여 동일한 메서드를 구현해 보겠습니다.</p>
<h2 id="데이터-로더의-간결한-구현-concise-implementation-of-the-data-loader"><a class="header" href="#데이터-로더의-간결한-구현-concise-implementation-of-the-data-loader">데이터 로더의 간결한 구현 (Concise Implementation of the Data Loader)</a></h2>
<p>자체 반복기를 작성하는 대신, [<strong>데이터를 로드하기 위해 프레임워크의 기존 API를 호출할 수 있습니다.</strong>]
이전과 마찬가지로 특성 <code>X</code>와 레이블 <code>y</code>가 있는 데이터셋이 필요합니다.
그 외에도 내장 데이터 로더에서 <code>batch_size</code>를 설정하고 예제를 효율적으로 셔플링하도록 맡깁니다.</p>
<p>:begin_tab:<code>jax</code>
JAX는 장치 가속 및 함수 변환이 포함된 NumPy와 유사한 API이므로, 적어도 현재 버전에는 데이터 로딩 메서드가 포함되어 있지 않습니다. 다른 라이브러리에는 이미 훌륭한 데이터 로더가 있으며, JAX는 대신 그것들을 사용할 것을 제안합니다. 여기서는 TensorFlow의 데이터 로더를 가져와 JAX와 호환되도록 약간 수정하겠습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(d2l.DataModule)  #@save
def get_tensorloader(self, tensors, train, indices=slice(0, None)):
    tensors = tuple(a[indices] for a in tensors)
    if tab.selected('mxnet'):
        dataset = gluon.data.ArrayDataset(*tensors)
        return gluon.data.DataLoader(dataset, self.batch_size,
                                     shuffle=train)
    if tab.selected('pytorch'):
        dataset = torch.utils.data.TensorDataset(*tensors)
        return torch.utils.data.DataLoader(dataset, self.batch_size,
                                           shuffle=train)
    if tab.selected('jax'):
        # Tensorflow Datasets 및 Dataloader를 사용합니다. JAX나 Flax는
        # 어떠한 데이터 로딩 기능도 제공하지 않습니다.
        shuffle_buffer = tensors[0].shape[0] if train else 1
        return tfds.as_numpy(
            tf.data.Dataset.from_tensor_slices(tensors).shuffle(
                buffer_size=shuffle_buffer).batch(self.batch_size))

    if tab.selected('tensorflow'):
        shuffle_buffer = tensors[0].shape[0] if train else 1
        return tf.data.Dataset.from_tensor_slices(tensors).shuffle(
            buffer_size=shuffle_buffer).batch(self.batch_size)
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(SyntheticRegressionData)  #@save
def get_dataloader(self, train):
    i = slice(0, self.num_train) if train else slice(self.num_train, None)
    return self.get_tensorloader((self.X, self.y), train, i)
</code></pre>
<p>새 데이터 로더는 더 효율적이고 몇 가지 기능이 추가되었다는 점을 제외하면 이전 데이터 로더와 똑같이 동작합니다.</p>
<pre><code class="language-{.python .input  n=4}">%%tab all
X, y = next(iter(data.train_dataloader()))
print('X shape:', X.shape, '_y shape:', y.shape)
</code></pre>
<p>예를 들어, 프레임워크 API가 제공하는 데이터 로더는 내장 <code>__len__</code> 메서드를 지원하므로, 길이, 즉 배치 수를 쿼리할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
len(data.train_dataloader())
</code></pre>
<h2 id="요약-summary-2"><a class="header" href="#요약-summary-2">요약 (Summary)</a></h2>
<p>데이터 로더는 데이터 로딩 및 조작 과정을 추상화하는 편리한 방법입니다.
이렇게 하면 동일한 머신러닝 <em>알고리즘</em>이 수정 없이도 다양한 유형과 소스의 데이터를 처리할 수 있습니다.
데이터 로더의 좋은 점 중 하나는 결합될 수 있다는 것입니다.
예를 들어 이미지를 로드한 다음 이미지를 자르거나 다른 방식으로 수정하는 후처리 필터를 가질 수 있습니다.
따라서 데이터 로더는 전체 데이터 처리 파이프라인을 설명하는 데 사용될 수 있습니다.</p>
<p>모델 자체에 관해서는, 2차원 선형 모델은 우리가 마주칠 수 있는 가장 간단한 모델 중 하나입니다.
데이터 양이 불충분하거나 방정식 시스템이 불충분하게 결정되는 것에 대해 걱정하지 않고 회귀 모델의 정확도를 테스트할 수 있게 해줍니다.
우리는 다음 섹션에서 이를 유용하게 활용할 것입니다.</p>
<h2 id="연습-문제-exercises-2"><a class="header" href="#연습-문제-exercises-2">연습 문제 (Exercises)</a></h2>
<ol>
<li>예제의 수가 배치 크기로 나누어떨어지지 않으면 어떻게 될까요? 프레임워크의 API를 사용하여 다른 인수를 지정함으로써 이 동작을 어떻게 바꾸겠습니까?</li>
<li>파라미터 벡터 <code>w</code>의 크기와 예제 수 <code>num_examples</code>가 모두 큰 거대한 데이터셋을 생성하고 싶다고 가정해 봅시다.
<ol>
<li>모든 데이터를 메모리에 보유할 수 없으면 어떻게 됩니까?</li>
<li>데이터가 디스크에 있는 경우 어떻게 셔플링하시겠습니까? 무작위 읽기나 쓰기가 너무 많이 필요하지 않은 <em>효율적인</em> 알고리즘을 설계하는 것이 과제입니다. 힌트: <a href="https://en.wikipedia.org/wiki/Pseudorandom_permutation">의사 난수 순열 생성기(pseudorandom permutation generators)</a>를 사용하면 순열 표를 명시적으로 저장할 필요 없이 재셔플을 설계할 수 있습니다 :cite:<code>Naor.Reingold.1999</code>.</li>
</ol>
</li>
<li>반복자가 호출될 때마다 즉석에서 새로운 데이터를 생성하는 데이터 생성기를 구현하십시오.</li>
<li>호출될 때마다 <em>동일한</em> 데이터를 생성하는 무작위 데이터 생성기를 어떻게 설계하시겠습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/6662">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/6663">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/6664">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17975">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="밑바닥부터-시작하는-선형-회귀-구현-linear-regression-implementation-from-scratch"><a class="header" href="#밑바닥부터-시작하는-선형-회귀-구현-linear-regression-implementation-from-scratch">밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></h1>
<p>:label:<code>sec_linear_scratch</code></p>
<p>이제 선형 회귀의 완전히 작동하는 구현을 살펴볼 준비가 되었습니다.
이 섹션에서는 (<strong>(i) 모델; (ii) 손실 함수; (iii) 미니배치 확률적 경사 하강법 최적화기; (iv) 이 모든 조각들을 하나로 묶는 훈련 함수를 포함하여 전체 메서드를 밑바닥부터 구현할 것입니다.</strong>)
마지막으로 :numref:<code>sec_synthetic-regression-data</code>에서 만든 합성 데이터 생성기를 실행하고 결과 데이터셋에 모델을 적용할 것입니다.
현대 딥러닝 프레임워크는 이 작업의 거의 모든 부분을 자동화할 수 있지만, 밑바닥부터 구현하는 것이 여러분이 정말로 무엇을 하고 있는지 알 수 있는 유일한 방법입니다.
게다가 자체 레이어나 손실 함수를 정의하여 모델을 커스터마이징할 때, 내부적으로 어떻게 작동하는지 이해하는 것이 유용할 것입니다.
이 섹션에서는 텐서와 자동 미분만을 사용할 것입니다.
나중에는 아래 구조를 유지하면서 딥러닝 프레임워크의 편리한 기능을 활용하는 더 간결한 구현을 소개할 것입니다.</p>
<pre><code class="language-{.python .input  n=2}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=3}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input  n=4}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input  n=5}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
import optax
</code></pre>
<h2 id="모델-정의하기-defining-the-model"><a class="header" href="#모델-정의하기-defining-the-model">모델 정의하기 (Defining the Model)</a></h2>
<p>미니배치 SGD로 [<strong>모델의 파라미터를 최적화하기 전에</strong>], (<strong>우선 파라미터가 있어야 합니다.</strong>)
다음에서는 평균 0, 표준 편차 0.01의 정규 분포에서 난수를 추출하여 가중치를 초기화합니다.
마법의 숫자 0.01은 실제 상황에서 종종 잘 작동하지만, <code>sigma</code> 인수를 통해 다른 값을 지정할 수 있습니다.
또한 편향은 0으로 설정합니다.
객체 지향 설계를 위해, (:numref:<code>subsec_oo-design-models</code>에서 소개된) <code>d2l.Module</code>의 서브클래스의 <code>__init__</code> 메서드에 코드를 추가합니다.</p>
<pre><code class="language-{.python .input  n=6}">%%tab pytorch, mxnet, tensorflow
class LinearRegressionScratch(d2l.Module):  #@save
    """밑바닥부터 구현된 선형 회귀 모델입니다."""
    def __init__(self, num_inputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.w = d2l.normal(0, sigma, (num_inputs, 1))
            self.b = d2l.zeros(1)
            self.w.attach_grad()
            self.b.attach_grad()
        if tab.selected('pytorch'):
            self.w = d2l.normal(0, sigma, (num_inputs, 1), requires_grad=True)
            self.b = d2l.zeros(1, requires_grad=True)
        if tab.selected('tensorflow'):
            w = tf.random.normal((num_inputs, 1), mean=0, stddev=0.01)
            b = tf.zeros(1)
            self.w = tf.Variable(w, trainable=True)
            self.b = tf.Variable(b, trainable=True)
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab jax
class LinearRegressionScratch(d2l.Module):  #@save
    """밑바닥부터 구현된 선형 회귀 모델입니다."""
    num_inputs: int
    lr: float
    sigma: float = 0.01

    def setup(self):
        self.w = self.param('w', nn.initializers.normal(self.sigma),
                            (self.num_inputs, 1))
        self.b = self.param('b', nn.initializers.zeros, (1))
</code></pre>
<p>다음으로 [<strong>입력과 파라미터를 출력과 연결하는 모델을 정의</strong>]해야 합니다.
선형 모델에 대해 :eqref:<code>eq_linreg-y-vec</code>와 동일한 표기법을 사용하여, 단순히 입력 특성 $\mathbf{X}$와 모델 가중치 $\mathbf{w}$의 행렬-벡터 곱을 취하고 각 예제에 오프셋 $b$를 더합니다.
곱 $\mathbf{Xw}$는 벡터이고 $b$는 스칼라입니다.
브로드캐스팅 메커니즘(:numref:<code>subsec_broadcasting</code> 참조)으로 인해 벡터와 스칼라를 더하면 벡터의 각 성분에 스칼라가 더해집니다.
결과인 <code>forward</code> 메서드는 (:numref:<code>oo-design-utilities</code>에서 소개된) <code>add_to_class</code>를 통해 <code>LinearRegressionScratch</code> 클래스에 등록됩니다.</p>
<pre><code class="language-{.python .input  n=8}">%%tab all
@d2l.add_to_class(LinearRegressionScratch)  #@save
def forward(self, X):
    return d2l.matmul(X, self.w) + self.b
</code></pre>
<h2 id="손실-함수-정의하기-defining-the-loss-function"><a class="header" href="#손실-함수-정의하기-defining-the-loss-function">손실 함수 정의하기 (Defining the Loss Function)</a></h2>
<p>[<strong>모델을 업데이트하려면 손실 함수의 기울기를 취해야 하므로</strong>], (<strong>먼저 손실 함수를 정의</strong>)해야 합니다.
여기서는 :eqref:<code>eq_mse</code>의 제곱 손실 함수를 사용합니다.
구현에서 실제 값 <code>y</code>를 예측 값의 모양 <code>y_hat</code>으로 변환해야 합니다.
다음 메서드에서 반환되는 결과도 <code>y_hat</code>과 동일한 모양을 갖게 됩니다.
또한 미니배치의 모든 예제에 대한 평균 손실 값을 반환합니다.</p>
<pre><code class="language-{.python .input  n=9}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(LinearRegressionScratch)  #@save
def loss(self, y_hat, y):
    l = (y_hat - y) ** 2 / 2
    return d2l.reduce_mean(l)
</code></pre>
<pre><code class="language-{.python .input  n=10}">%%tab jax
@d2l.add_to_class(LinearRegressionScratch)  #@save
def loss(self, params, X, y, state):
    y_hat = state.apply_fn({'params': params}, *X)  # 튜플에서 X를 언팩함
    l = (y_hat - d2l.reshape(y, y_hat.shape)) ** 2 / 2
    return d2l.reduce_mean(l)
</code></pre>
<h2 id="최적화-알고리즘-정의하기-defining-the-optimization-algorithm"><a class="header" href="#최적화-알고리즘-정의하기-defining-the-optimization-algorithm">최적화 알고리즘 정의하기 (Defining the Optimization Algorithm)</a></h2>
<p>:numref:<code>sec_linear_regression</code>에서 논의했듯이 선형 회귀는 닫힌 형식의 해를 갖습니다.
하지만 우리의 목표는 더 일반적인 신경망을 훈련하는 방법을 설명하는 것이며, 이를 위해서는 미니배치 SGD를 사용하는 방법을 가르쳐야 합니다.
따라서 이번 기회에 SGD의 첫 번째 실행 예제를 소개하겠습니다.
각 단계에서 데이터셋에서 무작위로 추출한 미니배치를 사용하여 파라미터에 대한 손실의 기울기를 추정합니다.
다음으로 손실을 줄일 수 있는 방향으로 파라미터를 업데이트합니다.</p>
<p>다음 코드는 파라미터 세트와 학습률 <code>lr</code>이 주어졌을 때 업데이트를 적용합니다.
손실이 미니배치에 대한 평균으로 계산되므로 배치 크기에 맞춰 학습률을 조정할 필요가 없습니다.
나중 장에서 분산 대규모 학습에서 발생하는 매우 큰 미니배치에 대해 학습률을 어떻게 조정해야 하는지 조사할 것입니다.
지금은 이 의존성을 무시할 수 있습니다.</p>
<p>:begin_tab:<code>mxnet</code>
내장 SGD 최적화기와 유사한 API를 갖도록 (:numref:<code>oo-design-utilities</code>에서 소개된) <code>d2l.HyperParameters</code>의 서브클래스인 <code>SGD</code> 클래스를 정의합니다.
<code>step</code> 메서드에서 파라미터를 업데이트합니다. 무시할 수 있는 <code>batch_size</code> 인수를 받습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
내장 SGD 최적화기와 유사한 API를 갖도록 (:numref:<code>oo-design-utilities</code>에서 소개된) <code>d2l.HyperParameters</code>의 서브클래스인 <code>SGD</code> 클래스를 정의합니다.
<code>step</code> 메서드에서 파라미터를 업데이트합니다. <code>zero_grad</code> 메서드는 모든 기울기를 0으로 설정하며, 역전파 단계 전에 실행해야 합니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
내장 SGD 최적화기와 유사한 API를 갖도록 (:numref:<code>oo-design-utilities</code>에서 소개된) <code>d2l.HyperParameters</code>의 서브클래스인 <code>SGD</code> 클래스를 정의합니다.
<code>apply_gradients</code> 메서드에서 파라미터를 업데이트합니다. 파라미터와 기울기 쌍의 리스트를 받습니다.
:end_tab:</p>
<pre><code class="language-{.python .input  n=11}">%%tab mxnet, pytorch
class SGD(d2l.HyperParameters):  #@save
    """미니배치 확률적 경사 하강법."""
    def __init__(self, params, lr):
        self.save_hyperparameters()

    if tab.selected('mxnet'):
        def step(self, _):
            for param in self.params:
                param -= self.lr * param.grad

    if tab.selected('pytorch'):
        def step(self):
            for param in self.params:
                param -= self.lr * param.grad

        def zero_grad(self):
            for param in self.params:
                if param.grad is not None:
                    param.grad.zero_()
</code></pre>
<pre><code class="language-{.python .input  n=12}">%%tab tensorflow
class SGD(d2l.HyperParameters):  #@save
    """미니배치 확률적 경사 하강법."""
    def __init__(self, lr):
        self.save_hyperparameters()

    def apply_gradients(self, grads_and_vars):
        for grad, param in grads_and_vars:
            param.assign_sub(self.lr * grad)
</code></pre>
<pre><code class="language-{.python .input  n=13}">%%tab jax
class SGD(d2l.HyperParameters):  #@save
    """미니배치 확률적 경사 하강법."""
    # Optax의 핵심 변환은 `init`과 `update` 두 메서드로 정의되는 
    # `GradientTransformation`입니다.
    # `init`은 상태를 초기화하고 `update`는 기울기를 변환합니다.
    # https://github.com/deepmind/optax/blob/master/optax/_src/transform.py
    def __init__(self, lr):
        self.save_hyperparameters()

    def init(self, params):
        # 사용되지 않는 파라미터 삭제
        del params
        return optax.EmptyState

    def update(self, updates, state, params=None):
        del params
        # flax의 `train_state` 객체를 업데이트하기 위해 `state.apply_gradients` 메서드가 호출되면,
        # 내부적으로 `optax.apply_updates` 메서드를 호출하여
        # 아래 정의된 업데이트 식에 파라미터를 추가합니다.
        updates = jax.tree_util.tree_map(lambda g: -self.lr * g, updates)
        return updates, state

    def __call__():
        return optax.GradientTransformation(self.init, self.update)
</code></pre>
<p>다음으로 <code>SGD</code> 클래스의 인스턴스를 반환하는 <code>configure_optimizers</code> 메서드를 정의합니다.</p>
<pre><code class="language-{.python .input  n=14}">%%tab all
@d2l.add_to_class(LinearRegressionScratch)  #@save
def configure_optimizers(self):
    if tab.selected('mxnet') or tab.selected('pytorch'):
        return SGD([self.w, self.b], self.lr)
    if tab.selected('tensorflow', 'jax'):
        return SGD(self.lr)
</code></pre>
<h2 id="훈련-training-1"><a class="header" href="#훈련-training-1">훈련 (Training)</a></h2>
<p>이제 모든 부품(파라미터, 손실 함수, 모델, 최적화기)이 준비되었으므로, [<strong>메인 훈련 루프를 구현</strong>]할 준비가 되었습니다.
이 책에서 다루는 모든 딥러닝 모델에 대해 유사한 훈련 루프를 사용할 것이므로 이 코드를 완전히 이해하는 것이 중요합니다.
각 *에폭(epoch)*마다 전체 훈련 데이터셋을 순회하며 모든 예제를 한 번씩 거칩니다(예제 수가 배치 크기로 나누어떨어진다고 가정).
각 *반복(iteration)*마다 훈련 예제의 미니배치를 가져와 모델의 <code>training_step</code> 메서드를 통해 손실을 계산합니다.
그런 다음 각 파라미터에 대한 기울기를 계산합니다.
마지막으로 최적화 알고리즘을 호출하여 모델 파라미터를 업데이트합니다.
요약하자면 다음 루프를 실행할 것입니다:</p>
<ul>
<li>파라미터 $(\mathbf{w}, b)$ 초기화</li>
<li>완료될 때까지 반복
<ul>
<li>기울기 계산 $\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)$</li>
<li>파라미터 업데이트 $(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}$</li>
</ul>
</li>
</ul>
<p>:numref:<code>sec_synthetic-regression-data</code>에서 생성한 합성 회귀 데이터셋은 검증 데이터셋을 제공하지 않는다는 점을 상기하십시오.
하지만 대부분의 경우 모델 품질을 측정하기 위해 검증 데이터셋을 원할 것입니다.
여기서는 모델 성능을 측정하기 위해 각 에폭에서 한 번씩 검증 데이터 로더를 전달합니다.
우리의 객체 지향 설계에 따라, <code>prepare_batch</code>와 <code>fit_epoch</code> 메서드는 (:numref:<code>oo-design-training</code>에서 소개된) <code>d2l.Trainer</code> 클래스에 등록됩니다.</p>
<pre><code class="language-{.python .input  n=15}">%%tab all    
@d2l.add_to_class(d2l.Trainer)  #@save
def prepare_batch(self, batch):
    return batch
</code></pre>
<pre><code class="language-{.python .input  n=16}">%%tab pytorch
@d2l.add_to_class(d2l.Trainer)  #@save
def fit_epoch(self):
    self.model.train()        
    for batch in self.train_dataloader:
        loss = self.model.training_step(self.prepare_batch(batch))
        self.optim.zero_grad()
        with torch.no_grad():
            loss.backward()
            if self.gradient_clip_val &gt; 0:  # 나중에 논의될 예정
                self.clip_gradients(self.gradient_clip_val, self.model)
            self.optim.step()
        self.train_batch_idx += 1
    if self.val_dataloader is None:
        return
    self.model.eval()
    for batch in self.val_dataloader:
        with torch.no_grad():            
            self.model.validation_step(self.prepare_batch(batch))
        self.val_batch_idx += 1
</code></pre>
<pre><code class="language-{.python .input  n=17}">%%tab mxnet
@d2l.add_to_class(d2l.Trainer)  #@save
def fit_epoch(self):
    for batch in self.train_dataloader:
        with autograd.record():
            loss = self.model.training_step(self.prepare_batch(batch))
        loss.backward()
        if self.gradient_clip_val &gt; 0:
            self.clip_gradients(self.gradient_clip_val, self.model)
        self.optim.step(1)
        self.train_batch_idx += 1
    if self.val_dataloader is None:
        return
    for batch in self.val_dataloader:        
        self.model.validation_step(self.prepare_batch(batch))
        self.val_batch_idx += 1
</code></pre>
<pre><code class="language-{.python .input  n=18}">%%tab tensorflow
@d2l.add_to_class(d2l.Trainer)  #@save
def fit_epoch(self):
    self.model.training = True
    for batch in self.train_dataloader:            
        with tf.GradientTape() as tape:
            loss = self.model.training_step(self.prepare_batch(batch))
        grads = tape.gradient(loss, self.model.trainable_variables)
        if self.gradient_clip_val &gt; 0:
            grads = self.clip_gradients(self.gradient_clip_val, grads)
        self.optim.apply_gradients(zip(grads, self.model.trainable_variables))
        self.train_batch_idx += 1
    if self.val_dataloader is None:
        return
    self.model.training = False
    for batch in self.val_dataloader:        
        self.model.validation_step(self.prepare_batch(batch))
        self.val_batch_idx += 1
</code></pre>
<pre><code class="language-{.python .input  n=19}">%%tab jax
@d2l.add_to_class(d2l.Trainer)  #@save
def fit_epoch(self):
    self.model.training = True
    if self.state.batch_stats:
        # 가변 상태(Mutable states)는 나중에 사용됩니다(예: 배치 정규화)
        for batch in self.train_dataloader:
            (_, mutated_vars), grads = self.model.training_step(self.state.params,
                                                           self.prepare_batch(batch),
                                                           self.state)
            self.state = self.state.apply_gradients(grads=grads)
            # 드롭아웃 레이어가 없는 모델에서는 무시할 수 있음
            self.state = self.state.replace(
                dropout_rng=jax.random.split(self.state.dropout_rng)[0])
            self.state = self.state.replace(batch_stats=mutated_vars['batch_stats'])
            self.train_batch_idx += 1
    else:
        for batch in self.train_dataloader:
            _, grads = self.model.training_step(self.state.params,
                                                self.prepare_batch(batch),
                                                self.state)
            self.state = self.state.apply_gradients(grads=grads)
            # 드롭아웃 레이어가 없는 모델에서는 무시할 수 있음
            self.state = self.state.replace(
                dropout_rng=jax.random.split(self.state.dropout_rng)[0])
            self.train_batch_idx += 1

    if self.val_dataloader is None:
        return
    self.model.training = False
    for batch in self.val_dataloader:
        self.model.validation_step(self.state.params,
                                   self.prepare_batch(batch),
                                   self.state)
        self.val_batch_idx += 1
</code></pre>
<p>모델을 훈련할 준비가 거의 다 되었지만, 먼저 훈련 데이터가 필요합니다.
여기서는 <code>SyntheticRegressionData</code> 클래스를 사용하고 몇 가지 실제 파라미터를 전달합니다.
그런 다음 학습률 <code>lr=0.03</code>으로 모델을 훈련하고 <code>max_epochs=3</code>으로 설정합니다.
일반적으로 에폭 수와 학습률은 모두 하이퍼파라미터라는 점에 유의하십시오.
일반적으로 하이퍼파라미터를 설정하는 것은 까다로우며, 우리는 대개 훈련을 위한 한 세트, 하이퍼파라미터 선택을 위한 두 번째 세트, 그리고 최종 평가를 위해 예약된 세 번째 세트라는 3-way split을 사용하기를 원할 것입니다.
지금은 이러한 세부 사항을 생략하지만 나중에 다시 살펴볼 것입니다.</p>
<pre><code class="language-{.python .input  n=20}">%%tab all
model = LinearRegressionScratch(2, lr=0.03)
data = d2l.SyntheticRegressionData(w=d2l.tensor([2, -3.4]), b=4.2)
trainer = d2l.Trainer(max_epochs=3)
trainer.fit(model, data)
</code></pre>
<p>데이터셋을 우리가 직접 합성했기 때문에 실제 파라미터가 무엇인지 정확히 알고 있습니다.
따라서 훈련 루프를 통해 [<strong>학습한 파라미터와 실제 파라미터를 비교하여 훈련 성공 여부를 평가</strong>]할 수 있습니다.
실제로 그들은 서로 매우 가깝다는 것이 밝혀졌습니다.</p>
<pre><code class="language-{.python .input  n=21}">%%tab pytorch
with torch.no_grad():
    print(f'w 추정 오차: {data.w - d2l.reshape(model.w, data.w.shape)}')
    print(f'b 추정 오차: {data.b - model.b}')
</code></pre>
<pre><code class="language-{.python .input  n=22}">%%tab mxnet, tensorflow
print(f'w 추정 오차: {data.w - d2l.reshape(model.w, data.w.shape)}')
print(f'b 추정 오차: {data.b - model.b}')
</code></pre>
<pre><code class="language-{.python .input  n=23}">%%tab jax
params = trainer.state.params
print(f"w 추정 오차: {data.w - d2l.reshape(params['w'], data.w.shape)}")
print(f"b 추정 오차: {data.b - params['b']}")
</code></pre>
<p>실제 파라미터를 정확하게 복구할 수 있는 능력을 당연하게 여겨서는 안 됩니다.
일반적으로 심층 모델의 경우 파라미터에 대한 고유한 해가 존재하지 않으며, 선형 모델의 경우에도 어떤 특성도 다른 특성에 선형적으로 종속되지 않을 때만 파라미터를 정확하게 복구할 수 있습니다.
그러나 머신러닝에서 우리는 종종 실제 기본 파라미터를 복구하는 것보다 매우 정확한 예측으로 이어지는 파라미터에 더 관심을 갖습니다 :cite:<code>Vapnik.1992</code>.
다행히도 어려운 최적화 문제에서도 확률적 경사 하강법은 종종 놀랍도록 좋은 솔루션을 찾을 수 있는데, 이는 부분적으로 심층 네트워크의 경우 매우 정확한 예측으로 이어지는 파라미터 설정이 많이 존재하기 때문입니다.</p>
<h2 id="요약-summary-3"><a class="header" href="#요약-summary-3">요약 (Summary)</a></h2>
<p>이 섹션에서 우리는 완벽하게 작동하는 신경망 모델과 훈련 루프를 구현함으로써 딥러닝 시스템 설계에 있어 중요한 단계를 밟았습니다.
이 과정에서 우리는 데이터 로더, 모델, 손실 함수, 최적화 절차, 시각화 및 모니터링 도구를 구축했습니다.
모델을 훈련하기 위한 모든 관련 구성 요소를 포함하는 Python 객체를 구성하여 이를 수행했습니다.
아직 프로 수준의 구현은 아니지만 완벽하게 작동하며 이와 같은 코드는 이미 작은 문제를 빠르게 해결하는 데 도움이 될 수 있습니다.
가까운 섹션에서 (상용구 코드를 피하면서) <em>더 간결하게</em>, 그리고 (GPU를 최대한 활용하여) <em>더 효율적으로</em> 이를 수행하는 방법을 볼 것입니다.</p>
<h2 id="연습-문제-exercises-3"><a class="header" href="#연습-문제-exercises-3">연습 문제 (Exercises)</a></h2>
<ol>
<li>가중치를 0으로 초기화하면 어떻게 될까요? 알고리즘이 여전히 작동할까요? 파라미터를 0.01이 아니라 분산 1000으로 초기화하면 어떻게 될까요?</li>
<li>전압과 전류를 관련시키는 저항 모델을 고안하려는 <a href="https://en.wikipedia.org/wiki/Georg_Ohm">게오르크 시몬 옴(Georg Simon Ohm)</a>이라고 가정해 봅시다. 자동 미분을 사용하여 모델의 파라미터를 학습할 수 있습니까?</li>
<li><a href="https://en.wikipedia.org/wiki/Planck%27s_law">플랑크 법칙(Planck's Law)</a>을 사용하여 분광 에너지 밀도를 통해 물체의 온도를 결정할 수 있습니까? 참고로 흑체에서 방출되는 복사의 분광 밀도 $B$는 $B(\lambda, T) = \frac{2 hc^2}{\lambda^5} \cdot \left(\exp \frac{h c}{\lambda k T} - 1\right)^{-1}$입니다. 여기서 $\lambda$는 파장, $T$는 온도, $c$는 빛의 속도, $h$는 플랑크 상수, $k$는 볼츠만 상수입니다. 여러분은 다양한 파장 $\lambda$에 대한 에너지를 측정하고 이제 분광 밀도 곡선을 플랑크 법칙에 맞춰야 합니다.</li>
<li>손실의 2계 도함수를 계산하려고 할 때 마주칠 수 있는 문제는 무엇입니까? 어떻게 고칠 수 있을까요?</li>
<li><code>loss</code> 함수에서 <code>reshape</code> 메서드가 필요한 이유는 무엇입니까?</li>
<li>손실 함수 값이 얼마나 빨리 떨어지는지 알아보기 위해 다양한 학습률을 사용하여 실험해 보십시오. 훈련 에폭 수를 늘려 오차를 줄일 수 있습니까?</li>
<li>예제 수가 배치 크기로 나누어떨어지지 않으면 에폭 끝에서 <code>data_iter</code>에 어떤 일이 발생합니까?</li>
<li>절댓값 손실 <code>(y_hat - d2l.reshape(y, y_hat.shape)).abs().sum()</code>과 같은 다른 손실 함수를 구현해 보십시오.
<ol>
<li>일반 데이터에 대해 어떤 일이 일어나는지 확인하십시오.</li>
<li>$\mathbf{y}$의 일부 항목(예: $y_5 = 10000$)을 적극적으로 교란시켰을 때 동작에 차이가 있는지 확인하십시오.</li>
<li>제곱 손실과 절댓값 손실의 장점을 결합한 저렴한 솔루션을 생각할 수 있습니까? 힌트: 정말 큰 기울기 값을 어떻게 피할 수 있을까요?</li>
</ol>
</li>
<li>왜 데이터셋을 재셔플해야 할까요? 그렇지 않으면 악의적으로 구성된 데이터셋이 최적화 알고리즘을 망가뜨리는 사례를 설계할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/42">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/43">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/201">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17976">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="선형-회귀의-간결한-구현-concise-implementation-of-linear-regression"><a class="header" href="#선형-회귀의-간결한-구현-concise-implementation-of-linear-regression">선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></h1>
<p>:label:<code>sec_linear_concise</code></p>
<p>딥러닝은 지난 10년 동안 일종의 캄브리아기 폭발을 목격했습니다.
수많은 기술, 응용 프로그램 및 알고리즘의 수는 이전 수십 년의 발전을 훨씬 능가합니다.
이는 여러 요인의 행운 섞인 조합 덕분이며, 그중 하나는 여러 오픈 소스 딥러닝 프레임워크가 제공하는 강력한 무료 도구입니다.
Theano :cite:<code>Bergstra.Breuleux.Bastien.ea.2010</code>, DistBelief :cite:<code>Dean.Corrado.Monga.ea.2012</code>, Caffe :cite:<code>Jia.Shelhamer.Donahue.ea.2014</code>는 널리 채택된 그러한 모델의 1세대라고 할 수 있습니다.
Lisp와 유사한 프로그래밍 경험을 제공했던 초기(중요한) 작품인 SN2 (Simulateur Neuristique) :cite:<code>Bottou.Le-Cun.1988</code>와 대조적으로, 현대 프레임워크는 자동 미분과 Python의 편리함을 제공합니다.
이러한 프레임워크를 통해 우리는 경사 기반 학습 알고리즘을 구현하는 반복적인 작업을 자동화하고 모듈화할 수 있습니다.</p>
<p>:numref:<code>sec_linear_scratch</code>에서 우리는 (i) 데이터 저장 및 선형 대수를 위한 텐서, (ii) 기울기 계산을 위한 자동 미분에만 의존했습니다.
실제로 데이터 반복자, 손실 함수, 최적화기 및 신경망 레이어는 매우 일반적이기 때문에 현대 라이브러리는 우리를 위해 이러한 구성 요소도 구현합니다.
이 섹션에서는 딥러닝 프레임워크의 고수준 API를 사용하여 :numref:<code>sec_linear_scratch</code>의 (<strong>선형 회귀 모델을 간결하게 구현하는 방법을 보여드리겠습니다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import numpy as np
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import numpy as np
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
import optax
</code></pre>
<h2 id="모델-정의하기-defining-the-model-1"><a class="header" href="#모델-정의하기-defining-the-model-1">모델 정의하기 (Defining the Model)</a></h2>
<p>:numref:<code>sec_linear_scratch</code>에서 선형 회귀를 밑바닥부터 구현했을 때, 우리는 모델 파라미터를 명시적으로 정의하고 기본 선형 대수 연산을 사용하여 출력을 생성하는 계산을 코딩했습니다.
이를 수행하는 방법은 반드시 알고 <em>있어야</em> 합니다.
하지만 모델이 복잡해지고 거의 매일 이 작업을 수행해야 한다면 도구의 도움을 받는 것이 기쁠 것입니다.
상황은 자체 블로그를 밑바닥부터 코딩하는 것과 비슷합니다.
한두 번 해보는 것은 보람 있고 유익하지만, 바퀴를 재발명하는 데 한 달을 보낸다면 서투른 웹 개발자가 될 것입니다.</p>
<p>표준 연산의 경우, 우리는 모델을 구성하는 레이어에 집중할 수 있게 해주는 [<strong>프레임워크의 미리 정의된 레이어를 사용할 수 있습니다.</strong>]
그 구현에 대해 걱정할 필요가 없습니다.
:numref:<code>fig_single_neuron</code>에 설명된 단일 레이어 네트워크의 아키텍처를 상기해 보십시오.
각 입력이 행렬-벡터 곱셈을 통해 각 출력에 연결되기 때문에 이 레이어를 *완전 연결(fully connected)*이라고 합니다.</p>
<p>:begin_tab:<code>mxnet</code>
Gluon에서 완전 연결 레이어는 <code>Dense</code> 클래스에 정의되어 있습니다.
단일 스칼라 출력을 생성하고 싶으므로 그 숫자를 1로 설정합니다.
편의를 위해 Gluon은 각 레이어의 입력 모양을 지정할 것을 요구하지 않는다는 점에 유의할 가치가 있습니다.
따라서 이 선형 레이어에 몇 개의 입력이 들어가는지 Gluon에 말해줄 필요가 없습니다.
나중에 <code>net(X)</code>를 실행할 때처럼 모델을 통해 처음으로 데이터를 전달할 때, Gluon은 자동으로 각 레이어의 입력 수를 추론하여 올바른 모델을 인스턴스화합니다.
이것이 어떻게 작동하는지는 나중에 더 자세히 설명하겠습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
PyTorch에서 완전 연결 레이어는 <code>Linear</code> 및 <code>LazyLinear</code> 클래스(버전 1.8.0부터 사용 가능)에 정의되어 있습니다.
후자는 사용자가 <em>오직</em> 출력 차원만 지정할 수 있게 해주는 반면, 전자는 추가로 이 레이어에 몇 개의 입력이 들어가는지 묻습니다.
입력 모양을 지정하는 것은 불편하고 (합성곱 레이어와 같이) 복잡한 계산이 필요할 수 있습니다.
따라서 단순함을 위해 가능할 때마다 이러한 "lazy" 레이어를 사용할 것입니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
Keras에서 완전 연결 레이어는 <code>Dense</code> 클래스에 정의되어 있습니다.
단일 스칼라 출력을 생성하고 싶으므로 그 숫자를 1로 설정합니다.
편의를 위해 Keras는 각 레이어의 입력 모양을 지정할 것을 요구하지 않는다는 점에 유의할 가치가 있습니다.
이 선형 레이어에 몇 개의 입력이 들어가는지 Keras에 말해줄 필요가 없습니다.
나중에 <code>net(X)</code>를 실행할 때처럼 모델을 통해 처음으로 데이터를 전달하려고 할 때, Keras는 자동으로 각 레이어의 입력 수를 추론합니다.
이것이 어떻게 작동하는지는 나중에 더 자세히 설명하겠습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class LinearRegression(d2l.Module):  #@save
    """고수준 API로 구현된 선형 회귀 모델입니다."""
    def __init__(self, lr):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.net = nn.Dense(1)
            self.net.initialize(init.Normal(sigma=0.01))
        if tab.selected('tensorflow'):
            initializer = tf.initializers.RandomNormal(stddev=0.01)
            self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)
        if tab.selected('pytorch'):
            self.net = nn.LazyLinear(1)
            self.net.weight.data.normal_(0, 0.01)
            self.net.bias.data.fill_(0)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class LinearRegression(d2l.Module):  #@save
    """고수준 API로 구현된 선형 회귀 모델입니다."""
    lr: float

    def setup(self):
        self.net = nn.Dense(1, kernel_init=nn.initializers.normal(0.01))
</code></pre>
<p><code>forward</code> 메서드에서는 단순히 미리 정의된 레이어의 내장 <code>__call__</code> 메서드를 호출하여 출력을 계산합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(LinearRegression)  #@save
def forward(self, X):
    return self.net(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(LinearRegression)  #@save
def forward(self, X):
    return self.net(X)
</code></pre>
<h2 id="손실-함수-정의하기-defining-the-loss-function-1"><a class="header" href="#손실-함수-정의하기-defining-the-loss-function-1">손실 함수 정의하기 (Defining the Loss Function)</a></h2>
<p>:begin_tab:<code>mxnet</code>
<code>loss</code> 모듈은 많은 유용한 손실 함수를 정의합니다.
속도와 편의를 위해 자체 구현은 포기하고 대신 내장된 <code>loss.L2Loss</code>를 선택합니다.
이 함수가 반환하는 <code>loss</code>는 각 예제에 대한 제곱 오차이므로, <code>mean</code>을 사용하여 미니배치에 대해 손실을 평균냅니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
[<strong><code>MSELoss</code> 클래스는 평균 제곱 오차를 계산합니다(:eqref:<code>eq_mse</code>에서 1/2 계수 제외).</strong>]
기본적으로 <code>MSELoss</code>는 예제에 대한 평균 손실을 반환합니다.
직접 구현하는 것보다 빠르고 사용하기 쉽습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<code>MeanSquaredError</code> 클래스는 평균 제곱 오차를 계산합니다(:eqref:<code>eq_mse</code>에서 1/2 계수 제외).
기본적으로 예제에 대한 평균 손실을 반환합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(LinearRegression)  #@save
def loss(self, y_hat, y):
    if tab.selected('mxnet'):
        fn = gluon.loss.L2Loss()
        return fn(y_hat, y).mean()
    if tab.selected('pytorch'):
        fn = nn.MSELoss()
        return fn(y_hat, y)
    if tab.selected('tensorflow'):
        fn = tf.keras.losses.MeanSquaredError()
        return fn(y, y_hat)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(LinearRegression)  #@save
def loss(self, params, X, y, state):
    y_hat = state.apply_fn({'params': params}, *X)
    return d2l.reduce_mean(optax.l2_loss(y_hat, y))
</code></pre>
<h2 id="최적화-알고리즘-정의하기-defining-the-optimization-algorithm-1"><a class="header" href="#최적화-알고리즘-정의하기-defining-the-optimization-algorithm-1">최적화 알고리즘 정의하기 (Defining the Optimization Algorithm)</a></h2>
<p>:begin_tab:<code>mxnet</code>
미니배치 SGD는 신경망을 최적화하기 위한 표준 도구이므로 Gluon은 <code>Trainer</code> 클래스를 통해 이 알고리즘의 다양한 변형과 함께 이를 지원합니다.
Gluon의 <code>Trainer</code> 클래스는 최적화 알고리즘을 의미하며, :numref:<code>sec_oo-design</code>에서 만든 <code>Trainer</code> 클래스는 훈련 메서드, 즉 모델 파라미터를 업데이트하기 위해 최적화기를 반복적으로 호출하는 것을 포함한다는 점에 유의하십시오.
<code>Trainer</code>를 인스턴스화할 때 최적화할 파라미터(우리 모델 <code>net</code>에서 <code>net.collect_params()</code>를 통해 얻을 수 있음), 사용하려는 최적화 알고리즘(<code>sgd</code>), 그리고 최적화 알고리즘에 필요한 하이퍼파라미터 딕셔너리를 지정합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
미니배치 SGD는 신경망을 최적화하기 위한 표준 도구이므로 PyTorch는 <code>optim</code> 모듈에서 이 알고리즘의 다양한 변형과 함께 이를 지원합니다.
우리가 (<strong><code>SGD</code> 인스턴스를 인스턴스화할 때,</strong>) 최적화할 파라미터(우리 모델에서 <code>self.parameters()</code>를 통해 얻을 수 있음)와 최적화 알고리즘에 필요한 학습률(<code>self.lr</code>)을 지정합니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
미니배치 SGD는 신경망을 최적화하기 위한 표준 도구이므로 Keras는 <code>optimizers</code> 모듈에서 이 알고리즘의 다양한 변형과 함께 이를 지원합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(LinearRegression)  #@save
def configure_optimizers(self):
    if tab.selected('mxnet'):
        return gluon.Trainer(self.collect_params(),
                             'sgd', {'learning_rate': self.lr})
    if tab.selected('pytorch'):
        return torch.optim.SGD(self.parameters(), self.lr)
    if tab.selected('tensorflow'):
        return tf.keras.optimizers.SGD(self.lr)
    if tab.selected('jax'):
        return optax.sgd(self.lr)
</code></pre>
<h2 id="훈련-training-2"><a class="header" href="#훈련-training-2">훈련 (Training)</a></h2>
<p>딥러닝 프레임워크의 고수준 API를 통해 우리 모델을 표현하면 더 적은 줄의 코드가 필요하다는 것을 눈치챘을 것입니다.
파라미터를 개별적으로 할당하거나, 손실 함수를 정의하거나, 미니배치 SGD를 구현할 필요가 없었습니다.
훨씬 더 복잡한 모델로 작업하기 시작하면 고수준 API의 이점은 상당히 커질 것입니다.</p>
<p>이제 모든 기본적인 부분이 준비되었으므로, [<strong>훈련 루프 자체는 밑바닥부터 구현한 것과 동일합니다.</strong>]
따라서 모델을 훈련하기 위해 (:numref:<code>oo-design-training</code>에서 소개된) <code>fit</code> 메서드를 호출하기만 하면 됩니다. 이 메서드는 :numref:<code>sec_linear_scratch</code>에서의 <code>fit_epoch</code> 메서드 구현에 의존합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
model = LinearRegression(lr=0.03)
data = d2l.SyntheticRegressionData(w=d2l.tensor([2, -3.4]), b=4.2)
trainer = d2l.Trainer(max_epochs=3)
trainer.fit(model, data)
</code></pre>
<p>아래에서는 [<strong>유한한 데이터에서 훈련하여 학습한 모델 파라미터와 우리 데이터셋을 생성한 실제 파라미터를 비교</strong>]합니다.
파라미터에 액세스하려면 우리가 필요한 레이어의 가중치와 편향에 액세스합니다.
밑바닥부터 구현한 경우와 마찬가지로, 추정된 파라미터가 실제 대응물과 가깝다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(LinearRegression)  #@save
def get_w_b(self):
    if tab.selected('mxnet'):
        return (self.net.weight.data(), self.net.bias.data())
    if tab.selected('pytorch'):
        return (self.net.weight.data, self.net.bias.data)
    if tab.selected('tensorflow'):
        return (self.get_weights()[0], self.get_weights()[1])

w, b = model.get_w_b()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(LinearRegression)  #@save
def get_w_b(self, state):
    net = state.params['net']
    return net['kernel'], net['bias']

w, b = model.get_w_b(trainer.state)
</code></pre>
<pre><code class="language-{.python .input}">print(f'w 추정 오차: {data.w - d2l.reshape(w, data.w.shape)}')
print(f'b 추정 오차: {data.b - b}')
</code></pre>
<h2 id="요약-summary-4"><a class="header" href="#요약-summary-4">요약 (Summary)</a></h2>
<p>이 섹션은 MXNet :cite:<code>Chen.Li.Li.ea.2015</code>, JAX :cite:<code>Frostig.Johnson.Leary.2018</code>, PyTorch :cite:<code>Paszke.Gross.Massa.ea.2019</code>, Tensorflow :cite:<code>Abadi.Barham.Chen.ea.2016</code>와 같은 현대 딥러닝 프레임워크가 제공하는 편리함을 활용한 (이 책에서의) 첫 번째 심층 네트워크 구현을 포함합니다.
우리는 데이터 로딩, 레이어 정의, 손실 함수, 최적화기 및 훈련 루프를 위해 프레임워크 기본값을 사용했습니다.
프레임워크가 필요한 모든 기능을 제공할 때마다 이를 사용하는 것이 일반적으로 좋은 아이디어입니다. 이러한 구성 요소의 라이브러리 구현은 성능을 위해 고도로 최적화되고 신뢰성을 위해 적절하게 테스트되는 경향이 있기 때문입니다.
동시에 이러한 모듈들을 직접 구현할 수 <em>있음</em>을 잊지 마십시오.
이는 현재 어떤 라이브러리에도 존재할 수 없는 새로운 구성 요소를 발명하게 될, 모델 개발의 최첨단에서 살고자 하는 야심 찬 연구자들에게 특히 중요합니다.</p>
<p>:begin_tab:<code>mxnet</code>
Gluon에서 <code>data</code> 모듈은 데이터 처리를 위한 도구를 제공하고, <code>nn</code> 모듈은 많은 수의 신경망 레이어를 정의하며, <code>loss</code> 모듈은 많은 일반적인 손실 함수를 정의합니다.
게다가 <code>initializer</code>는 파라미터 초기화를 위한 많은 선택권을 제공합니다.
사용자에게 편리하게도, 차원과 저장소는 자동으로 추론됩니다.
이러한 지연 초기화(lazy initialization)의 결과로, 파라미터가 인스턴스화(및 초기화)되기 전에 액세스하려고 시도해서는 안 됩니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
PyTorch에서 <code>data</code> 모듈은 데이터 처리를 위한 도구를 제공하고, <code>nn</code> 모듈은 많은 수의 신경망 레이어와 일반적인 손실 함수를 정의합니다.
우리는 <code>_</code>로 끝나는 메서드로 파라미터 값을 대체하여 초기화할 수 있습니다.
네트워크의 입력 차원을 지정해야 한다는 점에 유의하십시오.
지금은 사소해 보이지만, 많은 레이어를 가진 복잡한 네트워크를 설계할 때 상당한 연쇄 효과를 가질 수 있습니다.
이러한 네트워크를 파라미터화하는 방법에 대한 신중한 고려가 이식성을 허용하기 위해 필요합니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
TensorFlow에서 <code>data</code> 모듈은 데이터 처리를 위한 도구를 제공하고, <code>keras</code> 모듈은 많은 수의 신경망 레이어와 일반적인 손실 함수를 정의합니다.
게다가 <code>initializers</code> 모듈은 모델 파라미터 초기화를 위한 다양한 메서드를 제공합니다.
네트워크의 차원과 저장소는 자동으로 추론됩니다(하지만 초기화되기 전에 파라미터에 액세스하려고 시도하지 않도록 주의하십시오).
:end_tab:</p>
<h2 id="연습-문제-exercises-4"><a class="header" href="#연습-문제-exercises-4">연습 문제 (Exercises)</a></h2>
<ol>
<li>미니배치에 대한 총 손실을 미니배치에 대한 손실 평균으로 바꾸면 학습률을 어떻게 변경해야 합니까?</li>
<li>프레임워크 문서를 검토하여 어떤 손실 함수가 제공되는지 확인하십시오. 특히 제곱 손실을 후버(Huber)의 강건한 손실 함수로 대체해 보십시오. 즉, 다음 손실 함수를 사용하십시오.
$$l(y,y') = \begin{cases}|y-y'| -\frac{\sigma}{2} &amp; \textrm{ if } |y-y'| &gt; \sigma \ \frac{1}{2 \sigma} (y-y')^2 &amp; \textrm{ otherwise}\end{cases}$$</li>
<li>모델 가중치의 기울기에 어떻게 액세스합니까?</li>
<li>학습률과 에폭 수를 변경하면 해에 어떤 영향을 미칩니까? 계속 개선되나요?</li>
<li>생성된 데이터의 양을 변경함에 따라 해가 어떻게 변합니까?
<ol>
<li>데이터 양의 함수로서 $\hat{\mathbf{w}} - \mathbf{w}$ 및 $\hat{b} - b$에 대한 추정 오차를 플롯하십시오. 힌트: 데이터 양을 선형적이 아니라 로그적으로 증가시키십시오. 즉, 1000, 2000, ..., 10,000이 아니라 5, 10, 20, 50, ..., 10,000으로 하십시오.</li>
<li>힌트의 제안이 왜 적절할까요?</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/44">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/45">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/204">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17977">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="일반화-generalization"><a class="header" href="#일반화-generalization">일반화 (Generalization)</a></h1>
<p>:label:<code>sec_generalization_basics</code></p>
<p>기말고사를 성실히 준비하는 두 대학생을 생각해 보십시오.
일반적으로 이 준비는 예년의 시험을 치르며 실력을 연습하고 테스트하는 것으로 구성됩니다.
그럼에도 불구하고 과거 시험에서 잘하는 것이 정작 중요한 때에 잘할 것이라는 보장은 아닙니다.
예를 들어, 준비 과정이 예년의 시험 문제 정답을 모두 외우는 것이었던 학생 'Extraordinary Ellie'를 상상해 보십시오.
비록 Ellie가 비범한 기억력을 타고나서 <em>이전에 본</em> 어떤 문제의 답도 완벽하게 기억해낼 수 있더라도, 새로운 (<em>이전에 본 적 없는</em>) 문제에 직면하면 얼어붙을지도 모릅니다.
그에 비해, 암기 능력은 비슷하게 부족하지만 패턴을 파악하는 재주가 있는 또 다른 학생 'Inductive Irene'을 상상해 보십시오.
만약 시험이 정말로 전년도 문제를 재활용한 것이라면, Ellie가 Irene을 가뿐히 이길 것입니다.
Irene의 추론된 패턴이 90% 정확한 예측을 낸다고 해도 Ellie의 100% 암기를 결코 따라올 수 없습니다.
그러나 시험이 전적으로 새로운 문제로 구성된다면, Irene은 여전히 90%의 평균을 유지할 수 있습니다.</p>
<p>머신러닝 과학자로서 우리의 목표는 <em>패턴</em>을 발견하는 것입니다.
하지만 우리가 단순히 데이터를 암기한 것이 아니라 진정으로 <em>일반적인</em> 패턴을 발견했는지 어떻게 확신할 수 있을까요?
대부분의 경우, 우리의 예측은 모델이 그러한 패턴을 발견했을 때만 유용합니다.
우리는 어제의 주가가 아니라 내일의 주가를 예측하고 싶어 합니다.
이전에 본 환자들에 대해 이미 진단된 질병을 인식할 필요는 없으며, 그보다 이전에 본 적 없는 환자들에게서 이전에 진단되지 않은 질병을 인식해야 합니다.
패턴을 발견하여 어떻게 <em>일반화</em>할 것인가 하는 이 문제는 머신러닝의 근본적인 문제이며, 아마도 모든 통계학의 근본적인 문제일 것입니다.
우리는 이 문제를 과학 전체를 집어삼키는 훨씬 더 거대한 질문의 한 조각으로 던질 수 있습니다: 우리가 특정한 관찰에서 더 일반적인 진술로 도약하는 것이 정당화되는 때는 언제일까요?</p>
<p>실제 생활에서 우리는 한정된 데이터 모음을 사용하여 모델을 맞춰야 합니다.
그 데이터의 일반적인 규모는 도메인에 따라 크게 다릅니다.
많은 중요한 의학 문제의 경우, 수천 개의 데이터 포인트만 사용할 수 있습니다.
희귀 질병을 연구할 때는 수백 개에 접근할 수 있다면 운이 좋은 편일 수도 있습니다.
대조적으로, 레이블이 지정된 사진으로 구성된 가장 큰 공개 데이터셋(예: ImageNet :cite:<code>Deng.Dong.Socher.ea.2009</code>)은 수백만 개의 이미지를 포함합니다.
그리고 Flickr YFC100M 데이터셋과 같은 일부 레이블이 지정되지 않은 이미지 모음은 심지어 1억 개 이상의 이미지를 포함할 수 있습니다 :cite:<code>thomee2016yfcc100m</code>.
그러나 이 극한의 규모에서도 사용 가능한 데이터 포인트의 수는 메가픽셀 해상도의 모든 가능한 이미지 공간에 비하면 무한히 작습니다.
한정된 샘플로 작업할 때마다, 우리는 훈련 데이터에는 잘 맞지만 일반화 가능한 패턴을 발견하는 데 실패할 수 있다는 위험을 염두에 두어야 합니다.</p>
<p>기저의 분포보다 훈련 데이터에 더 가깝게 맞추는 현상을 *과대적합(overfitting)*이라고 하며, 과대적합과 싸우는 기술을 종종 <em>정규화(regularization)</em> 방법이라고 합니다.
이것이 통계적 학습 이론(statistical learning theory, :citet:<code>Vapnik98,boucheron2005theory</code> 참조)에 대한 적절한 소개를 대신할 수는 없지만, 여러분이 시작하기에 충분한 직관을 제공할 것입니다.
우리는 책 전반에 걸쳐 많은 장에서 일반화를 다시 다룰 것이며, 다양한 모델에서 일반화의 기초가 되는 원리에 대해 알려진 내용과 실무적인 작업에서 일반화를 개선하는 것으로 (경험적으로) 밝혀진 휴리스틱 기술들을 모두 살펴볼 것입니다.</p>
<h2 id="훈련-오차와-일반화-오차-training-error-and-generalization-error"><a class="header" href="#훈련-오차와-일반화-오차-training-error-and-generalization-error">훈련 오차와 일반화 오차 (Training Error and Generalization Error)</a></h2>
<p>표준 지도 학습 설정에서, 우리는 훈련 데이터와 테스트 데이터가 <em>동일한</em> 분포에서 <em>독립적으로</em> 추출되었다고 가정합니다.
이를 일반적으로 <em>IID 가정</em>이라고 합니다.
이 가정이 강력하기는 하지만, 그러한 가정이 없다면 우리는 아무것도 할 수 없다는 점에 유의할 가치가 있습니다.
분포 $P(X,Y)$에서 샘플링된 훈련 데이터가 <em>다른 분포</em> $Q(X,Y)$에 의해 생성된 테스트 데이터에 대해 예측하는 방법을 알려줄 것이라고 왜 믿어야 할까요?
그러한 도약을 하는 데는 $P$와 $Q$가 어떻게 관련되어 있는지에 대한 강력한 가정이 필요한 것으로 밝혀졌습니다.
나중에 분포의 이동을 허용하는 몇 가지 가정에 대해 논의하겠지만, 먼저 $P(\cdot) = Q(\cdot)$인 IID 사례를 이해해야 합니다.</p>
<p>우선, 훈련 데이터셋에서 계산되는 <em>통계량</em>인 <em>훈련 오차(training error)</em> $R_\textrm{emp}$와, 기저의 분포에 대해 취해진 <em>기댓값</em>인 <em>일반화 오차(generalization error)</em> $R$을 구별해야 합니다.
일반화 오차는 모델을 동일한 기저 데이터 분포에서 추출된 추가 데이터 예제의 무한한 스트림에 적용했을 때 보게 될 오차라고 생각할 수 있습니다.
공식적으로 훈련 오차는 (:numref:<code>sec_linear_regression</code>과 동일한 표기법을 사용하여) <em>합</em>으로 표현됩니다:</p>
<p>$$R_\textrm{emp}[\mathbf{X}, \mathbf{y}, f] = \frac{1}{n} \sum_{i=1}^n l(\mathbf{x}^{(i)}, y^{(i)}, f(\mathbf{x}^{(i)})),$$</p>
<p>반면 일반화 오차는 적분으로 표현됩니다:</p>
<p>$$R[p, f] = E_{(\mathbf{x}, y) \sim P} [l(\mathbf{x}, y, f(\mathbf{x}))] =
\int \int l(\mathbf{x}, y, f(\mathbf{x})) p(\mathbf{x}, y) ;d\mathbf{x} dy.$$</p>
<p>문제는 우리가 일반화 오차 $R$을 정확하게 계산할 수 없다는 것입니다.
아무도 우리에게 밀도 함수 $p(\mathbf{x}, y)$의 정확한 형태를 말해주지 않습니다.
게다가 우리는 무한한 데이터 포인트 스트림을 샘플링할 수 없습니다.
따라서 실제로는 훈련 세트에서 제외된 무작위 예제 선택 $\mathbf{X}'$ 및 레이블 $\mathbf{y}'$로 구성된 독립적인 테스트 세트에 모델을 적용하여 일반화 오차를 <em>추정</em>해야 합니다.
이는 경험적 훈련 오차를 계산하는 데 사용된 것과 동일한 공식을 테스트 세트 $\mathbf{X}', \mathbf{y}'$에 적용하는 것으로 구성됩니다.</p>
<p>결정적으로, 테스트 세트에서 분류기를 평가할 때 우리는 <em>고정된</em> 분류기(테스트 세트의 샘플에 의존하지 않음)로 작업하고 있으므로, 그 오차를 추정하는 것은 단순히 평균 추정 문제입니다.
하지만 훈련 세트에 대해서는 그렇게 말할 수 없습니다.
우리가 얻게 되는 모델은 훈련 세트의 선택에 명시적으로 의존하므로, 훈련 오차는 일반적으로 기저 모집단에 대한 실제 오차의 편향된 추정치가 될 것입니다.
일반화의 중심 질문은 언제 우리의 훈련 오차가 모집단 오차(따라서 일반화 오차)와 가까울 것으로 기대해야 하는가입니다.</p>
<h3 id="모델-복잡도-model-complexity"><a class="header" href="#모델-복잡도-model-complexity">모델 복잡도 (Model Complexity)</a></h3>
<p>고전 이론에서 단순한 모델과 풍부한 데이터가 있을 때, 훈련 오차와 일반화 오차는 가까워지는 경향이 있습니다.
하지만 더 복잡한 모델 및/또는 더 적은 예제로 작업할 때, 훈련 오차는 내려가지만 일반화 갭(generalization gap)은 커질 것으로 예상합니다.
이는 놀라운 일이 아닙니다.
어떤 $n$개 예제의 데이터셋에 대해서도, 설령 무작위로 할당되었더라도 임의의 레이블에 완벽하게 맞출 수 있는 파라미터 세트를 찾을 수 있을 만큼 표현력이 뛰어난 모델 클래스를 상상해 보십시오.
이 경우 훈련 데이터를 완벽하게 맞췄다고 한들, 일반화 오차에 대해 무엇을 결론지을 수 있을까요?
우리가 아는 한, 우리의 일반화 오차는 무작위 추측보다 나을 것이 없을지도 모릅니다.</p>
<p>일반적으로 우리 모델 클래스에 어떠한 제한도 없다면, 훈련 데이터에만 맞춘 것을 근거로 모델이 일반화 가능한 패턴을 발견했다고 결론지을 수 없습니다 :cite:<code>vapnik1994measuring</code>.
반면에 만약 우리 모델 클래스가 임의의 레이블에 맞출 능력이 없었다면, 그것은 반드시 패턴을 발견했어야 합니다.
모델 복잡도에 대한 학습 이론적 아이디어는 반증 가능성(falsifiability)의 기준을 공식화한 영향력 있는 과학 철학자 칼 포퍼(Karl Popper)의 아이디어에서 영감을 얻었습니다.
포퍼에 따르면, 모든 관찰을 설명할 수 있는 이론은 결코 과학적 이론이 아닙니다!
결국 어떤 가능성도 배제하지 못했다면 세상에 대해 무엇을 말해준 것일까요?
요컨대 우리가 원하는 것은 우리가 생각할 수 있는 어떤 관찰도 설명할 수는 <em>없지만</em>, 그럼에도 불구하고 우리가 <em>실제로</em> 내놓은 관찰들과는 우연히 호환되는 가설입니다.</p>
<p>이제 무엇이 적절한 모델 복잡도 개념을 구성하는지는 복잡한 문제입니다.
종종 더 많은 파라미터를 가진 모델이 더 많은 수의 임의로 할당된 레이블에 맞출 수 있습니다.
하지만 이것이 반드시 사실인 것은 아닙니다.
예를 들어 커널 방법은 무한한 수의 파라미터 공간에서 작동하지만, 그 복잡도는 다른 수단에 의해 제어됩니다 :cite:<code>Scholkopf.Smola.2002</code>.
종종 유용하다고 증명되는 복잡도의 한 가지 개념은 파라미터가 취할 수 있는 값의 범위입니다.
여기서 파라미터가 임의의 값을 취하도록 허용되는 모델이 더 복잡할 것입니다.
우리는 다음 섹션에서 첫 번째 실용적인 정규화 기술인 *가중치 감쇠(weight decay)*를 소개할 때 이 아이디어를 다시 다룰 것입니다.
특히, 상당히 다른 모델 클래스(예: 결정 트리 vs. 신경망) 간에 복잡도를 비교하는 것은 어려울 수 있습니다.</p>
<p>이 시점에서 심층 신경망을 소개할 때 다시 다룰 또 다른 중요한 점을 강조해야 합니다.
모델이 임의의 레이블에 맞출 수 있을 때, 낮은 훈련 오차가 반드시 낮은 일반화 오차를 의미하는 것은 아닙니다.
<em>하지만 그렇다고 해서 반드시 높은 일반화 오차를 의미하는 것도 아닙니다!</em>
우리가 자신 있게 말할 수 있는 전부는 낮은 훈련 오차만으로는 낮은 일반화 오차를 증명하기에 충분하지 않다는 것입니다.
심층 신경망은 바로 그러한 모델인 것으로 밝혀졌습니다: 실제로는 일반화가 잘 되지만, 훈련 오차만으로는 많은 결론을 내리기에는 너무 강력합니다.
이러한 경우 우리는 사후에 일반화를 증명하기 위해 홀드아웃(holdout) 데이터에 더 많이 의존해야 합니다.
홀드아웃 데이터, 즉 검증 세트에서의 오차를 *검증 오차(validation error)*라고 합니다.</p>
<h2 id="과소적합-또는-과대적합-underfitting-or-overfitting"><a class="header" href="#과소적합-또는-과대적합-underfitting-or-overfitting">과소적합 또는 과대적합? (Underfitting or Overfitting?)</a></h2>
<p>훈련 오차와 검증 오차를 비교할 때, 우리는 두 가지 일반적인 상황을 유념해야 합니다.
먼저, 훈련 오차와 검증 오차가 모두 상당하지만 그 사이의 격차가 거의 없는 경우를 주의해야 합니다.
모델이 훈련 오차를 줄이지 못한다면, 이는 우리 모델이 모델링하려는 패턴을 포착하기에 너무 단순하다(즉, 충분히 표현력이 없다)는 의미일 수 있습니다.
게다가 훈련 오차와 일반화 오차 사이의 <em>일반화 갭</em>($R_\textrm{emp} - R$)이 작기 때문에, 더 복잡한 모델을 사용해도 괜찮을 것이라고 믿을 근거가 있습니다.
이 현상을 *과소적합(underfitting)*이라고 합니다.</p>
<p>반면에 위에서 논의했듯이, 훈련 오차가 검증 오차보다 현저히 낮은 경우를 주의해야 하며, 이는 심각한 *과대적합(overfitting)*을 나타냅니다.
과대적합이 항상 나쁜 것만은 아니라는 점에 유의하십시오.
특히 딥러닝에서 가장 우수한 예측 모델은 종종 홀드아웃 데이터보다 훈련 데이터에서 훨씬 더 나은 성능을 보입니다.
궁극적으로 우리는 대개 일반화 오차를 낮추는 것에 관심을 가지며, 그 갭에 대해서는 그것이 그 목적에 장애가 될 때만 신경을 씁니다.
만약 훈련 오차가 0이라면, 일반화 갭은 정확히 일반화 오차와 같으며 우리는 오직 갭을 줄임으로써만 진전을 이룰 수 있습니다.</p>
<h3 id="다항식-곡선-맞춤-polynomial-curve-fitting"><a class="header" href="#다항식-곡선-맞춤-polynomial-curve-fitting">다항식 곡선 맞춤 (Polynomial Curve Fitting)</a></h3>
<p>:label:<code>subsec_polynomial-curve-fitting</code></p>
<p>과대적합과 모델 복잡도에 대한 몇 가지 고전적인 직관을 설명하기 위해 다음을 고려해 보십시오:
단일 특성 $x$와 그에 해당하는 실수 값 레이블 $y$로 구성된 훈련 데이터가 주어졌을 때, 레이블 $y$를 추정하기 위해 $d$차 다항식을 찾으려고 합니다.</p>
<p>$$\hat{y}= \sum_{i=0}^d x^i w_i$$</p>
<p>이는 특성이 $x$의 거듭제곱으로 주어지고 모델의 가중치가 $w_i$로 주어지며, 모든 $x$에 대해 $x^0 = 1$이므로 편향이 $w_0$으로 주어지는 선형 회귀 문제일 뿐입니다.
이것은 선형 회귀 문제이므로 제곱 오차를 손실 함수로 사용할 수 있습니다.</p>
<p>고차 다항식 함수는 저차 다항식 함수보다 파라미터가 더 많고 모델 함수의 선택 범위가 더 넓기 때문에 더 복잡합니다.
훈련 데이터셋을 고정했을 때, 고차 다항식 함수는 항상 저차 다항식에 비해 더 낮은(최악의 경우 동일한) 훈련 오차를 달성해야 합니다.
사실, 각 데이터 예제가 서로 다른 $x$ 값을 가질 때마다 데이터 예제 수와 동일한 차수의 다항식 함수는 훈련 세트를 완벽하게 맞출 수 있습니다.
우리는 :numref:<code>fig_capacity_vs_error</code>에서 다항식 차수(모델 복잡도)와 과소적합 및 과대적합 사이의 관계를 비교합니다.</p>
<p><img src="chapter_linear-regression/../img/capacity-vs-error.svg" alt="과소적합과 과대적합에 대한 모델 복잡도의 영향." />
:label:<code>fig_capacity_vs_error</code></p>
<h3 id="데이터셋-크기-dataset-size"><a class="header" href="#데이터셋-크기-dataset-size">데이터셋 크기 (Dataset Size)</a></h3>
<p>위의 경계가 이미 나타내듯이, 유념해야 할 또 다른 큰 고려 사항은 데이터셋 크기입니다.
모델을 고정했을 때, 훈련 데이터셋의 샘플 수가 적을수록 과대적합이 발생할 가능성이 더 높고 더 심각해집니다.
훈련 데이터의 양을 늘리면 일반적으로 일반화 오차가 감소합니다.
게다가 일반적으로 데이터는 많을수록 좋습니다.
고정된 작업과 데이터 분포에 대해, 모델 복잡도는 데이터의 양보다 더 빠르게 증가해서는 안 됩니다.
데이터가 많아지면 더 복잡한 모델을 맞추려고 시도할 수 있습니다.
충분한 데이터가 없으면 단순한 모델을 이기기가 더 어려울 수 있습니다.
많은 작업에서 딥러닝은 수만 개의 훈련 예제를 사용할 수 있을 때만 선형 모델보다 뛰어난 성능을 보입니다.
부분적으로 현재 딥러닝의 성공은 인터넷 기업, 저렴한 저장소, 연결된 장치 및 경제의 광범위한 디지털화에서 비롯된 막대한 데이터셋의 풍부함에 크게 힘입었습니다.</p>
<h2 id="모델-선택-model-selection"><a class="header" href="#모델-선택-model-selection">모델 선택 (Model Selection)</a></h2>
<p>:label:<code>subsec_generalization-model-selection</code></p>
<p>일반적으로 우리는 다양한 방식(서로 다른 아키텍처, 훈련 목표, 선택된 특성, 데이터 전처리, 학습률 등)으로 서로 다른 여러 모델을 평가한 후에만 최종 모델을 선택합니다.
많은 모델 중에서 선택하는 것을 적절하게 *모델 선택(model selection)*이라고 합니다.</p>
<p>원칙적으로 우리는 모든 하이퍼파라미터를 선택할 때까지 테스트 세트를 건드려서는 안 됩니다.
만약 모델 선택 과정에서 테스트 데이터를 사용했다면, 테스트 데이터에 과대적합될 위험이 있습니다.
그렇게 되면 심각한 문제에 빠지게 됩니다.
훈련 데이터에 과대적합되더라도 우리를 정직하게 유지해 줄 테스트 데이터에 대한 평가가 항상 있지만, 테스트 데이터에 과대적합되면 우리가 그것을 어떻게 알 수 있겠습니까?
복잡도가 엄격하게 제어될 수 있는 모델에서조차 이것이 어떻게 황당한 결과로 이어질 수 있는지에 대한 예는 :citet:<code>ong2005learning</code>를 참조하십시오.</p>
<p>따라서 모델 선택을 위해 테스트 데이터에 결코 의존해서는 안 됩니다.
그렇다고 모델을 훈련하는 데 사용하는 바로 그 데이터에서 일반화 오차를 추정할 수 없기 때문에 훈련 데이터에만 전적으로 의존할 수도 없습니다.</p>
<p>실제 응용 분야에서 그림은 더 흐릿해집니다.
이상적으로는 가장 좋은 모델을 평가하거나 소수의 모델을 서로 비교하기 위해 테스트 데이터를 단 한 번만 건드려야 하겠지만, 실제 테스트 데이터는 한 번 사용한 후에 폐기되는 경우가 거의 없습니다.
우리는 매번 실험할 때마다 새로운 테스트 세트를 마련할 여력이 거의 없습니다.
사실, 수십 년 동안 벤치마크 데이터를 재활용하는 것은 <a href="https://paperswithcode.com/sota/image-classification-on-imagenet">이미지 분류</a> 및 <a href="https://paperswithcode.com/sota/image-classification-on-mnist">광학 문자 인식</a>과 같은 알고리즘 개발에 상당한 영향을 미칠 수 있습니다.</p>
<p><em>테스트 세트에서의 훈련</em> 문제를 해결하기 위한 일반적인 관행은 데이터를 세 가지로 나누어 훈련 및 테스트 데이터셋 외에 *검증 세트(validation set)*를 포함하는 것입니다.
그 결과 검증 데이터와 테스트 데이터 사이의 경계가 걱정스러울 정도로 모호해지는 불분명한 상황이 벌어집니다.
이 책의 실험에서 명시적으로 언급되지 않는 한, 우리는 실제로는 훈련 데이터와 검증 데이터로 작업하고 있으며 진정한 테스트 세트는 없습니다.
따라서 이 책의 각 실험에서 보고된 정확도는 실제로는 검증 정확도이며 진정한 테스트 세트 정확도가 아닙니다.</p>
<h3 id="교차-검증-cross-validation"><a class="header" href="#교차-검증-cross-validation">교차 검증 (Cross-Validation)</a></h3>
<p>훈련 데이터가 부족할 때, 우리는 적절한 검증 세트를 구성할 만큼 충분한 데이터를 떼어놓을 여유조차 없을 수도 있습니다.
이 문제에 대한 한 가지 대중적인 해결책은 *$K$-겹 교차 검증($K$-fold cross-validation)*을 사용하는 것입니다.
여기서 원래 훈련 데이터는 겹치지 않는 $K$개의 부분 집합으로 나뉩니다.
그런 다음 모델 훈련과 검증이 $K$번 실행되는데, 매번 $K-1$개의 부분 집합에서 훈련하고 다른 부분 집합(해당 라운드에서 훈련에 사용되지 않은 것)에서 검증합니다.
마지막으로 $K$번의 실험 결과를 평균내어 훈련 및 검증 오차를 추정합니다.</p>
<h2 id="요약-summary-5"><a class="header" href="#요약-summary-5">요약 (Summary)</a></h2>
<p>이 섹션에서는 머신러닝에서 일반화의 토대 중 일부를 살펴보았습니다.
이러한 아이디어 중 일부는 심층 모델로 갈수록 복잡해지고 직관에 반하게 됩니다. 여기서는 모델이 데이터를 심하게 과대적합할 수 있으며, 관련 복잡도 개념이 암시적이고 직관에 반할 수 있습니다(예: 파라미터가 더 많은 더 큰 아키텍처가 더 잘 일반화됨).
몇 가지 경험 법칙을 남겨드립니다:</p>
<ol>
<li>모델 선택을 위해 검증 세트(또는 <em>$K$-겹 교차 검증</em>)를 사용하십시오.</li>
<li>복잡한 모델은 종종 더 많은 데이터를 필요로 합니다.</li>
<li>관련 복잡도 개념에는 파라미터 수와 파라미터가 허용되는 값의 범위가 모두 포함됩니다.</li>
<li>다른 모든 조건이 동일할 때, 데이터가 많을수록 거의 항상 더 나은 일반화로 이어집니다.</li>
<li>이 모든 일반화에 대한 이야기는 IID 가정을 전제로 합니다. 만약 이 가정을 완화하여 훈련 기간과 테스트 기간 사이에 분포가 이동하는 것을 허용한다면, 추가적인 (아마도 더 완만한) 가정 없이는 일반화에 대해 아무것도 말할 수 없습니다.</li>
</ol>
<h2 id="연습-문제-exercises-5"><a class="header" href="#연습-문제-exercises-5">연습 문제 (Exercises)</a></h2>
<ol>
<li>다항식 회귀 문제를 정확하게 풀 수 있는 때는 언제입니까?</li>
<li>종속 확률 변수로 인해 문제를 IID 데이터로 취급하는 것이 바람직하지 않은 예시를 최소 다섯 가지 드십시오.</li>
<li>훈련 오차가 0이 될 것으로 기대할 수 있습니까? 어떤 상황에서 일반화 오차가 0이 되는 것을 볼 수 있을까요?</li>
<li>$K$-겹 교차 검증이 왜 계산 비용이 매우 많이 들까요?</li>
<li>$K$-겹 교차 검증 오차 추정치가 왜 편향되어 있을까요?</li>
<li>VC 차원(VC dimension)은 일련의 함수들의 함수에 의해 임의의 레이블 {± 1}로 분류될 수 있는 최대 포인트 수로 정의됩니다. 이것이 함수 클래스가 얼마나 복잡한지 측정하는 데 좋은 아이디어가 아닌 이유는 무엇일까요? 힌트: 함수의 크기를 고려하십시오.</li>
<li>매니저가 현재 알고리즘이 잘 작동하지 않는 어려운 데이터셋을 줍니다. 더 많은 데이터가 필요하다는 것을 그에게 어떻게 정당화하시겠습니까? 힌트: 데이터를 늘릴 수는 없지만 줄일 수는 있습니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/96">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/97">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/234">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17978">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="가중치-감쇠-weight-decay"><a class="header" href="#가중치-감쇠-weight-decay">가중치 감쇠 (Weight Decay)</a></h1>
<p>:label:<code>sec_weight_decay</code></p>
<p>이제 과대적합 문제를 규명했으므로, 첫 번째 <em>정규화(regularization)</em> 기술을 소개할 수 있습니다.
더 많은 훈련 데이터를 수집함으로써 언제나 과대적합을 완화할 수 있다는 점을 상기하십시오.
그러나 이는 비용이 많이 들고 시간이 오래 걸리거나, 우리의 통제 밖일 수 있어 단기적으로는 불가능할 수 있습니다.
지금은 이미 리소스가 허용하는 만큼의 고품질 데이터를 확보했다고 가정하고, 데이터셋이 주어진 것으로 간주될 때 사용할 수 있는 도구에 집중해 보겠습니다.</p>
<p>다항식 회귀 예제(:numref:<code>subsec_polynomial-curve-fitting</code>)에서 적합된 다항식의 차수를 조정하여 모델의 용량을 제한할 수 있었다는 점을 상기하십시오.
실제로 특성 수를 제한하는 것은 과대적합을 완화하기 위한 대중적인 기술입니다.
하지만 단순히 특성을 버리는 것은 너무 무딘 도구일 수 있습니다.
다항식 회귀 예제를 계속해서 생각해보면, 고차원 입력에서 어떤 일이 일어날지 고려해 보십시오.
다변수 데이터로의 다항식의 자연스러운 확장을 *단항식(monomials)*이라고 하며, 이는 단순히 변수의 거듭제곱들의 곱입니다.
단항식의 차수는 거듭제곱의 합입니다. 예를 들어, $x_1^2 x_2$와 $x_3 x_5^2$는 모두 3차 단항식입니다.</p>
<p>$d$차 항의 수는 $d$가 커짐에 따라 급격히 늘어납니다.
$k$개의 변수가 주어졌을 때 $d$차 단항식의 수는 ${k - 1 + d} \choose {k - 1}$입니다.
차수가 2에서 3으로 조금만 변해도 모델의 복잡도는 비약적으로 증가합니다.
따라서 우리는 종종 함수 복잡도를 조정하기 위한 더 미세한 도구가 필요합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
import optax
</code></pre>
<h2 id="노름과-가중치-감쇠-norms-and-weight-decay"><a class="header" href="#노름과-가중치-감쇠-norms-and-weight-decay">노름과 가중치 감쇠 (Norms and Weight Decay)</a></h2>
<p>(<strong>파라미터 수를 직접 조작하는 대신, *가중치 감쇠(weight decay)*는 파라미터가 취할 수 있는 값을 제한함으로써 작동합니다.</strong>)
딥러닝 이외의 분야에서 미니배치 확률적 경사 하강법으로 최적화될 때 더 흔히 $\ell_2$ 정규화라고 불리는 가중치 감쇠는, 파라미터화된 머신러닝 모델을 정규화하기 위해 가장 널리 사용되는 기술일 것입니다.
이 기술은 모든 함수 $f$ 중에서 함수 $f = 0$ (모든 입력에 값 0을 할당)이 어떤 의미에서 가장 <em>단순</em>하며, 파라미터가 0에서 떨어진 거리로 함수의 복잡도를 측정할 수 있다는 기본적인 직관에 근거합니다.
하지만 함수와 0 사이의 거리를 정확히 어떻게 측정해야 할까요?
정답은 하나가 아닙니다. 사실 함수 해석학의 일부와 바나흐 공간(Banach spaces) 이론을 포함한 수학의 전체 분야가 이러한 문제를 다루는 데 헌신하고 있습니다.</p>
<p>하나의 간단한 해석은 선형 함수 $f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$의 복잡도를 그 가중치 벡터의 어떤 노름, 예를 들어 $| \mathbf{w} |^2$으로 측정하는 것일 수 있습니다.
우리는 :numref:<code>subsec_lin-algebra-norms</code>에서 더 일반적인 $\ell_p$ 노름의 특수한 경우인 $\ell_2$ 노름과 $\ell_1$ 노름을 소개했습니다.
가중치 벡터를 작게 유지하는 가장 일반적인 방법은 손실을 최소화하는 문제에 노름을 페널티 항으로 추가하는 것입니다.
따라서 우리는 원래의 목표인 <em>훈련 레이블에 대한 예측 손실 최소화</em>를 새로운 목표인 <em>예측 손실과 페널티 항의 합 최소화</em>로 대체합니다.
이제 가중치 벡터가 너무 커지면, 학습 알고리즘은 훈련 오차를 최소화하는 것보다 가중치 노름 $| \mathbf{w} |^2$을 최소화하는 데 집중할 수 있습니다.
그것이 바로 우리가 원하는 것입니다.
이를 코드로 설명하기 위해 :numref:<code>sec_linear_regression</code>의 선형 회귀 예제를 다시 가져와 보겠습니다.
거기서 우리의 손실은 다음과 같이 주어졌습니다.</p>
<p>$$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$</p>
<p>$\mathbf{x}^{(i)}$는 특성, $y^{(i)}$는 데이터 예제 $i$에 대한 레이블, $(\mathbf{w}, b)$는 각각 가중치와 편향 파라미터임을 상기하십시오.
가중치 벡터의 크기에 페널티를 주려면 어떻게든 손실 함수에 $| \mathbf{w} |^2$을 추가해야 하지만, 모델이 표준 손실과 이 새로운 가산 페널티 사이에서 어떻게 절충(trade-off)해야 할까요?
실제로는 검증 데이터를 사용하여 맞추는 음이 아닌 하이퍼파라미터인 <em>정규화 상수</em> $\lambda$를 통해 이 절충을 특성화합니다:</p>
<p>$$L(\mathbf{w}, b) + \frac{\lambda}{2} |\mathbf{w}|^2.$$</p>
<p>$\lambda = 0$인 경우 원래의 손실 함수를 복구합니다.
$\lambda &gt; 0$인 경우 $| \mathbf{w} |$의 크기를 제한합니다.
관례적으로 2로 나눕니다: 이차 함수의 도함수를 취할 때 2와 1/2이 상쇄되어 업데이트 식이 깔끔하고 단순해지기 때문입니다.
기민한 독자는 왜 표준 노름(유클리드 거리)이 아니라 제곱 노름을 사용하는지 궁금할 수 있습니다.
우리는 계산적 편의를 위해 이렇게 합니다. $\ell_2$ 노름을 제곱함으로써 제곱근을 제거하고 가중치 벡터의 각 성분의 제곱 합만 남깁니다.
이는 페널티의 도함수를 계산하기 쉽게 만듭니다: 합의 도함수는 도함수의 합과 같기 때문입니다.</p>
<p>게다가 왜 애초에 $\ell_1$ 노름 등이 아니라 $\ell_2$ 노름을 사용하는지 물을 수도 있습니다.
사실 다른 선택들도 유효하며 통계학 전반에서 인기가 있습니다.
$\ell_2$ 정규화된 선형 모델은 고전적인 <em>릿지 회귀(ridge regression)</em> 알고리즘을 구성하는 반면, $\ell_1$ 정규화된 선형 회귀는 통계학에서 마찬가지로 근본적인 방법으로 흔히 *라소 회귀(lasso regression)*로 알려져 있습니다.
$\ell_2$ 노름을 사용하는 한 가지 이유는 가중치 벡터의 큰 성분에 대해 과도한 페널티를 부여하기 때문입니다.
이는 학습 알고리즘이 더 많은 수의 특성에 가중치를 고르게 분산시키는 모델을 선호하게 만듭니다.
실제로 이는 단일 변수의 측정 오차에 대해 모델을 더 강건하게 만들 수 있습니다.
대조적으로, $\ell_1$ 페널티는 다른 가중치를 0으로 만듦으로써 작은 특성 세트에 가중치를 집중시키는 모델로 이어집니다.
이는 다른 이유로 바람직할 수 있는 *특성 선택(feature selection)*을 위한 효과적인 방법을 제공합니다.
예를 들어 모델이 몇 가지 특성에만 의존한다면, 다른 (버려진) 특성에 대한 데이터를 수집, 저장 또는 전송할 필요가 없기 때문입니다.</p>
<p>:eqref:<code>eq_linreg_batch_update</code>와 동일한 표기법을 사용하여, $\ell_2$ 정규화된 회귀에 대한 미니배치 확률적 경사 하강법 업데이트는 다음과 같습니다:</p>
<p>$$\begin{aligned}
\mathbf{w} &amp; \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).
\end{aligned}$$</p>
<p>이전과 마찬가지로, 추정치가 관찰값과 다른 정도에 따라 $\mathbf{w}$를 업데이트합니다.
하지만 우리는 또한 $\mathbf{w}$의 크기를 0을 향해 수축시킵니다.
그래서 이 방법을 때때로 "가중치 감쇠"라고 부릅니다: 페널티 항만 주어졌을 때, 최적화 알고리즘이 훈련의 각 단계에서 가중치를 *감쇠(decays)*시키기 때문입니다.
특성 선택과 대조적으로, 가중치 감쇠는 함수의 복잡도를 연속적으로 조정하는 메커니즘을 제공합니다.
작은 $\lambda$ 값은 덜 제한된 $\mathbf{w}$에 해당하고, 큰 $\lambda$ 값은 $\mathbf{w}$를 더 상당히 제한합니다.
해당하는 편향 페널티 $b^2$을 포함할지 여부는 구현마다 다를 수 있으며 신경망의 레이어마다 다를 수 있습니다.
종종 우리는 편향 항을 정규화하지 않습니다.
게다가 다른 최적화 알고리즘의 경우 $\ell_2$ 정규화가 가중치 감쇠와 동일하지 않을 수 있지만, 가중치의 크기를 줄여 정규화한다는 아이디어는 여전히 유효합니다.</p>
<h2 id="고차원-선형-회귀-high-dimensional-linear-regression"><a class="header" href="#고차원-선형-회귀-high-dimensional-linear-regression">고차원 선형 회귀 (High-Dimensional Linear Regression)</a></h2>
<p>간단한 합성 예제를 통해 가중치 감쇠의 이점을 설명할 수 있습니다.</p>
<p>먼저, 이전처럼 데이터를 생성합니다:</p>
<p>(<strong>$$y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \textrm{ 여기서 }
\epsilon \sim \mathcal{N}(0, 0.01^2).$$</strong>)</p>
<p>이 합성 데이터셋에서 레이블은 입력의 기저 선형 함수에 평균 0, 표준 편차 0.01의 가우스 노이즈가 더해져 주어집니다.
설명을 위해 문제의 차원을 $d = 200$으로 늘리고 예제가 20개뿐인 작은 훈련 세트를 사용하여 과대적합의 효과를 두드러지게 만들 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
class Data(d2l.DataModule):
    def __init__(self, num_train, num_val, num_inputs, batch_size):
        self.save_hyperparameters()                
        n = num_train + num_val 
        if tab.selected('mxnet') or tab.selected('pytorch'):
            self.X = d2l.randn(n, num_inputs)
            noise = d2l.randn(n, 1) * 0.01
        if tab.selected('tensorflow'):
            self.X = d2l.normal((n, num_inputs))
            noise = d2l.normal((n, 1)) * 0.01
        if tab.selected('jax'):
            self.X = jax.random.normal(jax.random.PRNGKey(0), (n, num_inputs))
            noise = jax.random.normal(jax.random.PRNGKey(0), (n, 1)) * 0.01
        w, b = d2l.ones((num_inputs, 1)) * 0.01, 0.05
        self.y = d2l.matmul(self.X, w) + b + noise

    def get_dataloader(self, train):
        i = slice(0, self.num_train) if train else slice(self.num_train, None)
        return self.get_tensorloader([self.X, self.y], train, i)
</code></pre>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>이제 가중치 감쇠를 밑바닥부터 구현해 보겠습니다.
미니배치 확률적 경사 하강법이 우리의 최적화기이므로, 원래 손실 함수에 제곱 $\ell_2$ 페널티를 추가하기만 하면 됩니다.</p>
<h3 id="ell_2-노름-페널티-정의하기"><a class="header" href="#ell_2-노름-페널티-정의하기">(<strong>$\ell_2$ 노름 페널티 정의하기</strong>)</a></h3>
<p>이 페널티를 구현하는 가장 편리한 방법은 모든 항을 제자리에서 제곱하고 합산하는 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def l2_penalty(w):
    return d2l.reduce_sum(w**2) / 2
</code></pre>
<h3 id="모델-정의하기"><a class="header" href="#모델-정의하기">모델 정의하기</a></h3>
<p>최종 모델에서 선형 회귀와 제곱 손실은 :numref:<code>sec_linear_scratch</code> 이후로 바뀌지 않았으므로, <code>d2l.LinearRegressionScratch</code>의 서브클래스를 정의하기만 하면 됩니다. 유일한 변경 사항은 이제 손실에 페널티 항이 포함된다는 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class WeightDecayScratch(d2l.LinearRegressionScratch):
    def __init__(self, num_inputs, lambd, lr, sigma=0.01):
        super().__init__(num_inputs, lr, sigma)
        self.save_hyperparameters()
        
    def loss(self, y_hat, y):
        return (super().loss(y_hat, y) +
                self.lambd * l2_penalty(self.w))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class WeightDecayScratch(d2l.LinearRegressionScratch):
    lambd: int = 0
        
    def loss(self, params, X, y, state):
        return (super().loss(params, X, y, state) +
                self.lambd * l2_penalty(params['w']))
</code></pre>
<p>다음 코드는 20개 예제가 있는 훈련 세트에서 모델을 맞추고 100개 예제가 있는 검증 세트에서 평가합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)
trainer = d2l.Trainer(max_epochs=10)

def train_scratch(lambd):    
    model = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)
    model.board.yscale='log'
    trainer.fit(model, data)
    if tab.selected('pytorch', 'mxnet', 'tensorflow'):
        print('w의 L2 노름:', float(l2_penalty(model.w)))
    if tab.selected('jax'):
        print('w의 L2 노름:',
              float(l2_penalty(trainer.state.params['w'])))
</code></pre>
<h3 id="정규화-없이-훈련하기"><a class="header" href="#정규화-없이-훈련하기">[<strong>정규화 없이 훈련하기</strong>]</a></h3>
<p>이제 <code>lambd = 0</code>으로 이 코드를 실행하여 가중치 감쇠를 비활성화합니다.
훈련 오차는 감소하지만 검증 오차는 감소하지 않는 심각한 과대적합이 발생합니다. 이는 과대적합의 교과서적인 사례입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
train_scratch(0)
</code></pre>
<h3 id="가중치-감쇠-사용하기"><a class="header" href="#가중치-감쇠-사용하기">[<strong>가중치 감쇠 사용하기</strong>]</a></h3>
<p>아래에서는 상당한 가중치 감쇠를 사용하여 실행합니다.
훈련 오차는 증가하지만 검증 오차는 감소하는 것을 보십시오.
이것이 바로 우리가 정규화에서 기대하는 효과입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
train_scratch(3)
</code></pre>
<h2 id="간결한-구현"><a class="header" href="#간결한-구현">[<strong>간결한 구현</strong>]</a></h2>
<p>가중치 감쇠는 신경망 최적화에서 어디에나 존재하기 때문에, 딥러닝 프레임워크는 이를 특히 편리하게 만들어 최적화 알고리즘 자체에 가중치 감쇠를 통합하여 모든 손실 함수와 함께 쉽게 사용할 수 있게 합니다.
게다가 이 통합은 계산적인 이점을 제공하여, 추가적인 계산 오버헤드 없이 알고리즘에 가중치 감쇠를 추가하는 구현 트릭을 가능하게 합니다.
업데이트의 가중치 감쇠 부분은 각 파라미터의 현재 값에만 의존하기 때문에, 최적화기는 어쨌든 각 파라미터를 한 번 건드려야 합니다.</p>
<p>:begin_tab:<code>mxnet</code>
아래에서는 <code>Trainer</code>를 인스턴스화할 때 <code>wd</code>를 통해 가중치 감쇠 하이퍼파라미터를 직접 지정합니다.
기본적으로 Gluon은 가중치와 편향을 동시에 감쇠시킵니다.
하이퍼파라미터 <code>wd</code>는 모델 파라미터를 업데이트할 때 <code>wd_mult</code>와 곱해진다는 점에 유의하십시오.
따라서 <code>wd_mult</code>를 0으로 설정하면 편향 파라미터 $b$는 감쇠하지 않습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
아래에서는 최적화기를 인스턴스화할 때 <code>weight_decay</code>를 통해 가중치 감쇠 하이퍼파라미터를 직접 지정합니다.
기본적으로 PyTorch는 가중치와 편향을 동시에 감쇠시키지만, 서로 다른 정책에 따라 서로 다른 파라미터를 처리하도록 최적화기를 구성할 수 있습니다.
여기서는 가중치(<code>net.weight</code> 파라미터)에 대해서만 <code>weight_decay</code>를 설정하므로 편향(<code>net.bias</code> 파라미터)은 감쇠하지 않습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
아래에서는 가중치 감쇠 하이퍼파라미터 <code>wd</code>를 사용하여 $\ell_2$ 정규화기를 만들고, <code>kernel_regularizer</code> 인수를 통해 레이어의 가중치에 적용합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class WeightDecay(d2l.LinearRegression):
    def __init__(self, wd, lr):
        super().__init__(lr)
        self.save_hyperparameters()
        self.wd = wd
        
    def configure_optimizers(self):
        self.collect_params('.*bias').setattr('wd_mult', 0)
        return gluon.Trainer(self.collect_params(),
                             'sgd', 
                             {'learning_rate': self.lr, 'wd': self.wd})
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class WeightDecay(d2l.LinearRegression):
    def __init__(self, wd, lr):
        super().__init__(lr)
        self.save_hyperparameters()
        self.wd = wd

    def configure_optimizers(self):
        return torch.optim.SGD([
            {'params': self.net.weight, 'weight_decay': self.wd},
            {'params': self.net.bias}], lr=self.lr)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class WeightDecay(d2l.LinearRegression):
    def __init__(self, wd, lr):
        super().__init__(lr)
        self.save_hyperparameters()
        self.net = tf.keras.layers.Dense(
            1, kernel_regularizer=tf.keras.regularizers.l2(wd),
            kernel_initializer=tf.keras.initializers.RandomNormal(0, 0.01)
        )
        
    def loss(self, y_hat, y):
        return super().loss(y_hat, y) + self.net.losses
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class WeightDecay(d2l.LinearRegression):
    wd: int = 0
    
    def configure_optimizers(self):
        # 가중치 감쇠는 optax.sgd 내에서 직접 사용할 수 없지만,
        # optax를 사용하면 여러 변환을 함께 연결할 수 있습니다.
        return optax.chain(optax.additive_weight_decay(self.wd),
                           optax.sgd(self.lr))
</code></pre>
<p>[<strong>플롯은 가중치 감쇠를 밑바닥부터 구현했을 때와 비슷해 보입니다</strong>].
하지만 이 버전은 더 빠르게 실행되고 구현하기 더 쉬우며, 더 큰 문제를 다루고 이 작업이 일상화됨에 따라 이러한 이점은 더욱 두드러질 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
model = WeightDecay(wd=3, lr=0.01)
model.board.yscale='log'
trainer.fit(model, data)

if tab.selected('jax'):
    print('w의 L2 노름:', float(l2_penalty(model.get_w_b(trainer.state)[0])))
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    print('w의 L2 노름:', float(l2_penalty(model.get_w_b()[0])))
</code></pre>
<p>지금까지 우리는 단순한 선형 함수가 무엇인지에 대한 한 가지 개념을 다루었습니다.
하지만 단순한 비선형 함수에 대해서도 상황은 훨씬 더 복잡할 수 있습니다. 이를 확인하기 위해, <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">재생 커널 힐베르트 공간(RKHS)</a> 개념을 사용하면 선형 함수를 위해 도입된 도구를 비선형 맥락에서 적용할 수 있습니다.
불행히도 RKHS 기반 알고리즘은 크고 고차원적인 데이터로 잘 확장되지 않는 경향이 있습니다.
이 책에서 우리는 심층 네트워크의 모든 레이어에 가중치 감쇠를 적용하는 일반적인 휴리스틱을 종종 채택할 것입니다.</p>
<h2 id="요약-summary-6"><a class="header" href="#요약-summary-6">요약 (Summary)</a></h2>
<p>정규화는 과대적합을 다루기 위한 일반적인 방법입니다. 고전적인 정규화 기술은 (훈련 시) 손실 함수에 페널티 항을 추가하여 학습된 모델의 복잡도를 줄입니다.
모델을 단순하게 유지하기 위한 한 가지 특별한 선택은 $\ell_2$ 페널티를 사용하는 것입니다. 이는 미니배치 확률적 경사 하강법 알고리즘의 업데이트 단계에서 가중치 감쇠로 이어집니다.
실제로 가중치 감쇠 기능은 딥러닝 프레임워크의 최적화기에서 제공됩니다.
동일한 훈련 루프 내에서 서로 다른 파라미터 세트가 서로 다른 업데이트 동작을 가질 수 있습니다.</p>
<h2 id="연습-문제-exercises-6"><a class="header" href="#연습-문제-exercises-6">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 섹션의 추정 문제에서 $\lambda$ 값을 실험해 보십시오. $\lambda$의 함수로서 훈련 및 검증 정확도를 플롯하십시오. 무엇을 관찰하셨습니까?</li>
<li>검증 세트를 사용하여 $\lambda$의 최적 값을 찾으십시오. 정말 최적의 값인가요? 이것이 중요한가요?</li>
<li>$|\mathbf{w}|^2$ 대신 페널티로 $\sum_i |w_i|$를 사용한다면 업데이트 식은 어떻게 보일까요 ($\ell_1$ 정규화)?</li>
<li>우리는 $|\mathbf{w}|^2 = \mathbf{w}^\top \mathbf{w}$임을 알고 있습니다. 행렬에 대해서도 유사한 식을 찾을 수 있습니까 (:numref:<code>subsec_lin-algebra-norms</code>의 프로베니우스 노름 참조)?</li>
<li>훈련 오차와 일반화 오차의 관계를 검토하십시오. 가중치 감쇠, 훈련량 증가, 적절한 복잡도의 모델 사용 외에 과대적합을 처리하는 데 도움이 될 수 있는 다른 방법은 무엇이 있을까요?</li>
<li>베이지안 통계에서는 $P(w \mid x) \propto P(x \mid w) P(w)$를 통해 사후 확률에 도달하기 위해 사전 확률과 우도의 곱을 사용합니다. $P(w)$를 정규화와 어떻게 연관 지을 수 있을까요?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/98">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/99">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/236">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17979">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="분류를-위한-선형-신경망-linear-neural-networks-for-classification"><a class="header" href="#분류를-위한-선형-신경망-linear-neural-networks-for-classification">분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a></h1>
<p>:label:<code>chap_classification</code></p>
<p>이제 모든 메커니즘을 살펴봤으므로, 배운 기술을 더 넓은 종류의 작업에 적용할 준비가 되었습니다.
분류로 방향을 전환하더라도 데이터 로딩, 모델 통과, 출력 생성, 손실 계산, 가중치에 대한 기울기 계산, 모델 업데이트와 같은 대부분의 배관 작업은 동일하게 유지됩니다.
하지만 타겟의 정확한 형태, 출력 레이어의 파라미터화, 손실 함수의 선택은 <em>분류(classification)</em> 설정에 맞게 조정될 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

softmax-regression
image-classification-dataset
classification
softmax-regression-scratch
softmax-regression-concise
generalization-classification
environment-and-distribution-shift
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="소프트맥스-회귀-softmax-regression"><a class="header" href="#소프트맥스-회귀-softmax-regression">소프트맥스 회귀 (Softmax Regression)</a></h1>
<p>:label:<code>sec_softmax</code></p>
<p>:numref:<code>sec_linear_regression</code>에서 우리는 선형 회귀를 소개하고, :numref:<code>sec_linear_scratch</code>에서 밑바닥부터 구현했으며, :numref:<code>sec_linear_concise</code>에서 딥러닝 프레임워크의 고수준 API를 사용하여 구현했습니다.</p>
<p>회귀는 *얼마나 많이?*라는 질문에 답하고 싶을 때 사용하는 도구입니다.
주택이 판매될 가격(달러), 야구팀의 승수, 환자가 퇴원하기 전까지 입원해 있을 일수 등을 예측하고 싶다면 아마도 회귀 모델을 찾고 있을 것입니다.
하지만 회귀 모델 내에서도 중요한 구분이 있습니다.
예를 들어, 주택 가격은 결코 음수가 될 수 없으며 변화는 종종 기준 가격에 대해 <em>상대적</em>일 수 있습니다.
따라서 가격의 로그에 대해 회귀를 수행하는 것이 더 효과적일 수 있습니다.
마찬가지로 환자가 병원에서 보내는 일수는 <em>이산형 음이 아닌</em> 확률 변수입니다.
따라서 최소 평균 제곱(least mean squares)이 이상적인 접근 방식이 아닐 수도 있습니다.
이러한 종류의 사건 발생 시간(time-to-event) 모델링은 *생존 모델링(survival modeling)*이라는 전문 하위 분야에서 다루는 다른 많은 복잡한 문제를 동반합니다.</p>
<p>여기서 요점은 여러분을 압도하려는 것이 아니라, 단순히 제곱 오차를 최소화하는 것보다 추정에는 훨씬 더 많은 것이 있음을 알려드리는 것입니다.
그리고 더 넓게는 지도 학습에는 회귀보다 훨씬 더 많은 것이 있습니다.
이 섹션에서는 *얼마나 많이?*라는 질문을 제쳐두고 대신 *어떤 범주?*라는 질문에 집중하는 <em>분류(classification)</em> 문제에 초점을 맞춥니다.</p>
<ul>
<li>이 이메일은 스팸 폴더에 속할까요, 아니면 받은 편지함에 속할까요?</li>
<li>이 고객은 구독 서비스에 가입할 가능성이 높을까요, 아니면 가입하지 않을 가능성이 높을까요?</li>
<li>이 이미지는 당나귀, 개, 고양이, 수탉 중 무엇을 묘사하고 있을까요?</li>
<li>Aston이 다음에 볼 가능성이 가장 높은 영화는 무엇일까요?</li>
<li>책의 다음 섹션 중 어느 섹션을 읽을 예정인가요?</li>
</ul>
<p>구어체로 머신러닝 실무자들은 <em>분류</em>라는 단어를 두 가지 미묘하게 다른 문제를 설명하기 위해 중복해서 사용합니다:
(i) 예제를 범주(클래스)에 하드 할당(hard assignments)하는 데만 관심이 있는 문제;
(ii) 소프트 할당(soft assignments)을 하고자 하는 문제, 즉 각 범주가 적용될 확률을 평가하는 문제.
종종 하드 할당에만 신경 쓸 때도 소프트 할당을 하는 모델을 사용하기 때문에 이 구분은 모호해지는 경향이 있습니다.</p>
<p>더 나아가 둘 이상의 레이블이 참일 수 있는 경우도 있습니다.
예를 들어, 뉴스 기사는 엔터테인먼트, 비즈니스, 우주 비행 주제를 동시에 다룰 수 있지만 의학이나 스포츠 주제는 다루지 않을 수 있습니다.
따라서 이를 위의 범주 중 하나로만 분류하는 것은 그다지 유용하지 않을 것입니다.
이 문제는 흔히 <a href="https://en.wikipedia.org/wiki/Multi-label_classification">다중 레이블 분류(multi-label classification)</a>로 알려져 있습니다.
개요는 :citet:<code>Tsoumakas.Katakis.2007</code>를, 이미지 태깅 시 효과적인 알고리즘은 :citet:<code>Huang.Xu.Yu.2015</code>를 참조하십시오.</p>
<h2 id="분류-classification"><a class="header" href="#분류-classification">분류 (Classification)</a></h2>
<p>:label:<code>subsec_classification-problem</code></p>
<p>맛보기로 간단한 이미지 분류 문제부터 시작해 봅시다.
여기서 각 입력은 $2\times2$ 그레이스케일 이미지로 구성됩니다.
각 픽셀 값을 단일 스칼라로 표현하여 4개의 특성 $x_1, x_2, x_3, x_4$를 얻을 수 있습니다.
더 나아가 각 이미지가 "고양이", "닭", "개"라는 범주 중 하나에 속한다고 가정해 봅시다.</p>
<p>다음으로 레이블을 어떻게 표현할지 선택해야 합니다.
두 가지 분명한 선택지가 있습니다.
아마도 가장 자연스러운 충동은 $y \in {1, 2, 3}$을 선택하는 것일 것입니다. 여기서 정수는 각각 {\textrm{개}, \textrm{고양이}, \textrm{닭}}을 나타냅니다.
이는 컴퓨터에 그러한 정보를 <em>저장</em>하는 훌륭한 방법입니다.
만약 범주들 사이에 어떤 자연스러운 순서가 있다면,
예를 들어 {\textrm{아기}, \textrm{유아}, \textrm{청소년}, \textrm{청년}, \textrm{성인}, \textrm{노인}}을 예측하려고 한다면, 이를 <a href="https://en.wikipedia.org/wiki/Ordinal_regression">서순 회귀(ordinal regression)</a> 문제로 던지고 레이블을 이 형식으로 유지하는 것이 타당할 수도 있습니다.
다양한 유형의 순위 지정 손실 함수(ranking loss functions)에 대한 개요는 :citet:<code>Moon.Smola.Chang.ea.2010</code>를, 둘 이상의 모드(mode)를 가진 응답을 다루는 베이지안 접근 방식은 :citet:<code>Beutel.Murray.Faloutsos.ea.2014</code>를 참조하십시오.</p>
<p>일반적으로 분류 문제에는 클래스 간에 자연스러운 순서가 없습니다.
다행히도 통계학자들은 오래전에 범주형 데이터를 표현하는 간단한 방법인 *원-핫 인코딩(one-hot encoding)*을 발명했습니다.
원-핫 인코딩은 범주의 수만큼의 성분을 가진 벡터입니다.
특정 인스턴스의 범주에 해당하는 성분은 1로 설정되고 다른 모든 성분은 0으로 설정됩니다.
우리 예제에서 레이블 $y$는 3차원 벡터가 되며, (1, 0, 0)은 "고양이", (0, 1, 0)은 "닭", (0, 0, 1)은 "개"에 해당합니다:</p>
<p>$$y \in {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.$$</p>
<h3 id="선형-모델-linear-model"><a class="header" href="#선형-모델-linear-model">선형 모델 (Linear Model)</a></h3>
<p>가능한 모든 클래스와 관련된 조건부 확률을 추정하려면, 클래스당 하나씩 여러 개의 출력을 가진 모델이 필요합니다.
선형 모델로 분류를 다루려면 출력 수만큼의 아핀 함수가 필요할 것입니다.
엄밀히 말하면 마지막 범주는 1에서 다른 범주들의 합을 뺀 값이어야 하므로 하나가 적게 필요하지만, 대칭성을 위해 약간 중복된 파라미터화를 사용합니다.
각 출력은 고유한 아핀 함수에 대응합니다.
우리 예제에서 4개의 특성과 3개의 가능한 출력 범주가 있으므로, 가중치를 나타내기 위해 12개의 스칼라($w$와 아래첨자)가 필요하고 편향을 나타내기 위해 3개의 스칼라($b$와 아래첨자)가 필요합니다. 결과는 다음과 같습니다:</p>
<p>$$
\begin{aligned}
o_1 &amp;= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,<br />
o_2 &amp;= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,<br />
o_3 &amp;= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$</p>
<p>이에 대응하는 신경망 다이어그램은 :numref:<code>fig_softmaxreg</code>에 나와 있습니다.
선형 회귀와 마찬가지로 단일 레이어 신경망을 사용합니다.
그리고 각 출력 $o_1, o_2, o_3$의 계산이 모든 입력 $x_1, x_2, x_3, x_4$에 의존하므로, 출력 레이어는 *완전 연결 레이어(fully connected layer)*로 설명될 수도 있습니다.</p>
<p><img src="chapter_linear-classification/../img/softmaxreg.svg" alt="소프트맥스 회귀는 단일 레이어 신경망입니다." />
:label:<code>fig_softmaxreg</code></p>
<p>더 간결한 표기법을 위해 벡터와 행렬을 사용합니다: $\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$는 수학과 코드에 훨씬 더 적합합니다.
우리는 모든 가중치를 $3 \times 4$ 행렬에 모으고 모든 편향을 벡터 $\mathbf{b} \in \mathbb{R}^3$에 모았습니다.</p>
<h3 id="소프트맥스-the-softmax"><a class="header" href="#소프트맥스-the-softmax">소프트맥스 (The Softmax)</a></h3>
<p>:label:<code>subsec_softmax_operation</code></p>
<p>적절한 손실 함수를 가정하고, $\mathbf{o}$와 레이블 $\mathbf{y}$ 사이의 차이를 직접 최소화하려고 시도할 수 있습니다.
분류를 벡터 값 회귀 문제로 취급하는 것이 놀라울 정도로 잘 작동하는 것으로 밝혀졌지만, 그럼에도 불구하고 다음과 같은 면에서 불만족스럽습니다:</p>
<ul>
<li>출력 $o_i$가 우리가 확률에 기대하는 방식대로 합이 1이 된다는 보장이 없습니다.</li>
<li>출력의 합이 1이 되더라도 출력 $o_i$가 음수가 아니거나 1을 초과하지 않는다는 보장이 없습니다.</li>
</ul>
<p>두 측면 모두 추정 문제를 해결하기 어렵게 만들고 솔루션을 이상값에 매우 취약하게 만듭니다.
예를 들어 침실 수와 누군가가 집을 살 가능성 사이에 양의 선형 종속성이 있다고 가정하면, 대저택을 사는 경우에는 확률이 1을 초과할 수도 있습니다!
따라서 우리는 출력을 "구겨 넣을(squish)" 메커니즘이 필요합니다.</p>
<p>이 목표를 달성할 수 있는 방법은 많습니다.
예를 들어, 출력이 $\mathbf{o}$인 경우 $\mathbf{y}$의 오염된 버전이라고 가정할 수 있습니다. 여기서 오염은 정규 분포에서 추출된 노이즈 $\boldsymbol{\epsilon}$을 더함으로써 발생합니다.
즉, $\mathbf{y} = \mathbf{o} + \boldsymbol{\epsilon}$이며, 여기서 $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$입니다.
이것은 :citet:<code>Fechner.1860</code>에 의해 처음 도입된 소위 <a href="https://en.wikipedia.org/wiki/Probit_model">프로빗 모델(probit model)</a>입니다.
매력적이기는 하지만, 소프트맥스와 비교했을 때 그렇게 잘 작동하지도 않고 특히 좋은 최적화 문제로 이어지지도 않습니다.</p>
<p>이 목표를 달성하는(그리고 비음수성을 보장하는) 또 다른 방법은 지수 함수 $P(y = i) \propto \exp o_i$를 사용하는 것입니다.
이는 $o_i$가 증가함에 따라 조건부 클래스 확률이 증가해야 한다는 요구 사항을 실제로 만족하며, 단조롭고 모든 확률이 음수가 아닙니다.
그런 다음 각 값을 그들의 합으로 나눔으로써 합이 1이 되도록 이러한 값을 변환할 수 있습니다.
이 과정을 *정규화(normalization)*라고 합니다.
이 두 부분을 합치면 <em>소프트맥스(softmax)</em> 함수를 얻게 됩니다:</p>
<p>$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o}) \quad \textrm{여기서}\quad \hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)}.$$
:eqlabel:<code>eq_softmax_y_and_o</code></p>
<p>$\mathbf{o}$의 가장 큰 좌표가 $\hat{\mathbf{y}}$에 따른 가장 가능성 있는 클래스에 대응한다는 점에 유의하십시오.
게다가 소프트맥스 연산은 인수들 사이의 순서를 보존하기 때문에, 어떤 클래스에 가장 높은 확률이 할당되었는지 결정하기 위해 소프트맥스를 계산할 필요는 없습니다. 따라서 다음과 같습니다:</p>
<p>$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$</p>
<p>소프트맥스에 대한 아이디어는 물리학의 아이디어를 차용한 :citet:<code>Gibbs.1902</code>로 거슬러 올라갑니다.
더 거슬러 올라가면 현대 통계 물리학의 아버지인 볼츠만(Boltzmann)은 가스 분자의 에너지 상태에 대한 분포를 모델링하기 위해 이 트릭을 사용했습니다.
특히 그는 가스의 분자와 같은 열역학적 앙상블에서 에너지 상태의 유병률이 $\exp(-E/kT)$에 비례한다는 것을 발견했습니다.
여기서 $E$는 상태의 에너지이고, $T$는 온도이며, $k$는 볼츠만 상수입니다.
통계학자들이 통계 시스템의 "온도"를 높이거나 낮추는 것에 대해 이야기할 때, 그들은 더 낮거나 높은 에너지 상태를 선호하기 위해 $T$를 변경하는 것을 의미합니다.
깁스(Gibbs)의 아이디어를 따르면 에너지는 오차와 같습니다.
에너지 기반 모델 :cite:<code>Ranzato.Boureau.Chopra.ea.2007</code>은 딥러닝의 문제를 설명할 때 이 관점을 사용합니다.</p>
<h3 id="벡터화-vectorization"><a class="header" href="#벡터화-vectorization">벡터화 (Vectorization)</a></h3>
<p>:label:<code>subsec_softmax_vectorization</code></p>
<p>계산 효율성을 높이기 위해 데이터 미니배치에서 계산을 벡터화합니다.
차원(입력 수)이 $d$인 $n$개 예제로 구성된 미니배치 $\mathbf{X} \in \mathbb{R}^{n \times d}$가 주어졌다고 가정합시다.
또한 출력에 $q$개의 범주가 있다고 가정합시다.
그러면 가중치는 $\mathbf{W} \in \mathbb{R}^{d \times q}$를 만족하고 편향은 $\mathbf{b} \in \mathbb{R}^{1\times q}$를 만족합니다.</p>
<p>$$
\begin{aligned}
\mathbf{O} &amp;= \mathbf{X} \mathbf{W} + \mathbf{b}, \
\hat{\mathbf{Y}} &amp; = \mathrm{softmax}(\mathbf{O}).
\end{aligned}
$$
:eqlabel:<code>eq_minibatch_softmax_reg</code></p>
<p>이는 지배적인 연산을 행렬-행렬 곱 $\mathbf{X} \mathbf{W}$로 가속화합니다.
게다가 $\mathbf{X}$의 각 행이 데이터 예제를 나타내므로, 소프트맥스 연산 자체는 *행별(rowwise)*로 계산될 수 있습니다: $\mathbf{O}$의 각 행에 대해 모든 항목을 지수화한 다음 합계로 정규화합니다.
하지만 큰 숫자의 지수와 로그를 취할 때는 수치적 오버플로(overflow)나 언더플로(underflow)를 유발할 수 있으므로 주의해야 합니다.
딥러닝 프레임워크는 이를 자동으로 처리합니다.</p>
<h2 id="손실-함수-loss-function-1"><a class="header" href="#손실-함수-loss-function-1">손실 함수 (Loss Function)</a></h2>
<p>:label:<code>subsec_softmax-regression-loss-func</code></p>
<p>이제 특성 $\mathbf{x}$에서 확률 $\mathbf{\hat{y}}$로의 매핑이 있으므로, 이 매핑의 정확도를 최적화할 방법이 필요합니다.
우리는 :numref:<code>subsec_normal_distribution_and_squared_loss</code>에서 평균 제곱 오차 손실에 대한 확률적 정당성을 제공할 때 만났던 것과 동일한 방법인 최대 우도 추정(maximum likelihood estimation)에 의존할 것입니다.</p>
<h3 id="로그-우도-log-likelihood"><a class="header" href="#로그-우도-log-likelihood">로그 우도 (Log-Likelihood)</a></h3>
<p>소프트맥스 함수는 벡터 $\hat{\mathbf{y}}$를 제공하며, 이를 $\hat{y}_1$ = $P(y=\textrm{고양이} \mid \mathbf{x})$와 같이 임의의 입력 $\mathbf{x}$가 주어졌을 때 각 클래스의 (추정된) 조건부 확률로 해석할 수 있습니다.
다음에서는 특성 $\mathbf{X}$가 있는 데이터셋에 대해 레이블 $\mathbf{Y}$가 원-핫 인코딩 레이블 벡터를 사용하여 표현된다고 가정합니다.
특성이 주어졌을 때 모델에 따라 실제 클래스가 얼마나 가능성이 있는지 확인함으로써 추정치를 실제와 비교할 수 있습니다:</p>
<p>$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$</p>
<p>각 레이블이 각각의 분포 $P(\mathbf{y}\mid\mathbf{x}^{(i)})$에서 독립적으로 추출되었다고 가정하므로 인수 분해를 사용할 수 있습니다.
항들의 곱을 최대화하는 것은 다루기 어렵기 때문에, 음의 로그를 취하여 음의 로그 우도를 최소화하는 등가 문제로 변환합니다:</p>
<p>$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$</p>
<p>여기서 $q$개 클래스에 대한 임의의 레이블 $\mathbf{y}$와 모델 예측 $\hat{\mathbf{y}}$ 쌍에 대해 손실 함수 $l$은 다음과 같습니다.</p>
<p>$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
:eqlabel:<code>eq_l_cross_entropy</code></p>
<p>나중에 설명할 이유들로 인해, :eqref:<code>eq_l_cross_entropy</code>의 손실 함수를 흔히 *크로스 엔트로피 손실(cross-entropy loss)*이라고 합니다.
$\mathbf{y}$는 길이가 $q$인 원-핫 벡터이므로, 모든 좌표 $j$에 대한 합은 한 항을 제외하고 모두 사라집니다.
$\hat{\mathbf{y}}$가 확률 벡터일 때마다 손실 $l(\mathbf{y}, \hat{\mathbf{y}})$는 아래로 $0$에 의해 제한된다는 점에 유의하십시오: 어떤 항목도 $1$보다 크지 않으므로 그들의 음의 로그는 $0$보다 낮을 수 없습니다; $l(\mathbf{y}, \hat{\mathbf{y}}) = 0$은 실제 레이블을 <em>확실성</em>을 가지고 예측할 때만 가능합니다.
이는 파라미터의 어떤 유한한 설정에서도 결코 일어날 수 없습니다. 소프트맥스 출력을 $1$로 가져가려면 해당 입력 $o_i$를 무한대로(또는 $j \neq i$인 다른 모든 출력 $o_j$를 음의 무한대로) 가져가야 하기 때문입니다.
설령 우리 모델이 출력 확률 $0$을 할당할 수 있더라도, 그렇게 높은 확신을 가지고 할당했을 때 발생하는 오차는 무한한 손실($-\log 0 = \infty$)을 초래할 것입니다.</p>
<h3 id="소프트맥스와-크로스-엔트로피-손실-softmax-and-cross-entropy-loss"><a class="header" href="#소프트맥스와-크로스-엔트로피-손실-softmax-and-cross-entropy-loss">소프트맥스와 크로스 엔트로피 손실 (Softmax and Cross-Entropy Loss)</a></h3>
<p>:label:<code>subsec_softmax_and_derivatives</code></p>
<p>소프트맥스 함수와 그에 대응하는 크로스 엔트로피 손실은 매우 흔하기 때문에, 이들이 어떻게 계산되는지 조금 더 잘 이해할 가치가 있습니다.
:eqref:<code>eq_softmax_y_and_o</code>를 :eqref:<code>eq_l_cross_entropy</code>의 손실 정의에 대입하고 소프트맥스의 정의를 사용하면 다음을 얻습니다.</p>
<p>$$
\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &amp;=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \
&amp;= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j \
&amp;= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}
$$</p>
<p>무슨 일이 일어나고 있는지 조금 더 잘 이해하기 위해, 임의의 로짓(logit) $o_j$에 대한 도함수를 고려해 봅시다. 우리는 다음을 얻습니다.</p>
<p>$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$</p>
<p>즉, 도함수는 소프트맥스 연산으로 표현되는 우리 모델에 의해 할당된 확률과, 원-핫 레이블 벡터의 원소들로 표현되는 실제 일어난 일 사이의 차이입니다.
이런 의미에서 이는 회귀에서 보았던 것과 매우 유사한데, 거기서 기울기는 관찰 $y$와 추정치 $\hat{y}$ 사이의 차이였습니다.
이것은 우연이 아닙니다. 임의의 지수족(exponential family) 모델에서 로그 우도의 기울기는 정확히 이 항으로 주어집니다. 이 사실은 실제로 기울기를 계산하는 것을 쉽게 만듭니다.</p>
<p>이제 단일 결과가 아니라 결과에 대한 전체 분포를 관찰하는 경우를 고려해 봅시다.
레이블 $\mathbf{y}$에 대해 이전과 동일한 표현을 사용할 수 있습니다.
유일한 차이점은 $(0, 0, 1)$과 같은 이진 항목만 포함된 벡터 대신, $(0.1, 0.2, 0.7)$과 같은 일반적인 확률 벡터를 갖는다는 것입니다.
:eqref:<code>eq_l_cross_entropy</code>에서 손실 $l$을 정의하기 위해 이전에 사용했던 수학은 여전히 잘 작동하며, 해석만 약간 더 일반적입니다.
이는 레이블에 대한 분포에 대한 손실의 기대값입니다.
이 손실을 <em>크로스 엔트로피 손실</em>이라고 하며 분류 문제에서 가장 흔히 사용되는 손실 중 하나입니다.
우리는 정보 이론의 기초만 도입함으로써 이 이름을 명확히 할 수 있습니다.
간단히 말해, 이는 우리가 예측한 일($\hat{\mathbf{y}}$)에 대해 우리가 본 것($\mathbf{y}$)을 인코딩하는 데 필요한 비트 수를 측정합니다.
다음에서 매우 기본적인 설명을 제공합니다. 정보 이론에 대한 자세한 내용은 :citet:<code>Cover.Thomas.1999</code> 또는 :citet:<code>mackay2003information</code>를 참조하십시오.</p>
<h2 id="정보-이론-기초-information-theory-basics"><a class="header" href="#정보-이론-기초-information-theory-basics">정보 이론 기초 (Information Theory Basics)</a></h2>
<p>:label:<code>subsec_info_theory_basics</code></p>
<p>많은 딥러닝 논문들이 정보 이론의 직관과 용어를 사용합니다.
이를 이해하기 위해서는 공통된 언어가 필요합니다.
이것은 생존 가이드입니다.
*정보 이론(Information theory)*은 정보(데이터라고도 함)를 인코딩, 디코딩, 전송 및 조작하는 문제를 다룹니다.</p>
<h3 id="엔트로피-entropy"><a class="header" href="#엔트로피-entropy">엔트로피 (Entropy)</a></h3>
<p>정보 이론의 핵심 아이디어는 데이터에 포함된 정보의 양을 정량화하는 것입니다.
이는 데이터를 압축하는 능력에 한계를 둡니다.
분포 $P$에 대해 그 <em>엔트로피(entropy)</em> $H[P]$는 다음과 같이 정의됩니다:</p>
<p>$$H[P] = \sum_j - P(j) \log P(j).$$
:eqlabel:<code>eq_softmax_reg_entropy</code></p>
<p>정보 이론의 근본적인 정리 중 하나는 분포 $P$에서 무작위로 추출된 데이터를 인코딩하기 위해 이를 인코딩하는 데 최소 $H[P]$ "나츠(nats)"가 필요하다는 것입니다 :cite:<code>Shannon.1948</code>.
"나츠"가 무엇인지 궁금하다면, 이는 밑이 2인 코드 대신 밑이 $e$인 코드를 사용할 때의 비트(bit)와 동등한 것입니다.
따라서 1 나츠는 $\frac{1}{\log(2)} \approx 1.44$ 비트입니다.</p>
<h3 id="놀람-surprisal"><a class="header" href="#놀람-surprisal">놀람 (Surprisal)</a></h3>
<p>압축이 예측과 무슨 상관이 있는지 궁금할 수 있습니다.
우리가 압축하고 싶은 데이터 스트림이 있다고 상상해 보십시오.
우리가 다음 토큰을 예측하는 것이 항상 쉽다면, 이 데이터는 압축하기 쉽습니다.
스트림의 모든 토큰이 항상 동일한 값을 갖는 극단적인 예를 들어봅시다.
그것은 매우 지루한 데이터 스트림입니다!
지루할 뿐만 아니라 예측하기도 쉽습니다.
토큰이 항상 동일하기 때문에 스트림의 내용을 전달하기 위해 어떠한 정보도 전송할 필요가 없습니다.
예측하기 쉬우면 압축하기 쉽습니다.</p>
<p>하지만 우리가 모든 사건을 완벽하게 예측할 수 없다면, 때때로 놀랄 수도 있습니다.
사건에 낮은 확률이 할당될 때 우리의 놀람은 더 큽니다.
클로드 섀넌(Claude Shannon)은 사건 $j$에 (주관적) 확률 $P(j)$를 할당했을 때 그 사건을 관찰하는 사람의 *놀람(surprisal)*을 정량화하기 위해 $\log \frac{1}{P(j)} = -\log P(j)$를 정했습니다.
:eqref:<code>eq_softmax_reg_entropy</code>에 정의된 엔트로피는 데이터 생성 프로세스와 진정으로 일치하는 올바른 확률을 할당했을 때의 *기대 놀람(expected surprisal)*입니다.</p>
<h3 id="크로스-엔트로피-다시-보기-cross-entropy-revisited"><a class="header" href="#크로스-엔트로피-다시-보기-cross-entropy-revisited">크로스 엔트로피 다시 보기 (Cross-Entropy Revisited)</a></h3>
<p>엔트로피가 실제 확률을 아는 사람이 경험하는 놀람의 수준이라면, 크로스 엔트로피란 무엇일까요?
$P$에서 $Q$로의 크로스 엔트로피(cross-entropy) $H(P, Q)$는, 확률 $P$에 따라 실제로 생성된 데이터를 보았을 때 주관적 확률 $Q$를 가진 관찰자의 기대 놀람입니다.
이는 $H(P, Q) \stackrel{\textrm{def}}{=} \sum_j - P(j) \log Q(j)$로 주어집니다.
가장 낮은 크로스 엔트로피는 $P=Q$일 때 달성됩니다.
이 경우 $P$에서 $Q$로의 크로스 엔트로피는 $H(P, P)= H(P)$입니다.</p>
<p>요컨대, 우리는 크로스 엔트로피 분류 목표를 두 가지 방식으로 생각할 수 있습니다: (i) 관찰된 데이터의 우도를 최대화하는 것; (ii) 레이블을 전달하는 데 필요한 우리의 놀람(따라서 비트 수)을 최소화하는 것.</p>
<h2 id="요약-및-토론-summary-and-discussion"><a class="header" href="#요약-및-토론-summary-and-discussion">요약 및 토론 (Summary and Discussion)</a></h2>
<p>이 섹션에서 우리는 <em>이산형</em> 출력 공간에 대해 최적화할 수 있게 해주는 첫 번째 비자명한 손실 함수를 만났습니다.
이 설계의 핵심은 우리가 확률론적 접근 방식을 취하여 이산 범주를 확률 분포에서 추출된 인스턴스로 취급했다는 것입니다.
부수적인 효과로, 일반적인 신경망 레이어의 출력을 유효한 이산 확률 분포로 변환하는 편리한 활성화 함수인 소프트맥스를 만났습니다.
우리는 소프트맥스와 결합된 크로스 엔트로피 손실의 도함수가 기대 행동과 그 예측 사이의 차이를 취함으로써 제곱 오차의 도함수와 매우 유사하게 동작한다는 것을 보았습니다.
그리고 비록 겉핥기만 할 수 있었지만, 통계 물리학 및 정보 이론과의 흥미로운 연결도 만났습니다.</p>
<p>비록 이것이 여러분이 나아가는 데 충분하고 여러분의 흥미를 끌기에 충분하기를 바라지만, 여기서는 깊이 파고들지 못했습니다.
무엇보다도 계산적인 고려 사항을 건너뛰었습니다.
구체적으로 $d$개의 입력과 $q$개의 출력이 있는 완전 연결 레이어의 경우 파라미터화 및 계산 비용은 $\mathcal{O}(dq)$이며, 이는 실제 상황에서 감당하기 힘들 정도로 높을 수 있습니다.
다행히 $d$개의 입력을 $q$개의 출력으로 변환하는 이 비용은 근사 및 압축을 통해 줄일 수 있습니다.
예를 들어 Deep Fried Convnets :cite:<code>Yang.Moczulski.Denil.ea.2015</code>는 순열, 푸리에 변환 및 스케일링의 조합을 사용하여 비용을 2차(quadratic)에서 로그-선형(log-linear)으로 줄입니다.
유사한 기술이 더 발전된 구조적 행렬 근사(structural matrix approximations)에도 적용됩니다 :cite:<code>sindhwani2015structured</code>.
마지막으로 쿼터니언(quaternion)과 유사한 분해를 사용하여 비용을 $\mathcal{O}(\frac{dq}{n})$으로 줄일 수 있는데, 이 역시 압축 계수 $n$을 기반으로 계산 및 저장 비용을 위해 약간의 정확도를 희생할 의향이 있다면 가능합니다 :cite:<code>Zhang.Tay.Zhang.ea.2021</code>.
이는 활발한 연구 분야입니다.
도전적인 점은 우리가 반드시 가장 간결한 표현이나 가장 적은 부동 소수점 연산 수를 추구하는 것이 아니라, 현대 GPU에서 가장 효율적으로 실행될 수 있는 솔루션을 추구한다는 것입니다.</p>
<h2 id="연습-문제-exercises-7"><a class="header" href="#연습-문제-exercises-7">연습 문제 (Exercises)</a></h2>
<ol>
<li>지수족(exponential families)과 소프트맥스 사이의 연결을 좀 더 깊이 탐구할 수 있습니다.
<ol>
<li>소프트맥스에 대한 크로스 엔트로피 손실 $l(\mathbf{y},\hat{\mathbf{y}})$의 2계 도함수를 계산하십시오.</li>
<li>$\mathrm{softmax}(\mathbf{o})$에 의해 주어지는 분포의 분산을 계산하고 위에서 계산된 2계 도함수와 일치함을 보이십시오.</li>
</ol>
</li>
<li>동일한 확률로 발생하는 세 개의 클래스가 있다고 가정합니다. 즉, 확률 벡터는 $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$입니다.
<ol>
<li>이에 대해 이진 코드를 설계하려고 할 때의 문제는 무엇입니까?</li>
<li>더 나은 코드를 설계할 수 있습니까? 힌트: 두 개의 독립적인 관찰을 인코딩하려고 하면 어떻게 될까요? $n$개의 관찰을 공동으로 인코딩하면 어떻게 될까요?</li>
</ol>
</li>
<li>물리적 전선을 통해 전송되는 신호를 인코딩할 때 엔지니어들은 항상 이진 코드를 사용하지는 않습니다. 예를 들어 <a href="https://en.wikipedia.org/wiki/Ternary_signal">PAM-3</a>는 두 개의 레벨 {0, 1} 대신 세 개의 신호 레벨 ${-1, 0, 1}$을 사용합니다. {0, ..., 7} 범위의 정수를 전송하려면 몇 개의 3진(ternary) 유닛이 필요합니까? 전자공학 측면에서 이것이 왜 더 좋은 아이디어일까요?</li>
<li><a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">브래들리-테리 모델(Bradley--Terry model)</a>은 선호도를 파악하기 위해 로지스틱 모델을 사용합니다. 사용자가 사과와 오렌지 중에서 선택하기 위해 점수 $o_{\textrm{apple}}$과 $o_{\textrm{orange}}$를 가정합니다. 우리의 요구 사항은 점수가 클수록 관련 항목을 선택할 가능성이 높아야 하고 점수가 가장 큰 항목이 선택될 가능성이 가장 높아야 한다는 것입니다 :cite:<code>Bradley.Terry.1952</code>.
<ol>
<li>소프트맥스가 이 요구 사항을 만족함을 증명하십시오.</li>
<li>사과와 오렌지 중 어느 것도 선택하지 않는 기본 옵션을 허용하려면 어떻게 해야 할까요? 힌트: 이제 사용자에게는 세 가지 선택지가 있습니다.</li>
</ol>
</li>
<li>소프트맥스는 다음 매핑에서 그 이름을 얻었습니다: $\textrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$.
<ol>
<li>$\textrm{RealSoftMax}(a, b) &gt; \mathrm{max}(a, b)$임을 증명하십시오.</li>
<li>두 함수 사이의 차이를 얼마나 작게 만들 수 있습니까? 힌트: 일반성을 잃지 않고 $b = 0$ 및 $a \geq b$로 설정할 수 있습니다.</li>
<li>$\lambda &gt; 0$일 때, $\lambda^{-1} \textrm{RealSoftMax}(\lambda a, \lambda b)$에 대해 이것이 성립함을 증명하십시오.</li>
<li>$\lambda \to \infty$에 대해 $\lambda^{-1} \textrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$임을 보이십시오.</li>
<li>유사한 softmin 함수를 구성하십시오.</li>
<li>이를 둘 이상의 숫자로 확장하십시오.</li>
</ol>
</li>
<li>함수 $g(\mathbf{x}) \stackrel{\textrm{def}}{=} \log \sum_i \exp x_i$는 때때로 <a href="https://en.wikipedia.org/wiki/Partition_function_(mathematics)">로그-분할 함수(log-partition function)</a>라고도 불립니다.
<ol>
<li>함수가 볼록(convex)함을 증명하십시오. 힌트: 이를 위해 1계 도함수가 소프트맥스 함수의 확률에 해당한다는 사실을 사용하고 2계 도함수가 분산임을 보이십시오.</li>
<li>$g$가 이동 불변(translation invariant)임을 보이십시오. 즉, $g(\mathbf{x} + b) = g(\mathbf{x})$.</li>
<li>좌표 $x_i$ 중 일부가 매우 크면 어떻게 됩니까? 모두 매우 작으면 어떻게 됩니까?</li>
<li>$b = \mathrm{max}_i x_i$를 선택하면 수치적으로 안정적인 구현을 얻게 됨을 보이십시오.</li>
</ol>
</li>
<li>어떤 확률 분포 $P$가 있다고 가정합니다. $\alpha &gt; 0$에 대해 $Q(i) \propto P(i)^\alpha$인 다른 분포 $Q$를 선택한다고 가정합시다.
<ol>
<li>어떤 $\alpha$ 선택이 온도를 두 배로 높이는 것에 해당합니까? 어떤 선택이 온도를 절반으로 줄이는 것에 해당합니까?</li>
<li>온도가 0에 가까워지게 하면 어떻게 됩니까?</li>
<li>온도가 $\infty$에 가까워지게 하면 어떻게 됩니까?</li>
</ol>
</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/46">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="이미지-분류-데이터셋-the-image-classification-dataset"><a class="header" href="#이미지-분류-데이터셋-the-image-classification-dataset">이미지 분류 데이터셋 (The Image Classification Dataset)</a></h1>
<p>:label:<code>sec_fashion_mnist</code></p>
<p>이미지 분류를 위해 널리 사용되는 데이터셋 중 하나는 손글씨 숫자로 구성된 <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST 데이터셋</a> :cite:<code>LeCun.Bottou.Bengio.ea.1998</code>입니다. 1990년대 출시 당시에는 60,000개의 $28 \times 28$ 픽셀 해상도 이미지(와 10,000개의 테스트 데이터셋)로 구성되어 대부분의 머신러닝 알고리즘에 만만치 않은 도전을 안겨주었습니다. 당시 상황을 돌이켜보면, 1995년에 무려 64MB의 RAM과 5 MFLOPs의 성능을 가진 Sun SPARCStation 5가 AT&amp;T Bell 연구소에서 머신러닝을 위한 최첨단 장비로 간주되었습니다. 숫자 인식에서 높은 정확도를 달성하는 것은 1990년대 USPS의 우편물 분류 자동화의 핵심 구성 요소였습니다. LeNet-5 :cite:<code>LeCun.Jackel.Bottou.ea.1995</code>와 같은 심층 네트워크, 불변성을 가진 서포트 벡터 머신(SVM) :cite:<code>Scholkopf.Burges.Vapnik.1996</code>, 탄젠트 거리 분류기 :cite:<code>Simard.LeCun.Denker.ea.1998</code> 등은 모두 1% 미만의 오차율에 도달할 수 있었습니다.</p>
<p>10년 넘게 MNIST는 머신러닝 알고리즘을 비교하는 <em>기준점</em> 역할을 했습니다. 하지만 벤치마크 데이터셋으로서의 명성이 무색하게도 오늘날의 기준으로는 단순한 모델조차 95% 이상의 분류 정확도를 달성하여, 강력한 모델과 약한 모델을 구분하기에 부적절해졌습니다. 더욱이 이 데이터셋은 많은 분류 문제에서 일반적으로 볼 수 없는 <em>매우</em> 높은 수준의 정확도를 허용합니다. 이는 active set methods나 boundary-seeking active set algorithms와 같이 깨끗한 데이터셋을 활용할 수 있는 특정 알고리즘 제품군 위주로 알고리즘 개발을 편향시켰습니다. 오늘날 MNIST는 벤치마크라기보다는 정상성 확인(sanity check) 용도에 가깝습니다. ImageNet :cite:<code>Deng.Dong.Socher.ea.2009</code>이 훨씬 더 유의미한 도전을 제시합니다. 불행히도 ImageNet은 이 책의 많은 예제와 그림에 사용하기에는 너무 커서, 예제를 대화식으로 만들기 위해 훈련하는 데 너무 오랜 시간이 걸립니다. 대용으로 우리는 다음 섹션에서 질적으로는 유사하지만 훨씬 작은 Fashion-MNIST 데이터셋 :cite:<code>Xiao.Rasul.Vollgraf.2017</code>에 초점을 맞출 것입니다. 2017년에 출시된 이 데이터셋은 $28 \times 28$ 픽셀 해상도의 10가지 범주의 의류 이미지를 포함하고 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
import time
from d2l import mxnet as d2l
from mxnet import gluon, npx
from mxnet.gluon.data.vision import transforms
npx.set_np()

d2l.use_svg_display()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
import time
from d2l import torch as d2l
import torch
import torchvision
from torchvision import transforms

d2l.use_svg_display()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
import time
from d2l import tensorflow as d2l
import tensorflow as tf

d2l.use_svg_display()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
import numpy as np
import time
import tensorflow as tf
import tensorflow_datasets as tfds

d2l.use_svg_display()
</code></pre>
<h2 id="데이터셋-로드하기-loading-the-dataset"><a class="header" href="#데이터셋-로드하기-loading-the-dataset">데이터셋 로드하기 (Loading the Dataset)</a></h2>
<p>Fashion-MNIST 데이터셋은 매우 유용하기 때문에 모든 주요 프레임워크에서 전처리된 버전을 제공합니다. 우리는 [<strong>프레임워크의 내장 유틸리티를 사용하여 이를 다운로드하고 메모리로 읽어올 수 있습니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class FashionMNIST(d2l.DataModule):  #@save
    """Fashion-MNIST 데이터셋입니다."""
    def __init__(self, batch_size=64, resize=(28, 28)):
        super().__init__()
        self.save_hyperparameters()
        trans = transforms.Compose([transforms.Resize(resize),
                                    transforms.ToTensor()])
        self.train = gluon.data.vision.FashionMNIST(
            train=True).transform_first(trans)
        self.val = gluon.data.vision.FashionMNIST(
            train=False).transform_first(trans)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class FashionMNIST(d2l.DataModule):  #@save
    """Fashion-MNIST 데이터셋입니다."""
    def __init__(self, batch_size=64, resize=(28, 28)):
        super().__init__()
        self.save_hyperparameters()
        trans = transforms.Compose([transforms.Resize(resize),
                                    transforms.ToTensor()])
        self.train = torchvision.datasets.FashionMNIST(
            root=self.root, train=True, transform=trans, download=True)
        self.val = torchvision.datasets.FashionMNIST(
            root=self.root, train=False, transform=trans, download=True)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow, jax
class FashionMNIST(d2l.DataModule):  #@save
    """Fashion-MNIST 데이터셋입니다."""
    def __init__(self, batch_size=64, resize=(28, 28)):
        super().__init__()
        self.save_hyperparameters()
        self.train, self.val = tf.keras.datasets.fashion_mnist.load_data()
</code></pre>
<p>Fashion-MNIST는 10개의 카테고리로 구성되며, 각 카테고리는 훈련 데이터셋에 6000개, 테스트 데이터셋에 1000개의 이미지로 표현됩니다. <em>테스트 데이터셋</em>은 모델 성능을 평가하는 데 사용됩니다(훈련에 사용되어서는 안 됩니다). 결과적으로 훈련 세트와 테스트 세트는 각각 60,000개와 10,000개의 이미지를 포함합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
data = FashionMNIST(resize=(32, 32))
len(data.train), len(data.val)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow, jax
data = FashionMNIST(resize=(32, 32))
len(data.train[0]), len(data.val[0])
</code></pre>
<p>이미지는 그레이스케일이며 위에서 $32 \times 32$ 픽셀 해상도로 업스케일되었습니다. 이는 (이진) 흑백 이미지로 구성된 원래의 MNIST 데이터셋과 유사합니다. 그러나 대부분의 현대적인 이미지 데이터는 3개의 채널(빨강, 초록, 파랑)을 가지며 초분광(hyperspectral) 이미지는 100개 이상의 채널을 가질 수 있습니다(HyMap 센서는 126개 채널을 가짐). 관례에 따라 이미지는 $c \times h \times w$ 텐서로 저장됩니다. 여기서 $c$는 색상 채널 수, $h$는 높이, $w$는 너비입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data.train[0][0].shape
</code></pre>
<p>Fashion-MNIST의 카테고리는 사람이 이해할 수 있는 이름을 가지고 있습니다. 다음 편의 메서드는 숫자 레이블과 그 이름 사이를 변환합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(FashionMNIST)  #@save
def text_labels(self, indices):
    """텍스트 레이블을 반환합니다."""
    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
              'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [labels[int(i)] for i in indices]
</code></pre>
<h2 id="미니배치-읽기-reading-a-minibatch"><a class="header" href="#미니배치-읽기-reading-a-minibatch">미니배치 읽기 (Reading a Minibatch)</a></h2>
<p>훈련 세트와 테스트 세트에서 읽을 때 편의를 위해, 처음부터 만드는 대신 내장 데이터 반복기를 사용합니다. 매 반복마다 데이터 반복기는 [<strong><code>batch_size</code> 크기의 데이터 미니배치를 읽습니다.</strong>] 또한 훈련 데이터 반복기에 대해 예제를 무작위로 셔플링합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
@d2l.add_to_class(FashionMNIST)  #@save
def get_dataloader(self, train):
    data = self.train if train else self.val
    return gluon.data.DataLoader(data, self.batch_size, shuffle=train,
                                 num_workers=self.num_workers)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
@d2l.add_to_class(FashionMNIST)  #@save
def get_dataloader(self, train):
    data = self.train if train else self.val
    return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,
                                       num_workers=self.num_workers)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow, jax
@d2l.add_to_class(FashionMNIST)  #@save
def get_dataloader(self, train):
    data = self.train if train else self.val
    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,
                            tf.cast(y, dtype='int32'))
    resize_fn = lambda X, y: (tf.image.resize_with_pad(X, *self.resize), y)
    shuffle_buf = len(data[0]) if train else 1
    if tab.selected('tensorflow'):
        return tf.data.Dataset.from_tensor_slices(process(*data)).batch(
            self.batch_size).map(resize_fn).shuffle(shuffle_buf)
    if tab.selected('jax'):
        return tfds.as_numpy(
            tf.data.Dataset.from_tensor_slices(process(*data)).batch(
                self.batch_size).map(resize_fn).shuffle(shuffle_buf))
</code></pre>
<p>어떻게 작동하는지 확인하기 위해 <code>train_dataloader</code> 메서드를 호출하여 이미지 미니배치를 로드해 보겠습니다. 64개의 이미지를 포함하고 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
X, y = next(iter(data.train_dataloader()))
print(X.shape, X.dtype, y.shape, y.dtype)
</code></pre>
<p>이미지를 읽는 데 걸리는 시간을 살펴봅시다. 내장 로더임에도 불구하고 엄청나게 빠르지는 않습니다. 그럼에도 불구하고 심층 네트워크로 이미지를 처리하는 데는 상당한 시간이 더 걸리기 때문에 이는 충분합니다. 따라서 네트워크 훈련이 I/O에 의해 제한되지 않을 정도로 충분히 빠릅니다.</p>
<pre><code class="language-{.python .input}">%%tab all
tic = time.time()
for X, y in data.train_dataloader():
    continue
f'{time.time() - tic:.2f} 초'
</code></pre>
<h2 id="시각화-visualization"><a class="header" href="#시각화-visualization">시각화 (Visualization)</a></h2>
<p>우리는 종종 Fashion-MNIST 데이터셋을 사용할 것입니다. 편의 함수 <code>show_images</code>를 사용하여 이미지와 관련 레이블을 시각화할 수 있습니다. 구현 세부 사항은 생략하고 아래 인터페이스만 보여드립니다. 이러한 유틸리티 함수에 대해서는 작동 방식보다는 <code>d2l.show_images</code>를 호출하는 방법만 알면 됩니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
    """이미지 리스트를 플롯합니다."""
    raise NotImplementedError
</code></pre>
<p>유용하게 활용해 봅시다. 일반적으로 여러분이 훈련하고 있는 데이터를 시각화하고 검사하는 것이 좋은 아이디어입니다. 인간은 이상한 점을 발견하는 데 매우 능숙하며, 그 덕분에 시각화는 실험 설계의 실수와 오차에 대한 추가적인 안전장치 역할을 합니다. 여기 훈련 데이터셋의 처음 몇 가지 예제에 대한 [<strong>이미지와 그에 대응하는 (텍스트 형태의) 레이블</strong>]이 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(FashionMNIST)  #@save
def visualize(self, batch, nrows=1, ncols=8, labels=[]):
    X, y = batch
    if not labels:
        labels = self.text_labels(y)
    if tab.selected('mxnet', 'pytorch'):
        d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)
    if tab.selected('tensorflow'):
        d2l.show_images(tf.squeeze(X), nrows, ncols, titles=labels)
    if tab.selected('jax'):
        d2l.show_images(jnp.squeeze(X), nrows, ncols, titles=labels)

batch = next(iter(data.val_dataloader()))
data.visualize(batch)
</code></pre>
<p>이제 다음 섹션에서 Fashion-MNIST 데이터셋을 사용하여 작업할 준비가 되었습니다.</p>
<h2 id="요약-summary-7"><a class="header" href="#요약-summary-7">요약 (Summary)</a></h2>
<p>이제 분류에 사용할 수 있는 약간 더 현실적인 데이터셋을 확보했습니다. Fashion-MNIST는 10가지 범주를 나타내는 이미지로 구성된 의류 분류 데이터셋입니다. 우리는 이후 섹션과 장에서 단순한 선형 모델부터 고급 잔차 네트워크(residual networks)까지 다양한 네트워크 설계를 평가하기 위해 이 데이터셋을 사용할 것입니다. 이미지에서 흔히 하듯이, 이미지를 (배치 크기, 채널 수, 높이, 너비) 모양의 텐서로 읽어옵니다. 현재 이미지는 그레이스케일이므로 채널이 하나뿐입니다(위의 시각화는 가독성을 높이기 위해 가상 컬러 팔레트를 사용했습니다).</p>
<p>마지막으로, 데이터 반복기는 효율적인 성능을 위한 핵심 구성 요소입니다. 예를 들어, 효율적인 이미지 압축 해제, 비디오 트랜스코딩 또는 기타 전처리를 위해 GPU를 사용할 수 있습니다. 가능할 때마다 훈련 루프의 속도가 느려지지 않도록 고성능 컴퓨팅을 활용하는 잘 구현된 데이터 반복기에 의존해야 합니다.</p>
<h2 id="연습-문제-exercises-8"><a class="header" href="#연습-문제-exercises-8">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>batch_size</code>를 줄이면(예를 들어 1로) 읽기 성능에 영향을 미칩니까?</li>
<li>데이터 반복기 성능은 중요합니다. 현재 구현이 충분히 빠르다고 생각하십니까? 성능을 개선하기 위한 다양한 옵션을 탐색해 보십시오. 시스템 프로파일러를 사용하여 병목 지점이 어디인지 찾아보십시오.</li>
<li>프레임워크의 온라인 API 문서를 확인해 보십시오. 다른 어떤 데이터셋들을 사용할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/48">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/49">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/224">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17980">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="기본-분류-모델-the-base-classification-model"><a class="header" href="#기본-분류-모델-the-base-classification-model">기본 분류 모델 (The Base Classification Model)</a></h1>
<p>:label:<code>sec_classification</code></p>
<p>회귀의 경우 밑바닥부터의 구현과 프레임워크 기능을 사용한 간결한 구현이 상당히 유사했다는 점을 눈치채셨을 것입니다. 분류도 마찬가지입니다. 이 책의 많은 모델이 분류를 다루기 때문에, 이 설정을 구체적으로 지원하는 기능을 추가하는 것이 가치가 있습니다. 이 섹션에서는 향후 코드를 단순화하기 위해 분류 모델을 위한 기본 클래스를 제공합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, gluon
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from functools import partial
from jax import numpy as jnp
import jax
import optax
</code></pre>
<h2 id="classifier-클래스"><a class="header" href="#classifier-클래스"><code>Classifier</code> 클래스</a></h2>
<p>:begin_tab:<code>pytorch, mxnet, tensorflow</code>
아래에 <code>Classifier</code> 클래스를 정의합니다. <code>validation_step</code>에서는 검증 배치에 대한 손실 값과 분류 정확도를 모두 보고합니다. 우리는 매 <code>num_val_batches</code> 배치마다 업데이트를 그립니다. 이는 전체 검증 데이터에 대해 평균화된 손실과 정확도를 생성하는 이점이 있습니다. 마지막 배치에 더 적은 예제가 포함되어 있다면 이 평균 수치들이 정확히 맞지는 않겠지만, 코드를 단순하게 유지하기 위해 이 사소한 차이는 무시합니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
아래에 <code>Classifier</code> 클래스를 정의합니다. <code>validation_step</code>에서는 검증 배치에 대한 손실 값과 분류 정확도를 모두 보고합니다. 우리는 매 <code>num_val_batches</code> 배치마다 업데이트를 그립니다. 이는 전체 검증 데이터에 대해 평균화된 손실과 정확도를 생성하는 이점이 있습니다. 마지막 배치에 더 적은 예제가 포함되어 있다면 이 평균 수치들이 정확히 맞지는 않겠지만, 코드를 단순하게 유지하기 위해 이 사소한 차이는 무시합니다.</p>
<p>또한 JAX를 위해 <code>training_step</code> 메서드를 재정의합니다. 나중에 <code>Classifier</code>를 상속받을 모든 모델은 보조 데이터(auxiliary data)를 반환하는 손실을 가질 것이기 때문입니다. 이 보조 데이터는 (:numref:<code>sec_batch_norm</code>에서 설명할) 배치 정규화(batch normalization)가 있는 모델에 사용될 수 있으며, 그 외의 모든 경우에는 손실이 보조 데이터를 나타내는 플레이스홀더(빈 딕셔너리)를 반환하도록 할 것입니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class Classifier(d2l.Module):  #@save
    """분류 모델의 기본 클래스입니다."""
    def validation_step(self, batch):
        Y_hat = self(*batch[:-1])
        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)
        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Classifier(d2l.Module):  #@save
    """분류 모델의 기본 클래스입니다."""
    def training_step(self, params, batch, state):
        # 배치 정규화 레이어가 있는 모델은 손실이 보조 데이터를 반환해야 하므로 
        # 여기서 value는 튜플입니다.
        value, grads = jax.value_and_grad(
            self.loss, has_aux=True)(params, batch[:-1], batch[-1], state)
        l, _ = value
        self.plot("loss", l, train=True)
        return value, grads

    def validation_step(self, params, batch, state):
        # 두 번째로 반환된 값은 버립니다. 이는 손실이 보조 데이터도 반환하는 
        # 배치 정규화 레이어가 있는 모델을 훈련할 때 사용됩니다.
        l, _ = self.loss(params, batch[:-1], batch[-1], state)
        self.plot('loss', l, train=False)
        self.plot('acc', self.accuracy(params, batch[:-1], batch[-1], state),
                  train=False)
</code></pre>
<p>기본적으로 우리는 선형 회귀 맥락에서 했던 것처럼 미니배치에서 작동하는 확률적 경사 하강법 최적화기를 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
@d2l.add_to_class(d2l.Module)  #@save
def configure_optimizers(self):
    params = self.parameters()
    if isinstance(params, list):
        return d2l.SGD(params, self.lr)
    return gluon.Trainer(params, 'sgd', {'learning_rate': self.lr})
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
@d2l.add_to_class(d2l.Module)  #@save
def configure_optimizers(self):
    return torch.optim.SGD(self.parameters(), lr=self.lr)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
@d2l.add_to_class(d2l.Module)  #@save
def configure_optimizers(self):
    return tf.keras.optimizers.SGD(self.lr)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(d2l.Module)  #@save
def configure_optimizers(self):
    return optax.sgd(self.lr)
</code></pre>
<h2 id="정확도-accuracy"><a class="header" href="#정확도-accuracy">정확도 (Accuracy)</a></h2>
<p>예측된 확률 분포 <code>y_hat</code>이 주어졌을 때, 하드 예측(hard prediction)을 출력해야 할 때마다 우리는 일반적으로 예측 확률이 가장 높은 클래스를 선택합니다. 실제로 많은 응용 분야에서 선택을 해야 합니다. 예를 들어 Gmail은 이메일을 "기본", "소셜", "업데이트", "포럼" 또는 "스팸"으로 분류해야 합니다. 내부적으로는 확률을 추정할 수 있겠지만, 결국에는 클래스 중 하나를 선택해야 합니다.</p>
<p>예측이 레이블 클래스 <code>y</code>와 일치하면 올바른 것입니다. 분류 정확도(classification accuracy)는 모든 예측 중 올바른 예측의 비율입니다. 정확도를 직접 최적화하는 것은 어려울 수 있지만(미분 불가능하므로), 우리가 가장 중요하게 생각하는 성능 측정치인 경우가 많습니다. 이는 벤치마크에서 종종 <em>핵심적인</em> 수치입니다. 따라서 분류기를 훈련할 때 거의 항상 이를 보고할 것입니다.</p>
<p>정확도는 다음과 같이 계산됩니다. 먼저 <code>y_hat</code>이 행렬인 경우, 두 번째 차원에 각 클래스에 대한 예측 점수가 저장되어 있다고 가정합니다. 각 행에서 가장 큰 항목의 인덱스로 예측된 클래스를 얻기 위해 <code>argmax</code>를 사용합니다. 그런 다음 [<strong>예측된 클래스를 실제 값 <code>y</code>와 요소별로 비교합니다.</strong>] 등호 연산자 <code>==</code>는 데이터 유형에 민감하므로, <code>y_hat</code>의 데이터 유형을 <code>y</code>와 일치하도록 변환합니다. 결과는 0(거짓)과 1(참) 항목을 포함하는 텐서입니다. 합계를 구하면 올바른 예측의 수가 나옵니다.</p>
<pre><code class="language-{.python .input  n=9}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(Classifier)  #@save
def accuracy(self, Y_hat, Y, averaged=True):
    """올바른 예측의 수를 계산합니다."""
    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))
    preds = d2l.astype(d2l.argmax(Y_hat, axis=1), Y.dtype)
    compare = d2l.astype(preds == d2l.reshape(Y, -1), d2l.float32)
    return d2l.reduce_mean(compare) if averaged else compare
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab jax
@d2l.add_to_class(Classifier)  #@save
@partial(jax.jit, static_argnums=(0, 5))
def accuracy(self, params, X, Y, state, averaged=True):
    """올바른 예측의 수를 계산합니다."""
    Y_hat = state.apply_fn({'params': params,
                            'batch_stats': state.batch_stats},  # 배치 정규화 전용
                           *X)
    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))
    preds = d2l.astype(d2l.argmax(Y_hat, axis=1), Y.dtype)
    compare = d2l.astype(preds == d2l.reshape(Y, -1), d2l.float32)
    return d2l.reduce_mean(compare) if averaged else compare
</code></pre>
<pre><code class="language-{.python .input  n=10}">%%tab mxnet

@d2l.add_to_class(d2l.Module)  #@save
def get_scratch_params(self):
    params = []
    for attr in dir(self):
        a = getattr(self, attr)
        if isinstance(a, np.ndarray):
            params.append(a)
        if isinstance(a, d2l.Module):
            params.extend(a.get_scratch_params())
    return params

@d2l.add_to_class(d2l.Module)  #@save
def parameters(self):
    params = self.collect_params()
    return params if isinstance(params, gluon.parameter.ParameterDict) and len(
        params.keys()) else self.get_scratch_params()
</code></pre>
<h2 id="요약-summary-8"><a class="header" href="#요약-summary-8">요약 (Summary)</a></h2>
<p>분류는 별도의 편의 함수가 필요할 정도로 충분히 일반적인 문제입니다. 분류에서 핵심적으로 중요한 것은 분류기의 <em>정확도</em>입니다. 비록 우리가 주로 정확도에 관심을 갖지만, 통계적 및 계산적 이유로 다양한 다른 목표를 최적화하도록 분류기를 훈련시킨다는 점에 유의하십시오. 하지만 훈련 중에 어떤 손실 함수가 최소화되었는지에 관계없이, 분류기의 정확도를 경험적으로 평가하기 위한 편의 메서드를 갖는 것은 유용합니다.</p>
<h2 id="연습-문제-exercises-9"><a class="header" href="#연습-문제-exercises-9">연습 문제 (Exercises)</a></h2>
<ol>
<li>$L_\textrm{v}$를 검증 손실로 표시하고, $L_\textrm{v}^\textrm{q}$를 이 섹션의 손실 함수 평균으로 계산된 대략적인 추정치라고 합시다. 마지막으로 $l_\textrm{v}^\textrm{b}$를 마지막 미니배치의 손실이라고 합시다. $L_\textrm{v}$를 $L_\textrm{v}^\textrm{q}$, $l_\textrm{v}^\textrm{b}$, 그리고 샘플 및 미니배치 크기 측면에서 표현하십시오.</li>
<li>대략적인 추정치 $L_\textrm{v}^\textrm{q}$가 불편 추정량(unbiased estimator)임을 보이십시오. 즉, $E[L_\textrm{v}] = E[L_\textrm{v}^\textrm{q}]$임을 보이십시오. 그럼에도 불구하고 왜 $L_\textrm{v}$를 대신 사용하고 싶어 할까요?</li>
<li>다중 클래스 분류 손실이 주어졌을 때, $y$를 보았을 때 $y'$를 추정하는 페널티를 $l(y,y')$로 표시하고 확률 $p(y \mid x)$가 주어졌을 때, $y'$의 최적 선택을 위한 규칙을 공식화하십시오. 힌트: $l$과 $p(y \mid x)$를 사용하여 기대 손실을 표현하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/6808">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/6809">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/6810">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17981">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="밑바닥부터-시작하는-소프트맥스-회귀-구현-softmax-regression-implementation-from-scratch"><a class="header" href="#밑바닥부터-시작하는-소프트맥스-회귀-구현-softmax-regression-implementation-from-scratch">밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></h1>
<p>:label:<code>sec_softmax_scratch</code></p>
<p>소프트맥스 회귀는 매우 기초적이기 때문에, 여러분이 이를 직접 구현하는 방법을 알아야 한다고 믿습니다. 여기서는 모델의 소프트맥스 관련 측면을 정의하는 것으로 제한하고, 훈련 루프를 포함한 다른 구성 요소들은 선형 회귀 섹션의 것들을 재사용하겠습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, gluon
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
from functools import partial
</code></pre>
<h2 id="소프트맥스-the-softmax-1"><a class="header" href="#소프트맥스-the-softmax-1">소프트맥스 (The Softmax)</a></h2>
<p>가장 중요한 부분인 스칼라를 확률로 매핑하는 것부터 시작해 봅시다. 복습을 위해 :numref:<code>subsec_lin-alg-reduction</code> 및 :numref:<code>subsec_lin-alg-non-reduction</code>에서 논의한 텐서의 특정 차원을 따른 합계 연산자를 상기해 보십시오. [<strong>행렬 <code>X</code>가 주어졌을 때 우리는 (기본적으로) 모든 요소를 합산하거나 동일한 축에 있는 요소들만 합산할 수 있습니다.</strong>] <code>axis</code> 변수를 사용하면 행 합계와 열 합계를 계산할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
X = d2l.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
d2l.reduce_sum(X, 0, keepdims=True), d2l.reduce_sum(X, 1, keepdims=True)
</code></pre>
<p>소프트맥스를 계산하려면 세 단계가 필요합니다:
(i) 각 항의 지수화;
(ii) 각 예제에 대한 정규화 상수를 계산하기 위해 각 행에 대한 합계;
(iii) 각 행을 정규화 상수로 나누어 결과의 합이 1이 되도록 함:</p>
<p>(**
$$\mathrm{softmax}(\mathbf{X})<em>{ij} = \frac{\exp(\mathbf{X}</em>{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.$$
**)</p>
<p>분모(의 로그)를 (로그) *분할 함수(partition function)*라고 합니다. 이는 통계 물리학에서 열역학적 앙상블의 모든 가능한 상태를 합산하기 위해 도입되었습니다. 구현은 간단합니다:</p>
<pre><code class="language-{.python .input}">%%tab all
def softmax(X):
    X_exp = d2l.exp(X)
    partition = d2l.reduce_sum(X_exp, 1, keepdims=True)
    return X_exp / partition  # 여기서 브로드캐스팅 메커니즘이 적용됩니다.
</code></pre>
<p>임의의 입력 <code>X</code>에 대해 [<strong>우리는 각 요소를 음이 아닌 숫자로 바꿉니다. 각 행의 합은 확률에 필요한 대로 1이 됩니다.</strong>] 주의: 위 코드는 매우 크거나 작은 인수에 대해 강건하지 <em>않습니다</em>. 무슨 일이 일어나는지 설명하기에는 충분하지만, 심각한 용도로 이 코드를 그대로 사용해서는 안 됩니다. 딥러닝 프레임워크에는 이러한 보호 기능이 내장되어 있으며 앞으로는 내장된 소프트맥스를 사용할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
X = d2l.rand(2, 5)
X_prob = softmax(X)
X_prob, d2l.reduce_sum(X_prob, 1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow, pytorch
X = d2l.rand((2, 5))
X_prob = softmax(X)
X_prob, d2l.reduce_sum(X_prob, 1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X = jax.random.uniform(jax.random.PRNGKey(d2l.get_seed()), (2, 5))
X_prob = softmax(X)
X_prob, d2l.reduce_sum(X_prob, 1)
</code></pre>
<h2 id="모델-the-model"><a class="header" href="#모델-the-model">모델 (The Model)</a></h2>
<p>이제 [<strong>소프트맥스 회귀 모델</strong>]을 구현하는 데 필요한 모든 것을 갖추었습니다. 선형 회귀 예제에서와 같이, 각 인스턴스는 고정 길이 벡터로 표현됩니다. 원시 데이터가 $28 \times 28$ 픽셀 이미지로 구성되므로, [<strong>각 이미지를 평탄화(flatten)하여 길이가 784인 벡터로 취급합니다.</strong>] 나중 장에서는 공간 구조를 더 만족스러운 방식으로 활용하는 합성곱 신경망을 소개할 것입니다.</p>
<p>소프트맥스 회귀에서 우리 네트워크의 출력 수는 클래스의 수와 같아야 합니다. (<strong>우리 데이터셋에는 10개의 클래스가 있으므로, 우리 네트워크는 10의 출력 차원을 갖습니다.</strong>) 결과적으로 우리의 가중치는 $784 \times 10$ 행렬이 되고 편향을 위한 $1 \times 10$ 행 벡터가 추가됩니다. 선형 회귀와 마찬가지로 가중치 <code>W</code>를 가우스 노이즈로 초기화합니다. 편향은 0으로 초기화됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class SoftmaxRegressionScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W = np.random.normal(0, sigma, (num_inputs, num_outputs))
        self.b = np.zeros(num_outputs)
        self.W.attach_grad()
        self.b.attach_grad()

    def collect_params(self):
        return [self.W, self.b]
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class SoftmaxRegressionScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W = torch.normal(0, sigma, size=(num_inputs, num_outputs),
                              requires_grad=True)
        self.b = torch.zeros(num_outputs, requires_grad=True)

    def parameters(self):
        return [self.W, self.b]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class SoftmaxRegressionScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W = tf.random.normal((num_inputs, num_outputs), 0, sigma)
        self.b = tf.zeros(num_outputs)
        self.W = tf.Variable(self.W)
        self.b = tf.Variable(self.b)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class SoftmaxRegressionScratch(d2l.Classifier):
    num_inputs: int
    num_outputs: int
    lr: float
    sigma: float = 0.01

    def setup(self):
        self.W = self.param('W', nn.initializers.normal(self.sigma),
                            (self.num_inputs, self.num_outputs))
        self.b = self.param('b', nn.initializers.zeros, self.num_outputs)
</code></pre>
<p>아래 코드는 네트워크가 각 입력을 출력에 매핑하는 방식을 정의합니다. 모델을 통해 데이터를 전달하기 전에 배치의 각 $28 \times 28$ 픽셀 이미지를 <code>reshape</code>을 사용하여 벡터로 평탄화합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(SoftmaxRegressionScratch)
def forward(self, X):
    X = d2l.reshape(X, (-1, self.W.shape[0]))
    return softmax(d2l.matmul(X, self.W) + self.b)
</code></pre>
<h2 id="크로스-엔트로피-손실-the-cross-entropy-loss"><a class="header" href="#크로스-엔트로피-손실-the-cross-entropy-loss">크로스 엔트로피 손실 (The Cross-Entropy Loss)</a></h2>
<p>다음으로 크로스 엔트로피 손실 함수를 구현해야 합니다(:numref:<code>subsec_softmax-regression-loss-func</code>에서 소개됨). 이는 모든 딥러닝에서 가장 흔한 손실 함수일 것입니다. 현재로서는 분류 문제로 쉽게 던질 수 있는 딥러닝 응용 분야가 회귀 문제로 처리하는 것이 더 나은 분야보다 훨씬 많습니다.</p>
<p>크로스 엔트로피는 실제 레이블에 할당된 예측 확률의 음의 로그 우도를 취한다는 점을 상기하십시오. 효율성을 위해 Python for-루프를 피하고 대신 인덱싱을 사용합니다. 특히 $\mathbf{y}$의 원-핫 인코딩을 통해 $\hat{\mathbf{y}}$에서 일치하는 항을 선택할 수 있습니다.</p>
<p>작동 방식을 확인하기 위해 [<strong>3개 클래스에 대한 예측 확률이 있는 2개의 예제와 그에 대응하는 레이블 <code>y</code>를 포함한 샘플 데이터 <code>y_hat</code>을 생성합니다.</strong>] 올바른 레이블은 각각 0과 2입니다(즉, 첫 번째와 세 번째 클래스). [<strong><code>y</code>를 <code>y_hat</code>의 확률 인덱스로 사용하여</strong>] 항을 효율적으로 뽑아낼 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
y = d2l.tensor([0, 2])
y_hat = d2l.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0, 1], y]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
y_hat = tf.constant([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y = tf.constant([0, 2])
tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))
</code></pre>
<p>:begin_tab:<code>pytorch, mxnet, tensorflow</code>
이제 선택된 확률의 로그에 대해 평균을 내어 (<strong>크로스 엔트로피 손실 함수를 구현</strong>)할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
이제 선택된 확률의 로그에 대해 평균을 내어 (<strong>크로스 엔트로피 손실 함수를 구현</strong>)할 수 있습니다.</p>
<p>JAX 구현의 속도를 높이기 위해 <code>jax.jit</code>을 활용하고 <code>loss</code>가 순수 함수(pure function)임을 보장하기 위해, <code>loss</code> 함수를 불순하게 만들 수 있는 전역 변수나 함수의 사용을 피하고자 <code>cross_entropy</code> 함수를 <code>loss</code> 내부에 다시 정의했습니다. <code>jax.jit</code>과 순수 함수에 대한 관심 있는 독자들은 <a href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions">JAX 문서</a>를 참조하십시오.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
def cross_entropy(y_hat, y):
    return -d2l.reduce_mean(d2l.log(y_hat[list(range(len(y_hat))), y]))

cross_entropy(y_hat, y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def cross_entropy(y_hat, y):
    return -tf.reduce_mean(tf.math.log(tf.boolean_mask(
        y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))))

cross_entropy(y_hat, y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(SoftmaxRegressionScratch)
def loss(self, y_hat, y):
    return cross_entropy(y_hat, y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(SoftmaxRegressionScratch)
@partial(jax.jit, static_argnums=(0))
def loss(self, params, X, y, state):
    def cross_entropy(y_hat, y):
        return -d2l.reduce_mean(d2l.log(y_hat[list(range(len(y_hat))), y]))
    y_hat = state.apply_fn({'params': params}, *X)
    # 반환된 빈 딕셔너리는 나중에 사용될(예: 배치 정규화) 보조 데이터를 위한 플레이스홀더입니다.
    return cross_entropy(y_hat, y), {}
</code></pre>
<h2 id="훈련-training-3"><a class="header" href="#훈련-training-3">훈련 (Training)</a></h2>
<p>:numref:<code>sec_linear_scratch</code>에서 정의된 <code>fit</code> 메서드를 재사용하여 [<strong>모델을 10 에폭 동안 훈련합니다.</strong>] 에폭 수(<code>max_epochs</code>), 미니배치 크기(<code>batch_size</code>), 학습률(<code>lr</code>)은 조정 가능한 하이퍼파라미터입니다. 즉, 이러한 값들은 기본 훈련 루프 중에 학습되지 않지만, 훈련 및 일반화 성능 측면에서 우리 모델의 성능에 여전히 영향을 미칩니다. 실제로는 데이터의 <em>검증(validation)</em> 분할을 기반으로 이러한 값들을 선택하고, 최종적으로 <em>테스트(test)</em> 분할에서 최종 모델을 평가하고 싶을 것입니다. :numref:<code>subsec_generalization-model-selection</code>에서 논의했듯이, 우리는 Fashion-MNIST의 테스트 데이터를 검증 세트로 간주하여 이 분할에 대한 검증 손실과 검증 정확도를 보고할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.FashionMNIST(batch_size=256)
model = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model, data)
</code></pre>
<h2 id="예측-prediction"><a class="header" href="#예측-prediction">예측 (Prediction)</a></h2>
<p>훈련이 완료되었으므로, 이제 우리 모델은 [<strong>일부 이미지를 분류</strong>]할 준비가 되었습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
X, y = next(iter(data.val_dataloader()))
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    preds = d2l.argmax(model(X), axis=1)
if tab.selected('jax'):
    preds = d2l.argmax(model.apply({'params': trainer.state.params}, X), axis=1)
preds.shape
</code></pre>
<p>우리는 모델이 <em>잘못</em> 분류한 이미지들에 더 관심이 있습니다. 실제 레이블(텍스트 출력의 첫 번째 줄)과 모델의 예측(텍스트 출력의 두 번째 줄)을 비교하여 시각화해 보겠습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
wrong = d2l.astype(preds, y.dtype) != y
X, y, preds = X[wrong], y[wrong], preds[wrong]
labels = [a+'\n'+b for a, b in zip(
    data.text_labels(y), data.text_labels(preds))]
data.visualize([X, y], labels=labels)
</code></pre>
<h2 id="요약-summary-9"><a class="header" href="#요약-summary-9">요약 (Summary)</a></h2>
<p>이제 우리는 선형 회귀와 분류 문제를 해결하는 데 어느 정도 경험을 쌓기 시작했습니다. 이를 통해 우리는 1960-1970년대의 최첨단 통계 모델링 수준에 도달했습니다. 다음 섹션에서는 딥러닝 프레임워크를 활용하여 이 모델을 훨씬 더 효율적으로 구현하는 방법을 보여드리겠습니다.</p>
<h2 id="연습-문제-exercises-10"><a class="header" href="#연습-문제-exercises-10">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 섹션에서는 소프트맥스 연산의 수학적 정의를 기반으로 소프트맥스 함수를 직접 구현했습니다. :numref:<code>sec_softmax</code>에서 논의했듯이 이는 수치적 불안정성을 유발할 수 있습니다.
<ol>
<li>입력 값 중 하나가 100일 때 <code>softmax</code>가 여전히 올바르게 작동하는지 테스트하십시오.</li>
<li>모든 입력 중 가장 큰 값이 -100보다 작을 때 <code>softmax</code>가 여전히 올바르게 작동하는지 테스트하십시오.</li>
<li>인수의 가장 큰 항목에 대한 상대적인 값을 살펴봄으로써 해결책을 구현하십시오.</li>
</ol>
</li>
<li>크로스 엔트로피 손실 함수의 정의 $\sum_i y_i \log \hat{y}_i$를 따르는 <code>cross_entropy</code> 함수를 구현하십시오.
<ol>
<li>이 섹션의 코드 예제에서 시도해 보십시오.</li>
<li>왜 더 느리게 실행된다고 생각하십니까?</li>
<li>이 함수를 사용해야 할까요? 어떤 경우에 사용하는 것이 타당할까요?</li>
<li>무엇을 주의해야 할까요? 힌트: 로그의 정의역을 고려하십시오.</li>
</ol>
</li>
<li>항상 가장 가능성 높은 레이블을 반환하는 것이 좋은 아이디어일까요? 예를 들어 의료 진단에서도 이렇게 하시겠습니까? 이를 어떻게 해결하려고 시도하시겠습니까?</li>
<li>일부 특성을 기반으로 다음 단어를 예측하기 위해 소프트맥스 회귀를 사용하고 싶다고 가정해 봅시다. 어휘 사전(vocabulary)이 클 때 어떤 문제가 발생할 수 있을까요?</li>
<li>이 섹션 코드의 하이퍼파라미터를 실험해 보십시오. 특히:
<ol>
<li>학습률을 변경함에 따라 검증 손실이 어떻게 변하는지 플롯하십시오.</li>
<li>미니배치 크기를 변경함에 따라 검증 및 훈련 손실이 변합니까? 효과가 나타나기 전까지 얼마나 크거나 작게 조정해야 합니까?</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/50">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/51">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/225">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17982">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="선형-회귀의-간결한-구현-concise-implementation-of-softmax-regression"><a class="header" href="#선형-회귀의-간결한-구현-concise-implementation-of-softmax-regression">선형 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></h1>
<p>:label:<code>sec_softmax_concise</code></p>
<p>고수준 딥러닝 프레임워크가 선형 회귀 구현을 쉽게 만들어주었듯이(:numref:<code>sec_linear_concise</code> 참조), 여기에서도 마찬가지로 편리합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, init, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import numpy as np
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from functools import partial
import jax
from jax import numpy as jnp
import optax
</code></pre>
<h2 id="모델-정의하기-defining-the-model-2"><a class="header" href="#모델-정의하기-defining-the-model-2">모델 정의하기 (Defining the Model)</a></h2>
<p>:numref:<code>sec_linear_concise</code>에서처럼, 내장 레이어를 사용하여 완전 연결 레이어를 구성합니다. 내장된 <code>__call__</code> 메서드는 네트워크를 입력에 적용해야 할 때마다 <code>forward</code>를 호출합니다.</p>
<p>:begin_tab:<code>mxnet</code>
입력 <code>X</code>가 4차 텐서이더라도, 내장된 <code>Dense</code> 레이어는 첫 번째 축을 따른 차원을 유지하면서 자동으로 <code>X</code>를 2차 텐서로 변환합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
우리는 <code>Flatten</code> 레이어를 사용하여 첫 번째 축을 따른 차원을 유지하면서 4차 텐서 <code>X</code>를 2차 텐서로 변환합니다.</p>
<p>:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
우리는 <code>Flatten</code> 레이어를 사용하여 첫 번째 축을 따른 차원을 유지하면서 4차 텐서 <code>X</code>를 변환합니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
Flax는 <code>@nn.compact</code> 데코레이터를 사용하여 네트워크 클래스를 더 간결하게 작성할 수 있게 해줍니다. <code>@nn.compact</code>를 사용하면 데이터 클래스에 표준 <code>setup</code> 메서드를 정의할 필요 없이, 단일 "순방향 패스(forward pass)" 메서드 내에 모든 네트워크 로직을 작성할 수 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch
class SoftmaxRegression(d2l.Classifier):  #@save
    """소프트맥스 회귀 모델입니다."""
    def __init__(self, num_outputs, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(nn.Flatten(),
                                 nn.LazyLinear(num_outputs))

    def forward(self, X):
        return self.net(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow
class SoftmaxRegression(d2l.Classifier):  #@save
    """소프트맥스 회귀 모델입니다."""
    def __init__(self, num_outputs, lr):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.net = nn.Dense(num_outputs)
            self.net.initialize()
        if tab.selected('tensorflow'):
            self.net = tf.keras.models.Sequential()
            self.net.add(tf.keras.layers.Flatten())
            self.net.add(tf.keras.layers.Dense(num_outputs))

    def forward(self, X):
        return self.net(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class SoftmaxRegression(d2l.Classifier):  #@save
    num_outputs: int
    lr: float

    @nn.compact
    def __call__(self, X):
        X = X.reshape((X.shape[0], -1))  # Flatten
        X = nn.Dense(self.num_outputs)(X)
        return X
</code></pre>
<h2 id="소프트맥스-다시-보기-softmax-revisited"><a class="header" href="#소프트맥스-다시-보기-softmax-revisited">소프트맥스 다시 보기 (Softmax Revisited)</a></h2>
<p>:label:<code>subsec_softmax-implementation-revisited</code></p>
<p>:numref:<code>sec_softmax_scratch</code>에서 우리는 모델의 출력을 계산하고 크로스 엔트로피 손실을 적용했습니다. 이는 수학적으로는 완전히 합리적이지만, 지수 계산에서의 수치적 언더플로와 오버플로 때문에 계산적으로는 위험합니다.</p>
<p>소프트맥스 함수가 $\hat y_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$를 통해 확률을 계산한다는 점을 상기하십시오. 만약 일부 $o_k$가 매우 크면(즉, 매우 큰 양수이면), $\exp(o_k)$는 특정 데이터 유형이 가질 수 있는 가장 큰 숫자보다 커질 수 있습니다. 이를 *오버플로(overflow)*라고 합니다. 마찬가지로 모든 인수가 매우 큰 음수이면 *언더플로(underflow)*가 발생합니다. 예를 들어 단정밀도 부동 소수점 숫자는 대략 $10^{-38}$에서 $10^{38}$의 범위를 커버합니다. 따라서 $\mathbf{o}$의 가장 큰 항이 구간 $[-90, 90]$을 벗어나면 결과가 안정적이지 않습니다. 이 문제를 해결하는 방법은 모든 항목에서 $\bar{o} \stackrel{\textrm{def}}{=} \max_k o_k$를 빼는 것입니다:</p>
<p>$$
\hat y_j = \frac{\exp o_j}{\sum_k \exp o_k} =
\frac{\exp(o_j - \bar{o}) \exp \bar{o}}{\sum_k \exp (o_k - \bar{o}) \exp \bar{o}} =
\frac{\exp(o_j - \bar{o})}{\sum_k \exp (o_k - \bar{o})}.
$$</p>
<p>구조상 모든 $j$에 대해 $o_j - \bar{o} \leq 0$임을 압니다. 따라서 $q$-클래스 분류 문제의 경우 분모는 구간 $[1, q]$에 포함됩니다. 게다가 분자는 1을 넘지 않으므로 수치적 오버플로를 방지합니다. 수치적 언더플로는 $\exp(o_j - \bar{o})$가 수치적으로 0으로 평가될 때만 발생합니다. 그럼에도 불구하고 나중에 $\log \hat{y}_j$를 $\log 0$으로 계산하고 싶을 때 문제가 발생할 수 있습니다. 특히 역전파에서 무시무시한 <code>NaN</code> (Not a Number) 결과로 가득 찬 화면을 보게 될 수도 있습니다.</p>
<p>다행히도 지수 함수를 계산하고 있더라도 궁극적으로는 그들의 로그를 취하려 한다는 사실(크로스 엔트로피 손실을 계산할 때) 덕분에 구원받을 수 있습니다. 소프트맥스와 크로스 엔트로피를 결합함으로써 수치 안정성 문제를 완전히 벗어날 수 있습니다. 다음과 같습니다:</p>
<p>$$
\log \hat{y}_j =
\log \frac{\exp(o_j - \bar{o})}{\sum_k \exp (o_k - \bar{o})} =
o_j - \bar{o} - \log \sum_k \exp (o_k - \bar{o}).
$$</p>
<p>이는 오버플로와 언더플로를 모두 방지합니다. 모델에 의한 출력 확률을 평가하고 싶을 때를 대비해 기존의 소프트맥스 함수를 계속 유용하게 사용할 것입니다. 하지만 새 손실 함수에 소프트맥스 확률을 전달하는 대신, [<strong>"LogSumExp 트릭"(https://en.wikipedia.org/wiki/LogSumExp)과 같은 영리한 일을 수행하는 크로스 엔트로피 손실 함수 내부에서 로짓(logits)을 전달하고 소프트맥스와 그 로그를 한 번에 계산합니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(d2l.Classifier)  #@save
def loss(self, Y_hat, Y, averaged=True):
    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))
    Y = d2l.reshape(Y, (-1,))
    if tab.selected('mxnet'):
        fn = gluon.loss.SoftmaxCrossEntropyLoss()
        l = fn(Y_hat, Y)
        return l.mean() if averaged else l
    if tab.selected('pytorch'):
        return F.cross_entropy(
            Y_hat, Y, reduction='mean' if averaged else 'none')
    if tab.selected('tensorflow'):
        fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
        return fn(Y, Y_hat)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(d2l.Classifier)  #@save
@partial(jax.jit, static_argnums=(0, 5))
def loss(self, params, X, Y, state, averaged=True):
    # 나중에 사용될 예정(예: 배치 정규화)
    Y_hat = state.apply_fn({'params': params}, *X,
                           mutable=False, rngs=None)
    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))
    Y = d2l.reshape(Y, (-1,))
    fn = optax.softmax_cross_entropy_with_integer_labels
    # 반환된 빈 딕셔너리는 나중에 사용될(예: 배치 정규화) 보조 데이터를 위한 플레이스홀더입니다.
    return (fn(Y_hat, Y).mean(), {}) if averaged else (fn(Y_hat, Y), {})
</code></pre>
<h2 id="훈련-training-4"><a class="header" href="#훈련-training-4">훈련 (Training)</a></h2>
<p>다음으로 모델을 훈련합니다. 784차원 특성 벡터로 평탄화된 Fashion-MNIST 이미지를 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.FashionMNIST(batch_size=256)
model = SoftmaxRegression(num_outputs=10, lr=0.1)
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model, data)
</code></pre>
<p>이전처럼 이 알고리즘은 합리적으로 정확한 솔루션으로 수렴하지만, 이번에는 이전보다 훨씬 적은 줄의 코드를 사용합니다.</p>
<h2 id="요약-summary-10"><a class="header" href="#요약-summary-10">요약 (Summary)</a></h2>
<p>고수준 API는 수치 안정성과 같이 사용자에게 잠재적으로 위험한 측면을 숨기는 데 매우 편리합니다. 게다가 사용자가 아주 적은 줄의 코드로 모델을 간결하게 설계할 수 있게 해줍니다. 이는 축복이자 저주입니다. 분명한 이점은 통계학 수업을 한 번도 들어본 적 없는 엔지니어들에게도(실제로 그들이 이 책의 대상 독자 중 일부입니다) 접근성을 매우 높여준다는 것입니다. 하지만 날카로운 모서리를 숨기는 데는 대가가 따릅니다: 스스로 새로운 그리고 다른 구성 요소를 추가하려는 의욕을 꺾게 되는데, 이를 수행하기 위한 근육 기억이 거의 없기 때문입니다. 게다가 프레임워크의 보호 패딩이 모든 코너 케이스를 완벽하게 커버하지 못할 때마다 무언가를 <em>고치는</em> 것을 더 어렵게 만듭니다. 이 역시 익숙함의 부족 때문입니다.</p>
<p>따라서 우리는 여러분이 이어지는 많은 구현의 가감 없는 버전과 우아한 버전 <em>둘 다</em>를 검토할 것을 강력히 권장합니다. 우리는 이해의 용이성을 강조하지만, 그럼에도 불구하고 구현은 대개 상당히 우수한 성능을 보여줍니다(합성곱은 여기서 큰 예외입니다). 어떤 프레임워크도 제공할 수 없는 새로운 것을 발명할 때 여러분이 이러한 토대 위에 구축할 수 있도록 하는 것이 우리의 의도입니다.</p>
<h2 id="연습-문제-exercises-11"><a class="header" href="#연습-문제-exercises-11">연습 문제 (Exercises)</a></h2>
<ol>
<li>딥러닝은 FP64 배정밀도(매우 드물게 사용됨), FP32 단정밀도, BFLOAT16(압축된 표현에 좋음), FP16(매우 불안정함), TF32(NVIDIA의 새로운 형식), INT8을 포함한 많은 다양한 숫자 형식을 사용합니다. 결과가 수치적 언더플로나 오버플로로 이어지지 않는 지수 함수의 가장 작은 인수와 가장 큰 인수를 계산하십시오.</li>
<li>INT8은 1에서 255 사이의 0이 아닌 숫자로 구성된 매우 제한된 형식입니다. 더 많은 비트를 사용하지 않고 어떻게 동적 범위를 확장할 수 있을까요? 표준 곱셈과 덧셈이 여전히 작동합니까?</li>
<li>훈련 에폭 수를 늘리십시오. 왜 잠시 후에 검증 정확도가 떨어질 수 있을까요? 이를 어떻게 고칠 수 있을까요?</li>
<li>학습률을 높이면 어떻게 됩니까? 여러 학습률에 대한 손실 곡선을 비교하십시오. 어떤 것이 더 잘 작동합니까? 언제 그렇습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/52">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/53">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/260">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17983">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="분류에서의-일반화-generalization-in-classification"><a class="header" href="#분류에서의-일반화-generalization-in-classification">분류에서의 일반화 (Generalization in Classification)</a></h1>
<p>:label:<code>chap_classification_generalization</code></p>
<p>지금까지 우리는 여러 개의 출력과 소프트맥스 함수를 가진 (선형) 신경망을 훈련하여 다중 클래스 분류 문제를 다루는 방법에 집중했습니다. 모델의 출력을 확률론적 예측으로 해석하여, 모델이(고정된 파라미터 세트에 대해) 실제 레이블에 할당하는 음의 로그 우도를 계산하는 크로스 엔트로피 손실 함수를 유도하고 동기를 부여했습니다. 마지막으로 이러한 도구들을 훈련 세트에 모델을 맞춤으로써 실습에 적용했습니다. 하지만 언제나 그렇듯 우리의 목표는 이전에 본 적 없는 데이터(테스트 세트)에서 경험적으로 평가된 <em>일반적인 패턴</em>을 학습하는 것입니다. 훈련 세트에서의 높은 정확도는 아무 의미가 없습니다. 각 입력이 고유할 때마다(실제로 대부분의 고차원 데이터셋에서 그러함), 첫 번째 훈련 에폭에서 데이터셋을 단순히 암기한 다음 새로운 이미지를 볼 때마다 레이블을 찾아보는 것만으로 훈련 세트에서 완벽한 정확도를 얻을 수 있습니다. 하지만 정확한 훈련 예제와 관련된 정확한 레이블을 암기하는 것은 새로운 예제를 어떻게 분류해야 하는지 알려주지 않습니다. 추가적인 지침이 없다면 새로운 예제를 만날 때마다 무작위 추측에 의존해야 할 수도 있습니다.</p>
<p>몇 가지 시급한 질문들이 즉각적인 관심을 요구합니다:</p>
<ol>
<li>모집단에 대한 분류기의 정확도를 잘 추정하기 위해 얼마나 많은 테스트 예제가 필요할까요?</li>
<li>동일한 테스트 세트에서 모델을 반복해서 계속 평가하면 어떻게 될까요?</li>
<li>훈련 세트에 우리 선형 모델을 맞추는 것이 단순한 암기 체계보다 더 나을 것이라고 왜 기대해야 할까요?</li>
</ol>
<p>:numref:<code>sec_generalization_basics</code>가 선형 회귀의 맥락에서 과대적합과 일반화의 기초를 소개했다면, 이 장에서는 조금 더 깊이 들어가 통계적 학습 이론의 기초적인 아이디어 몇 가지를 소개할 것입니다. 우리는 종종 사후적으로 일반화를 보장할 수 있는 것으로 밝혀졌습니다: 많은 모델에 대해, 그리고 원하는 일반화 갭의 상한 $\epsilon$에 대해, 훈련 세트가 최소 $n$개의 샘플을 포함한다면 <em>어떠한 데이터 생성 분포에 대해서도</em> 우리의 경험적 오차가 실제 오차의 $\epsilon$ 범위 내에 있게 되는 필요한 샘플 수 $n$을 결정할 수 있는 경우가 많습니다. 불행히도 이러한 종류의 보장은 심오한 지적 토대를 제공하지만, 딥러닝 실무자에게는 실제적인 유용성이 제한적이라는 사실도 밝혀졌습니다. 간단히 말해, 이러한 보장은 심층 신경망의 일반화를 <em>사전적으로</em> 보장하려면 (아마도 수조 개 이상의) 터무니없는 수의 예제가 필요함을 시사합니다. 하지만 우리가 관심을 갖는 작업에서 심층 신경망은 일반적으로 훨씬 적은 수의 예제(수천 개)로도 놀라울 정도로 잘 일반화된다는 것을 발견합니다. 따라서 딥러닝 실무자들은 종종 사전적 보장을 아예 포기하고, 대신 과거에 유사한 문제에서 잘 일반화되었던 방법들을 채택하며, 경험적 평가를 통해 <em>사후적으로</em> 일반화를 인증합니다. :numref:<code>chap_perceptrons</code>에 도달하면 일반화를 다시 다루고, 왜 심층 신경망이 실제 상황에서 일반화되는지 설명하려는 시도에서 생겨난 방대한 과학 문헌에 대해 가볍게 소개할 것입니다.</p>
<h2 id="테스트-세트-the-test-set"><a class="header" href="#테스트-세트-the-test-set">테스트 세트 (The Test Set)</a></h2>
<p>우리는 이미 일반화 오차를 평가하는 표준 방법으로서 테스트 세트에 의존하기 시작했으므로, 그러한 오차 추정치의 속성에 대해 논의하며 시작해 봅시다. 어떻게 얻었는지에 대해서는 걱정하지 말고 고정된 분류기 $f$에 집중해 봅시다. 또한 분류기 $f$를 훈련하는 데 사용되지 않은 예제들의 <em>신선한</em> 데이터셋 $\mathcal{D} = {(\mathbf{x}^{(i)},y^{(i)})}_{i=1}^n$을 가지고 있다고 가정합시다. $\mathcal{D}$에서 우리 분류기 $f$의 *경험적 오차(empirical error)*는 단순히 예측 $f(\mathbf{x}^{(i)})$가 실제 레이블 $y^{(i)}$와 일치하지 않는 인스턴스의 비율이며, 다음 식으로 주어집니다:</p>
<p>$$\epsilon_\mathcal{D}(f) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(f(\mathbf{x}^{(i)}) \neq y^{(i)}).$$</p>
<p>대조적으로, *모집단 오차(population error)*는 기저 모집단(확률 밀도 함수 $p(\mathbf{x},y)$로 특징지어지는 어떤 분포 $P(X,Y)$)에서 우리 분류기가 실제 레이블과 일치하지 않는 예제들의 <em>기대</em> 비율입니다:</p>
<p>$$\epsilon(f) =  E_{(\mathbf{x}, y) \sim P} \mathbf{1}(f(\mathbf{x}) \neq y) =
\int\int \mathbf{1}(f(\mathbf{x}) \neq y) p(\mathbf{x}, y) ;d\mathbf{x} dy.$$</p>
<p>$\epsilon(f)$가 우리가 실제로 관심을 갖는 양이지만, 모든 사람을 측정하지 않고는 큰 모집단의 평균 키를 직접 관찰할 수 없는 것처럼 이를 직접 관찰할 수는 없습니다. 우리는 샘플을 바탕으로 이 양을 추정할 수 있을 뿐입니다. 우리 테스트 세트 $\mathcal{D}$가 기저 모집단을 통계적으로 대표하기 때문에, $\epsilon_\mathcal{D}(f)$를 모집단 오차 $\epsilon(f)$의 통계적 추정량(statistical estimator)으로 볼 수 있습니다. 게다가 우리가 관심 있는 양 $\epsilon(f)$가 (확률 변수 $\mathbf{1}(f(X) \neq Y)$의) 기댓값이고 해당 추정량 $\epsilon_\mathcal{D}(f)$가 표본 평균이므로, 모집단 오차를 추정하는 것은 단순히 :numref:<code>sec_prob</code>에서 기억하실 수 있는 고전적인 평균 추정 문제입니다.</p>
<p>확률 이론의 중요한 고전적 결과인 *중심 극한 정리(central limit theorem)*는 평균 $\mu$와 표준 편차 $\sigma$를 가진 임의의 분포에서 추출된 $n$개의 무작위 샘플 $a_1, ..., a_n$을 가질 때마다, 샘플 수 $n$이 무한대에 가까워짐에 따라 표본 평균 $\hat{\mu}$가 실제 평균을 중심으로 하고 표준 편차가 $\sigma/\sqrt{n}$인 정규 분포에 대략적으로 근접함을 보장합니다. 이미 이는 중요한 사실을 말해줍니다: 예제 수가 많아짐에 따라 테스트 오차 $\epsilon_\mathcal{D}(f)$는 $\mathcal{O}(1/\sqrt{n})$의 비율로 실제 오차 $\epsilon(f)$에 접근해야 한다는 것입니다. 따라서 테스트 오차를 두 배 더 정확하게 추정하려면 테스트 세트를 4배 더 크게 수집해야 합니다. 테스트 오차를 100분의 1로 줄이려면 테스트 세트를 만 배 더 크게 수집해야 합니다. 일반적으로 그러한 $\mathcal{O}(1/\sqrt{n})$의 비율은 통계학에서 우리가 바랄 수 있는 최선의 결과인 경우가 많습니다.</p>
<p>이제 테스트 오차 $\epsilon_\mathcal{D}(f)$가 실제 오차 $\epsilon(f)$로 수렴하는 점진적 비율에 대해 알게 되었으므로, 몇 가지 중요한 세부 사항을 확대해 봅시다. 관심 있는 확률 변수 $\mathbf{1}(f(X) \neq Y)$가 0과 1 값만 취할 수 있으므로 이는 베르누이 확률 변수이며, 값 1을 취할 확률을 나타내는 파라미터로 특징지어집니다. 여기서 1은 우리 분류기가 오차를 범했음을 의미하므로, 우리 확률 변수의 파라미터는 실제로는 실제 오차율 $\epsilon(f)$입니다. 베르누이의 분산 $\sigma^2$은 식 $\epsilon(f)(1-\epsilon(f))$에 따라 그 파라미터(여기서는 $\epsilon(f)$)에 의존합니다. $\epsilon(f)$는 처음에 알려지지 않았지만 1보다 클 수 없음을 압니다. 이 함수를 조금 조사해 보면 실제 오차율이 0.5에 가까울 때 분산이 가장 높고, 0이나 1에 가까울 때 훨씬 낮아질 수 있음을 알 수 있습니다. 이는 ($n$개의 테스트 샘플 선택에 따른) 오차 $\epsilon(f)$에 대한 우리 추정치 $\epsilon_\mathcal{D}(f)$의 점진적 표준 편차가 $\sqrt{0.25/n}$보다 클 수 없음을 알려줍니다.</p>
<p>이 비율이 유한한 샘플을 가질 때가 아니라 테스트 세트 크기가 무한대에 가까워질 때의 행동을 특징짓는다는 사실을 무시한다면, 이는 테스트 오차 $\epsilon_\mathcal{D}(f)$가 모집단 오차 $\epsilon(f)$에 근사하여 한 표준 편차가 $\pm 0.01$의 구간에 해당하기를 원한다면 대략 2500개의 샘플을 수집해야 함을 알려줍니다. 만약 그 범위에 두 표준 편차를 맞추어 $\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$임을 95% 확신하고 싶다면 10,000개의 샘플이 필요할 것입니다!</p>
<p>이것이 머신러닝의 많은 인기 있는 벤치마크 테스트 세트 크기로 밝혀졌습니다. 매년 수천 편의 응용 딥러닝 논문이 발표되면서 0.01 이하의 오차율 개선을 대단하게 다루는 것을 보고 놀라실 수도 있습니다. 물론 오차율이 0에 훨씬 가까울 때는 0.01의 개선이 정말 큰 일일 수 있습니다.</p>
<p>지금까지 우리 분석의 한 가지 성가신 특징은 이것이 실제로는 점진적(asymptotics)인 것, 즉 샘플 크기가 무한대로 갈 때 $\epsilon_\mathcal{D}$와 $\epsilon$ 사이의 관계가 어떻게 진화하는지에 대해서만 알려준다는 점입니다. 다행히 우리 확률 변수가 유계(bounded)이므로, Hoeffding(1963)의 부등식을 적용하여 유효한 유한 샘플 경계를 얻을 수 있습니다:</p>
<p>$$P(\epsilon_\mathcal{D}(f) - \epsilon(f) \geq t) &lt; \exp\left( - 2n t^2 \right).$$</p>
<p>우리 추정치 $\epsilon_\mathcal{D}(f)$와 실제 오차율 $\epsilon(f)$ 사이의 거리 $t$가 0.01을 넘지 않는다고 95% 확신할 수 있게 해주는 가장 작은 데이터셋 크기를 구해보면, 위 점진적 분석에서 제안된 10,000개 예제에 비해 대략 15,000개의 예제가 필요함을 알게 될 것입니다. 통계학을 더 깊이 파고들면 이러한 경향이 일반적으로 유지됨을 알게 될 것입니다. 유한 샘플에서도 유지되는 보장은 일반적으로 약간 더 보수적입니다. 대세로 볼 때 이러한 수치들이 그렇게 멀지 않다는 점에 유의하십시오. 이는 점진적 분석이 비록 법정으로 가져갈 수 있는 보장은 아니더라도 우리에게 대략적인 수치를 제공하는 데 일반적으로 유용함을 반영합니다.</p>
<h2 id="테스트-세트-재사용-test-set-reuse"><a class="header" href="#테스트-세트-재사용-test-set-reuse">테스트 세트 재사용 (Test Set Reuse)</a></h2>
<p>어떤 의미에서 여러분은 이제 경험적 머신러닝 연구를 수행하는 데 성공할 준비가 되었습니다. 거의 모든 실제 모델은 테스트 세트 성능을 기반으로 개발되고 검증되며, 여러분은 이제 테스트 세트의 마스터입니다. 고정된 분류기 $f$에 대해 테스트 오차 $\epsilon_\mathcal{D}(f)$를 평가하는 방법을 알고, 그 모집단 오차 $\epsilon(f)$에 대해 무엇을 말할 수 있는지(그리고 없는지) 정확히 압니다.</p>
<p>따라서 여러분이 이 지식을 가지고 첫 번째 모델 $f_1$을 훈련할 준비를 한다고 합시다. 분류기의 오차율 성능에 대해 얼마나 확신이 필요한지 알고 있으므로, 테스트 세트를 위해 떼어놓을 적절한 예제 수를 결정하기 위해 위 분석을 적용합니다. 더욱이 여러분이 :numref:<code>sec_generalization_basics</code>의 교훈을 가슴에 새기고 모든 예비 분석, 하이퍼파라미터 튜닝, 심지어 경쟁하는 여러 모델 아키텍처 중 선택까지 검증 세트에서 수행함으로써 테스트 세트의 신성함을 지켰다고 가정해 봅시다. 마지막으로 테스트 세트에서 모델 $f_1$을 평가하고 관련 신뢰 구간과 함께 모집단 오차의 불편 추정치를 보고합니다.</p>
<p>지금까지는 모든 것이 잘 진행되는 것 같습니다. 하지만 그날 밤 새벽 3시에 새로운 모델링 접근 방식에 대한 기막힌 아이디어가 떠올라 잠에서 깹니다. 다음 날 새 모델을 코딩하고 검증 세트에서 하이퍼파라미터를 튜닝하며, 새 모델 $f_2$가 작동할 뿐만 아니라 오차율이 $f_1$보다 훨씬 낮아 보입니다. 하지만 최종 평가를 준비하면서 갑자기 발견의 전율이 사라집니다. 여러분에게는 테스트 세트가 없습니다!</p>
<p>비록 원래 테스트 세트 $\mathcal{D}$가 여전히 서버에 남아 있더라도, 이제 여러분은 두 가지 만만치 않은 문제에 직면합니다. 첫째, 테스트 세트를 수집할 때 여러분은 단일 분류기 $f$를 평가한다는 가정하에 필요한 정밀도 수준을 결정했습니다. 하지만 동일한 테스트 세트에서 여러 분류기 $f_1, ..., f_k$를 평가하는 일을 하게 된다면, 거짓 발견(false discovery) 문제를 고려해야 합니다. 이전에는 단일 분류기 $f$에 대해 $\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$임을 95% 확신했을 수 있고, 따라서 잘못된 결과가 나올 확률은 단 5%였습니다. $k$개의 분류기가 섞여 있으면, 그들 중 단 하나도 테스트 세트 성능이 오해를 불러일으키지 않는다고 보장하기 어려울 수 있습니다. 고려 중인 분류기가 20개라면, 그들 중 최소 하나가 오해의 소지가 있는 점수를 받았을 가능성을 배제할 힘이 전혀 없을 수도 있습니다. 이 문제는 다중 가설 검정(multiple hypothesis testing)과 관련이 있으며, 통계학의 방대한 문헌에도 불구하고 과학 연구를 괴롭히는 고질적인 문제로 남아 있습니다.</p>
<p>그것만으로 걱정하기 부족하다면, 후속 평가에서 얻는 결과를 불신해야 할 특별한 이유가 있습니다. 테스트 세트 성능에 대한 우리 분석은 분류기가 테스트 세트와의 접촉 없이 선택되었다는 가정에 근거했으므로 테스트 세트를 기저 모집단에서 무작위로 추출된 것으로 볼 수 있었다는 점을 상기하십시오. 여기서는 여러 함수를 테스트하고 있을 뿐만 아니라, 후속 함수 $f_2$가 $f_1$의 테스트 세트 성능을 관찰한 <em>후에</em> 선택되었습니다. 테스트 세트의 정보가 모델러에게 유출되고 나면, 엄격한 의미에서 그것은 더 이상 진정한 테스트 세트가 될 수 없습니다. 이 문제를 *적응형 과대적합(adaptive overfitting)*이라고 하며 최근 학습 이론가들과 통계학자들 사이에서 강렬한 관심 주제로 부상했습니다 :cite:<code>dwork2015preserving</code>. 다행히 홀드아웃 세트에서 모든 정보를 유출하는 것이 가능하고 이론적인 최악의 시나리오는 암울하지만, 이러한 분석들은 너무 보수적일 수 있습니다. 실제로는 진짜 테스트 세트를 만들고, 가능한 한 드물게 참조하며, 신뢰 구간을 보고할 때 다중 가설 검정을 고려하고, 이해관계가 높고 데이터셋 크기가 작을 때 경계심을 더 공격적으로 높이도록 주의하십시오. 일련의 벤치마크 챌린지를 실행할 때, 매 라운드 후에 오래된 테스트 세트를 검증 세트로 강등시킬 수 있도록 여러 테스트 세트를 유지하는 것이 종종 좋은 관행입니다.</p>
<h2 id="통계적-학습-이론-statistical-learning-theory"><a class="header" href="#통계적-학습-이론-statistical-learning-theory">통계적 학습 이론 (Statistical Learning Theory)</a></h2>
<p>단순히 말해서 <em>테스트 세트가 우리가 가진 전부</em>이지만, 이 사실은 이상하게도 불만족스럽게 느껴집니다. 첫째, 우리는 진정한 테스트 세트를 소유하는 경우가 드뭅니다. 우리가 데이터셋을 직접 만드는 사람이 아니라면, 다른 누군가가 이미 우리의 표면적인 "테스트 세트"에서 자신의 분류기를 평가했을 가능성이 높습니다. 그리고 우리가 우선권(first dibs)을 가졌을 때조차, 곧 우리 숫자를 믿을 수 없다는 찝찝한 기분 없이 후속 모델링 시도를 평가할 수 있기를 바라며 좌절하게 됩니다. 더욱이 진정한 테스트 세트조차 분류기가 실제로 모집단에 일반화되었는지를 <em>사후적으로</em> 알려줄 뿐이며, 일반화되어야 할 <em>사전적인</em> 이유가 있는지는 알려주지 않습니다.</p>
<p>이러한 우려를 염두에 둔다면, 여러분은 이제 통계적 학습 이론(statistical learning theory)의 매력을 볼 준비가 충분히 된 것일 수 있습니다. 통계적 학습 이론은 머신러닝의 수학적 하위 분야로, 실무자들이 경험적 데이터로 훈련된 모델이 왜/언제 보지 못한 데이터로 일반화될 수 있는/있을 것인지 설명하는 근본적인 원리를 밝히는 것을 목표로 합니다. 통계적 학습 연구자들의 주요 목표 중 하나는 모델 클래스의 속성을 데이터셋의 샘플 수와 연관 지어 일반화 갭을 제한(bound)하는 것이었습니다.</p>
<p>학습 이론가들은 훈련 세트 $\mathcal{S}$에서 훈련되고 평가된 학습된 분류기 $f_\mathcal{S}$의 <em>경험적 오차</em> $\epsilon_\mathcal{S}(f_\mathcal{S})$와, 기저 모집단에 대한 동일한 분류기의 실제 오차 $\epsilon(f_\mathcal{S})$ 사이의 차이를 제한하는 것을 목표로 합니다. 이는 우리가 방금 다룬 평가 문제와 비슷해 보일 수 있지만 큰 차이가 있습니다. 앞서 분류기 $f$는 고정되어 있었고 우리는 평가 목적으로만 데이터셋이 필요했습니다. 그리고 실제로 어떤 고정된 분류기는 일반화됩니다: (이전에 보지 못한) 데이터셋에서의 오차는 모집단 오차의 불편 추정치입니다. 하지만 분류기가 동일한 데이터셋에서 훈련되고 평가될 때는 무엇을 말할 수 있을까요? 훈련 오차가 테스트 오차와 가까울 것이라고 확신할 수 있을까요?</p>
<p>우리 학습된 분류기 $f_\mathcal{S}$가 미리 지정된 함수 집합 $\mathcal{F}$에서 선택되어야 한다고 가정합시다. 테스트 세트에 대한 논의에서 단일 분류기의 오차를 추정하는 것은 쉽지만, 분류기 모음을 고려하기 시작하면 상황이 까다로워진다는 점을 상기하십시오. 비록 어떤 하나의 (고정된) 분류기의 경험적 오차가 높은 확률로 실제 오차와 가까울지라도, 분류기 모음을 고려하게 되면 그들 중 <em>단 하나라도</em> 오차가 잘못 추정될 가능성을 걱정해야 합니다. 걱정되는 점은 우리가 그러한 분류기를 선택하여 모집단 오차를 크게 과소평가하게 될 수도 있다는 것입니다. 더욱이 선형 모델의 경우에도 그 파라미터가 연속적인 값을 갖기 때문에, 우리는 일반적으로 무한한 함수 클래스($|\mathcal{F}| = \infty$)에서 선택하게 됩니다.</p>
<p>이 문제에 대한 한 가지 야심 찬 해결책은 균등 수렴(uniform convergence)을 증명하기 위한 분석 도구를 개발하는 것입니다. 즉, 높은 확률로 클래스 $f\in\mathcal{F}$ 내의 모든 분류기에 대한 경험적 오차율이 실제 오차율로 <em>동시에</em> 수렴한다는 것입니다. 다시 말해, 우리는 (어떤 작은 $\delta$에 대해) 적어도 $1-\delta$의 확률로 클래스 $\mathcal{F}$ 내의 어떤 분류기의 오차율 $\epsilon(f)$도 어떤 작은 양 $\alpha$보다 더 크게 잘못 추정되지 않을 것이라고 말할 수 있게 해주는 이론적 원리를 찾습니다. 분명히 우리는 모든 모델 클래스 $\mathcal{F}$에 대해 그러한 진술을 할 수 없습니다. 항상 경험적 오차 0을 달성하지만 기저 모집단에 대해 무작위 추측보다 결코 나은 성능을 내지 못하는 암기 기계 클래스를 상기해 보십시오.</p>
<p>어떤 의미에서 암기자 클래스는 너무 유연합니다. 그러한 균등 수렴 결과가 유지될 수 없습니다. 반면에 고정된 분류기는 쓸모없습니다. 완벽하게 일반화되지만 훈련 데이터에도 테스트 데이터에도 맞지 않기 때문입니다. 따라서 학습의 중심 질문은 역사적으로 훈련 데이터에 더 잘 맞지만 과대적합의 위험이 있는 더 유연한(높은 분산) 모델 클래스와, 잘 일반화되지만 과소적합의 위험이 있는 더 경직된(높은 편향) 모델 클래스 사이의 절충으로 틀이 잡혀 왔습니다. 학습 이론의 핵심 질문은 모델이 이 스펙트럼의 어디에 위치하는지 정량화하고 관련 보장을 제공하기 위한 적절한 수학적 분석을 개발하는 것이었습니다.</p>
<p>일련의 중대한 논문들에서 Vapnik과 Chervonenkis는 상대 빈도의 수렴에 관한 이론을 더 일반적인 함수 클래스로 확장했습니다 :cite:<code>VapChe64,VapChe68,VapChe71,VapChe74b,VapChe81,VapChe91</code>. 이 연구 라인의 주요 기여 중 하나는 모델 클래스의 복잡성(유연성)을 측정하는 (하나의 개념인) Vapnik--Chervonenkis (VC) 차원입니다. 더욱이 그들의 주요 결과 중 하나는 경험적 오차와 모집단 오차 사이의 차이를 VC 차원과 샘플 수의 함수로 제한합니다:</p>
<p>$$P\left(R[p, f] - R_\textrm{emp}[\mathbf{X}, \mathbf{Y}, f] &lt; \alpha\right) \geq 1-\delta
\ 	extrm{ 여기서 }\ \alpha \geq c \sqrt{(\textrm{VC} - \log \delta)/n}.$$</p>
<p>여기서 $\delta &gt; 0$은 경계가 위반될 확률이고, $\alpha$는 일반화 갭의 상한이며, $n$은 데이터셋 크기입니다. 마지막으로 $c &gt; 0$은 발생할 수 있는 손실의 규모에만 의존하는 상수입니다. 이 경계의 한 가지 용도는 원하는 $\delta$와 $\alpha$ 값을 대입하여 수집할 샘플 수를 결정하는 것일 수 있습니다. VC 차원은 임의의 (이진) 레이블링을 할당할 수 있고 각 레이블링에 대해 해당 레이블링과 일치하는 클래스 내의 어떤 모델 $f$를 찾을 수 있는 데이터 포인트의 최대 수를 정량화합니다. 예를 들어, $d$차원 입력에 대한 선형 모델은 VC 차원이 $d+1$입니다. 직선이 2차원에서 세 점에 대해서는 가능한 모든 레이블링을 할당할 수 있지만, 네 점에 대해서는 그렇지 못함을 쉽게 알 수 있습니다. 불행히도 이 이론은 더 복잡한 모델에 대해 지나치게 비관적인 경향이 있으며, 이 보장을 얻으려면 일반적으로 원하는 오차율을 달성하는 데 실제로 필요한 것보다 훨씬 더 많은 예제가 필요합니다. 또한 모델 클래스와 $\delta$를 고정하면, 우리 오차율은 다시 통상적인 $\mathcal{O}(1/\sqrt{n})$ 비율로 감소한다는 점에 유의하십시오. $n$ 측면에서 이보다 더 잘하기는 어려워 보입니다. 하지만 모델 클래스를 변경함에 따라 VC 차원은 일반화 갭에 대해 비관적인 그림을 제시할 수 있습니다.</p>
<h2 id="요약-summary-11"><a class="header" href="#요약-summary-11">요약 (Summary)</a></h2>
<p>모델을 평가하는 가장 직접적인 방법은 이전에 본 적 없는 데이터로 구성된 테스트 세트를 참조하는 것입니다. 테스트 세트 평가는 실제 오차의 불편 추정치를 제공하며 테스트 세트가 커짐에 따라 원하는 $\mathcal{O}(1/\sqrt{n})$ 비율로 수렴합니다. 우리는 정확한 점진적 분포를 기반으로 대략적인 신뢰 구간을 제공하거나, (더 보수적인) 유한 샘플 보장을 기반으로 유효한 유한 샘플 신뢰 구간을 제공할 수 있습니다. 실제로 테스트 세트 평가는 현대 머신러닝 연구의 토대입니다. 하지만 테스트 세트가 (여러 연구자에 의해 반복해서 사용되는) 진정한 테스트 세트인 경우는 드뭅니다. 동일한 테스트 세트가 여러 모델을 평가하는 데 사용되고 나면, 거짓 발견을 제어하기 어려울 수 있습니다. 이는 이론적으로 큰 문제를 일으킬 수 있습니다. 실제로는 문제의 심각성이 해당 홀드아웃 세트의 크기와 그것이 단순히 하이퍼파라미터를 선택하는 데 사용되는지 아니면 정보를 더 직접적으로 유출하고 있는지에 따라 달라집니다. 그럼에도 불구하고 진짜 테스트 세트(또는 여러 개)를 관리하고 그것들이 사용되는 빈도에 대해 가능한 한 보수적인 태도를 취하는 것이 좋은 관행입니다.</p>
<p>더 만족스러운 해결책을 제공하기를 바라며, 통계적 학습 이론가들은 모델 클래스에 대한 균등 수렴을 보장하기 위한 방법들을 개발했습니다. 만약 실제로 모든 모델의 경험적 오차가 동시에 실제 오차로 수렴한다면, 우리는 훈련 오차를 최소화하여 가장 잘 수행되는 모델을 자유롭게 선택할 수 있으며, 그 모델이 홀드아웃 데이터에서도 비슷하게 잘 수행될 것임을 알 수 있습니다. 결정적으로, 그러한 결과들 중 어느 하나라도 모델 클래스의 어떤 속성에 의존해야 합니다. Vladimir Vapnik과 Alexey Chernovenkis는 VC 차원을 도입하여 VC 클래스의 모든 모델에 대해 유지되는 균등 수렴 결과를 제시했습니다. 클래스 내의 모든 모델에 대한 훈련 오차는 (동시에) 실제 오차와 가까울 것으로 보장되며, $\mathcal{O}(1/\sqrt{n})$의 비율로 더욱 가까워질 것으로 보장됩니다. VC 차원의 혁명적인 발견 이후, 수많은 대안적인 복잡도 척도들이 제안되었으며 각각은 유사한 일반화 보장을 용이하게 합니다. 함수 복잡도를 측정하는 여러 고급 방법에 대한 자세한 논의는 :citet:<code>boucheron2005theory</code>를 참조하십시오. 불행히도 이러한 복잡도 척도들은 통계 이론에서 폭넓게 유용한 도구가 되었지만, (직접적으로 적용될 때) 왜 심층 신경망이 일반화되는지 설명하는 데는 무력한 것으로 드러났습니다. 심층 신경망은 종종 수백만 개(또는 그 이상)의 파라미터를 가지며 큰 포인트 모음에 무작위 레이블을 쉽게 할당할 수 있습니다. 그럼에도 불구하고 실제 문제에서 잘 일반화되며, 놀랍게도 더 높은 VC 차원을 유발함에도 불구하고 더 크고 깊을 때 종종 더 잘 일반화됩니다. 다음 장에서는 딥러닝의 맥락에서 일반화를 다시 살펴보겠습니다.</p>
<h2 id="연습-문제-exercises-12"><a class="header" href="#연습-문제-exercises-12">연습 문제 (Exercises)</a></h2>
<ol>
<li>고정된 모델 $f$의 오차를 99.9% 이상의 확률로 0.0001 이내로 추정하고 싶다면 몇 개의 샘플이 필요할까요?</li>
<li>다른 누군가가 레이블이 지정된 테스트 세트 $\mathcal{D}$를 소유하고 있으며 레이블이 없는 입력(특성)만 제공한다고 가정해 봅시다. 이제 여러분이 (모델 클래스에 어떠한 제한도 두지 않고) 각 레이블이 없는 입력에 대해 모델 $f$를 실행하고 그에 대응하는 오차 $\epsilon_\mathcal{D}(f)$를 받음으로써만 테스트 세트 레이블에 접근할 수 있다고 가정해 봅시다. 여러분의 실제 오차와 상관없이 전체 테스트 세트를 유출하여 오차 0인 것처럼 보이게 하려면 얼마나 많은 모델을 평가해야 할까요?</li>
<li>5차 다항식 클래스의 VC 차원은 얼마입니까?</li>
<li>2차원 데이터에서 축에 평행한 직사각형(axis-aligned rectangles) 클래스의 VC 차원은 얼마입니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/6829">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="환경과-분포-이동-environment-and-distribution-shift"><a class="header" href="#환경과-분포-이동-environment-and-distribution-shift">환경과 분포 이동 (Environment and Distribution Shift)</a></h1>
<p>:label:<code>sec_environment-and-distribution-shift</code></p>
<p>이전 섹션들에서 우리는 다양한 데이터셋에 모델을 맞추며 머신러닝의 여러 실습 응용 사례를 살펴보았습니다. 하지만 데이터가 애초에 어디에서 왔는지, 혹은 모델의 출력으로 궁극적으로 무엇을 하려는지에 대해서는 한 번도 멈춰 서서 생각해보지 않았습니다. 데이터를 가진 머신러닝 개발자들은 이러한 근본적인 문제를 고려하기 위해 멈추지 않고 모델 개발에 서두르는 경우가 너무 많습니다.</p>
<p>실패한 많은 머신러닝 배포 사례가 이러한 고려의 부재로 거슬러 올라갈 수 있습니다. 때때로 모델은 테스트 세트 정확도로 측정했을 때 훌륭하게 수행되는 것처럼 보이지만, 데이터 분포가 갑자기 바뀌는 배포 상황에서는 처참하게 실패합니다. 더 교묘하게는, 모델의 배포 자체가 데이터 분포를 교란시키는 촉매제가 될 수도 있습니다. 예를 들어, 대출을 갚을 사람과 연체할 사람을 예측하도록 모델을 훈련시켰는데, 신청자의 신발 선택이 연체 위험과 관련이 있다는 것을 발견했다고 가정해 봅시다(옥스퍼드화는 상환을, 운동화는 연체를 나타냄). 이후 우리는 옥스퍼드화를 신은 모든 신청자에게 대출을 승인하고 운동화를 신은 모든 신청자에게 거절하고 싶어질 수도 있습니다.</p>
<p>이 경우, 패턴 인식에서 의사 결정으로의 신중하지 못한 도약과 환경에 대한 비판적 고려의 부재는 재앙적인 결과를 초래할 수 있습니다. 우선, 우리가 신발을 기준으로 결정을 내리기 시작하자마자 고객들은 이를 알아차리고 행동을 바꿀 것입니다. 머지않아 신용도에 어떠한 개선도 없이 모든 신청자가 옥스퍼드화를 신게 될 것입니다. 이를 이해하기 위해 잠시 시간을 내어 보십시오. 머신러닝의 많은 응용 분야에서 유사한 문제가 넘쳐나기 때문입니다: 환경에 모델 기반 결정을 도입함으로써 모델을 망가뜨릴 수 있습니다.</p>
<p>우리가 한 섹션에서 이러한 주제를 완벽하게 다룰 수는 없지만, 여기서는 일반적인 우려 사항들을 드러내고 이러한 상황을 조기에 감지하고 피해를 완화하며 머신러닝을 책임감 있게 사용하는 데 필요한 비판적 사고를 자극하는 것을 목표로 합니다. 해결책 중 일부는 간단하지만("올바른" 데이터를 요청하기), 일부는 기술적으로 어렵고(강화 학습 시스템 구현), 또 다른 일부는 통계적 예측의 영역을 벗어나 알고리즘의 윤리적 적용에 관한 어려운 철학적 질문들과 씨름해야 합니다.</p>
<h2 id="분포-이동의-유형-types-of-distribution-shift"><a class="header" href="#분포-이동의-유형-types-of-distribution-shift">분포 이동의 유형 (Types of Distribution Shift)</a></h2>
<p>먼저, 데이터 분포가 바뀔 수 있는 다양한 방식과 모델 성능을 구제하기 위해 무엇을 할 수 있는지 고려하며 수동적인 예측 설정을 고수해 봅시다. 고전적인 설정 중 하나에서, 우리는 훈련 데이터가 어떤 분포 $p_S(\mathbf{x},y)$에서 샘플링되었지만 테스트 데이터는 어떤 다른 분포 $p_T(\mathbf{x},y)$에서 추출된 레이블 없는 예제들로 구성될 것이라고 가정합니다. 이미 우리는 냉혹한 현실에 직면해야 합니다. $p_S$와 $p_T$가 서로 어떻게 관련되어 있는지에 대한 가정이 없다면 강건한 분류기를 학습하는 것은 불가능합니다.</p>
<p>개와 고양이를 구별하려는 이진 분류 문제를 생각해 보십시오. 분포가 임의의 방식으로 바뀔 수 있다면, 입력에 대한 분포는 일정하지만($p_S(\mathbf{x}) = p_T(\mathbf{x})$) 레이블이 모두 뒤집히는($p_S(y \mid \mathbf{x}) = 1 - p_T(y \mid \mathbf{x})$) 병적인 경우를 허용하게 됩니다. 다시 말해, 신이 갑자기 미래의 모든 "고양이"는 이제 개이고 우리가 이전에 "개"라고 불렀던 것은 이제 고양이라고 결정한다면(입력 분포 $p(\mathbf{x})$의 변화 없이), 우리는 이 설정을 분포가 전혀 바뀌지 않은 설정과 구별할 수 없습니다.</p>
<p>다행히 미래에 데이터가 변할 수 있는 방식에 대한 몇 가지 제한적인 가정 하에서, 원칙에 입각한 알고리즘은 변화를 감지하고 때로는 즉석에서 적응하여 원래 분류기의 정확도를 향상시킬 수 있습니다.</p>
<h3 id="공변량-이동-covariate-shift"><a class="header" href="#공변량-이동-covariate-shift">공변량 이동 (Covariate Shift)</a></h3>
<p>분포 이동의 범주 중에서 공변량 이동(covariate shift)이 가장 널리 연구되었을 것입니다. 여기서 우리는 입력의 분포는 시간이 지남에 따라 변할 수 있지만, 레이블링 함수 즉 조건부 분포 $P(y \mid \mathbf{x})$는 변하지 않는다고 가정합니다. 통계학자들은 이 문제가 공변량(특성) 분포의 변화로 인해 발생하기 때문에 이를 <em>공변량 이동</em>이라고 부릅니다. 인과 관계를 끌어들이지 않고도 분포 이동에 대해 추론할 수 있는 경우도 있지만, 공변량 이동은 $\mathbf{x}$가 $y$를 유발한다고 믿는 설정에서 호출하기에 자연스러운 가정입니다.</p>
<p>개와 고양이를 구별하는 도전을 고려해 보십시오. 우리의 훈련 데이터는 :numref:<code>fig_cat-dog-train</code>과 같은 이미지들로 구성될 수 있습니다.</p>
<p><img src="chapter_linear-classification/../img/cat-dog-train.png" alt="개와 고양이를 구별하기 위한 훈련 데이터 (그림: Lafeez Hossain / 500px / Getty Images; ilkermetinkursova / iStock / Getty Images Plus; GlobalP / iStock / Getty Images Plus; Musthafa Aboobakuru / 500px / Getty Images)." />
:label:<code>fig_cat-dog-train</code></p>
<p>테스트 시에 우리는 :numref:<code>fig_cat-dog-test</code>에 있는 이미지들을 분류하라는 요청을 받습니다.</p>
<p><img src="chapter_linear-classification/../img/cat-dog-test.png" alt="개와 고양이를 구별하기 위한 테스트 데이터 (그림: SIBAS_minich / iStock / Getty Images Plus; Ghrzuzudu / iStock / Getty Images Plus; id-work / DigitalVision Vectors / Getty Images; Yime / iStock / Getty Images Plus)." />
:label:<code>fig_cat-dog-test</code></p>
<p>훈련 세트는 사진으로 구성된 반면 테스트 세트는 만화만 포함하고 있습니다. 새로운 도메인에 어떻게 적응할 것인지에 대한 일관된 계획 없이 테스트 세트와 상당히 다른 특성을 가진 데이터셋에서 훈련하는 것은 문제를 일으킬 수 있습니다.</p>
<h3 id="레이블-이동-label-shift"><a class="header" href="#레이블-이동-label-shift">레이블 이동 (Label Shift)</a></h3>
<p>*레이블 이동(Label shift)*은 반대 문제를 설명합니다. 여기서 우리는 레이블 주변 분포 $P(y)$는 바뀔 수 있지만 클래스 조건부 분포 $P(\mathbf{x} \mid y)$는 도메인 간에 고정되어 있다고 가정합니다. 레이블 이동은 $y$가 $\mathbf{x}$를 유발한다고 믿을 때 내리기 합리적인 가정입니다. 예를 들어, 진단의 상대적 유병률이 시간이 지남에 따라 변하더라도 증상(또는 다른 징후)이 주어졌을 때 진단을 예측하고 싶을 수 있습니다. 질병이 증상을 유발하기 때문에 여기서 레이블 이동이 적절한 가정입니다. 일부 퇴행적인 경우에는 레이블 이동과 공변량 이동 가정이 동시에 성립할 수 있습니다. 예를 들어 레이블이 결정론적일 때 $y$가 $\mathbf{x}$를 유발하더라도 공변량 이동 가정이 충족될 것입니다. 흥미롭게도 이러한 경우 레이블 이동 가정에서 비롯된 방법으로 작업하는 것이 종종 유리합니다. 딥러닝에서 입력처럼 보이는 객체(종종 고차원)와 대조적으로 레이블처럼 보이는 객체(종종 저차원)를 조작하는 경향이 있기 때문입니다.</p>
<h3 id="개념-이동-concept-shift"><a class="header" href="#개념-이동-concept-shift">개념 이동 (Concept Shift)</a></h3>
<p>우리는 레이블의 정의 자체가 바뀔 수 있을 때 발생하는 관련 문제인 *개념 이동(concept shift)*에 직면할 수도 있습니다. 이것은 이상하게 들릴 수 있습니다. 고양이는 고양이 아닌가요? 하지만 다른 범주들은 시간이 지남에 따라 사용법이 변할 수 있습니다. 정신 질환에 대한 진단 기준, 유행하는 것, 직함 등은 모두 상당한 양의 개념 이동을 겪습니다. 미국 전역을 돌아다니며 지리에 따라 데이터 소스를 옮겨보면, :numref:<code>fig_popvssoda</code>에 표시된 것처럼 <em>탄산음료</em>의 이름 분포에 관한 상당한 개념 이동을 발견하게 될 것입니다.</p>
<p><img src="chapter_linear-classification/../img/popvssoda.png" alt="미국 내 탄산음료 명칭에 대한 개념 이동 (CC-BY: Alan McConchie, PopVsSoda.com)." />
:width:<code>400px</code>
:label:<code>fig_popvssoda</code></p>
<p>만약 우리가 기계 번역 시스템을 구축한다면 분포 $P(y \mid \mathbf{x})$는 우리 위치에 따라 달라질 수 있습니다. 이 문제는 발견하기 까다로울 수 있습니다. 변화가 시간적 또는 지리적 의미에서 점진적으로만 일어난다는 지식을 활용할 수 있기를 바랄 수 있습니다.</p>
<h2 id="분포-이동의-예시-examples-of-distribution-shift"><a class="header" href="#분포-이동의-예시-examples-of-distribution-shift">분포 이동의 예시 (Examples of Distribution Shift)</a></h2>
<p>형식주의와 알고리즘을 탐구하기 전에, 공변량이나 개념 이동이 명확하지 않을 수 있는 몇 가지 구체적인 상황을 논의해 보겠습니다.</p>
<h3 id="의료-진단-medical-diagnostics"><a class="header" href="#의료-진단-medical-diagnostics">의료 진단 (Medical Diagnostics)</a></h3>
<p>암을 탐지하는 알고리즘을 설계하고 싶다고 상상해 보십시오. 건강한 사람과 아픈 사람으로부터 데이터를 수집하고 알고리즘을 훈련합니다. 잘 작동하여 높은 정확도를 제공하며 여러분은 의료 진단 분야에서 성공적인 경력을 쌓을 준비가 되었다고 결론 내립니다. <em>잠시만요.</em></p>
<p>훈련 데이터를 생성한 분포와 실제 현장에서 마주하게 될 분포는 상당히 다를 수 있습니다. 수년 전 우리 저자들 중 일부와 함께 일했던 불운한 스타트업에 이런 일이 일어났습니다. 그들은 주로 노인 남성에게 영향을 미치는 질병에 대한 혈액 검사를 개발하고 있었고 환자들로부터 수집한 혈액 샘플을 사용하여 이를 연구하기를 원했습니다. 하지만 이미 시스템에 있는 아픈 환자들보다 건강한 남성들로부터 혈액 샘플을 얻는 것이 훨씬 더 어렵습니다. 이를 보완하기 위해 스타트업은 대학 캠퍼스의 학생들로부터 혈액 기증을 받아 검사 개발의 건강한 대조군으로 사용했습니다. 그러고 나서 우리에게 질병을 탐지하기 위한 분류기를 구축하는 데 도움을 줄 수 있는지 물었습니다.</p>
<p>우리가 그들에게 설명했듯이, 건강한 집단과 아픈 집단을 거의 완벽한 정확도로 구별하는 것은 실제로 쉬운 일일 것입니다. 하지만 그것은 실험 대상자들이 나이, 호르몬 수치, 신체 활동, 식단, 알코올 섭취량, 그리고 질병과 무관한 더 많은 요인에서 차이가 났기 때문입니다. 실제 환자들의 경우에는 그렇지 않을 가능성이 높았습니다. 그들의 샘플링 절차 때문에 우리는 극단적인 공변량 이동을 예상할 수 있었습니다. 게다가 이 사례는 일반적인 방법으로는 교정될 가능성이 낮았습니다. 요컨대 그들은 상당한 액수의 돈을 낭비했습니다.</p>
<h3 id="자율-주행차-self-driving-cars"><a class="header" href="#자율-주행차-self-driving-cars">자율 주행차 (Self-Driving Cars)</a></h3>
<p>어떤 회사가 자율 주행차 개발을 위해 머신러닝을 활용하고 싶어 한다고 합시다. 여기서 핵심 구성 요소 중 하나는 도로변 감지기입니다. 실제 주석이 달린 데이터는 얻기 비싸기 때문에, 그들은 게임 렌더링 엔진의 합성 데이터를 추가 훈련 데이터로 사용하겠다는 (똑똑하지만 의심스러운) 아이디어를 냈습니다. 이는 렌더링 엔진에서 추출한 "테스트 데이터"에서는 정말 잘 작동했습니다. 아뿔싸, 실제 차 안에서는 재앙이었습니다. 알고 보니 도로변이 매우 단순한 텍스처로 렌더링되어 있었습니다. 더 중요한 것은 <em>모든</em> 도로변이 <em>동일한</em> 텍스처로 렌더링되어 있었고 도로변 감지기는 이 "특성"을 매우 빠르게 학습해버린 것이었습니다.</p>
<p>미군이 숲속의 탱크를 처음 탐지하려고 했을 때도 비슷한 일이 일어났습니다. 그들은 탱크가 없는 숲의 항공 사진을 찍은 다음, 탱크를 숲으로 운전해 들여보내고 또 다른 사진 세트를 찍었습니다. 분류기는 <em>완벽하게</em> 작동하는 것처럼 보였습니다. 불행히도 그것은 단순히 그림자가 있는 나무와 그림자가 없는 나무를 구별하는 법을 배운 것뿐이었습니다. 첫 번째 사진 세트는 이른 아침에 찍혔고 두 번째 세트는 정오에 찍혔기 때문입니다.</p>
<h3 id="비정상-분포-nonstationary-distributions"><a class="header" href="#비정상-분포-nonstationary-distributions">비정상 분포 (Nonstationary Distributions)</a></h3>
<p>분포가 천천히 변하고(비정상 분포, nonstationary distribution라고도 함) 모델이 적절하게 업데이트되지 않을 때 훨씬 더 미묘한 상황이 발생합니다. 다음은 몇 가지 전형적인 사례입니다.</p>
<ul>
<li>전산 광고 모델을 훈련시킨 후 자주 업데이트하지 않는 경우(예: iPad라는 생소한 새 기기가 막 출시되었다는 사실을 반영하는 것을 잊음).</li>
<li>스팸 필터를 구축합니다. 지금까지 본 모든 스팸을 탐지하는 데는 잘 작동합니다. 하지만 스팸 메일 발송자들이 영리해져서 우리가 이전에 본 적 없는 새로운 메시지를 만들어냅니다.</li>
<li>제품 추천 시스템을 구축합니다. 겨울 내내 잘 작동하다가 크리스마스가 한참 지난 후에도 계속 산타 모자를 추천합니다.</li>
</ul>
<h3 id="더-많은-일화들"><a class="header" href="#더-많은-일화들">더 많은 일화들</a></h3>
<ul>
<li>얼굴 감지기를 구축합니다. 모든 벤치마크에서 잘 작동합니다. 불행히도 테스트 데이터에서는 실패합니다. 문제가 되는 예제는 얼굴이 전체 이미지를 채우는 근접 촬영 사진들입니다(훈련 세트에는 그런 데이터가 없었습니다).</li>
<li>미국 시장용 웹 검색 엔진을 구축하고 이를 영국에 배포하려고 합니다.</li>
<li>대규모 데이터셋을 컴파일하여 이미지 분류기를 훈련합니다. 여기서 방대한 클래스 집합 중 각각은 데이터셋에서 동일하게 대표됩니다. 예를 들어 1000개의 카테고리가 각각 1000개의 이미지로 표현됩니다. 그러고 나서 이 시스템을 실제 세계에 배포하는데, 실제 사진의 레이블 분포는 분명히 불균등합니다.</li>
</ul>
<h2 id="분포-이동의-교정-correction-of-distribution-shift"><a class="header" href="#분포-이동의-교정-correction-of-distribution-shift">분포 이동의 교정 (Correction of Distribution Shift)</a></h2>
<p>우리가 논의했듯이 훈련 및 테스트 분포 $P(\mathbf{x}, y)$가 다른 경우가 많습니다. 어떤 경우에는 운이 좋아 공변량, 레이블 또는 개념 이동에도 불구하고 모델이 작동합니다. 다른 경우에는 이동에 대처하기 위한 원칙적인 전략을 채택함으로써 더 잘할 수 있습니다. 이 섹션의 나머지는 상당히 더 기술적으로 변합니다. 성급한 독자는 이 내용이 이후 개념의 전제 조건이 아니므로 다음 섹션으로 넘어가도 좋습니다.</p>
<h3 id="경험적-위험과-위험-empirical-risk-and-risk"><a class="header" href="#경험적-위험과-위험-empirical-risk-and-risk">경험적 위험과 위험 (Empirical Risk and Risk)</a></h3>
<p>:label:<code>subsec_empirical-risk-and-risk</code></p>
<p>먼저 모델 훈련 중에 정확히 무슨 일이 일어나고 있는지 되새겨 봅시다: 우리는 훈련 데이터 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$의 특성과 관련 레이블을 반복하며 매 미니배치 후에 모델 $f$의 파라미터를 업데이트합니다. 단순함을 위해 정규화는 고려하지 않으므로 주로 훈련에서의 손실을 최소화합니다:</p>
<p>$$\mathop{\mathrm{minimize}}<em>f \frac{1}{n} \sum</em>{i=1}^n l(f(\mathbf{x}_i), y_i),$$
:eqlabel:<code>eq_empirical-risk-min</code></p>
<p>여기서 $l$은 관련 레이블 $y_i$가 주어졌을 때 예측 $f(\mathbf{x}_i)$가 "얼마나 나쁜지" 측정하는 손실 함수입니다. 통계학자들은 :eqref:<code>eq_empirical-risk-min</code>의 항을 *경험적 위험(empirical risk)*이라고 부릅니다. <em>경험적 위험</em>은 실제 분포 $p(\mathbf{x},y)$에서 추출된 전체 데이터 모집단에 대한 손실의 기댓값인 *위험(risk)*을 근사하기 위한 훈련 데이터에 대한 평균 손실입니다:</p>
<p>$$E_{p(\mathbf{x}, y)} [l(f(\mathbf{x}), y)] = \int\int l(f(\mathbf{x}), y) p(\mathbf{x}, y) ;d\mathbf{x}dy.$$
:eqlabel:<code>eq_true-risk</code></p>
<p>하지만 실제로는 일반적으로 전체 데이터 모집단을 얻을 수 없습니다. 따라서 :eqref:<code>eq_empirical-risk-min</code>에서 경험적 위험을 최소화하는 *경험적 위험 최소화(empirical risk minimization)*는 위험을 대략적으로 최소화하기를 바라는 머신러닝의 실질적인 전략입니다.</p>
<h3 id="공변량-이동-교정-covariate-shift-correction"><a class="header" href="#공변량-이동-교정-covariate-shift-correction">공변량 이동 교정 (Covariate Shift Correction)</a></h3>
<p>:label:<code>subsec_covariate-shift-correction</code></p>
<p>레이블이 지정된 데이터 $(\mathbf{x}_i, y_i)$가 있는 어떤 종속성 $P(y \mid \mathbf{x})$를 추정하고 싶다고 가정해 봅시다. 불행히도 관찰값 $\mathbf{x}_i$는 <em>타겟 분포</em> $p(\mathbf{x})$가 아닌 어떤 <em>소스 분포</em> $q(\mathbf{x})$에서 추출되었습니다. 다행히 종속성 가정은 조건부 분포가 변하지 않음을 의미합니다: $p(y \mid \mathbf{x}) = q(y \mid \mathbf{x})$. 소스 분포 $q(\mathbf{x})$가 "틀렸다면", 위험에서 다음과 같은 간단한 항등식을 사용하여 이를 교정할 수 있습니다:</p>
<p>$$
\begin{aligned}
\int\int l(f(\mathbf{x}), y) p(y \mid \mathbf{x})p(\mathbf{x}) ;d\mathbf{x}dy =
\int\int l(f(\mathbf{x}), y) q(y \mid \mathbf{x})q(\mathbf{x})\frac{p(\mathbf{x})}{q(\mathbf{x})} ;d\mathbf{x}dy.
\end{aligned}
$$</p>
<p>즉, 우리는 각 데이터 예제에 대해 올바른 분포에서 추출되었을 확률과 잘못된 분포에서 추출되었을 확률의 비율로 가중치를 다시 부여해야 합니다:</p>
<p>$$\beta_i \stackrel{\textrm{def}}{=} \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}.$$</p>
<p>각 데이터 예제 $(\mathbf{x}_i, y_i)$에 대해 가중치 $\beta_i$를 대입하면, *가중 경험적 위험 최소화(weighted empirical risk minimization)*를 사용하여 모델을 훈련할 수 있습니다:</p>
<p>$$\mathop{\mathrm{minimize}}<em>f \frac{1}{n} \sum</em>{i=1}^n \beta_i l(f(\mathbf{x}_i), y_i).$$
:eqlabel:<code>eq_weighted-empirical-risk-min</code></p>
<p>아쉽게도 우리는 그 비율을 모르기 때문에 유용한 일을 하기 전에 이를 추정해야 합니다. 최소 노름(minimum-norm)이나 최대 엔트로피 원리를 사용하여 기댓값 연산자를 직접 재조정하려는 몇 가지 화려한 연산자 이론적 접근 방식을 포함하여 많은 방법을 사용할 수 있습니다. 이러한 접근 방식의 경우, 두 분포 모두에서 추출된 샘플이 필요합니다. 즉, 테스트 데이터에 대한 접근을 통한 "진짜" $p$와 훈련 세트를 생성하는 데 사용된 $q$ (후자는 당연히 사용 가능함)입니다. 하지만 우리는 특성 $\mathbf{x} \sim p(\mathbf{x})$만 필요하며 레이블 $y \sim p(y)$에 접근할 필요는 없다는 점에 유의하십시오.</p>
<p>이 경우 원본만큼 좋은 결과를 제공하는 매우 효과적인 접근 방식이 존재합니다. 바로 이진 분류를 위한 소프트맥스 회귀(:numref:<code>sec_softmax</code> 참조)의 특수한 경우인 로지스틱 회귀입니다. 추정 확률 비율을 계산하는 데 필요한 것은 이것뿐입니다. 우리는 $p(\mathbf{x})$에서 추출된 데이터와 $q(\mathbf{x})$에서 추출된 데이터를 구별하는 분류기를 학습합니다. 두 분포를 구별하는 것이 불가능하다면 관련 인스턴스가 두 분포 중 어느 쪽에서 왔을 가능성도 동일하다는 것을 의미합니다. 반면에 잘 구별될 수 있는 인스턴스는 그에 따라 상당히 과중하게 가중되거나 가볍게 가중되어야 합니다.</p>
<p>단순함을 위해 각각 $p(\mathbf{x})$와 $q(\mathbf{x})$ 분포에서 동일한 수의 인스턴스를 가졌다고 가정합시다. 이제 $p$에서 추출된 데이터에 대해서는 레이블 $z$를 1로, $q$에서 추출된 데이터에 대해서는 -1로 표시합시다. 그러면 혼합 데이터셋에서의 확률은 다음과 같이 주어집니다.</p>
<p>$$P(z=1 \mid \mathbf{x}) = \frac{p(\mathbf{x})}{p(\mathbf{x})+q(\mathbf{x})} \textrm{ 이고 따라서 } \frac{P(z=1 \mid \mathbf{x})}{P(z=-1 \mid \mathbf{x})} = \frac{p(\mathbf{x})}{q(\mathbf{x})}.$$</p>
<p>따라서 $P(z=1 \mid \mathbf{x})=\frac{1}{1+\exp(-h(\mathbf{x}))}$ ($h$는 파라미터화된 함수)인 로지스틱 회귀 접근 방식을 사용하면 다음과 같이 됩니다.</p>
<p>$$
\beta_i = \frac{1/(1 + \exp(-h(\mathbf{x}_i)))}{\exp(-h(\mathbf{x}_i))/(1 + \exp(-h(\mathbf{x}_i)))} = \exp(h(\mathbf{x}_i)).
$$</p>
<p>결과적으로 우리는 두 가지 문제를 해결해야 합니다: 첫째, 두 분포에서 추출된 데이터를 구별하는 문제, 그리고 항들에 $\beta_i$로 가중치를 부여하는 :eqref:<code>eq_weighted-empirical-risk-min</code>의 가중 경험적 위험 최소화 문제입니다.</p>
<p>이제 교정 알고리즘을 설명할 준비가 되었습니다. 훈련 세트 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$와 레이블 없는 테스트 세트 ${\mathbf{u}_1, \ldots, \mathbf{u}_m}$가 있다고 가정해 봅시다. 공변량 이동의 경우, 모든 $1 \leq i \leq n$에 대해 $\mathbf{x}_i$는 어떤 소스 분포에서 추출되었고 모든 $1 \leq i \leq m$에 대해 $\mathbf{u}_i$는 타겟 분포에서 추출되었다고 가정합니다. 공변량 이동을 교정하기 위한 전형적인 알고리즘은 다음과 같습니다:</p>
<ol>
<li>이진 분류 훈련 세트를 만듭니다: ${(\mathbf{x}_1, -1), \ldots, (\mathbf{x}_n, -1), (\mathbf{u}_1, 1), \ldots, (\mathbf{u}_m, 1)}$.</li>
<li>로지스틱 회귀를 사용하여 이진 분류기를 훈련하여 함수 $h$를 얻습니다.</li>
<li>$\beta_i = \exp(h(\mathbf{x}_i))$ 또는 어떤 상수 $c$에 대해 더 나은 $\beta_i = \min(\exp(h(\mathbf{x}_i)), c)$를 사용하여 훈련 데이터에 가중치를 부여합니다.</li>
<li>:eqref:<code>eq_weighted-empirical-risk-min</code>에서 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$에 대해 훈련하기 위해 가중치 $\beta_i$를 사용합니다.</li>
</ol>
<p>위 알고리즘은 한 가지 결정적인 가정에 의존한다는 점에 유의하십시오. 이 체계가 작동하려면 타겟(예: 테스트 시) 분포의 각 데이터 예제가 훈련 시에 발생할 확률이 0이 아니어야 합니다. $p(\mathbf{x}) &gt; 0$이지만 $q(\mathbf{x}) = 0$인 지점을 발견하면 해당 중요도 가중치는 무한대가 되어야 하기 때문입니다.</p>
<h3 id="레이블-이동-교정-label-shift-correction"><a class="header" href="#레이블-이동-교정-label-shift-correction">레이블 이동 교정 (Label Shift Correction)</a></h3>
<p>우리가 $k$개의 범주를 가진 분류 작업을 다루고 있다고 가정해 봅시다. :numref:<code>subsec_covariate-shift-correction</code>과 동일한 표기법을 사용하여, $q$와 $p$는 각각 소스 분포(예: 훈련 시)와 타겟 분포(예: 테스트 시)입니다. 레이블의 분포가 시간이 지남에 따라 바뀐다고 가정해 봅시다: $q(y) \neq p(y)$, 하지만 클래스 조건부 분포는 동일하게 유지됩니다: $q(\mathbf{x} \mid y)=p(\mathbf{x} \mid y)$. 소스 분포 $q(y)$가 "틀렸다면", :eqref:<code>eq_true-risk</code>에 정의된 위험의 다음과 같은 항등식에 따라 이를 교정할 수 있습니다:</p>
<p>$$
\begin{aligned}
\int\int l(f(\mathbf{x}), y) p(\mathbf{x} \mid y)p(y) ;d\mathbf{x}dy =
\int\int l(f(\mathbf{x}), y) q(\mathbf{x} \mid y)q(y)\frac{p(y)}{q(y)} ;d\mathbf{x}dy.
\end{aligned}
$$</p>
<p>여기서 우리의 중요도 가중치는 레이블 우도 비율에 해당할 것입니다:</p>
<p>$$\beta_i \stackrel{\textrm{def}}{=} \frac{p(y_i)}{q(y_i)}.$$</p>
<p>레이블 이동의 좋은 점 중 하나는 소스 분포에 대해 상당히 좋은 모델을 가지고 있다면, 주변 차원을 다루지 않고도 이러한 가중치의 일관된 추정치를 얻을 수 있다는 것입니다. 딥러닝에서 입력은 이미지와 같은 고차원 객체인 경향이 있는 반면, 레이블은 범주와 같은 더 단순한 객체인 경우가 많기 때문입니다.</p>
<p>타겟 레이블 분포를 추정하기 위해, 먼저 (일반적으로 훈련 데이터로 훈련된) 합리적으로 우수한 기성 분류기를 가져와 검증 세트(훈련 분포에서 가져옴)를 사용하여 그 "혼동(confusion)" 행렬을 계산합니다. <em>혼동 행렬</em> $\mathbf{C}$는 단순히 $k \times k$ 행렬이며, 각 열은 레이블 범주(실제 값)에 대응하고 각 행은 우리 모델의 예측 범주에 대응합니다. 각 셀의 값 $c_{ij}$는 검증 세트에서 실제 레이블이 $j$이고 우리 모델이 $i$라고 예측한 전체 예측의 비율입니다.</p>
<p>이제 복잡한 실시간 주석 파이프라인에 투자하지 않는 한 실제 현장에서 마주치는 예제들에 대한 레이블을 볼 수 없으므로 타겟 데이터에서 혼동 행렬을 직접 계산할 수는 없습니다. 하지만 우리가 할 수 있는 일은 테스트 시의 모든 모델 예측을 평균하여 평균 모델 출력 $\mu(\hat{\mathbf{y}}) \in \mathbb{R}^k$를 얻는 것입니다. 여기서 $i$번째 원소 $\mu(\hat{y}_i)$는 우리 모델이 $i$라고 예측한 테스트 세트의 전체 예측 비율입니다.</p>
<p>몇 가지 완만한 조건 하에서(우리 분류기가 애초에 상당히 정확했고, 타겟 데이터가 우리가 이전에 본 범주들만 포함하고 있으며, 레이블 이동 가정이 애초에 성립한다면 - 여기서 가장 강력한 가정임), 우리는 간단한 선형 시스템을 풀어 테스트 세트 레이블 분포를 추정할 수 있음이 밝혀졌습니다.</p>
<p>$$\mathbf{C} p(\mathbf{y}) = \mu(\hat{\mathbf{y}}),$$</p>
<p>추정치로서 모든 $1 \leq i \leq k$에 대해 $\sum_{j=1}^k c_{ij} p(y_j) = \mu(\hat{y}_i)$가 성립하기 때문입니다. 여기서 $p(y_j)$는 $k$차원 레이블 분포 벡터 $p(\mathbf{y})$의 $j$번째 원소입니다. 우리 분류기가 처음부터 충분히 정확하다면 혼동 행렬 $\mathbf{C}$는 가역적일 것이며, $p(\mathbf{y}) = \mathbf{C}^{-1} \mu(\hat{\mathbf{y}})$라는 해를 얻게 됩니다.</p>
<p>소스 데이터에서 레이블을 관찰하므로 분포 $q(y)$를 추정하기 쉽습니다. 그런 다음 레이블 $y_i$를 가진 임의의 훈련 예제 $i$에 대해, 추정된 $p(y_i)/q(y_i)$의 비율을 취하여 가중치 $\beta_i$를 계산하고, 이를 :eqref:<code>eq_weighted-empirical-risk-min</code>의 가중 경험적 위험 최소화에 대입할 수 있습니다.</p>
<h3 id="개념-이동-교정-concept-shift-correction"><a class="header" href="#개념-이동-교정-concept-shift-correction">개념 이동 교정 (Concept Shift Correction)</a></h3>
<p>개념 이동은 원칙적인 방식으로 수정하기가 훨씬 더 어렵습니다. 예를 들어 문제가 갑자기 고양이와 개를 구별하는 것에서 흰색과 검은색 동물을 구별하는 것으로 바뀌는 상황에서, 단순히 새로운 레이블을 수집하고 처음부터 다시 훈련하는 것보다 훨씬 더 잘할 수 있다고 가정하는 것은 비합리적일 것입니다. 다행히 실제로는 그러한 극단적인 이동은 드뭅니다. 대신 보통 일어나는 일은 작업이 천천히 계속해서 바뀌는 것입니다. 상황을 더 구체적으로 만들기 위해 몇 가지 예시를 들어보겠습니다:</p>
<ul>
<li>전산 광고에서 새로운 제품이 출시되고 오래된 제품은 인기가 없어집니다. 이는 광고와 그 인기에 대한 분포가 점진적으로 변한다는 것을 의미하며, 어떠한 클릭률 예측기도 그에 따라 점진적으로 변해야 합니다.</li>
<li>교통 카메라 렌즈가 환경적 마모로 인해 점진적으로 노후화되어 이미지 품질에 점진적으로 영향을 미칩니다.</li>
<li>뉴스 콘텐츠가 점진적으로 바뀝니다(즉, 대부분의 뉴스는 그대로 유지되지만 새로운 이야기가 나타납니다).</li>
</ul>
<p>이러한 경우, 네트워크를 훈련하는 데 사용했던 것과 동일한 접근 방식을 사용하여 데이터의 변화에 적응하게 할 수 있습니다. 즉, 처음부터 훈련하는 대신 기존 네트워크 가중치를 사용하고 새로운 데이터로 몇 가지 업데이트 단계를 수행하면 됩니다.</p>
<h2 id="학습-문제의-분류-a-taxonomy-of-learning-problems"><a class="header" href="#학습-문제의-분류-a-taxonomy-of-learning-problems">학습 문제의 분류 (A Taxonomy of Learning Problems)</a></h2>
<p>분포의 변화를 다루는 방법에 대한 지식을 갖추었으니, 이제 머신러닝 문제 공식화의 다른 측면들을 고려해 볼 수 있습니다.</p>
<h3 id="배치-학습-batch-learning"><a class="header" href="#배치-학습-batch-learning">배치 학습 (Batch Learning)</a></h3>
<p>*배치 학습(batch learning)*에서는 동일한 분포에서 추출된 새로운 데이터 $(\mathbf{x}, y)$의 점수를 매기기 위해 모델 $f(\mathbf{x})$를 훈련하는 데 사용하는 훈련 특성과 레이블 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$에 접근할 수 있습니다. 이것은 우리가 여기서 논의하는 모든 문제에 대한 기본 가정입니다. 예를 들어, 많은 고양이와 개 사진을 기반으로 고양이 감지기를 훈련할 수 있습니다. 일단 훈련을 마치면, 고양이만 들여보내는 스마트 캣도어 컴퓨터 비전 시스템의 일부로 이를 출시합니다. 이는 고객의 집에 설치된 후 (극단적인 상황이 없는 한) 다시는 업데이트되지 않습니다.</p>
<h3 id="온라인-학습-online-learning"><a class="header" href="#온라인-학습-online-learning">온라인 학습 (Online Learning)</a></h3>
<p>이제 데이터 $(\mathbf{x}_i, y_i)$가 한 번에 한 샘플씩 도착한다고 상상해 보십시오. 더 구체적으로, 먼저 $\mathbf{x}_i$를 관찰한 다음 추정치 $f(\mathbf{x}_i)$를 내놓아야 한다고 가정합시다. 이를 수행한 후에야 $y_i$를 관찰하고 우리의 결정에 따라 보상을 받거나 손실을 입게 됩니다. 많은 실제 문제들이 이 범주에 속합니다. 예를 들어, 내일의 주가를 예측해야 하며, 그 추정치에 기반하여 거래를 할 수 있고 하루가 끝날 때 우리의 추정치가 이익을 냈는지 알게 됩니다. 즉, *온라인 학습(online learning)*에서는 새로운 관찰이 주어질 때마다 지속적으로 모델을 개선하는 다음과 같은 주기를 갖습니다:</p>
<p>$$\begin{aligned}&amp;\textrm{모델 } f_t \longrightarrow \textrm{데이터 }  \mathbf{x}_t \longrightarrow \textrm{추정치 } f_t(\mathbf{x}_t) \longrightarrow\ \textrm{관}&amp;\textrm{찰 } y_t \longrightarrow \textrm{손실 } l(y_t, f_t(\mathbf{x}<em>t)) \longrightarrow \textrm{모델 } f</em>{t+1}\end{aligned}$$</p>
<h3 id="밴딧-bandits"><a class="header" href="#밴딧-bandits">밴딧 (Bandits)</a></h3>
<p>*밴딧(Bandits)*은 위 문제의 특수한 경우입니다. 대부분의 학습 문제에서는 파라미터를 학습하고자 하는 연속적으로 파라미터화된 함수 $f$(예: 심층 네트워크)를 가지고 있지만, <em>밴딧</em> 문제에서는 우리가 당길 수 있는 팔의 수가 유한합니다. 즉, 우리가 취할 수 있는 행동의 수가 유한합니다. 이 더 단순한 문제에 대해 최적성 측면에서 더 강력한 이론적 보장을 얻을 수 있다는 사실은 그리 놀랍지 않습니다. 이 문제가 종종 (혼란스럽게도) 별개의 학습 설정인 것처럼 다루어지기 때문에 주로 나열했습니다.</p>
<h3 id="제어-control"><a class="header" href="#제어-control">제어 (Control)</a></h3>
<p>많은 경우 환경은 우리가 한 일을 기억합니다. 반드시 적대적인 방식은 아니지만 단순히 기억할 것이고 응답은 이전에 일어난 일에 달려 있을 것입니다. 예를 들어 커피 보일러 컨트롤러는 이전에 보일러를 가열했는지 여부에 따라 다른 온도를 관찰할 것입니다. PID(비례-적분-미분) 제어 알고리즘이 거기서 인기 있는 선택입니다. 마찬가지로 뉴스 사이트에서의 사용자 행동은 우리가 이전에 그들에게 무엇을 보여주었는지에 달려 있을 것입니다(예: 그들은 대부분의 뉴스를 한 번만 읽을 것입니다). 많은 그러한 알고리즘은 자신의 결정이 덜 무작위적으로 보이게 하기 위해 자신이 행동하는 환경의 모델을 형성합니다. 최근에는 더 나은 얽힘 해제(disentangling) 및 재구성 품질을 달성하고, 생성된 텍스트의 다양성과 생성된 이미지의 재구성 품질을 개선하기 위해 하이퍼파라미터를 자동으로 튜닝하는 데 제어 이론(예: PID 변형)이 사용되기도 했습니다 :cite:<code>Shao.Yao.Sun.ea.2020</code>.</p>
<h3 id="강화-학습-reinforcement-learning"><a class="header" href="#강화-학습-reinforcement-learning">강화 학습 (Reinforcement Learning)</a></h3>
<p>기억이 있는 환경의 더 일반적인 경우로, 환경이 우리와 협력하려고 노력하는 상황(협력 게임, 특히 비제로섬 게임의 경우)이나 환경이 이기려고 노력하는 상황을 만날 수 있습니다. 체스, 바둑, 백개먼 또는 스타크래프트는 *강화 학습(reinforcement learning)*의 일부 사례들입니다. 마찬가지로 자율 주행차를 위한 훌륭한 컨트롤러를 구축하고 싶을 수도 있습니다. 다른 차들은 자율 주행차의 주행 스타일에 대해 피하려 하거나, 사고를 유발하려 하거나, 협력하려 하는 등 비자명한 방식으로 대응할 가능성이 높습니다.</p>
<h3 id="환경-고려하기-considering-the-environment"><a class="header" href="#환경-고려하기-considering-the-environment">환경 고려하기 (Considering the Environment)</a></h3>
<p>위의 서로 다른 상황들 사이의 한 가지 핵심적인 구분은, 정지된 환경의 경우 전반적으로 작동했을 전략이 적응할 수 있는 환경에서는 전반적으로 작동하지 않을 수도 있다는 것입니다. 예를 들어 거래자가 발견한 차익 거래 기회는 그것이 활용되자마자 사라질 가능성이 높습니다. 환경이 변하는 속도와 방식은 우리가 동원할 수 있는 알고리즘의 유형을 크게 결정합니다. 예를 들어 상황이 천천히만 변할 수 있다는 것을 안다면, 어떠한 추정치도 천천히만 변하도록 강제할 수 있습니다. 환경이 즉각적으로 변할 수 있지만 아주 드물게만 변한다는 것을 안다면, 그에 대한 여유를 둘 수 있습니다. 이러한 종류의 지식은 개념 이동을 다루는 야심 찬 데이터 과학자에게 결정적입니다. 즉, 해결하려는 문제가 시간이 지남에 따라 변할 수 있을 때입니다.</p>
<h2 id="머신러닝에서의-공정성-책임성-투명성-fairness-accountability-and-transparency-in-machine-learning"><a class="header" href="#머신러닝에서의-공정성-책임성-투명성-fairness-accountability-and-transparency-in-machine-learning">머신러닝에서의 공정성, 책임성, 투명성 (Fairness, Accountability, and Transparency in Machine Learning)</a></h2>
<p>마지막으로, 머신러닝 시스템을 배포할 때 여러분은 단순히 예측 모델을 최적화하는 것이 아니라는 점을 기억하는 것이 중요합니다. 여러분은 일반적으로 결정을 (부분적으로 또는 완전히) 자동화하는 데 사용될 도구를 제공하는 것입니다. 이러한 기술 시스템은 결과적인 결정의 대상이 되는 개인의 삶에 영향을 미칠 수 있습니다. 예측을 고려하는 것에서 결정을 내리는 것으로의 도약은 새로운 기술적 질문뿐만 아니라 신중하게 고려해야 할 일련의 윤리적 질문을 제기합니다. 의료 진단 시스템을 배포하는 경우, 어떤 집단에 대해서는 작동하고 어떤 집단에 대해서는 작동하지 않을 수 있는지 알아야 합니다. 하위 집단의 복지에 대한 예측 가능한 위험을 간과하는 것은 우리가 열등한 치료를 제공하게 만들 수 있습니다. 더욱이 일단 의사 결정 시스템을 고려하게 되면, 우리는 한 걸음 물러나 우리 기술을 어떻게 평가하는지 재고해야 합니다. 이러한 범위 변화의 다른 결과들 중에서, 우리는 <em>정확도</em>가 적절한 척도인 경우가 드물다는 것을 발견할 것입니다. 예를 들어 예측을 행동으로 옮길 때, 우리는 종종 다양한 방식으로 오차를 범하는 것의 잠재적 비용 민감도를 고려하기를 원할 것입니다. 이미지를 잘못 분류하는 한 가지 방식이 인종적 차별로 인식될 수 있는 반면, 다른 범주로의 오분류는 무해하다면, 의사 결정 프로토콜을 설계할 때 사회적 가치를 고려하여 그에 따라 임계값을 조정하고 싶을 수 있습니다. 또한 예측 시스템이 피드백 루프로 이어지는 방식에 대해서도 주의하고 싶습니다. 예를 들어, 범죄 예측이 높은 지역에 순찰 대원을 할당하는 예측 치안 시스템을 생각해 보십시오. 걱정스러운 패턴이 어떻게 나타날 수 있는지 쉽게 알 수 있습니다:</p>
<ol>
<li>범죄가 더 많은 동네에 더 많은 순찰이 배치됩니다.</li>
<li>결과적으로 이 동네에서 더 많은 범죄가 발견되어 미래의 반복을 위한 훈련 데이터에 입력됩니다.</li>
<li>더 많은 양성 사례에 노출된 모델은 이 동네에서 더 많은 범죄를 예측합니다.</li>
<li>다음 반복에서 업데이트된 모델은 동일한 동네를 훨씬 더 집중적으로 타겟팅하여 더 많은 범죄가 발견되게 됩니다.</li>
</ol>
<p>종종 모델의 예측이 훈련 데이터와 결합되는 다양한 메커니즘은 모델링 과정에서 고려되지 않습니다. 이는 연구자들이 *폭주하는 피드백 루프(runaway feedback loops)*라고 부르는 상황으로 이어질 수 있습니다. 게다가 우리는 애초에 올바른 문제를 다루고 있는지에 대해서도 주의하고 싶습니다. 예측 알고리즘은 이제 정보의 보급을 중재하는 데 있어 막대한 역할을 합니다. 개인이 마주하는 뉴스가 그들이 <em>좋아요</em>를 누른 페이스북 페이지 세트에 의해 결정되어야 할까요? 이것들은 머신러닝 커리어를 쌓으며 마주칠 수 있는 수많은 시급한 윤리적 딜레마 중 몇 가지에 불과합니다.</p>
<h2 id="요약-summary-12"><a class="header" href="#요약-summary-12">요약 (Summary)</a></h2>
<p>많은 경우 훈련 세트와 테스트 세트는 동일한 분포에서 오지 않습니다. 이를 분포 이동이라고 합니다. 위험은 실제 분포에서 추출된 전체 데이터 모집단에 대한 손실의 기댓값입니다. 하지만 이 전체 모집단은 대개 사용할 수 없습니다. 경험적 위험은 위험을 근사하기 위해 훈련 데이터에 대한 평균 손실을 구하는 것입니다. 실제로는 경험적 위험 최소화를 수행합니다.</p>
<p>관련 가정 하에서, 공변량 이동과 레이블 이동은 테스트 시에 감지되고 교정될 수 있습니다. 이러한 편향을 고려하지 못하는 것은 테스트 시에 문제가 될 수 있습니다. 어떤 경우에는 환경이 자동화된 행동을 기억하고 놀라운 방식으로 대응할 수 있습니다. 모델을 구축할 때 이러한 가능성을 고려해야 하며, 우리 모델과 환경이 예기치 못한 방식으로 얽히게 될 가능성을 열어두고 운영 중인 시스템을 계속 모니터링해야 합니다.</p>
<h2 id="연습-문제-exercises-13"><a class="header" href="#연습-문제-exercises-13">연습 문제 (Exercises)</a></h2>
<ol>
<li>검색 엔진의 동작을 변경하면 어떤 일이 일어날 수 있을까요? 사용자는 어떻게 할까요? 광고주는 어떨까요?</li>
<li>공변량 이동 감지기를 구현하십시오. 힌트: 분류기를 구축하십시오.</li>
<li>공변량 이동 교정기를 구현하십시오.</li>
<li>분포 이동 외에 경험적 위험이 위험을 근사하는 방식에 영향을 줄 수 있는 다른 요소는 무엇일까요?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/105">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="다층-퍼셉트론-multilayer-perceptrons"><a class="header" href="#다층-퍼셉트론-multilayer-perceptrons">다층 퍼셉트론 (Multilayer Perceptrons)</a></h1>
<p>:label:<code>chap_perceptrons</code></p>
<p>이 장에서는 여러분의 첫 번째 진정한 <em>심층(deep)</em> 네트워크를 소개할 것입니다.
가장 단순한 심층 네트워크는 *다층 퍼셉트론(multilayer perceptrons)*이라고 불리며, 여러 층의 뉴런으로 구성되어 있습니다. 각 층은 아래 층(입력을 받는 층)과 위 층(영향을 주는 층)에 완전히 연결되어 있습니다.
자동 미분이 딥러닝 알고리즘의 구현을 상당히 단순화하지만, 우리는 심층 네트워크에서 이러한 기울기가 어떻게 계산되는지 깊이 파고들 것입니다.
그런 다음 심층 네트워크를 성공적으로 훈련하는 데 핵심적인 수치적 안정성과 파라미터 초기화에 관한 문제를 논의할 준비가 될 것입니다.
이러한 고용량 모델을 훈련할 때 우리는 과대적합의 위험을 감수합니다. 따라서 심층 네트워크를 위한 정규화와 일반화를 다시 살펴볼 것입니다.
전체적으로 우리는 개념뿐만 아니라 심층 네트워크 사용의 실제에 대해서도 확실한 이해를 제공하는 것을 목표로 합니다.
이 장의 끝에서, 우리는 지금까지 소개한 내용을 실제 사례인 주택 가격 예측에 적용합니다. 모델의 계산 성능, 확장성 및 효율성과 관련된 문제는 후속 장으로 미룹니다.</p>
<pre><code class="language-toc">:maxdepth: 2

mlp
mlp-implementation
backprop
numerical-stability-and-init
generalization-deep
dropout
kaggle-house-price
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="다층-퍼셉트론-multilayer-perceptrons-1"><a class="header" href="#다층-퍼셉트론-multilayer-perceptrons-1">다층 퍼셉트론 (Multilayer Perceptrons)</a></h1>
<p>:label:<code>sec_mlp</code></p>
<p>:numref:<code>sec_softmax</code>에서 우리는 소프트맥스 회귀를 소개하고, 알고리즘을 밑바닥부터 구현(:numref:<code>sec_softmax_scratch</code>)하거나 고수준 API를 사용(:numref:<code>sec_softmax_concise</code>)하여 구현했습니다. 이를 통해 저해상도 이미지에서 10가지 의류 카테고리를 인식할 수 있는 분류기를 훈련했습니다.
그 과정에서 데이터를 다루고, 출력을 유효한 확률 분포로 강제 변환하고, 적절한 손실 함수를 적용하고, 모델의 파라미터에 대해 이를 최소화하는 방법을 배웠습니다.
이제 단순한 선형 모델의 맥락에서 이러한 메커니즘을 마스터했으므로, 이 책의 주된 관심사이며 비교적 풍부한 모델 클래스인 심층 신경망에 대한 탐구를 시작할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
from jax import grad, vmap
</code></pre>
<h2 id="은닉층-hidden-layers"><a class="header" href="#은닉층-hidden-layers">은닉층 (Hidden Layers)</a></h2>
<p>우리는 :numref:<code>subsec_linear_model</code>에서 아핀 변환을 편향이 추가된 선형 변환으로 설명했습니다.
시작하기 위해, :numref:<code>fig_softmaxreg</code>에 설명된 소프트맥스 회귀 예제에 해당하는 모델 아키텍처를 상기해 보십시오.
이 모델은 단일 아핀 변환과 그 뒤를 잇는 소프트맥스 연산을 통해 입력을 출력에 직접 매핑합니다.
만약 우리의 레이블이 정말로 단순한 아핀 변환에 의해 입력 데이터와 관련이 있다면 이 접근 방식만으로 충분할 것입니다.
그러나 (아핀 변환에서의) 선형성은 <em>강력한</em> 가정입니다.</p>
<h3 id="선형-모델의-한계-limitations-of-linear-models"><a class="header" href="#선형-모델의-한계-limitations-of-linear-models">선형 모델의 한계 (Limitations of Linear Models)</a></h3>
<p>예를 들어, 선형성은 *단조성(monotonicity)*이라는 <em>더 약한</em> 가정을 내포합니다. 즉, 특성의 증가가 항상 모델 출력의 증가를 유발하거나(해당 가중치가 양수인 경우), 항상 모델 출력의 감소를 유발해야 합니다(해당 가중치가 음수인 경우).
때로는 이것이 타당합니다.
예를 들어, 개인이 대출을 갚을지 예측하려고 할 때, 다른 모든 조건이 동일하다면 소득이 높은 신청자가 낮은 신청자보다 항상 갚을 가능성이 더 높다고 합리적으로 가정할 수 있습니다.
단조적이기는 하지만, 이 관계가 상환 확률과 선형적으로 관련이 있을 가능성은 낮습니다. 소득이 0달러에서 5만 달러로 증가하는 것은 100만 달러에서 105만 달러로 증가하는 것보다 상환 가능성 증가에 더 큰 영향을 미칠 가능성이 높습니다.
이를 처리하는 한 가지 방법은 로지스틱 맵(따라서 결과 확률의 로그)을 사용하여 선형성이 더 그럴듯해지도록 결과를 후처리하는 것일 수 있습니다.</p>
<p>단조성을 위반하는 예제를 쉽게 생각해 낼 수 있다는 점에 유의하십시오.
예를 들어 체온의 함수로 건강을 예측하고 싶다고 가정해 봅시다.
체온이 37°C(98.6°F) 이상인 정상 체온을 가진 개인의 경우, 체온이 높을수록 위험이 큽니다.
그러나 체온이 37°C 아래로 떨어지면 체온이 낮을수록 위험이 큽니다!
다시 말하지만, 37°C로부터의 거리를 특성으로 사용하는 것과 같은 기발한 전처리를 통해 문제를 해결할 수도 있습니다.</p>
<p>하지만 고양이와 개 이미지를 분류하는 것은 어떨까요?
위치 (13, 17)에 있는 픽셀의 강도를 높이는 것이 이미지가 개를 묘사할 가능성을 항상 증가(또는 항상 감소)시켜야 할까요?
선형 모델에 의존하는 것은 고양이와 개를 구별하는 유일한 요구 사항이 개별 픽셀의 밝기를 평가하는 것이라는 암묵적인 가정에 해당합니다.
이 접근 방식은 이미지를 반전시켜도 범주가 보존되는 세상에서는 실패할 운명입니다.</p>
<p>그럼에도 불구하고 이전 예제들과 비교할 때 여기에서의 선형성이 명백히 터무니없음에도 불구하고, 간단한 전처리 수정으로 문제를 해결할 수 있을지는 덜 명확합니다.
그 이유는 픽셀의 중요성이 맥락(주변 픽셀의 값)에 복잡한 방식으로 의존하기 때문입니다.
데이터에 대한 표현이 존재하여 특성 간의 관련 상호 작용을 고려하고 그 위에 선형 모델이 적합할 수도 있겠지만, 우리는 단순히 그것을 손으로 계산하는 방법을 모릅니다.
심층 신경망을 사용하여, 우리는 관찰 데이터를 통해 은닉층을 통한 표현과 그 표현에 작용하는 선형 예측기를 공동으로 학습합니다.</p>
<p>이 비선형성 문제는 적어도 한 세기 동안 연구되어 왔습니다 :cite:<code>Fisher.1928</code>. 예를 들어, 가장 기본적인 형태의 결정 트리는 클래스 멤버십을 결정하기 위해 일련의 이진 결정을 사용합니다 :cite:<code>quinlan2014c4</code>. 마찬가지로 커널 방법은 수십 년 동안 비선형 의존성을 모델링하는 데 사용되었습니다 :cite:<code>Aronszajn.1950</code>. 이것은 비모수 스플라인 모델 :cite:<code>Wahba.1990</code>과 커널 방법 :cite:<code>Scholkopf.Smola.2002</code>으로 이어졌습니다. 이것은 또한 뇌가 아주 자연스럽게 해결하는 문제이기도 합니다. 결국 뉴런은 다른 뉴런으로 공급되고, 이는 다시 다른 뉴런으로 공급됩니다 :cite:<code>Cajal.Azoulay.1894</code>.
결과적으로 우리는 상대적으로 단순한 변환의 시퀀스를 갖게 됩니다.</p>
<h3 id="은닉층-통합하기-incorporating-hidden-layers"><a class="header" href="#은닉층-통합하기-incorporating-hidden-layers">은닉층 통합하기 (Incorporating Hidden Layers)</a></h3>
<p>우리는 하나 이상의 은닉층을 통합하여 선형 모델의 한계를 극복할 수 있습니다.
가장 쉬운 방법은 많은 완전 연결 레이어를 서로 쌓는 것입니다.
각 레이어는 출력을 생성할 때까지 그 위의 레이어로 공급됩니다.
처음 $L-1$개의 레이어를 표현으로 생각하고 마지막 레이어를 선형 예측기로 생각할 수 있습니다.
이 아키텍처는 일반적으로 *다층 퍼셉트론(multilayer perceptron)*이라고 불리며 종종 <em>MLP</em>로 축약됩니다 (:numref:<code>fig_mlp</code>).</p>
<p><img src="chapter_multilayer-perceptrons/../img/mlp.svg" alt="5개의 은닉 유닛이 있는 은닉층을 가진 MLP." />
:label:<code>fig_mlp</code></p>
<p>이 MLP는 4개의 입력, 3개의 출력을 가지며, 은닉층에는 5개의 은닉 유닛이 있습니다.
입력 레이어는 어떠한 계산도 포함하지 않으므로, 이 네트워크로 출력을 생성하려면 은닉층과 출력 레이어 모두에 대한 계산을 구현해야 합니다. 따라서 이 MLP의 레이어 수는 2개입니다.
두 레이어 모두 완전 연결되어 있다는 점에 유의하십시오.
모든 입력은 은닉층의 모든 뉴런에 영향을 미치고, 이들 각각은 다시 출력 레이어의 모든 뉴런에 영향을 미칩니다. 아쉽게도 아직 끝난 것은 아닙니다.</p>
<h3 id="선형에서-비선형으로-from-linear-to-nonlinear"><a class="header" href="#선형에서-비선형으로-from-linear-to-nonlinear">선형에서 비선형으로 (From Linear to Nonlinear)</a></h3>
<p>이전과 마찬가지로, 행렬 $\mathbf{X} \in \mathbb{R}^{n \times d}$를 각 예제가 $d$개의 입력(특성)을 가진 $n$개 예제의 미니배치라고 표시합니다.
$h$개의 은닉 유닛을 가진 은닉층이 있는 1-은닉층 MLP의 경우, $\mathbf{H} \in \mathbb{R}^{n \times h}$를 은닉층의 출력으로 표시하며, 이는 *은닉 표현(hidden representations)*입니다.
은닉층과 출력 레이어 모두 완전 연결되어 있으므로, 은닉층 가중치 $\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$와 편향 $\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$, 그리고 출력층 가중치 $\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$와 편향 $\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$를 갖습니다.
이를 통해 1-은닉층 MLP의 출력 $\mathbf{O} \in \mathbb{R}^{n \times q}$를 다음과 같이 계산할 수 있습니다:</p>
<p>$$
\begin{aligned}
\mathbf{H} &amp; = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \
\mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.<br />
\end{aligned}
$$</p>
<p>은닉층을 추가한 후, 이제 우리 모델은 추가적인 파라미터 세트를 추적하고 업데이트해야 합니다.
그렇다면 그 대가로 무엇을 얻었을까요?
위에서 정의된 모델에서는 <em>수고에 대해 아무것도 얻지 못했다</em>는 사실을 알게 되어 놀랄 수도 있습니다!
이유는 명백합니다.
위의 은닉 유닛은 입력의 아핀 함수로 주어지고, 출력(소프트맥스 전)은 단지 은닉 유닛의 아핀 함수일 뿐입니다.
아핀 함수의 아핀 함수는 그 자체로 아핀 함수입니다.
게다가 우리 선형 모델은 이미 모든 아핀 함수를 표현할 수 있었습니다.</p>
<p>이를 공식적으로 확인하기 위해 위 정의에서 은닉층을 붕괴시켜 파라미터 $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$와 $\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$를 가진 동등한 단일 레이어 모델을 얻을 수 있습니다:</p>
<p>$$
\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.
$$</p>
<p>다층 아키텍처의 잠재력을 실현하기 위해, 우리는 한 가지 핵심 요소가 더 필요합니다: 아핀 변환 후 각 은닉 유닛에 적용될 비선형 <em>활성화 함수(activation function)</em> $\sigma$입니다. 예를 들어 인기 있는 선택은 인수에 요소별로 작동하는 ReLU(rectified linear unit) 활성화 함수 :cite:<code>Nair.Hinton.2010</code> $\sigma(x) = \mathrm{max}(0, x)$입니다.
활성화 함수 $\sigma(\cdot)$의 출력을 *활성화(activations)*라고 합니다.
일반적으로 활성화 함수가 있으면 더 이상 MLP를 선형 모델로 붕괴시킬 수 없습니다:</p>
<p>$$
\begin{aligned}
\mathbf{H} &amp; = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \
\mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.<br />
\end{aligned}
$$</p>
<p>$\mathbf{X}$의 각 행은 미니배치의 예제에 해당하므로, 약간의 표기법 남용과 함께 비선형성 $\sigma$가 입력에 행별 방식으로, 즉 한 번에 한 예제씩 적용되도록 정의합니다.
:numref:<code>subsec_softmax_vectorization</code>에서 행별 연산을 나타낼 때 소프트맥스에 대해 동일한 표기법을 사용했다는 점에 유의하십시오.
우리가 사용하는 활성화 함수는 단순히 행별이 아니라 요소별로 적용되는 경우가 꽤 많습니다. 이는 레이어의 선형 부분을 계산한 후, 다른 은닉 유닛이 취한 값을 보지 않고 각 활성화를 계산할 수 있음을 의미합니다.</p>
<p>더 일반적인 MLP를 구축하기 위해, 우리는 이러한 은닉층을 계속 쌓을 수 있습니다.
예를 들어 $\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$ 및 $\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$와 같이 차곡차곡 쌓아 훨씬 더 표현력이 풍부한 모델을 만들 수 있습니다.</p>
<h3 id="보편적-근사자-universal-approximators"><a class="header" href="#보편적-근사자-universal-approximators">보편적 근사자 (Universal Approximators)</a></h3>
<p>우리는 뇌가 매우 정교한 통계 분석을 할 수 있다는 것을 알고 있습니다. 따라서 심층 네트워크가 <em>얼마나 강력할 수 있는지</em> 물어볼 가치가 있습니다. 이 질문은 MLP의 맥락에서 :citet:<code>Cybenko.1989</code>에 의해, 그리고 단일 은닉층을 가진 방사형 기저 함수(RBF) 네트워크로 볼 수 있는 방식으로 재생 커널 힐베르트 공간의 맥락에서 :citet:<code>micchelli1984interpolation</code>에 의해 여러 번 대답되었습니다.
이러한 (및 관련 결과)는 단일 은닉층 네트워크라도 충분한 노드(어쩌면 터무니없이 많은)와 올바른 가중치 세트가 주어지면 어떤 함수든 모델링할 수 있음을 시사합니다.
하지만 실제로 그 함수를 학습하는 것은 어려운 부분입니다.
신경망을 C 프로그래밍 언어와 비슷하다고 생각할 수 있습니다.
다른 모든 현대 언어와 마찬가지로 이 언어는 모든 계산 가능한 프로그램을 표현할 수 있습니다.
하지만 사양을 충족하는 프로그램을 실제로 생각해 내는 것이 어려운 부분입니다.</p>
<p>게다가 단일 은닉층 네트워크가 어떤 함수든 학습할 <em>수</em> 있다고 해서 모든 문제를 하나로 해결하려고 해서는 안 됩니다. 사실 이 경우 커널 방법은 무한 차원 공간에서도 <em>정확하게</em> 문제를 해결할 수 있으므로 훨씬 더 효과적입니다 :cite:<code>Kimeldorf.Wahba.1971,Scholkopf.Herbrich.Smola.2001</code>.
실제로 우리는 (넓은 것보다는) 더 깊은 네트워크를 사용하여 많은 함수를 훨씬 더 간결하게 근사할 수 있습니다 :cite:<code>Simonyan.Zisserman.2014</code>.
우리는 후속 장에서 더 엄격한 주장을 다룰 것입니다.</p>
<h2 id="활성화-함수-activation-functions"><a class="header" href="#활성화-함수-activation-functions">활성화 함수 (Activation Functions)</a></h2>
<p>:label:<code>subsec_activation-functions</code></p>
<p>활성화 함수는 가중 합을 계산하고 여기에 편향을 더함으로써 뉴런이 활성화되어야 하는지 여부를 결정합니다.
이들은 입력 신호를 출력으로 변환하기 위한 미분 가능한 연산자이며, 그들 대부분은 비선형성을 추가합니다.
활성화 함수는 딥러닝의 기본이므로, (<strong>몇 가지 일반적인 함수를 간략하게 살펴봅시다</strong>).</p>
<h3 id="relu-함수"><a class="header" href="#relu-함수">ReLU 함수</a></h3>
<p>구현의 단순성과 다양한 예측 작업에서의 우수한 성능으로 인해 가장 인기 있는 선택은 *ReLU(rectified linear unit)*입니다 :cite:<code>Nair.Hinton.2010</code>.
[<strong>ReLU는 매우 간단한 비선형 변환을 제공합니다</strong>].
요소 $x$가 주어지면 함수는 해당 요소와 $0$ 중 최댓값으로 정의됩니다:</p>
<p>$$\operatorname{ReLU}(x) = \max(x, 0).$$</p>
<p>비공식적으로 ReLU 함수는 양의 요소만 유지하고 해당 활성화를 0으로 설정하여 모든 음의 요소를 버립니다.
직관을 얻기 위해 함수를 플롯할 수 있습니다.
보시다시피 활성화 함수는 부분적으로 선형입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
    y = npx.relu(x)
d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)
y = tf.nn.relu(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'relu(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(-8.0, 8.0, 0.1)
y = jax.nn.relu(x)
d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))
</code></pre>
<p>입력이 음수일 때 ReLU 함수의 도함수는 0이고, 입력이 양수일 때 ReLU 함수의 도함수는 1입니다.
입력이 정확히 0인 값을 취할 때 ReLU 함수는 미분 불가능하다는 점에 유의하십시오.
이러한 경우, 우리는 기본적으로 좌측 도함수를 사용하여 입력이 0일 때 도함수가 0이라고 말합니다.
입력이 실제로 0이 되는 경우는 없을 수 있기 때문에(수학자들은 측도가 0인 집합에서 미분 불가능하다고 말할 것입니다) 우리는 이를 넘어갈 수 있습니다.
미묘한 경계 조건이 중요하다면 우리는 아마도 공학이 아니라 (<em>진짜</em>) 수학을 하고 있다는 옛 격언이 있습니다.
그 통념이 여기에 적용될 수 있거나, 적어도 우리가 제약 최적화 :cite:<code>Mangasarian.1965,Rockafellar.1970</code>를 수행하지 않는다는 사실이 적용될 수 있습니다.
아래에 ReLU 함수의 도함수를 플롯합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.relu(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of relu',
         figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
grad_relu = vmap(grad(jax.nn.relu))
d2l.plot(x, grad_relu(x), 'x', 'grad of relu', figsize=(5, 2.5))
</code></pre>
<p>ReLU를 사용하는 이유는 도함수가 특히 잘 작동하기 때문입니다: 사라지거나 인수를 그대로 통과시킵니다.
이는 최적화가 더 잘 작동하게 만들고 신경망의 이전 버전을 괴롭혔던 잘 문서화된 기울기 소실(vanishing gradients) 문제를 완화했습니다(나중에 자세히 설명).</p>
<p><em>파라미터화된 ReLU</em> (<em>pReLU</em>) 함수 :cite:<code>He.Zhang.Ren.ea.2015</code>를 포함하여 ReLU 함수에는 많은 변형이 있습니다.
이 변형은 ReLU에 선형 항을 추가하여 인수가 음수일 때도 일부 정보가 여전히 통과되도록 합니다:</p>
<p>$$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$$</p>
<h3 id="시그모이드-함수-sigmoid-function"><a class="header" href="#시그모이드-함수-sigmoid-function">시그모이드 함수 (Sigmoid Function)</a></h3>
<p>[** <em>시그모이드 함수(sigmoid function)<em>는 값이 도메인 $\mathbb{R}$에 있는 입력을</em></em>] (<strong>구간 (0, 1)에 있는 출력으로 변환합니다.</strong>)
그렇기 때문에 시그모이드는 종종 *스쿼싱 함수(squashing function)*라고 불립니다: (-inf, inf) 범위의 모든 입력을 (0, 1) 범위의 어떤 값으로 찌그러뜨립니다:</p>
<p>$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$</p>
<p>초기 신경망에서 과학자들은 <em>발화하거나</em> <em>발화하지 않는</em> 생물학적 뉴런을 모델링하는 데 관심이 있었습니다.
따라서 인공 뉴런의 발명가인 맥컬록과 피츠로 거슬러 올라가는 이 분야의 개척자들은 임계값 유닛에 집중했습니다 :cite:<code>McCulloch.Pitts.1943</code>.
임계값 활성화는 입력이 어떤 임계값보다 낮을 때 0 값을 갖고 입력이 임계값을 초과할 때 1 값을 갖습니다.</p>
<p>관심이 경사 기반 학습으로 옮겨갔을 때, 시그모이드 함수는 임계값 유닛에 대한 부드럽고 미분 가능한 근사치이기 때문에 자연스러운 선택이었습니다.
시그모이드는 이진 분류 문제에 대해 출력을 확률로 해석하고 싶을 때 출력 유닛의 활성화 함수로 여전히 널리 사용됩니다: 시그모이드를 소프트맥스의 특수한 경우로 생각할 수 있습니다.
그러나 시그모이드는 은닉층의 대부분의 용도에서 더 간단하고 훈련하기 쉬운 ReLU로 대체되었습니다.
이는 시그모이드가 큰 양수 <em>및</em> 음수 인수에 대해 기울기가 사라지기 때문에 최적화에 어려움을 준다는 사실과 관련이 있습니다 :cite:<code>LeCun.Bottou.Orr.ea.1998</code>.
이로 인해 탈출하기 어려운 고원(plateaus)이 생길 수 있습니다.
그럼에도 불구하고 시그모이드는 중요합니다. 순환 신경망에 대한 나중 장(예: :numref:<code>sec_lstm</code>)에서 우리는 시그모이드 유닛을 활용하여 시간에 따른 정보의 흐름을 제어하는 아키텍처를 설명할 것입니다.</p>
<p>아래에 시그모이드 함수를 플롯합니다.
입력이 0에 가까울 때 시그모이드 함수는 선형 변환에 접근한다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = npx.sigmoid(x)
d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = jax.nn.sigmoid(x)
d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))
</code></pre>
<p>시그모이드 함수의 도함수는 다음 방정식으로 주어집니다:</p>
<p>$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)(1-\operatorname{sigmoid}(x)).$$</p>
<p>시그모이드 함수의 도함수는 아래에 플롯되어 있습니다.
입력이 0일 때 시그모이드 함수의 도함수는 최대 0.25에 도달합니다.
입력이 어느 방향으로든 0에서 멀어질수록 도함수는 0에 접근합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 이전 기울기 지우기
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of sigmoid',
         figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
grad_sigmoid = vmap(grad(jax.nn.sigmoid))
d2l.plot(x, grad_sigmoid(x), 'x', 'grad of sigmoid', figsize=(5, 2.5))
</code></pre>
<h3 id="tanh-함수-tanh-function"><a class="header" href="#tanh-함수-tanh-function">Tanh 함수 (Tanh Function)</a></h3>
<p>:label:<code>subsec_tanh</code></p>
<p>시그모이드 함수처럼, [<strong>tanh(하이퍼볼릭 탄젠트) 함수도 입력을 찌그러뜨려</strong>] (<strong>$-1$과 $1$ 사이의</strong>) 구간의 요소로 변환합니다:</p>
<p>$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$</p>
<p>아래에 tanh 함수를 플롯합니다. 입력이 0에 가까워지면 tanh 함수는 선형 변환에 접근합니다. 함수의 모양은 시그모이드 함수의 모양과 유사하지만, tanh 함수는 좌표계 원점에 대해 점 대칭을 보입니다 :cite:<code>Kalman.Kwasny.1992</code>.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = np.tanh(x)
d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
y = tf.nn.tanh(x)
d2l.plot(x.numpy(), y.numpy(), 'x', 'tanh(x)', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = jax.nn.tanh(x)
d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))
</code></pre>
<p>tanh 함수의 도함수는 다음과 같습니다:</p>
<p>$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$</p>
<p>아래에 플롯되어 있습니다.
입력이 0에 가까워지면 tanh 함수의 도함수는 최대 1에 접근합니다.
그리고 시그모이드 함수에서 보았듯이 입력이 어느 방향으로든 0에서 멀어지면 tanh 함수의 도함수는 0에 접근합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 이전 기울기 지우기
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.nn.tanh(x)
d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of tanh',
         figsize=(5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
grad_tanh = vmap(grad(jax.nn.tanh))
d2l.plot(x, grad_tanh(x), 'x', 'grad of tanh', figsize=(5, 2.5))
</code></pre>
<h2 id="요약-및-토론-summary-and-discussion-1"><a class="header" href="#요약-및-토론-summary-and-discussion-1">요약 및 토론 (Summary and Discussion)</a></h2>
<p>우리는 이제 비선형성을 통합하여 표현력 있는 다층 신경망 아키텍처를 구축하는 방법을 알게 되었습니다.
참고로 여러분의 지식은 이미 여러분을 1990년경의 실무자와 비슷한 도구 세트를 다룰 수 있는 위치에 올려놓았습니다.
어떤 면에서 여러분은 그 당시 일하던 누구보다 유리한 위치에 있습니다. 강력한 오픈 소스 딥러닝 프레임워크를 활용하여 단 몇 줄의 코드로 모델을 빠르게 구축할 수 있기 때문입니다.
이전에는 이러한 네트워크를 훈련하기 위해 연구자들은 레이어와 도함수를 C, Fortran, 심지어 Lisp(LeNet의 경우)로 명시적으로 코딩해야 했습니다.</p>
<p>부차적인 이점은 ReLU가 시그모이드나 tanh 함수보다 최적화에 훨씬 더 적합하다는 것입니다.
이것이 지난 10년 동안 딥러닝의 부활을 도운 주요 혁신 중 하나라고 주장할 수 있습니다.
하지만 활성화 함수에 대한 연구는 멈추지 않았다는 점에 유의하십시오.
예를 들어, :citet:<code>Hendrycks.Gimpel.2016</code>의 GELU(Gaussian error linear unit) 활성화 함수 $x \Phi(x)$ ($\Phi(x)$는 표준 정규 누적 분포 함수)와 :citet:<code>Ramachandran.Zoph.Le.2017</code>에서 제안한 Swish 활성화 함수 $\sigma(x) = x \operatorname{sigmoid}(\beta x)$는 많은 경우에 더 나은 정확도를 산출할 수 있습니다.</p>
<h2 id="연습-문제-exercises-14"><a class="header" href="#연습-문제-exercises-14">연습 문제 (Exercises)</a></h2>
<ol>
<li><em>선형</em> 심층 네트워크, 즉 비선형성 $\sigma$가 없는 네트워크에 레이어를 추가해도 네트워크의 표현력이 결코 증가하지 않음을 보이십시오.
적극적으로 줄이는 예를 드십시오.</li>
<li>pReLU 활성화 함수의 도함수를 계산하십시오.</li>
<li>Swish 활성화 함수 $x \operatorname{sigmoid}(\beta x)$의 도함수를 계산하십시오.</li>
<li>ReLU(또는 pReLU)만 사용하는 MLP가 연속적인 조각별 선형 함수를 구성함을 보이십시오.</li>
<li>시그모이드와 tanh는 매우 유사합니다.
<ol>
<li>$\operatorname{tanh}(x) + 1 = 2 \operatorname{sigmoid}(2x)$임을 보이십시오.</li>
<li>두 비선형성에 의해 파라미터화된 함수 클래스가 동일함을 증명하십시오. 힌트: 아핀 레이어에는 편향 항도 있습니다.</li>
</ol>
</li>
<li>배치 정규화 :cite:<code>Ioffe.Szegedy.2015</code>와 같이 한 번에 하나의 미니배치에 적용되는 비선형성이 있다고 가정해 봅시다. 이것이 어떤 종류의 문제를 일으킬 것으로 예상하십니까?</li>
<li>시그모이드 활성화 함수에 대해 기울기가 사라지는 예를 제시하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/90">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/91">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/226">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17984">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="다층-퍼셉트론-구현-implementation-of-multilayer-perceptrons"><a class="header" href="#다층-퍼셉트론-구현-implementation-of-multilayer-perceptrons">다층 퍼셉트론 구현 (Implementation of Multilayer Perceptrons)</a></h1>
<p>:label:<code>sec_mlp-implementation</code></p>
<p>다층 퍼셉트론(MLP)은 단순 선형 모델보다 구현하기가 훨씬 복잡하지 않습니다. 핵심적인 개념적 차이점은 이제 여러 레이어를 연결한다는 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-1"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-1">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>다시 한번 밑바닥부터 구현해 봅시다.</p>
<h3 id="모델-파라미터-초기화-initializing-model-parameters"><a class="header" href="#모델-파라미터-초기화-initializing-model-parameters">모델 파라미터 초기화 (Initializing Model Parameters)</a></h3>
<p>Fashion-MNIST에는 10개의 클래스가 있고 각 이미지는 $28 \times 28 = 784$ 그리드의 그레이스케일 픽셀 값으로 구성되어 있음을 상기하십시오.
이전과 마찬가지로 지금은 픽셀 간의 공간 구조를 무시할 것이므로, 이를 784개의 입력 특성과 10개의 클래스가 있는 분류 데이터셋으로 생각할 수 있습니다.
시작하기 위해, [<strong>하나의 은닉층과 256개의 은닉 유닛을 가진 MLP를 구현</strong>]할 것입니다.
레이어의 수와 너비는 모두 조정 가능합니다(하이퍼파라미터로 간주됨).
일반적으로 레이어 너비는 더 큰 2의 거듭제곱으로 나누어떨어지도록 선택합니다.
이는 하드웨어에서 메모리가 할당되고 주소가 지정되는 방식 때문에 계산적으로 효율적입니다.</p>
<p>다시 한번 우리는 여러 텐서로 파라미터를 나타낼 것입니다.
<em>모든 레이어에 대해</em> 하나의 가중치 행렬과 하나의 편향 벡터를 추적해야 한다는 점에 유의하십시오.
언제나 그렇듯이 우리는 이러한 파라미터에 대한 손실의 기울기를 위해 메모리를 할당합니다.</p>
<p>:begin_tab:<code>mxnet</code>
아래 코드에서 먼저 파라미터를 정의하고 초기화한 다음 기울기 추적을 활성화합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
아래 코드에서 <code>nn.Parameter</code>를 사용하여 클래스 속성을 <code>autograd</code>(:numref:<code>sec_autograd</code>)에 의해 추적될 파라미터로 자동 등록합니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
아래 코드에서 <code>tf.Variable</code>을 사용하여 모델 파라미터를 정의합니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
아래 코드에서 <code>flax.linen.Module.param</code>을 사용하여 모델 파라미터를 정의합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class MLPScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W1 = np.random.randn(num_inputs, num_hiddens) * sigma
        self.b1 = np.zeros(num_hiddens)
        self.W2 = np.random.randn(num_hiddens, num_outputs) * sigma
        self.b2 = np.zeros(num_outputs)
        for param in self.get_scratch_params():
            param.attach_grad()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class MLPScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)
        self.b1 = nn.Parameter(torch.zeros(num_hiddens))
        self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)
        self.b2 = nn.Parameter(torch.zeros(num_outputs))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class MLPScratch(d2l.Classifier):
    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W1 = tf.Variable(
            tf.random.normal((num_inputs, num_hiddens)) * sigma)
        self.b1 = tf.Variable(tf.zeros(num_hiddens))
        self.W2 = tf.Variable(
            tf.random.normal((num_hiddens, num_outputs)) * sigma)
        self.b2 = tf.Variable(tf.zeros(num_outputs))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class MLPScratch(d2l.Classifier):
    num_inputs: int
    num_outputs: int
    num_hiddens: int
    lr: float
    sigma: float = 0.01

    def setup(self):
        self.W1 = self.param('W1', nn.initializers.normal(self.sigma),
                             (self.num_inputs, self.num_hiddens))
        self.b1 = self.param('b1', nn.initializers.zeros, self.num_hiddens)
        self.W2 = self.param('W2', nn.initializers.normal(self.sigma),
                             (self.num_hiddens, self.num_outputs))
        self.b2 = self.param('b2', nn.initializers.zeros, self.num_outputs)
</code></pre>
<h3 id="모델-model-1"><a class="header" href="#모델-model-1">모델 (Model)</a></h3>
<p>모든 것이 어떻게 작동하는지 확인하기 위해, 내장 <code>relu</code> 함수를 직접 호출하는 대신 [<strong>ReLU 활성화를 직접 구현</strong>]해 보겠습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def relu(X):
    return np.maximum(X, 0)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def relu(X):
    return tf.math.maximum(X, 0)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def relu(X):
    return jnp.maximum(X, 0)
</code></pre>
<p>공간 구조를 무시하고 있으므로, 각 2차원 이미지를 길이 <code>num_inputs</code>의 평면 벡터로 <code>reshape</code>합니다.
마지막으로 단 몇 줄의 코드로 (<strong>모델을 구현합니다</strong>). 프레임워크에 내장된 autograd를 사용하므로 이것으로 충분합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(MLPScratch)
def forward(self, X):
    X = d2l.reshape(X, (-1, self.num_inputs))
    H = relu(d2l.matmul(X, self.W1) + self.b1)
    return d2l.matmul(H, self.W2) + self.b2
</code></pre>
<h3 id="훈련-training-5"><a class="header" href="#훈련-training-5">훈련 (Training)</a></h3>
<p>다행히도 [<strong>MLP의 훈련 루프는 소프트맥스 회귀와 정확히 동일합니다.</strong>] 모델, 데이터, 트레이너를 정의하고 마지막으로 모델과 데이터에 대해 <code>fit</code> 메서드를 호출합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
model = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)
data = d2l.FashionMNIST(batch_size=256)
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model, data)
</code></pre>
<h2 id="간결한-구현-concise-implementation"><a class="header" href="#간결한-구현-concise-implementation">간결한 구현 (Concise Implementation)</a></h2>
<p>예상하시겠지만, 고수준 API를 활용하면 MLP를 훨씬 더 간결하게 구현할 수 있습니다.</p>
<h3 id="모델-model-2"><a class="header" href="#모델-model-2">모델 (Model)</a></h3>
<p>소프트맥스 회귀 구현의 간결한 구현(:numref:<code>sec_softmax_concise</code>)과 비교할 때, 유일한 차이점은 이전에 <em>하나</em>만 추가했던 곳에 <em>두 개의</em> 완전 연결 레이어를 추가한다는 것입니다.
첫 번째는 [<strong>은닉층</strong>]이고, 두 번째는 출력 레이어입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class MLP(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential()
        self.net.add(nn.Dense(num_hiddens, activation='relu'),
                     nn.Dense(num_outputs))
        self.net.initialize()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class MLP(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),
                                 nn.ReLU(), nn.LazyLinear(num_outputs))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class MLP(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = tf.keras.models.Sequential([
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(num_hiddens, activation='relu'),
            tf.keras.layers.Dense(num_outputs)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class MLP(d2l.Classifier):
    num_outputs: int
    num_hiddens: int
    lr: float

    @nn.compact
    def __call__(self, X):
        X = X.reshape((X.shape[0], -1))  # Flatten
        X = nn.Dense(self.num_hiddens)(X)
        X = nn.relu(X)
        X = nn.Dense(self.num_outputs)(X)
        return X
</code></pre>
<p>이전에 우리는 모델 파라미터를 사용하여 입력을 변환하기 위해 모델에 <code>forward</code> 메서드를 정의했습니다.
이러한 연산은 본질적으로 파이프라인입니다: 입력을 받아 변환(예: 가중치와의 행렬 곱셈 후 편향 덧셈)을 적용한 다음, 현재 변환의 출력을 다음 변환의 입력으로 반복적으로 사용합니다.
하지만 여기서 <code>forward</code> 메서드가 정의되지 않았다는 것을 눈치채셨을 것입니다.
사실 <code>MLP</code>는 <code>Module</code> 클래스(:numref:<code>subsec_oo-design-models</code>)에서 <code>forward</code> 메서드를 상속받아 단순히 <code>self.net(X)</code>(<code>X</code>는 입력)를 호출합니다. 여기서 <code>net</code>은 <code>Sequential</code> 클래스를 통해 일련의 변환으로 정의되었습니다.
<code>Sequential</code> 클래스는 순방향 프로세스를 추상화하여 변환에 집중할 수 있게 해줍니다.
<code>Sequential</code> 클래스가 작동하는 방식에 대해서는 :numref:<code>subsec_model-construction-sequential</code>에서 더 자세히 논의할 것입니다.</p>
<h3 id="훈련-training-6"><a class="header" href="#훈련-training-6">훈련 (Training)</a></h3>
<p>[<strong>훈련 루프</strong>]는 우리가 소프트맥스 회귀를 구현했을 때와 정확히 동일합니다.
이러한 모듈성을 통해 모델 아키텍처와 관련된 문제를 직교적인 고려 사항과 분리할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)
trainer.fit(model, data)
</code></pre>
<h2 id="요약-summary-13"><a class="header" href="#요약-summary-13">요약 (Summary)</a></h2>
<p>이제 심층 네트워크 설계에 대한 연습을 더 많이 했으므로, 단일 레이어에서 다중 레이어 심층 네트워크로의 단계는 더 이상 큰 어려움을 주지 않습니다. 특히 훈련 알고리즘과 데이터 로더를 재사용할 수 있습니다. 하지만 밑바닥부터 MLP를 구현하는 것은 여전히 지저분하다는 점에 유의하십시오: 모델 파라미터의 이름을 지정하고 추적하는 것은 모델 확장을 어렵게 만듭니다. 예를 들어 레이어 42와 43 사이에 다른 레이어를 삽입하고 싶다고 상상해 보십시오. 순차적으로 이름을 바꾸지 않으려면 이것은 레이어 42b가 될 수 있습니다. 게다가 네트워크를 밑바닥부터 구현하면 프레임워크가 의미 있는 성능 최적화를 수행하기가 훨씬 더 어렵습니다.</p>
<p>그럼에도 불구하고 여러분은 이제 완전 연결 심층 네트워크가 신경망 모델링을 위해 선택된 방법이었던 1980년대 후반의 최첨단 기술 수준에 도달했습니다. 우리의 다음 개념적 단계는 이미지를 고려하는 것입니다. 그렇게 하기 전에, 몇 가지 통계적 기초와 모델을 효율적으로 계산하는 방법에 대한 세부 사항을 검토해야 합니다.</p>
<h2 id="연습-문제-exercises-15"><a class="header" href="#연습-문제-exercises-15">연습 문제 (Exercises)</a></h2>
<ol>
<li>은닉 유닛 수 <code>num_hiddens</code>를 변경하고 그 수가 모델의 정확도에 어떤 영향을 미치는지 플롯하십시오. 이 하이퍼파라미터의 가장 좋은 값은 무엇입니까?</li>
<li>은닉층을 추가하여 결과에 어떤 영향을 미치는지 확인해 보십시오.</li>
<li>단 하나의 뉴런만 있는 은닉층을 삽입하는 것이 나쁜 생각인 이유는 무엇입니까? 무엇이 잘못될 수 있습니까?</li>
<li>학습률을 변경하면 결과가 어떻게 바뀝니까? 다른 모든 파라미터가 고정된 상태에서 어떤 학습률이 가장 좋은 결과를 제공합니까? 이것이 에폭 수와 어떤 관련이 있습니까?</li>
<li>모든 하이퍼파라미터, 즉 학습률, 에폭 수, 은닉층 수, 레이어당 은닉 유닛 수에 대해 공동으로 최적화해 봅시다.
<ol>
<li>모든 것을 최적화하여 얻을 수 있는 가장 좋은 결과는 무엇입니까?</li>
<li>여러 하이퍼파라미터를 다루는 것이 왜 훨씬 더 어려운가요?</li>
<li>여러 파라미터를 공동으로 최적화하기 위한 효율적인 전략을 설명하십시오.</li>
</ol>
</li>
<li>어려운 문제에 대해 프레임워크와 밑바닥부터의 구현 속도를 비교해 보십시오. 네트워크의 복잡성에 따라 어떻게 변합니까?</li>
<li>잘 정렬된 행렬과 잘못 정렬된 행렬에 대한 텐서-행렬 곱셈 속도를 측정하십시오. 예를 들어 차원이 1024, 1025, 1026, 1028, 1032인 행렬에 대해 테스트하십시오.
<ol>
<li>GPU와 CPU 사이에서 이것이 어떻게 변합니까?</li>
<li>CPU와 GPU의 메모리 버스 너비를 결정하십시오.</li>
</ol>
</li>
<li>다양한 활성화 함수를 사용해 보십시오. 어떤 것이 가장 잘 작동합니까?</li>
<li>네트워크의 가중치 초기화 사이에 차이가 있습니까? 중요합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/92">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/93">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/227">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17985">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="순전파-역전파-그리고-계산-그래프-forward-propagation-backward-propagation-and-computational-graphs"><a class="header" href="#순전파-역전파-그리고-계산-그래프-forward-propagation-backward-propagation-and-computational-graphs">순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></h1>
<p>:label:<code>sec_backprop</code></p>
<p>지금까지 우리는 미니배치 확률적 경사 하강법으로 모델을 훈련해 왔습니다.
하지만 알고리즘을 구현할 때, 모델을 통한 *순전파(forward propagation)*와 관련된 계산에만 신경 썼습니다.
기울기를 계산할 때가 되면 딥러닝 프레임워크에서 제공하는 역전파 함수를 호출하기만 했습니다.</p>
<p>기울기의 자동 계산은 딥러닝 알고리즘의 구현을 엄청나게 단순화합니다.
자동 미분 전에는 복잡한 모델을 조금만 변경해도 복잡한 도함수를 손으로 다시 계산해야 했습니다.
놀랍게도 학술 논문들은 업데이트 규칙을 유도하는 데 수많은 페이지를 할애하곤 했습니다.
흥미로운 부분에 집중하기 위해 자동 미분에 계속 의존해야 하지만,
딥러닝에 대한 피상적인 이해를 넘어 서려면 이러한 기울기가 내부적으로 어떻게 계산되는지 알아야 합니다.</p>
<p>이 섹션에서는 <em>역전파(backward propagation)</em> (더 일반적으로는 <em>backpropagation</em>이라고 함)의 세부 사항을 깊이 파고듭니다.
기술과 구현 모두에 대한 통찰력을 전달하기 위해 몇 가지 기본 수학과 계산 그래프에 의존합니다.
시작하기 위해 가중치 감쇠($\ell_2$ 정규화, 후속 장에서 설명됨)가 있는 단일 은닉층 MLP에 설명을 집중합니다.</p>
<h2 id="순전파-forward-propagation"><a class="header" href="#순전파-forward-propagation">순전파 (Forward Propagation)</a></h2>
<p><em>순전파</em> (또는 <em>순방향 패스</em>)는 입력 레이어에서 출력 레이어 순으로 신경망의 중간 변수(출력 포함)를 계산하고 저장하는 것을 말합니다.
이제 단일 은닉층 신경망의 메커니즘을 단계별로 살펴봅니다.
이것이 지루해 보일 수 있지만 펑크의 거장 제임스 브라운(James Brown)의 불멸의 말처럼, "보스가 되려면 대가를 치러야 합니다(you must 'pay the cost to be the boss')".</p>
<p>단순함을 위해 입력 예제가 $\mathbf{x}\in \mathbb{R}^d$이고 은닉층에 편향 항이 포함되지 않는다고 가정해 봅시다.
여기서 중간 변수는 다음과 같습니다:</p>
<p>$$\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x},$$</p>
<p>여기서 $\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$는 은닉층의 가중치 파라미터입니다.
중간 변수 $\mathbf{z}\in \mathbb{R}^h$를 활성화 함수 $\phi$에 통과시키면 길이 $h$의 은닉 활성화 벡터를 얻습니다:</p>
<p>$$\mathbf{h}= \phi (\mathbf{z}).$$</p>
<p>은닉층 출력 $\mathbf{h}$도 중간 변수입니다.
출력 레이어의 파라미터가 $\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$의 가중치만 가지고 있다고 가정하면, 길이 $q$의 벡터를 가진 출력 레이어 변수를 얻을 수 있습니다:</p>
<p>$$\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.$$</p>
<p>손실 함수가 $l$이고 예제 레이블이 $y$라고 가정하면, 단일 데이터 예제에 대한 손실 항을 계산할 수 있습니다.</p>
<p>$$L = l(\mathbf{o}, y).$$</p>
<p>나중에 소개될 $\ell_2$ 정규화의 정의에서 볼 수 있듯이, 하이퍼파라미터 $\lambda$가 주어지면 정규화 항은 다음과 같습니다.</p>
<p>$$s = \frac{\lambda}{2} \left(|\mathbf{W}^{(1)}|<em>\textrm{F}^2 + |\mathbf{W}^{(2)}|</em>\textrm{F}^2\right),$$
:eqlabel:<code>eq_forward-s</code></p>
<p>여기서 행렬의 프로베니우스 노름은 단순히 행렬을 벡터로 평탄화한 후 적용된 $\ell_2$ 노름입니다.
마지막으로 주어진 데이터 예제에 대한 모델의 정규화된 손실은 다음과 같습니다:</p>
<p>$$J = L + s.$$</p>
<p>다음 논의에서는 $J$를 *목적 함수(objective function)*라고 부릅니다.</p>
<h2 id="순전파의-계산-그래프-computational-graph-of-forward-propagation"><a class="header" href="#순전파의-계산-그래프-computational-graph-of-forward-propagation">순전파의 계산 그래프 (Computational Graph of Forward Propagation)</a></h2>
<p><em>계산 그래프</em>를 그리는 것은 계산 내에서 연산자와 변수의 종속성을 시각화하는 데 도움이 됩니다.
:numref:<code>fig_forward</code>는 위에서 설명한 간단한 네트워크와 관련된 그래프를 포함하고 있으며, 여기서 사각형은 변수를, 원은 연산자를 나타냅니다.
왼쪽 아래 모서리는 입력을 나타내고 오른쪽 위 모서리는 출력을 나타냅니다.
화살표 방향(데이터 흐름을 나타냄)이 주로 오른쪽과 위쪽임에 유의하십시오.</p>
<p><img src="chapter_multilayer-perceptrons/../img/forward.svg" alt="순전파의 계산 그래프." />
:label:<code>fig_forward</code></p>
<h2 id="역전파-backpropagation"><a class="header" href="#역전파-backpropagation">역전파 (Backpropagation)</a></h2>
<p><em>역전파</em>는 신경망 파라미터의 기울기를 계산하는 방법을 말합니다.
간단히 말해서, 이 방법은 미적분학의 <em>연쇄 법칙</em>에 따라 출력 레이어에서 입력 레이어로 네트워크를 역순으로 순회합니다.
알고리즘은 일부 파라미터에 대한 기울기를 계산하는 동안 필요한 중간 변수(편도함수)를 저장합니다.
입력과 출력 $\mathsf{Y}=f(\mathsf{X})$와 $\mathsf{Z}=g(\mathsf{Y})$가 임의의 모양의 텐서라고 가정해 봅시다.
연쇄 법칙을 사용하여 다음을 통해 $\mathsf{X}$에 대한 $\mathsf{Z}$의 도함수를 계산할 수 있습니다.</p>
<p>$$\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \textrm{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right).$$</p>
<p>여기서 우리는 전치 및 입력 위치 교환과 같은 필요한 연산이 수행된 후 인수를 곱하기 위해 $\textrm{prod}$ 연산자를 사용합니다.
벡터의 경우 이것은 간단합니다: 단순히 행렬-행렬 곱셈입니다.
고차원 텐서의 경우 적절한 대응물을 사용합니다.
연산자 $\textrm{prod}$는 모든 표기법상의 오버헤드를 숨깁니다.</p>
<p>:numref:<code>fig_forward</code>에 계산 그래프가 있는 단일 은닉층을 가진 단순 네트워크의 파라미터가 $\mathbf{W}^{(1)}$과 $\mathbf{W}^{(2)}$임을 상기하십시오.
역전파의 목표는 기울기 $\partial J/\partial \mathbf{W}^{(1)}$과 $\partial J/\partial \mathbf{W}^{(2)}$를 계산하는 것입니다.
이를 달성하기 위해 연쇄 법칙을 적용하고 각 중간 변수와 파라미터의 기울기를 차례로 계산합니다.
계산 순서는 순전파에서 수행된 순서와 반대인데, 계산 그래프의 결과에서 시작하여 파라미터 쪽으로 작업해야 하기 때문입니다.
첫 번째 단계는 손실 항 $L$과 정규화 항 $s$에 대한 목적 함수 $J=L+s$의 기울기를 계산하는 것입니다:</p>
<p>$$\frac{\partial J}{\partial L} = 1 ; \textrm{그리고} ; \frac{\partial J}{\partial s} = 1.$$</p>
<p>다음으로, 연쇄 법칙에 따라 출력 레이어 변수 $\mathbf{o}$에 대한 목적 함수의 기울기를 계산합니다:</p>
<p>$$
\frac{\partial J}{\partial \mathbf{o}}
= \textrm{prod}\left(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \mathbf{o}}\right)
= \frac{\partial L}{\partial \mathbf{o}}
\in \mathbb{R}^q.
$$</p>
<p>다음으로, 두 파라미터에 대한 정규화 항의 기울기를 계산합니다:</p>
<p>$$\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}
; \textrm{그리고} ;
\frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}.$$</p>
<p>이제 출력 레이어에 가장 가까운 모델 파라미터의 기울기 $\partial J/\partial \mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$를 계산할 수 있습니다.
연쇄 법칙을 사용하면 다음을 얻습니다:</p>
<p>$$\frac{\partial J}{\partial \mathbf{W}^{(2)}}= \textrm{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(2)}}\right) + \textrm{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(2)}}\right)= \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}.$$
:eqlabel:<code>eq_backprop-J-h</code></p>
<p>$\mathbf{W}^{(1)}$에 대한 기울기를 얻으려면 출력 레이어를 따라 은닉층으로 역전파를 계속해야 합니다.
은닉층 출력에 대한 기울기 $\partial J/\partial \mathbf{h} \in \mathbb{R}^h$는 다음과 같이 주어집니다.</p>
<p>$$
\frac{\partial J}{\partial \mathbf{h}}
= \textrm{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{h}}\right)
= {\mathbf{W}^{(2)}}^\top \frac{\partial J}{\partial \mathbf{o}}.
$$</p>
<p>활성화 함수 $\phi$가 요소별로 적용되므로, 중간 변수 $\mathbf{z}$의 기울기 $\partial J/\partial \mathbf{z} \in \mathbb{R}^h$를 계산하려면 요소별 곱셈 연산자를 사용해야 하며, 이를 $\odot$로 표시합니다:</p>
<p>$$
\frac{\partial J}{\partial \mathbf{z}}
= \textrm{prod}\left(\frac{\partial J}{\partial \mathbf{h}}, \frac{\partial \mathbf{h}}{\partial \mathbf{z}}\right)
= \frac{\partial J}{\partial \mathbf{h}} \odot \phi'\left(\mathbf{z}\right).
$$</p>
<p>마지막으로 입력 레이어에 가장 가까운 모델 파라미터의 기울기 $\partial J/\partial \mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$를 얻을 수 있습니다.
연쇄 법칙에 따라 다음을 얻습니다.</p>
<p>$$
\frac{\partial J}{\partial \mathbf{W}^{(1)}}
= \textrm{prod}\left(\frac{\partial J}{\partial \mathbf{z}}, \frac{\partial \mathbf{z}}{\partial \mathbf{W}^{(1)}}\right) + \textrm{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(1)}}\right)
= \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}.
$$</p>
<h2 id="신경망-훈련-training-neural-networks"><a class="header" href="#신경망-훈련-training-neural-networks">신경망 훈련 (Training Neural Networks)</a></h2>
<p>신경망을 훈련할 때, 순전파와 역전파는 서로 의존합니다.
특히 순전파의 경우, 종속성 방향으로 계산 그래프를 순회하며 경로상의 모든 변수를 계산합니다.
이들은 그래프의 계산 순서가 반대인 역전파에 사용됩니다.</p>
<p>앞서 언급한 간단한 네트워크를 예시로 들어보겠습니다.
한편으로, 순전파 중 정규화 항 :eqref:<code>eq_forward-s</code>를 계산하는 것은 모델 파라미터 $\mathbf{W}^{(1)}$과 $\mathbf{W}^{(2)}$의 현재 값에 의존합니다.
이들은 가장 최근 반복의 역전파에 따라 최적화 알고리즘에 의해 제공됩니다.
다른 한편으로, 역전파 중 파라미터에 대한 기울기 계산 :eqref:<code>eq_backprop-J-h</code>은 은닉층 출력 $\mathbf{h}$의 현재 값에 의존하며, 이는 순전파에 의해 제공됩니다.</p>
<p>따라서 신경망을 훈련할 때, 모델 파라미터가 초기화되면 순전파와 역전파를 번갈아 가며 수행하고, 역전파가 제공하는 기울기를 사용하여 모델 파라미터를 업데이트합니다.
역전파는 중복 계산을 피하기 위해 순전파의 저장된 중간 값을 재사용한다는 점에 유의하십시오.
결과 중 하나는 역전파가 완료될 때까지 중간 값을 유지해야 한다는 것입니다.
이것은 훈련이 단순 예측보다 훨씬 더 많은 메모리를 필요로 하는 이유 중 하나이기도 합니다.
게다가 그러한 중간 값의 크기는 네트워크 레이어 수와 배치 크기에 대략 비례합니다.
따라서 더 큰 배치 크기를 사용하여 더 깊은 네트워크를 훈련하면 <em>메모리 부족(out-of-memory)</em> 오류가 더 쉽게 발생합니다.</p>
<h2 id="요약-summary-14"><a class="header" href="#요약-summary-14">요약 (Summary)</a></h2>
<p>순전파는 신경망에 의해 정의된 계산 그래프 내의 중간 변수를 순차적으로 계산하고 저장합니다. 입력 레이어에서 출력 레이어로 진행됩니다.
역전파는 신경망 내의 중간 변수와 파라미터의 기울기를 역순으로 순차적으로 계산하고 저장합니다.
딥러닝 모델을 훈련할 때 순전파와 역전파는 상호 의존적이며, 훈련은 예측보다 훨씬 더 많은 메모리를 필요로 합니다.</p>
<h2 id="연습-문제-exercises-16"><a class="header" href="#연습-문제-exercises-16">연습 문제 (Exercises)</a></h2>
<ol>
<li>어떤 스칼라 함수 $f$에 대한 입력 $\mathbf{X}$가 $n \times m$ 행렬이라고 가정합니다. $\mathbf{X}$에 대한 $f$의 기울기의 차원(dimensionality)은 무엇입니까?</li>
<li>이 섹션에서 설명한 모델의 은닉층에 편향을 추가하십시오(정규화 항에 편향을 포함할 필요는 없습니다).
<ol>
<li>해당 계산 그래프를 그리십시오.</li>
<li>순전파 및 역전파 방정식을 유도하십시오.</li>
</ol>
</li>
<li>이 섹션에서 설명한 모델의 훈련 및 예측에 대한 메모리 사용량을 계산하십시오.</li>
<li>2계 도함수를 계산하고 싶다고 가정합니다. 계산 그래프에 어떤 일이 발생합니까? 계산에 얼마나 걸릴 것으로 예상합니까?</li>
<li>계산 그래프가 GPU에 비해 너무 크다고 가정합니다.
<ol>
<li>하나 이상의 GPU에 분할할 수 있습니까?</li>
<li>더 작은 미니배치에서 훈련하는 것에 비해 장단점은 무엇입니까?</li>
</ol>
</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/102">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="수치적-안정성과-초기화-numerical-stability-and-initialization"><a class="header" href="#수치적-안정성과-초기화-numerical-stability-and-initialization">수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></h1>
<p>:label:<code>sec_numerical_stability</code></p>
<p>지금까지 우리가 구현한 모든 모델은 미리 지정된 분포에 따라 파라미터를 초기화해야 했습니다.
지금까지는 이러한 선택이 어떻게 이루어지는지에 대한 세부 사항을 얼버무리며 초기화 방식을 당연하게 여겼습니다.
이러한 선택이 특별히 중요하지 않다는 인상을 받았을 수도 있습니다.
반대로, 초기화 방식의 선택은 신경망 학습에서 중요한 역할을 하며, 수치적 안정성을 유지하는 데 결정적일 수 있습니다.
게다가 이러한 선택은 비선형 활성화 함수의 선택과 흥미로운 방식으로 연결될 수 있습니다.
우리가 선택한 함수와 파라미터 초기화 방식은 최적화 알고리즘이 얼마나 빨리 수렴하는지를 결정할 수 있습니다.
여기서 잘못된 선택을 하면 훈련 중에 기울기 폭발이나 소실을 겪을 수 있습니다.
이 섹션에서는 이러한 주제를 더 자세히 살펴보고 딥러닝 경력 내내 유용하게 사용할 수 있는 몇 가지 유용한 휴리스틱에 대해 논의합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
from jax import grad, vmap
</code></pre>
<h2 id="기울기-소실-및-폭발-vanishing-and-exploding-gradients"><a class="header" href="#기울기-소실-및-폭발-vanishing-and-exploding-gradients">기울기 소실 및 폭발 (Vanishing and Exploding Gradients)</a></h2>
<p>$L$개의 레이어, 입력 $\mathbf{x}$, 출력 $\mathbf{o}$를 가진 심층 네트워크를 고려해 봅시다.
각 레이어 $l$이 가중치 $\mathbf{W}^{(l)}$로 파라미터화된 변환 $f_l$에 의해 정의되고, 은닉층 출력이 $\mathbf{h}^{(l)}$ ($\mathbf{h}^{(0)} = \mathbf{x}$라고 가정)일 때, 네트워크는 다음과 같이 표현될 수 있습니다:</p>
<p>$$\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \textrm{ 따라서 } \mathbf{o} = f_L \circ \cdots \circ f_1(\mathbf{x}).$$</p>
<p>모든 은닉층 출력과 입력이 벡터인 경우, 임의의 파라미터 세트 $\mathbf{W}^{(l)}$에 대한 $\mathbf{o}$의 기울기를 다음과 같이 쓸 수 있습니다:</p>
<p>$$\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}<em>{ \mathbf{M}^{(L)} \stackrel{\textrm{def}}{=}} \cdots \underbrace{\partial</em>{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}<em>{ \mathbf{M}^{(l+1)} \stackrel{\textrm{def}}{=}} \underbrace{\partial</em>{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\textrm{def}}{=}}.$$</p>
<p>즉, 이 기울기는 $L-l$개의 행렬 $\mathbf{M}^{(L)} \cdots \mathbf{M}^{(l+1)}$과 기울기 벡터 $\mathbf{v}^{(l)}$의 곱입니다.
따라서 너무 많은 확률을 함께 곱할 때 종종 발생하는 수치적 언더플로 문제에 취약합니다.
확률을 다룰 때 일반적인 트릭은 로그 공간으로 전환하는 것입니다. 즉, 수치 표현의 압력을 가수(mantissa)에서 지수(exponent)로 옮기는 것입니다.
불행히도 위의 문제는 더 심각합니다. 처음에 행렬 $\mathbf{M}^{(l)}$은 다양한 고유값을 가질 수 있습니다.
그것들은 작거나 클 수 있으며, 그들의 곱은 <em>매우 크거나</em> <em>매우 작을</em> 수 있습니다.</p>
<p>불안정한 기울기로 인한 위험은 수치 표현을 넘어섭니다.
예측할 수 없는 크기의 기울기는 최적화 알고리즘의 안정성도 위협합니다.
우리는 (i) 지나치게 커서 모델을 파괴하는 파라미터 업데이트(<em>기울기 폭발</em> 문제);
또는 (ii) 지나치게 작아서 파라미터가 각 업데이트에서 거의 움직이지 않아 학습을 불가능하게 만드는 파라미터 업데이트(<em>기울기 소실</em> 문제)에 직면할 수 있습니다.</p>
<h3 id="기울기-소실-vanishing-gradients"><a class="header" href="#기울기-소실-vanishing-gradients">(<strong>기울기 소실 (Vanishing Gradients)</strong>)</a></h3>
<p>기울기 소실 문제를 일으키는 한 가지 빈번한 범인은 각 레이어의 선형 연산 뒤에 추가되는 활성화 함수 $\sigma$의 선택입니다.
역사적으로 시그모이드 함수 $1/(1 + \exp(-x))$ (:numref:<code>sec_mlp</code>에서 소개됨)는 임계값 함수와 유사하기 때문에 인기가 있었습니다.
초기 인공 신경망은 생물학적 신경망에서 영감을 얻었기 때문에, <em>완전히</em> 발화하거나 <em>전혀</em> 발화하지 않는 뉴런(생물학적 뉴런처럼)이라는 아이디어가 매력적으로 보였습니다.
시그모이드가 왜 기울기 소실을 일으킬 수 있는지 자세히 살펴봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
    y = npx.sigmoid(x)
y.backward()

d2l.plot(x, [y, x.grad], legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)
y.backward(torch.ones_like(x))

d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.Variable(tf.range(-8.0, 8.0, 0.1))
with tf.GradientTape() as t:
    y = tf.nn.sigmoid(x)
d2l.plot(x.numpy(), [y.numpy(), t.gradient(y, x).numpy()],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(-8.0, 8.0, 0.1)
y = jax.nn.sigmoid(x)
grad_sigmoid = vmap(grad(jax.nn.sigmoid))
d2l.plot(x, [y, grad_sigmoid(x)],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
</code></pre>
<p>보시다시피, (<strong>시그모이드의 기울기는 입력이 크거나 작을 때 모두 사라집니다</strong>).
게다가 많은 레이어를 통해 역전파할 때, 많은 시그모이드의 입력이 0에 가까운 골디락스 존(Goldilocks zone)에 있지 않는 한 전체 곱의 기울기가 사라질 수 있습니다.
네트워크가 많은 레이어를 자랑할 때, 주의하지 않으면 기울기가 어떤 레이어에서 잘릴 가능성이 높습니다.
실제로 이 문제는 심층 네트워크 훈련을 괴롭히곤 했습니다.
결과적으로 더 안정적인(하지만 신경적으로는 덜 그럴듯한) ReLU가 실무자들의 기본 선택으로 부상했습니다.</p>
<h3 id="기울기-폭발-exploding-gradients"><a class="header" href="#기울기-폭발-exploding-gradients">[<strong>기울기 폭발 (Exploding Gradients)</strong>]</a></h3>
<p>반대 문제인 기울기 폭발도 마찬가지로 골치 아플 수 있습니다.
이를 좀 더 잘 설명하기 위해 100개의 가우스 랜덤 행렬을 그려 초기 행렬과 곱합니다.
우리가 선택한 스케일(분산 $\sigma^2=1$ 선택)에 대해 행렬 곱이 폭발합니다.
심층 네트워크의 초기화로 인해 이런 일이 발생하면 경사 하강법 최적화기를 수렴시킬 기회가 없습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
M = np.random.normal(size=(4, 4))
print('단일 행렬', M)
for i in range(100):
    M = np.dot(M, np.random.normal(size=(4, 4)))
print('100개의 행렬을 곱한 후', M)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
M = torch.normal(0, 1, size=(4, 4))
print('단일 행렬 \n ',M)
for i in range(100):
    M = M @ torch.normal(0, 1, size=(4, 4))
print('100개의 행렬을 곱한 후\n', M)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
M = tf.random.normal((4, 4))
print('단일 행렬 \n ', M)
for i in range(100):
    M = tf.matmul(M, tf.random.normal((4, 4)))
print('100개의 행렬을 곱한 후\n', M.numpy())
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
get_key = lambda: jax.random.PRNGKey(d2l.get_seed())  # PRNG 키 생성
M = jax.random.normal(get_key(), (4, 4))
print('단일 행렬 \n ', M)
for i in range(100):
    M = jnp.matmul(M, jax.random.normal(get_key(), (4, 4)))
print('100개의 행렬을 곱한 후\n', M)
</code></pre>
<h3 id="대칭-깨기-breaking-the-symmetry"><a class="header" href="#대칭-깨기-breaking-the-symmetry">대칭 깨기 (Breaking the Symmetry)</a></h3>
<p>신경망 설계의 또 다른 문제는 파라미터화에 내재된 대칭성입니다.
하나의 은닉층과 두 개의 유닛이 있는 간단한 MLP가 있다고 가정해 봅시다.
이 경우 첫 번째 레이어의 가중치 $\mathbf{W}^{(1)}$을 치환하고 출력 레이어의 가중치를 마찬가지로 치환하여 동일한 함수를 얻을 수 있습니다.
첫 번째 은닉 유닛과 두 번째 은닉 유닛을 구별하는 특별한 것은 없습니다.
즉, 각 레이어의 은닉 유닛 간에는 순열 대칭(permutation symmetry)이 있습니다.</p>
<p>이것은 단순한 이론적 성가심 이상입니다.
앞서 언급한 두 개의 은닉 유닛이 있는 1-은닉층 MLP를 고려해 보십시오.
설명을 위해 출력 레이어가 두 은닉 유닛을 단 하나의 출력 유닛으로 변환한다고 가정해 보십시오.
은닉층의 모든 파라미터를 어떤 상수 $c$에 대해 $\mathbf{W}^{(1)} = c$로 초기화하면 어떤 일이 일어날지 상상해 보십시오.
이 경우 순전파 동안 두 은닉 유닛은 동일한 입력과 파라미터를 받아 동일한 활성화를 생성하고 출력 유닛으로 공급됩니다.
역전파 동안 출력 유닛을 파라미터 $\mathbf{W}^{(1)}$로 미분하면 모든 요소가 동일한 값을 갖는 기울기를 얻습니다.
따라서 경사 기반 반복(예: 미니배치 확률적 경사 하강법) 후에도 $\mathbf{W}^{(1)}$의 모든 요소는 여전히 동일한 값을 갖습니다.
이러한 반복은 스스로 <em>대칭을 깨뜨릴</em> 수 없으며 우리는 네트워크의 표현력을 결코 실현할 수 없을지도 모릅니다.
은닉층은 마치 단 하나의 유닛만 있는 것처럼 동작할 것입니다.
미니배치 확률적 경사 하강법은 이 대칭을 깨뜨리지 못하지만, 드롭아웃 정규화(나중에 소개됨)는 깨뜨릴 수 있다는 점에 유의하십시오!</p>
<h2 id="파라미터-초기화-parameter-initialization"><a class="header" href="#파라미터-초기화-parameter-initialization">파라미터 초기화 (Parameter Initialization)</a></h2>
<p>위에서 제기된 문제를 해결하거나 적어도 완화하는 한 가지 방법은 신중한 초기화를 통하는 것입니다.
나중에 보겠지만 최적화 중의 추가적인 주의와 적절한 정규화가 안정성을 더욱 향상시킬 수 있습니다.</p>
<h3 id="기본-초기화-default-initialization"><a class="header" href="#기본-초기화-default-initialization">기본 초기화 (Default Initialization)</a></h3>
<p>이전 섹션들, 예: :numref:<code>sec_linear_concise</code>에서 우리는 가중치 값을 초기화하기 위해 정규 분포를 사용했습니다.
초기화 방법을 지정하지 않으면 프레임워크는 기본 무작위 초기화 방법을 사용하며, 이는 중간 규모의 문제 크기에 대해 실제로 잘 작동하는 경우가 많습니다.</p>
<h3 id="xavier-초기화-xavier-initialization"><a class="header" href="#xavier-초기화-xavier-initialization">Xavier 초기화 (Xavier Initialization)</a></h3>
<p>:label:<code>subsec_xavier</code></p>
<p><em>비선형성 없는</em> 완전 연결 레이어에 대한 출력 $o_{i}$의 스케일 분포를 살펴봅시다.
이 레이어에 대해 $n_\textrm{in}$개의 입력 $x_j$와 관련 가중치 $w_{ij}$가 있을 때, 출력은 다음과 같이 주어집니다.</p>
<p>$$o_{i} = \sum_{j=1}^{n_\textrm{in}} w_{ij} x_j.$$</p>
<p>가중치 $w_{ij}$는 모두 동일한 분포에서 독립적으로 추출됩니다.
또한 이 분포가 평균 0과 분산 $\sigma^2$을 갖는다고 가정해 봅시다.
이것은 분포가 가우시안이어야 한다는 것을 의미하는 것이 아니라, 평균과 분산이 존재해야 함을 의미합니다.
지금은 레이어에 대한 입력 $x_j$도 평균 0과 분산 $\gamma^2$을 가지며 $w_{ij}$와 독립적이고 서로 독립적이라고 가정해 봅시다.
이 경우 $o_i$의 평균을 계산할 수 있습니다:</p>
<p>$$
\begin{aligned}
E[o_i] &amp; = \sum_{j=1}^{n_\textrm{in}} E[w_{ij} x_j] \&amp;= \sum_{j=1}^{n_\textrm{in}} E[w_{ij}] E[x_j] \&amp;= 0,
\end{aligned}
$$</p>
<p>그리고 분산:</p>
<p>$$
\begin{aligned}
\textrm{Var}[o_i] &amp; = E[o_i^2] - (E[o_i])^2 \        &amp; = \sum_{j=1}^{n_\textrm{in}} E[w^2_{ij} x^2_j] - 0 \        &amp; = \sum_{j=1}^{n_\textrm{in}} E[w^2_{ij}] E[x^2_j] \        &amp; = n_\textrm{in} \sigma^2 \gamma^2.
\end{aligned}
$$</p>
<p>분산을 고정하는 한 가지 방법은 $n_\textrm{in} \sigma^2 = 1$로 설정하는 것입니다.
이제 역전파를 고려해 봅시다.
거기서 우리는 출력에 더 가까운 레이어에서 전파되는 기울기와 함께 유사한 문제에 직면합니다.
순전파와 동일한 추론을 사용하여, $n_\textrm{out} \sigma^2 = 1$이 아닌 한 기울기의 분산이 폭발할 수 있음을 알 수 있습니다. 여기서 $n_\textrm{out}$은 이 레이어의 출력 수입니다.
이는 우리를 딜레마에 빠뜨립니다: 두 조건을 동시에 만족시키는 것은 불가능합니다.
대신 우리는 단순히 다음을 만족시키려고 노력합니다:</p>
<p>$$
\begin{aligned}
\frac{1}{2} (n_\textrm{in} + n_\textrm{out}) \sigma^2 = 1 \textrm{ 또는 동등하게 }
\sigma = \sqrt{\frac{2}{n_\textrm{in} + n_\textrm{out}}}.
\end{aligned}
$$</p>
<p>이것이 창시자 중 제1 저자의 이름을 딴 현재 표준이자 실용적으로 유익한 *Xavier 초기화(Xavier initialization)*의 기본 추론입니다 :cite:<code>Glorot.Bengio.2010</code>.
일반적으로 Xavier 초기화는 평균 0과 분산 $\sigma^2 = \frac{2}{n_\textrm{in} + n_\textrm{out}}$인 가우스 분포에서 가중치를 샘플링합니다.
우리는 또한 균등 분포에서 가중치를 샘플링할 때 분산을 선택하도록 이를 조정할 수 있습니다.
균등 분포 $U(-a, a)$는 분산 $\frac{a^2}{3}$을 가짐을 상기하십시오.
$\sigma^2$에 대한 조건에 $\frac{a^2}{3}$을 대입하면 다음에 따라 초기화하도록 유도됩니다.</p>
<p>$$U\left(-\sqrt{\frac{6}{n_\textrm{in} + n_\textrm{out}}}, \sqrt{\frac{6}{n_\textrm{in} + n_\textrm{out}}}\right).$$</p>
<p>비록 위의 수학적 추론에서 비선형성이 존재하지 않는다는 가정이 신경망에서 쉽게 위반될 수 있지만, Xavier 초기화 방법은 실제로 잘 작동하는 것으로 밝혀졌습니다.</p>
<h3 id="그-너머-beyond"><a class="header" href="#그-너머-beyond">그 너머 (Beyond)</a></h3>
<p>위의 추론은 파라미터 초기화에 대한 현대적 접근 방식의 겉핥기에 불과합니다.
딥러닝 프레임워크는 종종 12개 이상의 서로 다른 휴리스틱을 구현합니다.
게다가 파라미터 초기화는 딥러닝의 근본적인 연구 분야로 계속 남아 있습니다.
그중에는 묶인(공유된) 파라미터, 초해상도, 시퀀스 모델 및 기타 상황에 특화된 휴리스틱이 있습니다.
예를 들어 :citet:<code>Xiao.Bahri.Sohl-Dickstein.ea.2018</code>는 신중하게 설계된 초기화 방법을 사용하여 아키텍처 트릭 없이 10,000개 레이어 신경망을 훈련할 가능성을 보여주었습니다.</p>
<p>이 주제에 관심이 있다면 이 모듈의 제공 사항을 깊이 파고들고, 각 휴리스틱을 제안하고 분석한 논문을 읽은 다음, 해당 주제에 대한 최신 간행물을 탐색할 것을 제안합니다.
어쩌면 기발한 아이디어를 우연히 발견하거나 발명하여 딥러닝 프레임워크에 구현을 기여할 수도 있을 것입니다.</p>
<h2 id="요약-summary-15"><a class="header" href="#요약-summary-15">요약 (Summary)</a></h2>
<p>기울기 소실 및 폭발은 심층 네트워크에서 흔한 문제들입니다. 기울기와 파라미터가 잘 제어되도록 하려면 파라미터 초기화에 각별한 주의가 필요합니다.
초기 기울기가 너무 크거나 작지 않도록 초기화 휴리스틱이 필요합니다.
무작위 초기화는 최적화 전에 대칭이 깨지도록 하는 데 핵심적입니다.
Xavier 초기화는 각 레이어에 대해 출력의 분산이 입력 수에 영향을 받지 않고, 기울기의 분산이 출력 수에 영향을 받지 않을 것을 제안합니다.
ReLU 활성화 함수는 기울기 소실 문제를 완화합니다. 이는 수렴을 가속화할 수 있습니다.</p>
<h2 id="연습-문제-exercises-17"><a class="header" href="#연습-문제-exercises-17">연습 문제 (Exercises)</a></h2>
<ol>
<li>MLP 레이어의 순열 대칭 외에 깨져야 할 대칭을 보일 수 있는 신경망의 다른 사례를 설계할 수 있습니까?</li>
<li>선형 회귀나 소프트맥스 회귀의 모든 가중치 파라미터를 동일한 값으로 초기화할 수 있습니까?</li>
<li>두 행렬 곱의 고유값에 대한 해석적 경계를 찾아보십시오. 이것은 기울기가 잘 조건화되도록 보장하는 것에 대해 무엇을 알려줍니까?</li>
<li>일부 항이 발산한다는 것을 안다면 사후에 이를 고칠 수 있습니까? 영감을 얻으려면 계층별 적응형 속도 스케일링(layerwise adaptive rate scaling)에 대한 논문을 살펴보십시오 :cite:<code>You.Gitman.Ginsburg.2017</code>.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/103">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/104">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/235">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17986">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="딥러닝에서의-일반화-generalization-in-deep-learning"><a class="header" href="#딥러닝에서의-일반화-generalization-in-deep-learning">딥러닝에서의 일반화 (Generalization in Deep Learning)</a></h1>
<p>:numref:<code>chap_regression</code>과 :numref:<code>chap_classification</code>에서, 우리는 선형 모델을 훈련 데이터에 맞춤으로써 회귀 및 분류 문제를 다루었습니다.
두 경우 모두 관찰된 훈련 레이블의 우도를 최대화하는 파라미터를 찾기 위한 실용적인 알고리즘을 제공했습니다.
그리고 각 장의 끝부분에서, 우리는 훈련 데이터를 맞추는 것이 중간 목표일 뿐임을 상기했습니다.
우리의 진짜 탐구는 내내 동일한 기저 모집단에서 추출된 새로운 예제에서도 정확한 예측을 할 수 있는 기반이 되는 <em>일반적인 패턴</em>을 발견하는 것이었습니다.
머신러닝 연구자들은 최적화 알고리즘의 <em>소비자</em>입니다.
때때로 우리는 새로운 최적화 알고리즘을 개발해야 합니다.
하지만 결국 최적화는 목적을 위한 수단일 뿐입니다.
핵심적으로 머신러닝은 통계적 학문이며, 우리는 어떤 통계적 원리(알려지거나 알려지지 않은)가 결과 모델을 훈련 세트 너머로 일반화하도록 이끄는 한에서만 훈련 손실을 최적화하기를 원합니다.</p>
<p>긍정적인 측면에서, 확률적 경사 하강법으로 훈련된 심층 신경망은 컴퓨터 비전, 자연어 처리, 시계열 데이터, 추천 시스템, 전자 건강 기록, 단백질 접힘, 비디오 게임 및 보드게임의 가치 함수 근사화, 그리고 수많은 다른 도메인에 걸친 무수한 예측 문제에서 놀라울 정도로 잘 일반화되는 것으로 밝혀졌습니다.
부정적인 측면에서, 최적화 이야기(왜 우리가 훈련 데이터에 맞출 수 있는지)나 일반화 이야기(왜 결과 모델이 보지 못한 예제로 일반화되는지)에 대한 직접적인 설명을 찾고 있다면 실망할지도 모릅니다.
선형 모델을 최적화하는 절차와 솔루션의 통계적 속성은 포괄적인 이론 체계에 의해 잘 설명되지만, 딥러닝에 대한 우리의 이해는 두 전선 모두에서 여전히 서부 개척 시대와 비슷합니다.</p>
<p>딥러닝의 이론과 실제는 빠르게 진화하고 있습니다.
이론가들은 무슨 일이 일어나고 있는지 설명하기 위해 새로운 전략을 채택하고 있으며, 실무자들은 맹렬한 속도로 혁신을 계속하며 심층 네트워크 훈련을 위한 휴리스틱의 무기고와 어떤 상황에 어떤 기술을 적용할지 결정하기 위한 지침을 제공하는 직관 및 민간 지식 체계를 구축하고 있습니다.</p>
<p>현 시점의 요약은 딥러닝 이론이 유망한 공격 라인과 흩어진 매혹적인 결과들을 낳았지만, (i) 왜 신경망을 최적화할 수 있는지, (ii) 경사 하강법으로 학습된 모델이 고차원 작업에서도 어떻게 그렇게 잘 일반화되는지에 대한 포괄적인 설명과는 여전히 거리가 멀어 보인다는 것입니다.
하지만 실제로는 (i)이 거의 문제가 되지 않으며(우리는 항상 모든 훈련 데이터에 맞는 파라미터를 찾을 수 있음), 따라서 일반화를 이해하는 것이 훨씬 더 큰 문제입니다.
반면에 일관된 과학적 이론의 위안이 없더라도, 실무자들은 실제 상황에서 잘 일반화되는 모델을 생성하는 데 도움이 될 수 있는 방대한 기술 컬렉션을 개발했습니다.
어떤 간결한 요약도 딥러닝의 일반화라는 방대한 주제를 제대로 다룰 수 없으며 전반적인 연구 상태가 해결된 것과는 거리가 멀지만, 우리는 이 섹션에서 연구 및 실제 상태에 대한 광범위한 개요를 제시하고자 합니다.</p>
<h2 id="과대적합-및-정규화-다시-보기-revisiting-overfitting-and-regularization"><a class="header" href="#과대적합-및-정규화-다시-보기-revisiting-overfitting-and-regularization">과대적합 및 정규화 다시 보기 (Revisiting Overfitting and Regularization)</a></h2>
<p>:citet:<code>wolpert1995no</code>의 "공짜 점심은 없다(no free lunch)" 정리에 따르면, 어떤 학습 알고리즘은 특정 분포를 가진 데이터에서는 더 잘 일반화하고 다른 분포에서는 더 나쁘게 일반화합니다.
따라서 유한한 훈련 세트가 주어졌을 때, 모델은 특정 가정에 의존합니다: 인간 수준의 성능을 달성하려면 인간이 세상을 생각하는 방식을 반영하는 *귀납적 편향(inductive biases)*을 식별하는 것이 유용할 수 있습니다.
그러한 귀납적 편향은 특정 속성을 가진 솔루션에 대한 선호도를 보여줍니다.
예를 들어 심층 MLP는 더 단순한 함수들의 합성으로 복잡한 함수를 구축하는 것에 대한 귀납적 편향을 가지고 있습니다.</p>
<p>귀납적 편향을 인코딩하는 머신러닝 모델을 사용하여 훈련하는 접근 방식은 일반적으로 두 단계로 구성됩니다: (i) 훈련 데이터를 맞춤; (ii) 홀드아웃 데이터에서 모델을 평가하여 <em>일반화 오차</em>(기저 모집단에 대한 실제 오차)를 추정함.
훈련 데이터에 대한 적합도와 테스트 데이터에 대한 적합도의 차이를 *일반화 갭(generalization gap)*이라고 하며, 이것이 클 때 우리는 모델이 훈련 데이터에 *과대적합(overfit)*되었다고 말합니다.
과대적합의 극단적인 경우, 테스트 오차가 여전히 상당한데도 훈련 데이터에 정확히 맞출 수 있습니다.
고전적인 관점에서 해석하자면 우리 모델이 너무 복잡하여 특성 수, 학습된 0이 아닌 파라미터 수, 또는 정량화된 파라미터 크기를 줄여야 한다는 것입니다.
:numref:<code>sec_generalization_basics</code>의 모델 복잡도 대 손실 플롯(:numref:<code>fig_capacity_vs_error</code>)을 상기해 보십시오.</p>
<p>그러나 딥러닝은 직관에 반하는 방식으로 이 그림을 복잡하게 만듭니다.
첫째, 분류 문제의 경우 우리 모델은 일반적으로 수백만 개로 구성된 데이터셋에서도 모든 훈련 예제를 완벽하게 맞출 수 있을 만큼 표현력이 뛰어납니다 :cite:<code>zhang2021understanding</code>.
고전적인 그림에서 우리는 이 설정이 모델 복잡도 축의 맨 오른쪽 끝에 있으며, 일반화 오차의 개선은 모델 클래스의 복잡도를 줄이거나 파라미터가 취할 수 있는 값 집합을 엄격하게 제한하는 페널티를 적용하는 등 정규화를 통해서만 이루어져야 한다고 생각할 수 있습니다.
하지만 여기서부터 이상해지기 시작합니다.</p>
<p>이상하게도 많은 딥러닝 작업(예: 이미지 인식 및 텍스트 분류)에서 우리는 일반적으로 모델 아키텍처 중에서 선택하는데, 그들 모두가 임의로 낮은 훈련 손실(및 0의 훈련 오차)을 달성할 수 있습니다.
고려 중인 모든 모델이 0의 훈련 오차를 달성하기 때문에, <em>추가적인 이득을 위한 유일한 길은 과대적합을 줄이는 것입니다</em>.
더욱 이상한 점은 훈련 데이터를 완벽하게 맞추었음에도 불구하고, 레이어나 노드를 추가하거나 더 많은 에폭 동안 훈련하는 등 모델을 <em>더욱 표현력 있게</em> 만듦으로써 실제로 <em>일반화 오차를 줄일</em> 수 있는 경우가 많다는 것입니다.
더욱 이상한 것은 일반화 갭과 (예를 들어 네트워크의 깊이나 너비로 포착되는) 모델의 <em>복잡도</em> 사이의 관계 패턴이 비단조적일 수 있다는 것입니다. 더 큰 복잡도가 처음에는 해를 끼치지만 이후에는 소위 "이중 하강(double-descent)" 패턴으로 도움을 줍니다 :cite:<code>nakkiran2021deep</code>.
따라서 딥러닝 실무자는 몇 가지는 모델을 어떤 식으로든 제한하는 것처럼 보이고 다른 몇 가지는 모델을 더 표현력 있게 만드는 것처럼 보이는, 하지만 어떤 의미에서는 모두 과대적합을 완화하기 위해 적용되는 요령 꾸러미를 가지고 있습니다.</p>
<p>상황을 더욱 복잡하게 만드는 것은, 고전적인 학습 이론이 제공하는 보장이 고전적인 모델에 대해서도 보수적일 수 있는 반면, 심층 신경망이 애초에 왜 일반화되는지 설명하는 데는 무력해 보인다는 것입니다.
심층 신경망은 대규모 데이터셋에 대해서도 임의의 레이블을 맞출 수 있으며 $\ell_2$ 정규화와 같은 익숙한 방법을 사용함에도 불구하고, VC 차원이나 가설 클래스의 라데마허(Rademacher) 복잡도에 기반한 것과 같은 전통적인 복잡도 기반 일반화 경계는 신경망이 왜 일반화되는지 설명할 수 없습니다.</p>
<h2 id="비모수nonparametrics로부터의-영감-inspiration-from-nonparametrics"><a class="header" href="#비모수nonparametrics로부터의-영감-inspiration-from-nonparametrics">비모수(Nonparametrics)로부터의 영감 (Inspiration from Nonparametrics)</a></h2>
<p>딥러닝에 처음 접근할 때, 이들을 파라메트릭 모델로 생각하고 싶어집니다.
결국 모델에는 수백만 개의 파라미터가 <em>있습니다</em>.
모델을 업데이트할 때 파라미터를 업데이트합니다.
모델을 저장할 때 파라미터를 디스크에 씁니다.
하지만 수학과 컴퓨터 과학은 직관에 반하는 관점의 변화와 겉보기에 다른 문제들 사이의 놀라운 동형성(isomorphisms)으로 가득 차 있습니다.
신경망이 분명히 파라미터를 <em>가지고</em> 있지만, 어떤 면에서는 비모수 모델처럼 행동한다고 생각하는 것이 더 유익할 수 있습니다.
그렇다면 모델을 비모수적으로 만드는 것은 정확히 무엇입니까?
이 이름이 다양한 접근 방식을 포괄하지만, 공통된 주제 하나는 비모수 방법이 사용 가능한 데이터 양이 증가함에 따라 복잡도 수준이 증가하는 경향이 있다는 것입니다.</p>
<p>아마도 비모수 모델의 가장 간단한 예는 $k$-최근접 이웃 알고리즘일 것입니다(나중에 예를 들어 :numref:<code>sec_attention-pooling</code>에서 더 많은 비모수 모델을 다룰 것입니다).
여기서 학습자는 훈련 시 단순히 데이터셋을 암기합니다.
그런 다음 예측 시 새로운 점 $\mathbf{x}$에 직면했을 때, 학습자는 $k$개의 가장 가까운 이웃(어떤 거리 $d(\mathbf{x}, \mathbf{x}_i')$를 최소화하는 $k$개의 점 $\mathbf{x}_i'$)을 찾습니다.
$k=1$일 때 이 알고리즘을 1-최근접 이웃이라고 하며, 알고리즘은 항상 0의 훈련 오차를 달성할 것입니다.
그렇다고 해서 알고리즘이 일반화되지 않는다는 의미는 아닙니다.
사실 몇 가지 완만한 조건 하에서 1-최근접 이웃 알고리즘은 일관성이 있음(결국 최적의 예측기로 수렴함)이 밝혀졌습니다.</p>
<p>1-최근접 이웃은 거리 함수 $d$를 지정하거나 동등하게 데이터를 특성 화하기 위한 벡터 값 기저 함수 $\phi(\mathbf{x})$를 지정해야 한다는 점에 유의하십시오.
거리 지표의 어떤 선택에 대해서도 0의 훈련 오차를 달성하고 결국 최적의 예측기에 도달하겠지만, 서로 다른 거리 지표 $d$는 서로 다른 귀납적 편향을 인코딩하며 유한한 양의 데이터로는 서로 다른 예측기를 산출할 것입니다.
거리 지표 $d$의 다른 선택은 기저 패턴에 대한 다른 가정을 나타내며, 다른 예측기의 성능은 가정이 관찰된 데이터와 얼마나 호환되는지에 달려 있습니다.</p>
<p>어떤 의미에서 신경망은 훈련 데이터를 맞추는 데 필요한 것보다 훨씬 더 많은 파라미터를 보유하고 있어 과도하게 파라미터화(over-parametrized)되어 있기 때문에, 훈련 데이터를 *보간(interpolate)*하는(완벽하게 맞추는) 경향이 있으며 따라서 어떤 면에서는 비모수 모델처럼 행동합니다.
보다 최근의 이론적 연구는 대규모 신경망과 비모수 방법, 특히 커널 방법 사이의 깊은 연결을 확립했습니다.
특히 :citet:<code>Jacot.Grabriel.Hongler.2018</code>는 무작위로 초기화된 가중치를 가진 다층 퍼셉트론이 무한히 넓어짐에 따라 극한에서 특정 커널 함수(본질적으로 거리 함수) 선택에 대한 (비모수) 커널 방법과 동일해짐을 입증했습니다. 그들은 이를 신경 탄젠트 커널(neural tangent kernel)이라고 부릅니다.
현재의 신경 탄젠트 커널 모델이 현대 심층 네트워크의 동작을 완전히 설명하지 못할 수도 있지만, 분석 도구로서의 성공은 과도하게 파라미터화된 심층 네트워크의 동작을 이해하는 데 있어 비모수 모델링의 유용성을 강조합니다.</p>
<h2 id="조기-종료-early-stopping"><a class="header" href="#조기-종료-early-stopping">조기 종료 (Early Stopping)</a></h2>
<p>심층 신경망은 레이블이 잘못되거나 무작위로 할당된 경우에도 임의의 레이블을 맞출 수 있지만 :cite:<code>zhang2021understanding</code>, 이 능력은 훈련의 많은 반복에 걸쳐서만 나타납니다.
새로운 연구 라인 :cite:<code>Rolnick.Veit.Belongie.Shavit.2017</code>은 레이블 노이즈 설정에서 신경망이 깨끗하게 레이블이 지정된 데이터를 먼저 맞추고 나중에야 잘못 레이블이 지정된 데이터를 보간하는 경향이 있음을 밝혔습니다.
더욱이 이 현상은 일반화에 대한 보장으로 직접 변환됨이 확립되었습니다: 모델이 깨끗하게 레이블이 지정된 데이터를 맞추었지만 훈련 세트에 포함된 무작위로 레이블이 지정된 예제는 맞추지 않은 경우, 실제로 일반화된 것입니다 :cite:<code>Garg.Balakrishnan.Kolter.Lipton.2021</code>.</p>
<p>이러한 발견들은 함께 심층 신경망을 정규화하는 고전적인 기술인 *조기 종료(early stopping)*에 동기를 부여하는 데 도움이 됩니다.
여기서 가중치 값을 직접 제한하는 대신 훈련 에폭 수를 제한합니다.
종료 기준을 결정하는 가장 일반적인 방법은 훈련 내내 검증 오차를 모니터링하고(일반적으로 각 에폭 후 한 번씩 확인), 검증 오차가 일정 에폭 수 동안 어떤 작은 양 $\epsilon$ 이상 감소하지 않으면 훈련을 중단하는 것입니다.
이를 때때로 *인내 기준(patience criterion)*이라고 합니다.
노이즈가 섞인 레이블 설정에서 더 나은 일반화로 이어질 수 있는 잠재력 외에도, 조기 종료의 또 다른 이점은 절약된 시간입니다.
인내 기준이 충족되면 훈련을 종료할 수 있습니다.
8개 이상의 GPU에서 동시에 며칠 동안 훈련해야 할 수도 있는 대형 모델의 경우, 잘 튜닝된 조기 종료는 연구자의 시간을 며칠 절약하고 고용주에게 수천 달러를 절약해 줄 수 있습니다.</p>
<p>특히 레이블 노이즈가 없고 데이터셋이 *실현 가능(realizable)*할 때(클래스가 진정으로 분리 가능할 때, 예: 고양이와 개 구별), 조기 종료는 일반화에 큰 개선을 가져오지 않는 경향이 있습니다.
반면에 레이블 노이즈가 있거나 레이블에 내재적 변동성이 있는 경우(예: 환자의 사망률 예측), 조기 종료는 중요합니다.
노이즈가 섞인 데이터를 보간할 때까지 모델을 훈련하는 것은 일반적으로 나쁜 생각입니다.</p>
<h2 id="심층-네트워크를-위한-고전적-정규화-방법-classical-regularization-methods-for-deep-networks"><a class="header" href="#심층-네트워크를-위한-고전적-정규화-방법-classical-regularization-methods-for-deep-networks">심층 네트워크를 위한 고전적 정규화 방법 (Classical Regularization Methods for Deep Networks)</a></h2>
<p>:numref:<code>chap_regression</code>에서 우리는 모델의 복잡도를 제한하기 위한 몇 가지 고전적인 정규화 기술을 설명했습니다.
특히 :numref:<code>sec_weight_decay</code>는 가중치의 큰 값에 페널티를 주기 위해 손실 함수에 정규화 항을 추가하는 가중치 감쇠라는 방법을 소개했습니다.
어떤 가중치 노름이 페널티를 받느냐에 따라 이 기술은 릿지 정규화($\ell_2$ 페널티) 또는 라소 정규화($\ell_1$ 페널티)로 알려져 있습니다.
이러한 정규화 기법에 대한 고전적 분석에서는 가중치가 취할 수 있는 값에 대해 모델이 임의의 레이블을 맞추지 못하게 할 만큼 충분히 제한적인 것으로 간주됩니다.</p>
<p>딥러닝 구현에서 가중치 감쇠는 여전히 인기 있는 도구입니다.
하지만 연구자들은 $\ell_2$ 정규화의 일반적인 강도가 네트워크가 데이터를 보간하는 것을 방지하기에 불충분하다는 점에 주목했습니다 :cite:<code>zhang2021understanding</code>. 따라서 정규화로 해석될 경우 그 이점은 조기 종료 기준과 결합될 때만 의미가 있을 수 있습니다.
조기 종료가 없다면, 레이어 수나 노드 수(딥러닝에서) 또는 거리 지표(1-최근접 이웃에서)와 마찬가지로, 이러한 방법들이 신경망의 힘을 의미 있게 제한하기 때문이 아니라 오히려 관심 있는 데이터셋에서 발견되는 패턴과 더 잘 호환되는 귀납적 편향을 인코딩하기 때문에 더 나은 일반화로 이어질 가능성이 있습니다.
따라서 고전적인 정규화 기법은 효능에 대한 이론적 근거가 근본적으로 다를 수 있더라도 딥러닝 구현에서 여전히 인기가 있습니다.</p>
<p>특히 딥러닝 연구자들은 모델 입력에 노이즈를 추가하는 것과 같이 고전적인 정규화 맥락에서 처음 대중화된 기술을 기반으로 구축해 왔습니다.
다음 섹션에서는 (:citet:<code>Srivastava.Hinton.Krizhevsky.ea.2014</code>에 의해 발명된) 유명한 드롭아웃(dropout) 기술을 소개할 것입니다. 이 기술은 효능에 대한 이론적 근거가 비슷하게 미스터리함에도 불구하고 딥러닝의 주류가 되었습니다.</p>
<h2 id="요약-summary-16"><a class="header" href="#요약-summary-16">요약 (Summary)</a></h2>
<p>예제보다 파라미터가 적은 경향이 있는 고전적인 선형 모델과 달리, 심층 네트워크는 과도하게 파라미터화되는 경향이 있으며 대부분의 작업에서 훈련 세트를 완벽하게 맞출 수 있습니다.
이 *보간 체제(interpolation regime)*는 많은 굳건한 직관에 도전합니다.
기능적으로 신경망은 파라메트릭 모델처럼 보입니다.
하지만 비모수 모델로 생각하는 것이 때로는 더 신뢰할 수 있는 직관의 원천이 될 수 있습니다.
고려 중인 모든 심층 네트워크가 모든 훈련 레이블을 맞출 수 있는 경우가 많기 때문에, 거의 모든 이득은 과대적합을 완화( <em>일반화 갭</em> 줄이기)함으로써 와야 합니다.
역설적이게도 일반화 갭을 줄이는 개입은 때로는 모델 복잡도를 증가시키는 것처럼 보이고 다른 때에는 복잡도를 감소시키는 것처럼 보입니다.
그러나 이러한 방법들이 고전 이론이 심층 네트워크의 일반화를 설명할 수 있을 만큼 복잡도를 충분히 감소시키는 경우는 드물며, <em>왜 특정 선택이 향상된 일반화로 이어지는지</em>는 많은 뛰어난 연구자들의 헌신적인 노력에도 불구하고 대부분 거대한 미해결 문제로 남아 있습니다.</p>
<h2 id="연습-문제-exercises-18"><a class="header" href="#연습-문제-exercises-18">연습 문제 (Exercises)</a></h2>
<ol>
<li>전통적인 복잡도 기반 측정은 어떤 의미에서 심층 신경망의 일반화를 설명하지 못합니까?</li>
<li><em>조기 종료</em>가 정규화 기술로 간주될 수 있는 이유는 무엇입니까?</li>
<li>연구자들은 일반적으로 종료 기준을 어떻게 결정합니까?</li>
<li>조기 종료가 일반화에 큰 개선을 가져오는 경우를 구별하는 중요한 요인은 무엇입니까?</li>
<li>일반화 외에 조기 종료의 다른 이점을 설명하십시오.</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/7473">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="드롭아웃-dropout"><a class="header" href="#드롭아웃-dropout">드롭아웃 (Dropout)</a></h1>
<p>:label:<code>sec_dropout</code></p>
<p>우리가 좋은 예측 모델에서 기대하는 것이 무엇인지 잠시 생각해 봅시다.
우리는 보지 못한 데이터에서 잘 수행되기를 원합니다.
고전적인 일반화 이론은 훈련 성능과 테스트 성능 사이의 격차를 줄이기 위해 단순한 모델을 목표로 해야 한다고 제안합니다.
단순함은 차원의 수가 적은 형태로 올 수 있습니다.
우리는 :numref:<code>sec_generalization_basics</code>에서 선형 모델의 단항 기저 함수를 논의할 때 이것을 탐구했습니다.
또한 :numref:<code>sec_weight_decay</code>에서 가중치 감쇠($\ell_2$ 정규화)를 논의할 때 보았듯이 파라미터의 (역) 노름도 단순함의 유용한 척도를 나타냅니다.
단순함의 또 다른 유용한 개념은 부드러움(smoothness), 즉 함수가 입력의 작은 변화에 민감하지 않아야 한다는 것입니다.
예를 들어 이미지를 분류할 때 픽셀에 약간의 무작위 노이즈를 추가해도 대부분 무해할 것으로 예상합니다.</p>
<p>:citet:<code>Bishop.1995</code>는 입력 노이즈를 사용한 훈련이 티호노프(Tikhonov) 정규화와 동일하다는 것을 증명하면서 이 아이디어를 공식화했습니다.
이 작업은 함수가 부드러워야 한다(따라서 단순해야 한다)는 요구 사항과 입력의 섭동(perturbation)에 탄력적이어야 한다는 요구 사항 사이에 명확한 수학적 연결을 도출했습니다.</p>
<p>그 후, :citet:<code>Srivastava.Hinton.Krizhevsky.ea.2014</code>는 Bishop의 아이디어를 네트워크의 내부 레이어에도 적용하는 영리한 아이디어를 개발했습니다.
*드롭아웃(dropout)*이라고 불리는 그들의 아이디어는 순전파 동안 각 내부 레이어를 계산하는 동안 노이즈를 주입하는 것을 포함하며, 신경망 훈련을 위한 표준 기술이 되었습니다.
이 방법은 훈련 중에 문자 그대로 일부 뉴런을 <em>떨어뜨리기(drop out)</em> 때문에 <em>드롭아웃</em>이라고 불립니다.
훈련 전반에 걸쳐 각 반복에서 표준 드롭아웃은 다음 레이어를 계산하기 전에 각 레이어의 노드 중 일부를 0으로 만드는 것으로 구성됩니다.</p>
<p>분명히 하자면, 우리는 Bishop과의 연결을 통해 우리 자신의 내러티브를 강요하고 있습니다.
드롭아웃에 대한 원본 논문은 유성 생식에 대한 놀라운 비유를 통해 직관을 제공합니다.
저자들은 신경망 과대적합이 각 레이어가 이전 레이어의 특정 활성화 패턴에 의존하는 상태로 특징지어지며, 이 상태를 *공동 적응(co-adaptation)*이라고 부릅니다.
그들은 유성 생식이 공동 적응 유전자를 분해한다고 주장되는 것처럼 드롭아웃이 공동 적응을 분해한다고 주장합니다.
이 이론에 대한 정당성은 확실히 논쟁의 여지가 있지만, 드롭아웃 기술 자체는 지속성이 있음이 입증되었으며 대부분의 딥러닝 라이브러리에 다양한 형태의 드롭아웃이 구현되어 있습니다.</p>
<p>핵심 과제는 이 노이즈를 어떻게 주입하느냐입니다.
한 가지 아이디어는 <em>편향되지 않은(unbiased)</em> 방식으로 주입하여 각 레이어의 기댓값(다른 레이어는 고정된 상태에서)이 노이즈가 없을 때 취했을 값과 같아지도록 하는 것입니다.
Bishop의 연구에서 그는 선형 모델의 입력에 가우스 노이즈를 추가했습니다.
각 훈련 반복에서 그는 평균이 0인 분포 $\epsilon \sim \mathcal{N}(0,\sigma^2)$에서 샘플링된 노이즈를 입력 $\mathbf{x}$에 추가하여 섭동된 점 $\mathbf{x}' = \mathbf{x} + \epsilon$을 산출했습니다.
기댓값에서 $E[\mathbf{x}'] = \mathbf{x}$입니다.</p>
<p>표준 드롭아웃 정규화에서는 각 레이어의 노드 중 일부를 0으로 만든 다음 유지된(떨어뜨리지 않은) 노드의 비율로 정규화하여 각 레이어의 편향을 *제거(debiases)*합니다.
즉, <em>드롭아웃 확률</em> $p$를 사용하여 각 중간 활성화 $h$는 다음과 같이 확률 변수 $h'$로 대체됩니다.</p>
<p>$$
\begin{aligned}
h' =
\begin{cases}
0 &amp; \textrm{ 확률 } p \
\frac{h}{1-p} &amp; \textrm{ 그 외}
\end{cases}
\end{aligned}
$$</p>
<p>설계상 기댓값은 변경되지 않습니다. 즉, $E[h'] = h$.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from functools import partial
import jax
from jax import numpy as jnp
import optax
</code></pre>
<h2 id="실제-드롭아웃-dropout-in-practice"><a class="header" href="#실제-드롭아웃-dropout-in-practice">실제 드롭아웃 (Dropout in Practice)</a></h2>
<p>:numref:<code>fig_mlp</code>의 은닉층과 5개의 은닉 유닛이 있는 MLP를 상기해 보십시오.
은닉층에 드롭아웃을 적용하여 각 은닉 유닛을 확률 $p$로 0으로 만들면, 그 결과는 원래 뉴런의 부분 집합만 포함하는 네트워크로 볼 수 있습니다.
:numref:<code>fig_dropout2</code>에서 $h_2$와 $h_5$가 제거되었습니다.
결과적으로 출력의 계산은 더 이상 $h_2$나 $h_5$에 의존하지 않으며 역전파를 수행할 때 각각의 기울기도 사라집니다.
이런 식으로 출력 레이어의 계산은 $h_1, \ldots, h_5$의 어느 한 요소에 과도하게 의존할 수 없습니다.</p>
<p><img src="chapter_multilayer-perceptrons/../img/dropout2.svg" alt="드롭아웃 전후의 MLP." />
:label:<code>fig_dropout2</code></p>
<p>일반적으로 테스트 시에는 드롭아웃을 비활성화합니다.
훈련된 모델과 새로운 예제가 주어지면, 우리는 어떤 노드도 떨어뜨리지 않으므로 정규화할 필요도 없습니다.
그러나 몇 가지 예외가 있습니다: 일부 연구자들은 신경망 예측의 <em>불확실성</em>을 추정하기 위한 휴리스틱으로 테스트 시 드롭아웃을 사용합니다: 예측이 서로 다른 많은 드롭아웃 출력에 걸쳐 일치한다면 네트워크가 더 확신한다고 말할 수 있습니다.</p>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-2"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-2">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>단일 레이어에 대한 드롭아웃 함수를 구현하려면 레이어가 가진 차원 수만큼 베르누이(이진) 확률 변수에서 샘플을 추출해야 합니다.
여기서 확률 변수는 확률 $1-p$로 값 $1$(유지)을, 확률 $p$로 $0$(제거)을 취합니다.
이를 구현하는 쉬운 방법 중 하나는 먼저 균등 분포 $U[0, 1]$에서 샘플을 추출하는 것입니다.
그런 다음 해당 샘플이 $p$보다 큰 노드를 유지하고 나머지는 떨어뜨릴 수 있습니다.</p>
<p>다음 코드에서 우리는 위에서 설명한 대로 나머지를 재조정하여 (<strong>확률 <code>dropout</code>으로 텐서 입력 <code>X</code>의 요소를 떨어뜨리는 <code>dropout_layer</code> 함수를 구현</strong>)합니다: 생존자들을 <code>1.0-dropout</code>으로 나눕니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def dropout_layer(X, dropout):
    assert 0 &lt;= dropout &lt;= 1
    if dropout == 1: return np.zeros_like(X)
    mask = np.random.uniform(0, 1, X.shape) &gt; dropout
    return mask.astype(np.float32) * X / (1.0 - dropout)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def dropout_layer(X, dropout):
    assert 0 &lt;= dropout &lt;= 1
    if dropout == 1: return torch.zeros_like(X)
    mask = (torch.rand(X.shape) &gt; dropout).float()
    return mask * X / (1.0 - dropout)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def dropout_layer(X, dropout):
    assert 0 &lt;= dropout &lt;= 1
    if dropout == 1: return tf.zeros_like(X)
    mask = tf.random.uniform(
        shape=tf.shape(X), minval=0, maxval=1) &lt; 1 - dropout
    return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def dropout_layer(X, dropout, key=d2l.get_key()):
    assert 0 &lt;= dropout &lt;= 1
    if dropout == 1: return jnp.zeros_like(X)
    mask = jax.random.uniform(key, X.shape) &gt; dropout
    return jnp.asarray(mask, dtype=jnp.float32) * X / (1.0 - dropout)
</code></pre>
<p>우리는 [<strong>몇 가지 예제에서 <code>dropout_layer</code> 함수를 테스트</strong>]할 수 있습니다.
다음 코드 줄에서는 입력 <code>X</code>를 각각 확률 0, 0.5, 1로 드롭아웃 연산에 통과시킵니다.</p>
<pre><code class="language-{.python .input}">%%tab all
if tab.selected('mxnet'):
    X = np.arange(16).reshape(2, 8)
if tab.selected('pytorch'):
    X = torch.arange(16, dtype = torch.float32).reshape((2, 8))
if tab.selected('tensorflow'):
    X = tf.reshape(tf.range(16, dtype=tf.float32), (2, 8))
if tab.selected('jax'):
    X = jnp.arange(16, dtype=jnp.float32).reshape(2, 8)
print('dropout_p = 0:', dropout_layer(X, 0))
print('dropout_p = 0.5:', dropout_layer(X, 0.5))
print('dropout_p = 1:', dropout_layer(X, 1))
</code></pre>
<h3 id="모델-정의하기-1"><a class="header" href="#모델-정의하기-1">모델 정의하기</a></h3>
<p>아래 모델은 각 은닉층의 출력(활성화 함수 다음)에 드롭아웃을 적용합니다.
각 레이어에 대해 드롭아웃 확률을 별도로 설정할 수 있습니다.
일반적인 선택은 입력 레이어에 가까울수록 낮은 드롭아웃 확률을 설정하는 것입니다.
우리는 훈련 중에만 드롭아웃이 활성화되도록 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class DropoutMLPScratch(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,
                 dropout_1, dropout_2, lr):
        super().__init__()
        self.save_hyperparameters()
        self.lin1 = nn.Dense(num_hiddens_1, activation='relu')
        self.lin2 = nn.Dense(num_hiddens_2, activation='relu')
        self.lin3 = nn.Dense(num_outputs)
        self.initialize()

    def forward(self, X):
        H1 = self.lin1(X)
        if autograd.is_training():
            H1 = dropout_layer(H1, self.dropout_1)
        H2 = self.lin2(H1)
        if autograd.is_training():
            H2 = dropout_layer(H2, self.dropout_2)
        return self.lin3(H2)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class DropoutMLPScratch(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,
                 dropout_1, dropout_2, lr):
        super().__init__()
        self.save_hyperparameters()
        self.lin1 = nn.LazyLinear(num_hiddens_1)
        self.lin2 = nn.LazyLinear(num_hiddens_2)
        self.lin3 = nn.LazyLinear(num_outputs)
        self.relu = nn.ReLU()

    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))
        if self.training:  
            H1 = dropout_layer(H1, self.dropout_1)
        H2 = self.relu(self.lin2(H1))
        if self.training:
            H2 = dropout_layer(H2, self.dropout_2)
        return self.lin3(H2)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class DropoutMLPScratch(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,
                 dropout_1, dropout_2, lr):
        super().__init__()
        self.save_hyperparameters()
        self.lin1 = tf.keras.layers.Dense(num_hiddens_1, activation='relu')
        self.lin2 = tf.keras.layers.Dense(num_hiddens_2, activation='relu')
        self.lin3 = tf.keras.layers.Dense(num_outputs)

    def forward(self, X):
        H1 = self.lin1(tf.reshape(X, (X.shape[0], -1)))
        if self.training:
            H1 = dropout_layer(H1, self.dropout_1)
        H2 = self.lin2(H1)
        if self.training:
            H2 = dropout_layer(H2, self.dropout_2)
        return self.lin3(H2)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class DropoutMLPScratch(d2l.Classifier):
    num_hiddens_1: int
    num_hiddens_2: int
    num_outputs: int
    dropout_1: float
    dropout_2: float
    lr: float
    training: bool = True

    def setup(self):
        self.lin1 = nn.Dense(self.num_hiddens_1)
        self.lin2 = nn.Dense(self.num_hiddens_2)
        self.lin3 = nn.Dense(self.num_outputs)
        self.relu = nn.relu

    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape(X.shape[0], -1)))
        if self.training:
            H1 = dropout_layer(H1, self.dropout_1)
        H2 = self.relu(self.lin2(H1))
        if self.training:
            H2 = dropout_layer(H2, self.dropout_2)
        return self.lin3(H2)
</code></pre>
<h3 id="훈련-training-7"><a class="header" href="#훈련-training-7">[<strong>훈련 (Training)</strong>]</a></h3>
<p>다음은 앞서 설명한 MLP의 훈련과 유사합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,
           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}
model = DropoutMLPScratch(**hparams)
data = d2l.FashionMNIST(batch_size=256)
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model, data)
</code></pre>
<h2 id="간결한-구현-1"><a class="header" href="#간결한-구현-1">[<strong>간결한 구현</strong>]</a></h2>
<p>고수준 API를 사용하면 각 완전 연결 레이어 뒤에 <code>Dropout</code> 레이어를 추가하고 생성자에 유일한 인수로 드롭아웃 확률을 전달하기만 하면 됩니다.
훈련 중에 <code>Dropout</code> 레이어는 지정된 드롭아웃 확률에 따라 이전 레이어의 출력(또는 동등하게 후속 레이어의 입력)을 무작위로 떨어뜨립니다.
훈련 모드가 아닐 때, <code>Dropout</code> 레이어는 테스트 중에 데이터를 단순히 통과시킵니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class DropoutMLP(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,
                 dropout_1, dropout_2, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential()
        self.net.add(nn.Dense(num_hiddens_1, activation="relu"),
                     nn.Dropout(dropout_1),
                     nn.Dense(num_hiddens_2, activation="relu"),
                     nn.Dropout(dropout_2),
                     nn.Dense(num_outputs))
        self.net.initialize()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class DropoutMLP(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,
                 dropout_1, dropout_2, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
            nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(), 
            nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(), 
            nn.Dropout(dropout_2), nn.LazyLinear(num_outputs))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class DropoutMLP(d2l.Classifier):
    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,
                 dropout_1, dropout_2, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = tf.keras.models.Sequential([
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(num_hiddens_1, activation=tf.nn.relu),
            tf.keras.layers.Dropout(dropout_1),
            tf.keras.layers.Dense(num_hiddens_2, activation=tf.nn.relu),
            tf.keras.layers.Dropout(dropout_2),
            tf.keras.layers.Dense(num_outputs)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class DropoutMLP(d2l.Classifier):
    num_hiddens_1: int
    num_hiddens_2: int
    num_outputs: int
    dropout_1: float
    dropout_2: float
    lr: float
    training: bool = True

    @nn.compact
    def __call__(self, X):
        x = nn.relu(nn.Dense(self.num_hiddens_1)(X.reshape((X.shape[0], -1))))
        x = nn.Dropout(self.dropout_1, deterministic=not self.training)(x)
        x = nn.relu(nn.Dense(self.num_hiddens_2)(x))
        x = nn.Dropout(self.dropout_2, deterministic=not self.training)(x)
        return nn.Dense(self.num_outputs)(x)
</code></pre>
<p>:begin_tab:<code>jax</code>
<code>Module.apply()</code>를 사용할 때 드롭아웃 레이어가 있는 네트워크는 PRNGKey가 필요하고, 이 RNG 시드의 이름은 명시적으로 <code>dropout</code>이어야 하므로 손실 함수를 재정의해야 한다는 점에 유의하십시오. 이 키는 Flax의 <code>dropout</code> 레이어에서 내부적으로 무작위 드롭아웃 마스크를 생성하는 데 사용됩니다. 훈련 루프의 모든 에폭마다 고유한 <code>dropout_rng</code> 키를 사용하는 것이 중요합니다. 그렇지 않으면 생성된 드롭아웃 마스크가 확률적이지 않고 에폭 실행 간에 다르지 않게 됩니다. 이 <code>dropout_rng</code>는 <code>TrainState</code> 객체(:numref:<code>oo-design-training</code>에 정의된 <code>d2l.Trainer</code> 클래스에 있음)에 속성으로 저장될 수 있으며 매 에폭마다 새 <code>dropout_rng</code>로 대체됩니다. 우리는 이미 :numref:<code>sec_linear_scratch</code>에 정의된 <code>fit_epoch</code> 메서드로 이를 처리했습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(d2l.Classifier)  #@save
@partial(jax.jit, static_argnums=(0, 5))
def loss(self, params, X, Y, state, averaged=True):
    Y_hat = state.apply_fn({'params': params}, *X,
                           mutable=False,  # 나중에 사용될 예정(예: 배치 정규화)
                           rngs={'dropout': state.dropout_rng})
    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))
    Y = d2l.reshape(Y, (-1,))
    fn = optax.softmax_cross_entropy_with_integer_labels
    # 반환된 빈 딕셔너리는 나중에 사용될(예: 배치 정규화) 보조 데이터를 위한 플레이스홀더입니다.
    return (fn(Y_hat, Y).mean(), {}) if averaged else (fn(Y_hat, Y), {})
</code></pre>
<p>다음으로 [<strong>모델을 훈련합니다</strong>].</p>
<pre><code class="language-{.python .input}">%%tab all
model = DropoutMLP(**hparams)
trainer.fit(model, data)
</code></pre>
<h2 id="요약-summary-17"><a class="header" href="#요약-summary-17">요약 (Summary)</a></h2>
<p>차원의 수와 가중치 벡터의 크기를 제어하는 것 외에도, 드롭아웃은 과대적합을 피하기 위한 또 다른 도구입니다. 종종 도구들은 함께 사용됩니다.
드롭아웃은 훈련 중에만 사용된다는 점에 유의하십시오:
활성화 $h$를 기댓값 $h$를 가진 확률 변수로 대체합니다.</p>
<h2 id="연습-문제-exercises-19"><a class="header" href="#연습-문제-exercises-19">연습 문제 (Exercises)</a></h2>
<ol>
<li>첫 번째와 두 번째 레이어의 드롭아웃 확률을 변경하면 어떻게 됩니까? 특히 두 레이어의 확률을 바꾸면 어떻게 됩니까? 이 질문에 답하기 위한 실험을 설계하고, 결과를 정량적으로 설명하고, 정성적인 시사점을 요약하십시오.</li>
<li>에폭 수를 늘리고 드롭아웃을 사용할 때와 사용하지 않을 때 얻은 결과를 비교하십시오.</li>
<li>드롭아웃이 적용될 때와 적용되지 않을 때 각 은닉층에서 활성화의 분산은 얼마입니까? 두 모델에 대해 이 양이 시간이 지남에 따라 어떻게 진화하는지 보여주는 플롯을 그리십시오.</li>
<li>테스트 시에 드롭아웃을 일반적으로 사용하지 않는 이유는 무엇입니까?</li>
<li>이 섹션의 모델을 예로 들어 드롭아웃과 가중치 감쇠 사용의 효과를 비교하십시오. 드롭아웃과 가중치 감쇠를 동시에 사용하면 어떻게 됩니까? 결과가 가산적입니까? 수익이 감소합니까(혹은 더 나쁩니까)? 서로 상쇄합니까?</li>
<li>활성화 대신 가중치 행렬의 개별 가중치에 드롭아웃을 적용하면 어떻게 됩니까?</li>
<li>표준 드롭아웃 기술과 다른, 각 레이어에 무작위 노이즈를 주입하는 다른 기술을 발명하십시오. Fashion-MNIST 데이터셋(고정된 아키텍처에 대해)에서 드롭아웃보다 성능이 뛰어난 방법을 개발할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/100">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/101">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/261">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17987">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="kaggle에서-주택-가격-예측하기-predicting-house-prices-on-kaggle"><a class="header" href="#kaggle에서-주택-가격-예측하기-predicting-house-prices-on-kaggle">Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></h1>
<p>:label:<code>sec_kaggle_house</code></p>
<p>심층 네트워크를 구축 및 훈련하고 가중치 감쇠 및 드롭아웃과 같은 기술로 정규화하는 몇 가지 기본 도구를 소개했으므로,
이제 Kaggle 대회에 참가하여 이 모든 지식을 실습에 적용할 준비가 되었습니다.
주택 가격 예측 대회는 시작하기에 좋은 곳입니다.
데이터가 상당히 일반적이며 전문 모델(오디오나 비디오처럼)이 필요할 수 있는 이국적인 구조를 보이지 않습니다.
:citet:<code>De-Cock.2011</code>이 수집한 이 데이터셋은 2006년부터 2010년까지 아이오와주 에임스(Ames)의 주택 가격을 다룹니다.
Harrison과 Rubinfeld(1978)의 유명한 <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names">보스턴 주택 데이터셋</a>보다 훨씬 크며 더 많은 예제와 특성을 자랑합니다.</p>
<p>이 섹션에서는 데이터 전처리, 모델 설계 및 하이퍼파라미터 선택에 대한 세부 사항을 안내해 드립니다.
실습 접근 방식을 통해 데이터 과학자로서의 경력에 지침이 될 몇 가지 직관을 얻으시길 바랍니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, autograd, init, np, npx
from mxnet.gluon import nn
import pandas as pd

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
from torch import nn
import pandas as pd
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
import pandas as pd
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
import numpy as np
import pandas as pd
</code></pre>
<h2 id="데이터-다운로드하기-downloading-data"><a class="header" href="#데이터-다운로드하기-downloading-data">데이터 다운로드하기 (Downloading Data)</a></h2>
<p>책 전반에 걸쳐 다양한 다운로드된 데이터셋에서 모델을 훈련하고 테스트할 것입니다.
여기서는 zip 또는 tar 파일을 다운로드하고 압축을 풀기 위한 (<strong>두 가지 유틸리티 함수를 구현</strong>)합니다.
다시 한번, 이러한 유틸리티 함수의 구현 세부 사항은 생략합니다.</p>
<pre><code class="language-{.python .input  n=2}">%%tab all
def download(url, folder, sha1_hash=None):
    """파일을 폴더로 다운로드하고 로컬 파일 경로를 반환합니다."""

def extract(filename, folder):
    """폴더에 zip/tar 파일을 풉니다."""
</code></pre>
<h2 id="kaggle"><a class="header" href="#kaggle">Kaggle</a></h2>
<p><a href="https://www.kaggle.com">Kaggle</a>은 머신러닝 대회를 주최하는 인기 있는 플랫폼입니다.
각 대회는 데이터셋을 중심으로 하며, 많은 대회가 우승 솔루션에 상금을 제공하는 이해 관계자들에 의해 후원됩니다.
플랫폼은 사용자가 포럼과 공유 코드를 통해 상호 작용하도록 도와 협업과 경쟁을 모두 촉진합니다.
리더보드 추격이 종종 통제 불능 상태가 되어 연구자들이 근본적인 질문을 던지기보다는 전처리 단계에 근시안적으로 집중하게 되기도 하지만,
경쟁하는 접근 방식 간의 직접적인 정량적 비교뿐만 아니라 코드 공유를 용이하게 하여 모든 사람이 무엇이 효과가 있었고 무엇이 효과가 없었는지 배울 수 있게 하는 플랫폼의 객관성에는 엄청난 가치가 있습니다.
Kaggle 대회에 참가하려면 먼저 계정을 등록해야 합니다(:numref:<code>fig_kaggle</code> 참조).</p>
<p><img src="chapter_multilayer-perceptrons/../img/kaggle.png" alt="Kaggle 웹사이트." />
:width:<code>400px</code>
:label:<code>fig_kaggle</code></p>
<p>:numref:<code>fig_house_pricing</code>에 설명된 주택 가격 예측 대회 페이지에서 데이터셋("Data" 탭 아래)을 찾고 예측을 제출하고 순위를 볼 수 있습니다. URL은 바로 여기에 있습니다:</p>
<blockquote>
<p>https://www.kaggle.com/c/house-prices-advanced-regression-techniques</p>
</blockquote>
<p><img src="chapter_multilayer-perceptrons/../img/house-pricing.png" alt="주택 가격 예측 대회 페이지." />
:width:<code>400px</code>
:label:<code>fig_house_pricing</code></p>
<h2 id="데이터셋-액세스-및-읽기-accessing-and-reading-the-dataset"><a class="header" href="#데이터셋-액세스-및-읽기-accessing-and-reading-the-dataset">데이터셋 액세스 및 읽기 (Accessing and Reading the Dataset)</a></h2>
<p>대회 데이터가 훈련 세트와 테스트 세트로 나뉘어 있다는 점에 유의하십시오.
각 레코드에는 주택의 자산 가치와 거리 유형, 건축 연도, 지붕 유형, 지하실 상태 등의 속성이 포함되어 있습니다.
특성은 다양한 데이터 유형으로 구성됩니다.
예를 들어 건축 연도는 정수로, 지붕 유형은 이산형 범주 할당으로, 기타 특성은 부동 소수점 숫자로 표현됩니다.
그리고 여기서 현실이 상황을 복잡하게 만듭니다: 일부 예제의 경우 일부 데이터가 완전히 누락되어 있으며 결측값은 단순히 "na"로 표시됩니다.
각 주택의 가격은 훈련 세트에만 포함되어 있습니다(어쨌든 대회니까요).
훈련 세트를 분할하여 검증 세트를 만들고 싶겠지만, Kaggle에 예측을 업로드한 후에만 공식 테스트 세트에서 모델을 평가할 수 있습니다.
:numref:<code>fig_house_pricing</code>의 대회 탭에 있는 "Data" 탭에는 데이터를 다운로드할 수 있는 링크가 있습니다.</p>
<p>시작하기 위해, :numref:<code>sec_pandas</code>에서 소개한 [<strong><code>pandas</code>를 사용하여 데이터를 읽고 처리</strong>]할 것입니다.
편의를 위해 Kaggle 주택 데이터셋을 다운로드하고 캐시할 수 있습니다.
이 데이터셋에 해당하는 파일이 이미 캐시 디렉토리에 존재하고 SHA-1이 <code>sha1_hash</code>와 일치하면, 우리 코드는 중복 다운로드로 인터넷을 막는 것을 방지하기 위해 캐시된 파일을 사용합니다.</p>
<pre><code class="language-{.python .input  n=30}">%%tab all
class KaggleHouse(d2l.DataModule):
    def __init__(self, batch_size, train=None, val=None):
        super().__init__()
        self.save_hyperparameters()
        if self.train is None:
            self.raw_train = pd.read_csv(d2l.download(
                d2l.DATA_URL + 'kaggle_house_pred_train.csv', self.root,
                sha1_hash='585e9cc93e70b39160e7921475f9bcd7d31219ce'))
            self.raw_val = pd.read_csv(d2l.download(
                d2l.DATA_URL + 'kaggle_house_pred_test.csv', self.root,
                sha1_hash='fa19780a7b011d9b009e8bff8e99922a8ee2eb90'))
</code></pre>
<p>훈련 데이터셋에는 1460개의 예제, 80개의 특성, 1개의 레이블이 포함되어 있으며, 검증 데이터에는 1459개의 예제와 80개의 특성이 포함되어 있습니다.</p>
<pre><code class="language-{.python .input  n=31}">%%tab all
data = KaggleHouse(batch_size=64)
print(data.raw_train.shape)
print(data.raw_val.shape)
</code></pre>
<h2 id="데이터-전처리-data-preprocessing-1"><a class="header" href="#데이터-전처리-data-preprocessing-1">데이터 전처리 (Data Preprocessing)</a></h2>
<p>처음 4개 예제에서 [<strong>처음 4개와 마지막 2개의 특성뿐만 아니라 레이블(SalePrice)을 살펴봅시다</strong>].</p>
<pre><code class="language-{.python .input  n=10}">%%tab all
print(data.raw_train.iloc[:4, [0, 1, 2, 3, -3, -2, -1]])
</code></pre>
<p>각 예제에서 첫 번째 특성이 식별자임을 알 수 있습니다.
이는 모델이 각 훈련 예제를 결정하는 데 도움이 됩니다.
편리하기는 하지만 예측 목적으로는 어떤 정보도 전달하지 않습니다.
따라서 모델에 데이터를 공급하기 전에 데이터셋에서 제거할 것입니다.
또한 다양한 데이터 유형을 감안할 때 모델링을 시작하기 전에 데이터를 전처리해야 합니다.</p>
<p>수치형 특성부터 시작해 봅시다.
먼저, [<strong>모든 결측값을 해당 특성의 평균으로 대체</strong>]하는 휴리스틱을 적용합니다.
그런 다음 모든 특성을 공통 스케일에 두기 위해, (<strong><em>데이터를 표준화</em>하여 특성을 평균 0 및 단위 분산으로 재조정</strong>)합니다:</p>
<p>$$x \leftarrow \frac{x - \mu}{\sigma},
$$</p>
<p>여기서 $\mu$와 $\sigma$는 각각 평균과 표준 편차를 나타냅니다.
이것이 실제로 우리 특성(변수)을 평균 0 및 단위 분산을 갖도록 변환하는지 확인하기 위해,
$E[\frac{x-\mu}{\sigma}] = \frac{\mu - \mu}{\sigma} = 0$이고
$E[(x-\mu)^2] = (\sigma^2 + \mu^2) - 2\mu^2+\mu^2 = \sigma^2$임을 유의하십시오.
직관적으로 우리는 두 가지 이유로 데이터를 표준화합니다.
첫째, 최적화에 편리함이 입증되었습니다.
둘째, 어떤 특성이 관련성이 있을지 <em>사전적으로</em> 알지 못하므로, 한 특성에 할당된 계수에 다른 특성보다 더 많은 페널티를 주고 싶지 않기 때문입니다.</p>
<p>[<strong>다음으로 이산 값을 다룹니다.</strong>]
여기에는 "MSZoning"과 같은 특성이 포함됩니다.
앞서 다중 클래스 레이블을 벡터로 변환했던 것과 같은 방식으로 (<strong>원-핫 인코딩으로 대체</strong>)합니다(:numref:<code>subsec_classification-problem</code> 참조).
예를 들어 "MSZoning"은 "RL"과 "RM" 값을 가정합니다.
"MSZoning" 특성을 삭제하고, 값이 0 또는 1인 두 개의 새로운 지시 특성 "MSZoning_RL"과 "MSZoning_RM"이 생성됩니다.
원-핫 인코딩에 따르면, "MSZoning"의 원래 값이 "RL"이면 "MSZoning_RL"은 1이고 "MSZoning_RM"은 0입니다.
<code>pandas</code> 패키지가 이를 자동으로 수행해 줍니다.</p>
<pre><code class="language-{.python .input  n=32}">%%tab all
@d2l.add_to_class(KaggleHouse)
def preprocess(self):
    # ID와 레이블 열 제거
    label = 'SalePrice'
    features = pd.concat(
        (self.raw_train.drop(columns=['Id', label]),
         self.raw_val.drop(columns=['Id'])))
    # 수치형 열 표준화
    numeric_features = features.dtypes[features.dtypes!='object'].index
    features[numeric_features] = features[numeric_features].apply(
        lambda x: (x - x.mean()) / (x.std()))
    # NAN 수치형 특성을 0으로 대체
    features[numeric_features] = features[numeric_features].fillna(0)
    # 이산형 특성을 원-핫 인코딩으로 대체
    features = pd.get_dummies(features, dummy_na=True)
    # 전처리된 특성 저장
    self.train = features[:self.raw_train.shape[0]].copy()
    self.train[label] = self.raw_train[label]
    self.val = features[self.raw_train.shape[0]:].copy()
</code></pre>
<p>이 변환으로 특성 수가 79개에서 331개로 늘어난 것을 볼 수 있습니다(ID 및 레이블 열 제외).</p>
<pre><code class="language-{.python .input  n=33}">%%tab all
data.preprocess()
data.train.shape
</code></pre>
<h2 id="오차-측정-error-measure"><a class="header" href="#오차-측정-error-measure">오차 측정 (Error Measure)</a></h2>
<p>시작하기 위해 제곱 손실을 사용하여 선형 모델을 훈련할 것입니다. 놀랍지 않게도 우리 선형 모델은 대회 우승 제출물로 이어지지는 않겠지만, 데이터에 의미 있는 정보가 있는지 확인하는 정상성 확인을 제공합니다. 여기서 무작위 추측보다 더 잘할 수 없다면 데이터 처리 버그가 있을 가능성이 큽니다. 그리고 일이 잘 풀린다면, 선형 모델은 단순한 모델이 가장 잘 보고된 모델들에 얼마나 근접하는지에 대한 직관을 제공하는 베이스라인 역할을 하는 것으로, 더 화려한 모델에서 얼마나 많은 이득을 기대해야 하는지에 대한 감각을 줄 것입니다.</p>
<p>주식 가격과 마찬가지로 주택 가격의 경우, 우리는 절대적인 양보다 상대적인 양에 관심을 갖습니다.
따라서 [<strong>우리는 절대 오차 $y - \hat{y}$보다 상대 오차 $\frac{y - \hat{y}}{y}$에 더 관심을 갖는 경향이 있습니다.</strong>]
예를 들어, 일반적인 주택 가치가 125,000달러인 오하이오 시골 지역의 주택 가격을 추정할 때 예측이 100,000달러 빗나간다면 아마도 끔찍한 일을 하고 있는 것입니다.
반면에 캘리포니아 로스 알토스 힐스(중위 주택 가격이 400만 달러를 초과함)에서 이 금액만큼 틀린다면, 이는 놀랍도록 정확한 예측을 나타낼 수 있습니다.</p>
<p>(<strong>이 문제를 해결하는 한 가지 방법은 가격 추정치의 로그 불일치를 측정하는 것입니다.</strong>)
사실 이는 제출물의 품질을 평가하기 위해 대회에서 사용하는 공식적인 오차 측정치이기도 합니다.
결국 $|\log y - \log \hat{y}| \leq \delta$에 대한 작은 값 $\delta$는 $e^{-\delta} \leq \frac{\hat{y}}{y} \leq e^{\delta}$로 변환됩니다.
이것은 예측 가격의 로그와 레이블 가격의 로그 사이의 다음과 같은 제곱근 평균 제곱 오차(root-mean-squared-error)로 이어집니다:</p>
<p>$$\sqrt{\frac{1}{n}\sum_{i=1}^n\left(\log y_i -\log \hat{y}_i\right)^2}.$$</p>
<pre><code class="language-{.python .input  n=60}">%%tab all
@d2l.add_to_class(KaggleHouse)
def get_dataloader(self, train):
    label = 'SalePrice'
    data = self.train if train else self.val
    if label not in data: return
    get_tensor = lambda x: d2l.tensor(x.values.astype(float),
                                      dtype=d2l.float32)
    # 가격의 로그
    tensors = (get_tensor(data.drop(columns=[label])),  # X
               d2l.reshape(d2l.log(get_tensor(data[label])), (-1, 1)))  # Y
    return self.get_tensorloader(tensors, train)
</code></pre>
<h2 id="k-겹-교차-검증-k-fold-cross-validation"><a class="header" href="#k-겹-교차-검증-k-fold-cross-validation">$K$-겹 교차 검증 ($K$-Fold Cross-Validation)</a></h2>
<p>모델 선택을 다루는 방법을 논의한 :numref:<code>subsec_generalization-model-selection</code>에서 [<strong>교차 검증</strong>]을 소개했던 것을 기억하실 것입니다.
우리는 이를 모델 설계를 선택하고 하이퍼파라미터를 조정하는 데 유용하게 사용할 것입니다.
먼저 $K$-겹 교차 검증 절차에서 데이터의 $i$번째 겹(fold)을 반환하는 함수가 필요합니다.
이 함수는 $i$번째 세그먼트를 검증 데이터로 잘라내고 나머지를 훈련 데이터로 반환하는 방식으로 진행됩니다.
이것이 데이터를 처리하는 가장 효율적인 방법은 아니며 데이터셋이 상당히 더 크다면 분명히 훨씬 더 똑똑한 방법을 사용할 것입니다.
하지만 이 추가된 복잡성은 불필요하게 코드를 난독화할 수 있으므로 문제의 단순성 덕분에 여기서는 안전하게 생략할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def k_fold_data(data, k):
    rets = []
    fold_size = data.train.shape[0] // k
    for j in range(k):
        idx = range(j * fold_size, (j+1) * fold_size)
        rets.append(KaggleHouse(data.batch_size, data.train.drop(index=idx),  
                                data.train.loc[idx]))    
    return rets
</code></pre>
<p>$K$-겹 교차 검증에서 $K$번 훈련할 때 [<strong>평균 검증 오차가 반환됩니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab all
def k_fold(trainer, data, k, lr):
    val_loss, models = [], []
    for i, data_fold in enumerate(k_fold_data(data, k)):
        model = d2l.LinearRegression(lr)
        model.board.yscale='log'
        if i != 0: model.board.display = False
        trainer.fit(model, data_fold)
        val_loss.append(float(model.board.data['val_loss'][-1].y))
        models.append(model)
    print(f'평균 검증 로그 mse = {sum(val_loss)/len(val_loss)}')
    return models
</code></pre>
<h2 id="모델-선택-model-selection-1"><a class="header" href="#모델-선택-model-selection-1">[<strong>모델 선택 (Model Selection)</strong>]</a></h2>
<p>이 예제에서는 조정되지 않은 하이퍼파라미터 세트를 선택하고 모델을 개선하는 것은 독자에게 맡깁니다.
좋은 선택을 찾는 것은 최적화하는 변수의 수에 따라 시간이 걸릴 수 있습니다.
충분히 큰 데이터셋과 일반적인 종류의 하이퍼파라미터가 있다면, $K$-겹 교차 검증은 다중 테스트에 대해 상당히 탄력적인 경향이 있습니다.
그러나 비합리적으로 많은 수의 옵션을 시도하면 검증 성능이 더 이상 실제 오차를 대표하지 않음을 알게 될 수도 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
trainer = d2l.Trainer(max_epochs=10)
models = k_fold(trainer, data, k=5, lr=0.01)
</code></pre>
<p>때로는 하이퍼파라미터 세트에 대한 훈련 오차 수가 매우 낮을 수 있지만, $K$-겹 교차 검증의 오차 수는 상당히 높아질 수 있음에 유의하십시오.
이는 과대적합되고 있음을 나타냅니다.
훈련 내내 두 숫자를 모두 모니터링하고 싶을 것입니다.
과대적합이 적다는 것은 데이터가 더 강력한 모델을 지원할 수 있음을 나타낼 수 있습니다.
엄청난 과대적합은 정규화 기술을 통합하여 이득을 얻을 수 있음을 시사할 수 있습니다.</p>
<h2 id="kaggle에-예측-제출하기"><a class="header" href="#kaggle에-예측-제출하기">[<strong>Kaggle에 예측 제출하기</strong>]</a></h2>
<p>이제 하이퍼파라미터의 좋은 선택이 무엇인지 알았으므로, 모든 $K$개 모델에 의한 테스트 세트에서의 평균 예측을 계산할 수 있습니다.
예측을 csv 파일에 저장하면 결과를 Kaggle에 업로드하는 것이 간편해집니다.
다음 코드는 <code>submission.csv</code>라는 파일을 생성합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    preds = [model(d2l.tensor(data.val.values.astype(float), dtype=d2l.float32))
             for model in models]
if tab.selected('jax'):
    preds = [model.apply({'params': trainer.state.params},
             d2l.tensor(data.val.values.astype(float), dtype=d2l.float32))
             for model in models]
# 로그 스케일에서의 예측을 지수화
ensemble_preds = d2l.reduce_mean(d2l.exp(d2l.concat(preds, 1)), 1)
submission = pd.DataFrame({'Id':data.raw_val.Id,
                           'SalePrice':d2l.numpy(ensemble_preds)})
submission.to_csv('submission.csv', index=False)
</code></pre>
<p>다음으로 :numref:<code>fig_kaggle_submit2</code>에 설명된 대로 Kaggle에 예측을 제출하고 테스트 세트의 실제 주택 가격(레이블)과 비교해 볼 수 있습니다.
단계는 꽤 간단합니다:</p>
<ul>
<li>Kaggle 웹사이트에 로그인하고 주택 가격 예측 대회 페이지를 방문합니다.</li>
<li>"Submit Predictions" 또는 "Late Submission" 버튼을 클릭합니다.</li>
<li>페이지 하단의 점선 상자에 있는 "Upload Submission File" 버튼을 클릭하고 업로드할 예측 파일을 선택합니다.</li>
<li>페이지 하단의 "Make Submission" 버튼을 클릭하여 결과를 확인합니다.</li>
</ul>
<p><img src="chapter_multilayer-perceptrons/../img/kaggle-submit2.png" alt="Kaggle에 데이터 제출하기." />
:width:<code>400px</code>
:label:<code>fig_kaggle_submit2</code></p>
<h2 id="요약-및-토론-summary-and-discussion-2"><a class="header" href="#요약-및-토론-summary-and-discussion-2">요약 및 토론 (Summary and Discussion)</a></h2>
<p>실제 데이터는 종종 다양한 데이터 유형의 혼합을 포함하며 전처리가 필요합니다.
실수 값 데이터를 평균 0 및 단위 분산으로 재조정하는 것은 좋은 기본값입니다. 결측값을 평균으로 대체하는 것도 마찬가지입니다.
또한 범주형 특성을 지시 특성으로 변환하면 원-핫 벡터처럼 취급할 수 있습니다.
절대 오차보다 상대 오차에 더 신경 쓸 때는 예측의 로그 불일치를 측정할 수 있습니다.
모델을 선택하고 하이퍼파라미터를 조정하기 위해 $K$-겹 교차 검증을 사용할 수 있습니다.</p>
<h2 id="연습-문제-exercises-20"><a class="header" href="#연습-문제-exercises-20">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 섹션에 대한 예측을 Kaggle에 제출하십시오. 얼마나 좋습니까?</li>
<li>결측값을 평균으로 대체하는 것이 항상 좋은 아이디어일까요? 힌트: 값이 무작위로 누락되지 않는 상황을 구성할 수 있습니까?</li>
<li>$K$-겹 교차 검증을 통해 하이퍼파라미터를 튜닝하여 점수를 개선하십시오.</li>
<li>모델(예: 레이어, 가중치 감쇠, 드롭아웃)을 개선하여 점수를 개선하십시오.</li>
<li>이 섹션에서 한 것처럼 연속 수치형 특성을 표준화하지 않으면 어떻게 됩니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/106">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/107">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/237">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17988">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="빌더-가이드-builders-guide"><a class="header" href="#빌더-가이드-builders-guide">빌더 가이드 (Builders' Guide)</a></h1>
<p>:label:<code>chap_computation</code></p>
<p>거대한 데이터셋 및 강력한 하드웨어와 더불어, 훌륭한 소프트웨어 도구는 딥러닝의 빠른 발전에 없어서는 안 될 역할을 해왔습니다.
2007년 출시된 획기적인 Theano 라이브러리를 시작으로, 유연한 오픈 소스 도구들은 연구자들이 표준 구성 요소를 재활용할 때 반복적인 작업을 피하면서도 저수준 수정을 할 수 있는 능력을 유지하여 모델을 신속하게 프로토타이핑할 수 있게 했습니다.
시간이 지남에 따라 딥러닝 라이브러리는 점점 더 거친 추상화를 제공하도록 진화했습니다.
반도체 설계자가 트랜지스터 지정에서 논리 회로로, 그리고 코드를 작성하는 것으로 나아간 것처럼, 신경망 연구자들은 개별 인공 뉴런의 동작에 대해 생각하는 것에서 전체 레이어 측면에서 네트워크를 구상하는 것으로 이동했으며, 이제는 종종 훨씬 더 거친 <em>블록</em>을 염두에 두고 아키텍처를 설계합니다.</p>
<p>지금까지 우리는 몇 가지 기본적인 머신러닝 개념을 소개하고 완전히 기능하는 딥러닝 모델로 나아갔습니다.
지난 장에서는 MLP의 각 구성 요소를 밑바닥부터 구현하고, 고수준 API를 활용하여 동일한 모델을 쉽게 출시하는 방법도 보여주었습니다.
그렇게 빨리 도달하기 위해 우리는 라이브러리를 <em>호출</em>했지만, <em>작동 방식</em>에 대한 고급 세부 사항은 건너뛰었습니다.
이 장에서는 커튼을 젖히고 딥러닝 계산의 핵심 구성 요소, 즉 모델 구성, 파라미터 액세스 및 초기화, 사용자 정의 레이어 및 블록 설계, 모델을 디스크에 읽고 쓰기, 극적인 속도 향상을 달성하기 위한 GPU 활용에 대해 더 깊이 파고들 것입니다.
이러한 통찰력은 여러분을 <em>최종 사용자</em>에서 <em>파워 유저</em>로 이동시켜, 성숙한 딥러닝 라이브러리의 이점을 누리면서도 여러분이 직접 발명한 것을 포함하여 더 복잡한 모델을 구현할 수 있는 유연성을 유지하는 데 필요한 도구를 제공할 것입니다.
이 장에서는 새로운 모델이나 데이터셋을 소개하지 않지만, 이어지는 고급 모델링 장에서는 이러한 기술에 크게 의존합니다.</p>
<pre><code class="language-toc">:maxdepth: 2

model-construction
parameters
init-param
lazy-init
custom-layer
read-write
use-gpu
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code>```{.python .input}
%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="레이어와-모듈-layers-and-modules"><a class="header" href="#레이어와-모듈-layers-and-modules">레이어와 모듈 (Layers and Modules)</a></h1>
<p>:label:<code>sec_model_construction</code></p>
<p>우리가 신경망을 처음 소개했을 때, 단일 출력이 있는 선형 모델에 집중했습니다.
여기서 전체 모델은 단 하나의 뉴런으로 구성됩니다.
단일 뉴런은 (i) 입력 세트를 받고; (ii) 해당하는 스칼라 출력을 생성하며; (iii) 관심 있는 목적 함수를 최적화하기 위해 업데이트할 수 있는 관련 파라미터 세트를 가지고 있습니다.
그 후, 다중 출력을 가진 네트워크에 대해 생각하기 시작했을 때, 벡터화된 산술을 활용하여 전체 뉴런 레이어를 특징지었습니다.
개별 뉴런과 마찬가지로 레이어는 (i) 입력 세트를 받고, (ii) 해당하는 출력을 생성하며, (iii) 조정 가능한 파라미터 세트로 설명됩니다.
소프트맥스 회귀를 작업할 때, 단일 레이어 자체가 모델이었습니다.
그러나 그 후 MLP를 소개했을 때도 여전히 모델이 이와 동일한 기본 구조를 유지한다고 생각할 수 있었습니다.</p>
<p>흥미롭게도 MLP의 경우, 전체 모델과 구성 레이어 모두 이 구조를 공유합니다.
전체 모델은 원시 입력(특성)을 받아 출력(예측)을 생성하고 파라미터(모든 구성 레이어의 결합된 파라미터)를 보유합니다.
마찬가지로 각 개별 레이어는 입력(이전 레이어에서 제공)을 섭취하여 출력(후속 레이어에 대한 입력)을 생성하고, 후속 레이어에서 역방향으로 흐르는 신호에 따라 업데이트되는 조정 가능한 파라미터 세트를 보유합니다.</p>
<p>뉴런, 레이어, 모델이 우리 업무를 수행하기에 충분한 추상화를 제공한다고 생각할 수 있지만, 개별 레이어보다는 크지만 전체 모델보다는 작은 구성 요소에 대해 이야기하는 것이 편리할 때가 많습니다.
예를 들어 컴퓨터 비전에서 매우 인기 있는 ResNet-152 아키텍처는 수백 개의 레이어를 보유하고 있습니다.
이 레이어들은 <em>레이어 그룹</em>의 반복되는 패턴으로 구성됩니다. 이러한 네트워크를 한 번에 한 레이어씩 구현하는 것은 지루할 수 있습니다.
이 우려는 단지 가상적인 것이 아닙니다. 이러한 디자인 패턴은 실제로 일반적입니다.
위에서 언급한 ResNet 아키텍처는 인식 및 검출 모두에 대해 2015 ImageNet 및 COCO 컴퓨터 비전 대회에서 우승했으며 :cite:<code>He.Zhang.Ren.ea.2016</code> 많은 비전 작업에서 여전히 선호되는 아키텍처입니다.
레이어가 다양한 반복 패턴으로 배열된 유사한 아키텍처는 이제 자연어 처리 및 음성을 포함한 다른 도메인에서도 어디에나 있습니다.</p>
<p>이러한 복잡한 네트워크를 구현하기 위해 신경망 <em>모듈(module)</em> 개념을 도입합니다.
모듈은 단일 레이어, 여러 레이어로 구성된 구성 요소, 또는 전체 모델 자체를 설명할 수 있습니다!
모듈 추상화로 작업하는 것의 한 가지 이점은 더 큰 아티팩트로 결합될 수 있으며 종종 재귀적으로 결합된다는 것입니다. 이것은 :numref:<code>fig_blocks</code>에 설명되어 있습니다. 필요에 따라 임의의 복잡도를 가진 모듈을 생성하는 코드를 정의함으로써, 놀랍도록 간결한 코드를 작성하면서도 복잡한 신경망을 구현할 수 있습니다.</p>
<p><img src="chapter_builders-guide/../img/blocks.svg" alt="여러 레이어가 모듈로 결합되어 더 큰 모델의 반복 패턴을 형성합니다." />
:label:<code>fig_blocks</code></p>
<p>프로그래밍 관점에서 모듈은 *클래스(class)*로 표현됩니다.
모든 서브클래스는 입력을 출력으로 변환하는 순전파 메서드를 정의해야 하며 필요한 모든 파라미터를 저장해야 합니다.
일부 모듈은 파라미터가 전혀 필요하지 않다는 점에 유의하십시오.
마지막으로 모듈은 기울기 계산을 위해 역전파 메서드를 보유해야 합니다.
다행히도 자체 모듈을 정의할 때 자동 미분(:numref:<code>sec_autograd</code>에서 소개됨)이 제공하는 무대 뒤의 마법 덕분에 파라미터와 순전파 메서드만 걱정하면 됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from typing import List
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<p>[<strong>시작하기 위해 MLP를 구현하는 데 사용했던 코드</strong>] (:numref:<code>sec_mlp</code>)를 다시 살펴봅니다.
다음 코드는 256개 유닛과 ReLU 활성화가 있는 하나의 완전 연결 은닉층과, 10개 유닛(활성화 함수 없음)이 있는 완전 연결 출력 레이어로 구성된 네트워크를 생성합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()

X = np.random.uniform(size=(2, 20))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))

X = torch.rand(2, 20)
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(10),
])

X = tf.random.uniform((2, 20))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(10)])

# get_key는 jax.random.PRNGKey(random_seed)를 반환하는 d2l 저장 함수입니다
X = jax.random.uniform(d2l.get_key(), (2, 20))
params = net.init(d2l.get_key(), X)
net.apply(params, X).shape
</code></pre>
<p>:begin_tab:<code>mxnet</code>
이 예제에서 우리는 <code>nn.Sequential</code>을 인스턴스화하여 모델을 구성하고 반환된 객체를 <code>net</code> 변수에 할당했습니다.
다음으로 <code>add</code> 메서드를 반복적으로 호출하여 실행되어야 할 순서대로 레이어를 추가합니다.
간단히 말해서 <code>nn.Sequential</code>은 Gluon에서 <em>모듈</em>을 나타내는 클래스인 <code>Block</code>의 특별한 종류를 정의합니다.
구성 <code>Block</code>의 정렬된 목록을 유지합니다.
<code>add</code> 메서드는 목록에 각 후속 <code>Block</code>을 추가하는 것을 용이하게 합니다.
각 레이어는 <code>Dense</code> 클래스의 인스턴스이며 그 자체로 <code>Block</code>의 서브클래스라는 점에 유의하십시오.
순전파(<code>forward</code>) 메서드도 놀랍도록 간단합니다. 목록에 있는 각 <code>Block</code>을 연결하여 각 출력을 다음 입력으로 전달합니다.
지금까지 우리는 출력을 얻기 위해 <code>net(X)</code> 구성을 통해 모델을 호출해 왔다는 점에 유의하십시오.
실제로는 <code>Block</code> 클래스의 <code>__call__</code> 메서드를 통해 달성되는 멋진 Python 트릭인 <code>net.forward(X)</code>의 약어일 뿐입니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
이 예제에서 우리는 실행되어야 할 순서대로 레이어를 인수로 전달하여 <code>nn.Sequential</code>을 인스턴스화함으로써 모델을 구성했습니다.
간단히 말해서, (<strong><code>nn.Sequential</code>은 특별한 종류의 <code>Module</code>을 정의합니다</strong>).
<code>Module</code>은 PyTorch에서 모듈을 나타내는 클래스입니다.
구성 <code>Module</code>의 정렬된 목록을 유지합니다.
두 완전 연결 레이어 각각은 <code>Linear</code> 클래스의 인스턴스이며 그 자체로 <code>Module</code>의 서브클래스라는 점에 유의하십시오.
순전파(<code>forward</code>) 메서드도 놀랍도록 간단합니다. 목록에 있는 각 모듈을 연결하여 각 출력을 다음 입력으로 전달합니다.
지금까지 우리는 출력을 얻기 위해 <code>net(X)</code> 구성을 통해 모델을 호출해 왔다는 점에 유의하십시오.
실제로는 <code>net.__call__(X)</code>의 약어일 뿐입니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
이 예제에서 우리는 실행되어야 할 순서대로 레이어를 인수로 전달하여 <code>keras.models.Sequential</code>을 인스턴스화함으로써 모델을 구성했습니다.
간단히 말해서, <code>Sequential</code>은 특별한 종류의 <code>keras.Model</code>을 정의합니다.
<code>Model</code>은 Keras에서 모듈을 나타내는 클래스입니다.
구성 <code>Model</code>의 정렬된 목록을 유지합니다.
두 완전 연결 레이어 각각은 <code>Dense</code> 클래스의 인스턴스이며 그 자체로 <code>Model</code>의 서브클래스라는 점에 유의하십시오.
순전파(<code>call</code>) 메서드도 놀랍도록 간단합니다. 목록에 있는 각 모듈을 연결하여 각 출력을 다음 입력으로 전달합니다.
지금까지 우리는 출력을 얻기 위해 <code>net(X)</code> 구성을 통해 모델을 호출해 왔다는 점에 유의하십시오.
실제로는 모듈 클래스의 <code>__call__</code> 메서드를 통해 달성되는 멋진 Python 트릭인 <code>net.call(X)</code>의 약어일 뿐입니다.
:end_tab:</p>
<h2 id="사용자-정의-모듈-a-custom-module"><a class="header" href="#사용자-정의-모듈-a-custom-module">[<strong>사용자 정의 모듈 (A Custom Module)</strong>]</a></h2>
<p>모듈이 작동하는 방식에 대한 직관을 개발하는 가장 쉬운 방법은 아마도 직접 구현해 보는 것일 겁니다.
그렇게 하기 전에, 각 모듈이 제공해야 하는 기본 기능을 간략하게 요약합니다:</p>
<ol>
<li>입력 데이터를 순전파 메서드의 인수로 섭취합니다.</li>
<li>순전파 메서드가 값을 반환하도록 하여 출력을 생성합니다. 출력은 입력과 다른 모양을 가질 수 있습니다. 예를 들어 위 모델의 첫 번째 완전 연결 레이어는 임의 차원의 입력을 섭취하지만 차원 256의 출력을 반환합니다.</li>
<li>역전파 메서드를 통해 액세스할 수 있는 입력에 대한 출력의 기울기를 계산합니다. 일반적으로 이는 자동으로 발생합니다.</li>
<li>순전파 계산을 실행하는 데 필요한 파라미터를 저장하고 액세스를 제공합니다.</li>
<li>필요에 따라 모델 파라미터를 초기화합니다.</li>
</ol>
<p>다음 스니펫에서는 256개 은닉 유닛이 있는 하나의 은닉층과 10차원 출력 레이어가 있는 MLP에 해당하는 모듈을 밑바닥부터 코딩합니다.
아래의 <code>MLP</code> 클래스는 모듈을 나타내는 클래스를 상속한다는 점에 유의하십시오.
우리는 부모 클래스의 메서드에 크게 의존하며 자체 생성자(Python의 <code>__init__</code> 메서드)와 순전파 메서드만 제공할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class MLP(nn.Block):
    def __init__(self):
        # 필요한 초기화를 수행하기 위해 MLP 부모 클래스 nn.Block의 생성자를 호출합니다
        super().__init__()
        self.hidden = nn.Dense(256, activation='relu')
        self.out = nn.Dense(10)

    # 모델의 순전파, 즉 입력 X를 기반으로 필요한 모델 출력을 반환하는 방법을 정의합니다
    def forward(self, X):
        return self.out(self.hidden(X))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class MLP(nn.Module):
    def __init__(self):
        # 필요한 초기화를 수행하기 위해 부모 클래스 nn.Module의 생성자를 호출합니다
        super().__init__()
        self.hidden = nn.LazyLinear(256)
        self.out = nn.LazyLinear(10)

    # 모델의 순전파, 즉 입력 X를 기반으로 필요한 모델 출력을 반환하는 방법을 정의합니다
    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class MLP(tf.keras.Model):
    def __init__(self):
        # 필요한 초기화를 수행하기 위해 부모 클래스 tf.keras.Model의 생성자를 호출합니다
        super().__init__()
        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)
        self.out = tf.keras.layers.Dense(units=10)

    # 모델의 순전파, 즉 입력 X를 기반으로 필요한 모델 출력을 반환하는 방법을 정의합니다
    def call(self, X):
        return self.out(self.hidden((X)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class MLP(nn.Module):
    def setup(self):
        # 레이어 정의
        self.hidden = nn.Dense(256)
        self.out = nn.Dense(10)

    # 모델의 순전파, 즉 입력 X를 기반으로 필요한 모델 출력을 반환하는 방법을 정의합니다
    def __call__(self, X):
        return self.out(nn.relu(self.hidden(X)))
</code></pre>
<p>먼저 순전파 메서드에 집중해 봅시다.
<code>X</code>를 입력으로 받아 활성화 함수가 적용된 은닉 표현을 계산하고 로짓을 출력한다는 점에 유의하십시오.
이 <code>MLP</code> 구현에서 두 레이어는 모두 인스턴스 변수입니다.
이것이 왜 합리적인지 알기 위해 두 개의 MLP <code>net1</code>과 <code>net2</code>를 인스턴스화하고 서로 다른 데이터로 훈련한다고 상상해 보십시오.
당연히 우리는 이들이 두 개의 서로 다른 학습된 모델을 나타낼 것으로 기대할 것입니다.</p>
<p>우리는 생성자에서 [<strong>MLP의 레이어를 인스턴스화하고</strong>] (<strong>이후에</strong>) 순전파 메서드를 호출할 때마다 (<strong>이 레이어들을 호출합니다</strong>).
몇 가지 주요 세부 사항에 유의하십시오.
먼저, 사용자 정의 <code>__init__</code> 메서드는 <code>super().__init__()</code>를 통해 부모 클래스의 <code>__init__</code> 메서드를 호출하여 대부분의 모듈에 적용되는 상용구 코드를 다시 작성하는 고통을 덜어줍니다.
그런 다음 두 개의 완전 연결 레이어를 인스턴스화하여 <code>self.hidden</code>과 <code>self.out</code>에 할당합니다.
새 레이어를 구현하지 않는 한 역전파 메서드나 파라미터 초기화에 대해 걱정할 필요가 없다는 점에 유의하십시오.
시스템이 이러한 메서드를 자동으로 생성합니다.
한 번 시도해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
net = MLP()
if tab.selected('mxnet'):
    net.initialize()
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = MLP()
params = net.init(d2l.get_key(), X)
net.apply(params, X).shape
</code></pre>
<p>모듈 추상화의 주요 장점은 다용도성입니다.
우리는 모듈을 서브클래스화하여 레이어(완전 연결 레이어 클래스 등), 전체 모델(위의 <code>MLP</code> 클래스 등) 또는 중간 복잡도의 다양한 구성 요소를 만들 수 있습니다.
우리는 합성곱 신경망을 다룰 때와 같이 앞으로 나올 장들에서 이 다용도성을 활용할 것입니다.</p>
<h2 id="sequential-모듈-the-sequential-module"><a class="header" href="#sequential-모듈-the-sequential-module">[<strong>Sequential 모듈 (The Sequential Module)</strong>]</a></h2>
<p>:label:<code>subsec_model-construction-sequential</code></p>
<p>이제 <code>Sequential</code> 클래스가 어떻게 작동하는지 자세히 살펴볼 수 있습니다.
<code>Sequential</code>은 다른 모듈들을 데이지 체인(daisy-chain) 방식으로 연결하도록 설계되었음을 상기하십시오.
우리만의 단순화된 <code>MySequential</code>을 구축하려면 두 가지 주요 메서드만 정의하면 됩니다:</p>
<ol>
<li>모듈을 하나씩 리스트에 추가하는 메서드.</li>
<li>추가된 것과 동일한 순서로 모듈 체인을 통해 입력을 전달하는 순전파 메서드.</li>
</ol>
<p>다음 <code>MySequential</code> 클래스는 기본 <code>Sequential</code> 클래스와 동일한 기능을 제공합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class MySequential(nn.Block):
    def add(self, block):
        # 여기서 block은 Block 서브클래스의 인스턴스이며 고유한 이름을 가지고 있다고 가정합니다.
        # Block 클래스의 멤버 변수 _children에 저장하며 그 타입은 OrderedDict입니다.
        # MySequential 인스턴스가 initialize 메서드를 호출하면 시스템은
        # _children의 모든 멤버를 자동으로 초기화합니다
        self._children[block.name] = block

    def forward(self, X):
        # OrderedDict는 멤버가 추가된 순서대로 순회될 것임을 보장합니다
        for block in self._children.values():
            X = block(X)
        return X
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            self.add_module(str(idx), module)

    def forward(self, X):
        for module in self.children():            
            X = module(X)
        return X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class MySequential(tf.keras.Model):
    def __init__(self, *args):
        super().__init__()
        self.modules = args

    def call(self, X):
        for module in self.modules:
            X = module(X)
        return X
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class MySequential(nn.Module):
    modules: List

    def __call__(self, X):
        for module in self.modules:
            X = module(X)
        return X
</code></pre>
<p>:begin_tab:<code>mxnet</code>
<code>add</code> 메서드는 단일 블록을 정렬된 딕셔너리 <code>_children</code>에 추가합니다.
모든 Gluon <code>Block</code>이 왜 <code>_children</code> 속성을 가지고 있는지, 그리고 왜 우리가 직접 Python 리스트를 정의하지 않고 그것을 사용했는지 궁금할 수 있습니다.
간단히 말해서 <code>_children</code>의 주된 장점은 블록의 파라미터 초기화 중에 Gluon이 <code>_children</code> 딕셔너리 내부를 살펴보고 파라미터도 초기화해야 하는 하위 블록을 찾는다는 것입니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<code>__init__</code> 메서드에서 우리는 <code>add_modules</code> 메서드를 호출하여 모든 모듈을 추가합니다. 이 모듈들은 나중에 <code>children</code> 메서드로 액세스할 수 있습니다.
이런 식으로 시스템은 추가된 모듈을 알게 되고 각 모듈의 파라미터를 적절하게 초기화할 것입니다.
:end_tab:</p>
<p><code>MySequential</code>의 순전파 메서드가 호출되면 추가된 각 모듈이 추가된 순서대로 실행됩니다.
이제 <code>MySequential</code> 클래스를 사용하여 MLP를 다시 구현할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = MySequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = MySequential(
    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),
    tf.keras.layers.Dense(10))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = MySequential([nn.Dense(256), nn.relu, nn.Dense(10)])
params = net.init(d2l.get_key(), X)
net.apply(params, X).shape
</code></pre>
<p>이 <code>MySequential</code> 사용법은 이전에 <code>Sequential</code> 클래스에 대해 작성한 코드(:numref:<code>sec_mlp</code>에 설명됨)와 동일합니다.</p>
<h2 id="순전파-메서드에서-코드-실행하기-executing-code-in-the-forward-propagation-method"><a class="header" href="#순전파-메서드에서-코드-실행하기-executing-code-in-the-forward-propagation-method">[<strong>순전파 메서드에서 코드 실행하기 (Executing Code in the Forward Propagation Method)</strong>]</a></h2>
<p><code>Sequential</code> 클래스는 모델 구성을 쉽게 만들어주며, 우리만의 클래스를 정의할 필요 없이 새로운 아키텍처를 조립할 수 있게 해줍니다.
그러나 모든 아키텍처가 단순한 데이지 체인인 것은 아닙니다.
더 큰 유연성이 필요할 때는 우리만의 블록을 정의하고 싶을 것입니다.
예를 들어 순전파 메서드 내에서 Python의 제어 흐름을 실행하고 싶을 수 있습니다.
더욱이 미리 정의된 신경망 레이어에만 의존하지 않고 임의의 수학적 연산을 수행하고 싶을 수도 있습니다.</p>
<p>지금까지 우리 네트워크의 모든 연산이 네트워크의 활성화와 파라미터에 작용했다는 것을 눈치채셨을 겁니다.
하지만 때로는 이전 레이어의 결과도 아니고 업데이트 가능한 파라미터도 아닌 항을 포함하고 싶을 때가 있습니다.
우리는 이를 *상수 파라미터(constant parameters)*라고 부릅니다.
예를 들어 함수 $f(\mathbf{x},\mathbf{w}) = c \cdot \mathbf{w}^{\top} \mathbf{x}$를 계산하는 레이어를 원한다고 가정해 봅시다. 여기서 $\mathbf{x}$는 입력, $\mathbf{w}$는 파라미터, $c$는 최적화 중에 업데이트되지 않는 지정된 상수입니다.
따라서 다음과 같이 <code>FixedHiddenMLP</code> 클래스를 구현합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class FixedHiddenMLP(nn.Block):
    def __init__(self):
        super().__init__()
        # get_constant 메서드로 생성된 무작위 가중치 파라미터는
        # 훈련 중에 업데이트되지 않습니다(즉, 상수 파라미터)
        self.rand_weight = self.params.get_constant(
            'rand_weight', np.random.uniform(size=(20, 20)))
        self.dense = nn.Dense(20, activation='relu')

    def forward(self, X):
        X = self.dense(X)
        # 생성된 상수 파라미터와 relu 및 dot 함수 사용
        X = npx.relu(np.dot(X, self.rand_weight.data()) + 1)
        # 완전 연결 레이어 재사용. 이는 두 완전 연결 레이어와 파라미터를 공유하는 것과 동일합니다
        X = self.dense(X)
        # 제어 흐름
        while np.abs(X).sum() &gt; 1:
            X /= 2
        return X.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 기울기를 계산하지 않아 훈련 중에 일정하게 유지되는 무작위 가중치 파라미터
        self.rand_weight = torch.rand((20, 20))
        self.linear = nn.LazyLinear(20)

    def forward(self, X):
        X = self.linear(X)        
        X = F.relu(X @ self.rand_weight + 1)
        # 완전 연결 레이어 재사용. 이는 두 완전 연결 레이어와 파라미터를 공유하는 것과 동일합니다
        X = self.linear(X)
        # 제어 흐름
        while X.abs().sum() &gt; 1:
            X /= 2
        return X.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class FixedHiddenMLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.flatten = tf.keras.layers.Flatten()
        # tf.constant로 생성된 무작위 가중치 파라미터는
        # 훈련 중에 업데이트되지 않습니다(즉, 상수 파라미터)
        self.rand_weight = tf.constant(tf.random.uniform((20, 20)))
        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)

    def call(self, inputs):
        X = self.flatten(inputs)
        # 생성된 상수 파라미터와 relu 및 matmul 함수 사용
        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + 1)
        # 완전 연결 레이어 재사용. 이는 두 완전 연결 레이어와 파라미터를 공유하는 것과 동일합니다
        X = self.dense(X)
        # 제어 흐름
        while tf.reduce_sum(tf.math.abs(X)) &gt; 1:
            X /= 2
        return tf.reduce_sum(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class FixedHiddenMLP(nn.Module):
    # 기울기를 계산하지 않아 훈련 중에 일정하게 유지되는 무작위 가중치 파라미터
    rand_weight: jnp.array = jax.random.uniform(d2l.get_key(), (20, 20))

    def setup(self):
        self.dense = nn.Dense(20)

    def __call__(self, X):
        X = self.dense(X)
        X = nn.relu(X @ self.rand_weight + 1)
        # 완전 연결 레이어 재사용. 이는 두 완전 연결 레이어와 파라미터를 공유하는 것과 동일합니다
        X = self.dense(X)
        # 제어 흐름
        while jnp.abs(X).sum() &gt; 1:
            X /= 2
        return X.sum()
</code></pre>
<p>이 모델에서 우리는 인스턴스화 시 무작위로 초기화되고 이후에는 일정하게 유지되는 가중치(<code>self.rand_weight</code>)를 가진 은닉층을 구현합니다.
이 가중치는 모델 파라미터가 아니므로 역전파에 의해 업데이트되지 않습니다.
그런 다음 네트워크는 이 "고정된" 레이어의 출력을 완전 연결 레이어를 통해 전달합니다.</p>
<p>출력을 반환하기 전에 우리 모델이 특이한 작업을 수행했다는 점에 유의하십시오.
우리는 while 루프를 실행하여 $\ell_1$ 노름이 1보다 크다는 조건을 테스트하고, 조건을 만족할 때까지 출력 벡터를 2로 나누었습니다.
마지막으로 <code>X</code> 항목의 합계를 반환했습니다.
우리가 아는 한, 이 연산을 수행하는 표준 신경망은 없습니다.
이 특정 작업은 실제 작업에서 유용하지 않을 수 있습니다.
우리의 요점은 신경망 계산 흐름에 임의의 코드를 통합하는 방법을 보여주는 것뿐입니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
net = FixedHiddenMLP()
if tab.selected('mxnet'):
    net.initialize()
net(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = FixedHiddenMLP()
params = net.init(d2l.get_key(), X)
net.apply(params, X)
</code></pre>
<p>우리는 [<strong>모듈을 조립하는 다양한 방법을 혼합하고 일치</strong>]시킬 수 있습니다.
다음 예제에서는 모듈을 창의적인 방식으로 중첩합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class NestMLP(nn.Block):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.net = nn.Sequential()
        self.net.add(nn.Dense(64, activation='relu'),
                     nn.Dense(32, activation='relu'))
        self.dense = nn.Dense(16, activation='relu')

    def forward(self, X):
        return self.dense(self.net(X))

chimera = nn.Sequential()
chimera.add(NestMLP(), nn.Dense(20), FixedHiddenMLP())
chimera.initialize()
chimera(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),
                                 nn.LazyLinear(32), nn.ReLU())
        self.linear = nn.LazyLinear(16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())
chimera(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class NestMLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.net = tf.keras.Sequential()
        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))
        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))
        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)

    def call(self, inputs):
        return self.dense(self.net(inputs))

chimera = tf.keras.Sequential()
chimera.add(NestMLP())
chimera.add(tf.keras.layers.Dense(20))
chimera.add(FixedHiddenMLP())
chimera(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class NestMLP(nn.Module):
    def setup(self):
        self.net = nn.Sequential([nn.Dense(64), nn.relu,
                                  nn.Dense(32), nn.relu])
        self.dense = nn.Dense(16)

    def __call__(self, X):
        return self.dense(self.net(X))


chimera = nn.Sequential([NestMLP(), nn.Dense(20), FixedHiddenMLP()])
params = chimera.init(d2l.get_key(), X)
chimera.apply(params, X)
</code></pre>
<h2 id="요약-summary-18"><a class="header" href="#요약-summary-18">요약 (Summary)</a></h2>
<p>개별 레이어는 모듈이 될 수 있습니다.
많은 레이어가 모듈을 구성할 수 있습니다.
많은 모듈이 모듈을 구성할 수 있습니다.</p>
<p>모듈은 코드를 포함할 수 있습니다.
모듈은 파라미터 초기화 및 역전파를 포함한 많은 관리 작업을 처리합니다.
레이어와 모듈의 순차적 연결은 <code>Sequential</code> 모듈에 의해 처리됩니다.</p>
<h2 id="연습-문제-exercises-21"><a class="header" href="#연습-문제-exercises-21">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>MySequential</code>을 변경하여 모듈을 Python 리스트에 저장하면 어떤 종류의 문제가 발생합니까?</li>
<li>두 개의 모듈을 인수로 받아, 예를 들어 <code>net1</code>과 <code>net2</code>, 순전파에서 두 네트워크의 연결된 출력을 반환하는 모듈을 구현하십시오. 이를 *병렬 모듈(parallel module)*이라고도 합니다.</li>
<li>동일한 네트워크의 여러 인스턴스를 연결하고 싶다고 가정해 봅시다. 동일한 모듈의 여러 인스턴스를 생성하는 팩토리 함수를 구현하고 그로부터 더 큰 네트워크를 구축하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/54">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/55">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/264">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17989">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="파라미터-관리-parameter-management"><a class="header" href="#파라미터-관리-parameter-management">파라미터 관리 (Parameter Management)</a></h1>
<p>아키텍처를 선택하고 하이퍼파라미터를 설정했으면 훈련 루프로 진행합니다.
여기서 우리의 목표는 손실 함수를 최소화하는 파라미터 값을 찾는 것입니다.
훈련 후에는 향후 예측을 위해 이러한 파라미터가 필요합니다.
또한 다른 맥락에서 재사용하거나, 다른 소프트웨어에서 실행될 수 있도록 모델을 디스크에 저장하거나, 과학적 이해를 얻기 위해 검사하기 위해 파라미터를 추출하고 싶을 때가 있습니다.</p>
<p>대부분의 경우, 우리는 파라미터가 선언되고 조작되는 구체적인 세부 사항을 무시하고 딥러닝 프레임워크에 무거운 작업을 맡길 수 있습니다.
하지만 표준 레이어가 쌓인 아키텍처에서 벗어날 때, 파라미터 선언 및 조작의 세부 사항으로 들어가야 할 때가 있습니다.
이 섹션에서는 다음 내용을 다룹니다:</p>
<ul>
<li>디버깅, 진단 및 시각화를 위한 파라미터 액세스.</li>
<li>서로 다른 모델 구성 요소 간의 파라미터 공유.</li>
</ul>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<p>(<strong>은닉층이 하나 있는 MLP에 집중하는 것으로 시작합니다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = nn.Sequential()
net.add(nn.Dense(8, activation='relu'))
net.add(nn.Dense(1))
net.initialize()  # 기본 초기화 방법 사용

X = np.random.uniform(size=(2, 4))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net = nn.Sequential(nn.LazyLinear(8),
                    nn.ReLU(),
                    nn.LazyLinear(1))

X = torch.rand(size=(2, 4))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(4, activation=tf.nn.relu),
    tf.keras.layers.Dense(1),
])

X = tf.random.uniform((2, 4))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = nn.Sequential([nn.Dense(8), nn.relu, nn.Dense(1)])

X = jax.random.uniform(d2l.get_key(), (2, 4))
params = net.init(d2l.get_key(), X)
net.apply(params, X).shape
</code></pre>
<h2 id="파라미터-액세스-parameter-access"><a class="header" href="#파라미터-액세스-parameter-access">[<strong>파라미터 액세스 (Parameter Access)</strong>]</a></h2>
<p>:label:<code>subsec_param-access</code></p>
<p>이미 알고 있는 모델에서 파라미터에 액세스하는 방법부터 시작해 봅시다.</p>
<p>:begin_tab:<code>mxnet, pytorch, tensorflow</code>
모델이 <code>Sequential</code> 클래스를 통해 정의되면, 모델을 리스트인 것처럼 인덱싱하여 모든 레이어에 먼저 액세스할 수 있습니다.
각 레이어의 파라미터는 해당 속성에 편리하게 위치해 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
이전에 정의한 모델에서 관찰했을 수 있듯이 Flax와 JAX는 모델과 파라미터를 분리합니다.
모델이 <code>Sequential</code> 클래스를 통해 정의되면, 먼저 네트워크를 초기화하여 파라미터 딕셔너리를 생성해야 합니다.
이 딕셔너리의 키를 통해 모든 레이어의 파라미터에 액세스할 수 있습니다.
:end_tab:</p>
<p>다음과 같이 두 번째 완전 연결 레이어의 파라미터를 검사할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net[1].params
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net[2].state_dict()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net.layers[2].weights
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
params['params']['layers_2']
</code></pre>
<p>이 완전 연결 레이어에는 두 개의 파라미터가 포함되어 있음을 알 수 있습니다.
각각 해당 레이어의 가중치와 편향에 해당합니다.</p>
<h3 id="타겟-파라미터-targeted-parameters"><a class="header" href="#타겟-파라미터-targeted-parameters">[<strong>타겟 파라미터 (Targeted Parameters)</strong>]</a></h3>
<p>각 파라미터는 파라미터 클래스의 인스턴스로 표현됩니다.
파라미터로 유용한 작업을 하려면 먼저 기본 수치 값에 액세스해야 합니다.
이를 수행하는 방법에는 여러 가지가 있습니다.
일부는 더 간단하고 다른 일부는 더 일반적입니다.
다음 코드는 두 번째 신경망 레이어에서 편향을 추출하여 파라미터 클래스 인스턴스를 반환하고,
더 나아가 해당 파라미터의 값에 액세스합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
type(net[1].bias), net[1].bias.data()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
type(net[2].bias), net[2].bias.data
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
type(net.layers[2].weights[1]), tf.convert_to_tensor(net.layers[2].weights[1])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
bias = params['params']['layers_2']['bias']
type(bias), bias
</code></pre>
<p>:begin_tab:<code>mxnet,pytorch</code>
파라미터는 값, 기울기 및 추가 정보를 포함하는 복잡한 객체입니다.
그렇기 때문에 값을 명시적으로 요청해야 합니다.</p>
<p>값 외에도 각 파라미터는 기울기에 액세스할 수 있게 해줍니다. 이 네트워크에 대해 아직 역전파를 호출하지 않았으므로 초기 상태입니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
다른 프레임워크와 달리 JAX는 신경망 파라미터에 대한 기울기를 추적하지 않고 대신 파라미터와 네트워크가 분리됩니다.
사용자가 계산을 Python 함수로 표현하고 동일한 목적을 위해 <code>grad</code> 변환을 사용할 수 있게 합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net[1].weight.grad()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net[2].weight.grad == None
</code></pre>
<h3 id="한꺼번에-모든-파라미터-all-parameters-at-once"><a class="header" href="#한꺼번에-모든-파라미터-all-parameters-at-once">[<strong>한꺼번에 모든 파라미터 (All Parameters at Once)</strong>]</a></h3>
<p>모든 파라미터에 대해 작업을 수행해야 할 때, 하나씩 액세스하는 것은 지루할 수 있습니다.
더 복잡한, 예를 들어 중첩된 모듈로 작업할 때 상황은 특히 다루기 어려워질 수 있습니다.
각 하위 모듈의 파라미터를 추출하기 위해 전체 트리를 재귀적으로 탐색해야 하기 때문입니다. 아래에서는 모든 레이어의 파라미터에 액세스하는 것을 보여줍니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net.collect_params()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
[(name, param.shape) for name, param in net.named_parameters()]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net.get_weights()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jax.tree_util.tree_map(lambda x: x.shape, params)
</code></pre>
<h2 id="묶인-파라미터-tied-parameters"><a class="header" href="#묶인-파라미터-tied-parameters">[<strong>묶인 파라미터 (Tied Parameters)</strong>]</a></h2>
<p>종종 여러 레이어 간에 파라미터를 공유하고 싶을 때가 있습니다.
우아하게 수행하는 방법을 알아봅시다.
다음에서는 완전 연결 레이어를 할당한 다음 해당 파라미터를 사용하여 다른 레이어의 파라미터를 설정합니다.
여기서 파라미터에 액세스하기 전에 순전파 <code>net(X)</code>를 실행해야 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = nn.Sequential()
# 공유 레이어의 파라미터를 참조할 수 있도록 이름을 지정해야 합니다
shared = nn.Dense(8, activation='relu')
net.add(nn.Dense(8, activation='relu'),
        shared,
        nn.Dense(8, activation='relu', params=shared.params),
        nn.Dense(10))
net.initialize()

X = np.random.uniform(size=(2, 20))

net(X)
# 파라미터가 동일한지 확인
print(net[1].weight.data()[0] == net[2].weight.data()[0])
net[1].weight.data()[0, 0] = 100
# 단순히 같은 값을 갖는 것이 아니라 실제로 같은 객체인지 확인
print(net[1].weight.data()[0] == net[2].weight.data()[0])
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 공유 레이어의 파라미터를 참조할 수 있도록 이름을 지정해야 합니다
shared = nn.LazyLinear(8)
net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.LazyLinear(1))

net(X)
# 파라미터가 동일한지 확인
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 단순히 같은 값을 갖는 것이 아니라 실제로 같은 객체인지 확인
print(net[2].weight.data[0] == net[4].weight.data[0])
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# tf.keras는 약간 다르게 동작합니다. 중복 레이어를 자동으로 제거합니다
shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)
net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    shared,
    shared,
    tf.keras.layers.Dense(1),
])

net(X)
# 파라미터가 다른지 확인
print(len(net.layers) == 3)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# 공유 레이어의 파라미터를 참조할 수 있도록 이름을 지정해야 합니다
shared = nn.Dense(8)
net = nn.Sequential([nn.Dense(8), nn.relu,
                     shared, nn.relu,
                     shared, nn.relu,
                     nn.Dense(1)])

params = net.init(jax.random.PRNGKey(d2l.get_seed()), X)

# 파라미터가 다른지 확인
print(len(params['params']) == 3)
</code></pre>
<p>이 예제는 두 번째와 세 번째 레이어의 파라미터가 묶여 있음을 보여줍니다.
그것들은 단순히 같은 것이 아니라 정확히 같은 텐서로 표현됩니다.
따라서 파라미터 중 하나를 변경하면 다른 하나도 변경됩니다.</p>
<p>:begin_tab:<code>mxnet, pytorch, tensorflow</code>
파라미터가 묶여 있을 때 기울기는 어떻게 되는지 궁금할 수 있습니다.
모델 파라미터에 기울기가 포함되어 있으므로,
역전파 중에 두 번째 은닉층과 세 번째 은닉층의 기울기가 함께 더해집니다.
:end_tab:</p>
<h2 id="요약-summary-19"><a class="header" href="#요약-summary-19">요약 (Summary)</a></h2>
<p>우리는 모델 파라미터에 액세스하고 묶는 몇 가지 방법을 가지고 있습니다.</p>
<h2 id="연습-문제-exercises-22"><a class="header" href="#연습-문제-exercises-22">연습 문제 (Exercises)</a></h2>
<ol>
<li>:numref:<code>sec_model_construction</code>에 정의된 <code>NestMLP</code> 모델을 사용하여 다양한 레이어의 파라미터에 액세스하십시오.</li>
<li>공유 파라미터 레이어를 포함하는 MLP를 구성하고 훈련하십시오. 훈련 과정 동안 각 레이어의 모델 파라미터와 기울기를 관찰하십시오.</li>
<li>파라미터를 공유하는 것이 왜 좋은 아이디어입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/56">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/57">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/269">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17990">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="파라미터-초기화-parameter-initialization-1"><a class="header" href="#파라미터-초기화-parameter-initialization-1">파라미터 초기화 (Parameter Initialization)</a></h1>
<p>이제 파라미터에 액세스하는 방법을 알았으니, 올바르게 초기화하는 방법을 살펴봅시다.
우리는 :numref:<code>sec_numerical_stability</code>에서 올바른 초기화의 필요성에 대해 논의했습니다.
딥러닝 프레임워크는 레이어에 기본 무작위 초기화를 제공합니다.
그러나 우리는 종종 다양한 다른 프로토콜에 따라 가중치를 초기화하고 싶어 합니다. 프레임워크는 가장 일반적으로 사용되는 프로토콜을 제공하며, 사용자 정의 초기화 생성기를 만들 수도 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<p>:begin_tab:<code>mxnet</code>
기본적으로 MXNet은 균등 분포 $U(-0.07, 0.07)$에서 무작위로 추출하여 가중치 파라미터를 초기화하고 편향 파라미터는 0으로 지웁니다.
MXNet의 <code>init</code> 모듈은 다양한 사전 설정 초기화 방법을 제공합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
기본적으로 PyTorch는 입력 및 출력 차원에 따라 계산된 범위에서 균등하게 추출하여 가중치와 편향 행렬을 초기화합니다.
PyTorch의 <code>nn.init</code> 모듈은 다양한 사전 설정 초기화 방법을 제공합니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
기본적으로 Keras는 입력 및 출력 차원에 따라 계산된 범위에서 균등하게 추출하여 가중치 행렬을 초기화하고, 편향 파라미터는 모두 0으로 설정됩니다.
TensorFlow는 루트 모듈과 <code>keras.initializers</code> 모듈 모두에서 다양한 초기화 방법을 제공합니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
기본적으로 Flax는 <code>jax.nn.initializers.lecun_normal</code>을 사용하여 가중치를 초기화합니다. 즉, 가중치 텐서의 입력 유닛 수인 <code>fan_in</code>에 대해 $1 / \textrm{fan}_{\textrm{in}}$의 제곱근으로 표준 편차를 설정하고 0을 중심으로 하는 절단된 정규 분포에서 샘플을 추출합니다. 편향 파라미터는 모두 0으로 설정됩니다.
Jax의 <code>nn.initializers</code> 모듈은 다양한 사전 설정 초기화 방법을 제공합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = nn.Sequential()
net.add(nn.Dense(8, activation='relu'))
net.add(nn.Dense(1))
net.initialize()  # 기본 초기화 방법 사용

X = np.random.uniform(size=(2, 4))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))
X = torch.rand(size=(2, 4))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(4, activation=tf.nn.relu),
    tf.keras.layers.Dense(1),
])

X = tf.random.uniform((2, 4))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = nn.Sequential([nn.Dense(8), nn.relu, nn.Dense(1)])
X = jax.random.uniform(d2l.get_key(), (2, 4))
params = net.init(d2l.get_key(), X)
net.apply(params, X).shape
</code></pre>
<h2 id="내장-초기화-built-in-initialization"><a class="header" href="#내장-초기화-built-in-initialization">[<strong>내장 초기화 (Built-in Initialization)</strong>]</a></h2>
<p>내장 초기화 생성기를 호출하여 시작해 봅시다.
아래 코드는 모든 가중치 파라미터를 표준 편차 0.01인 가우스 확률 변수로 초기화하고 편향 파라미터는 0으로 지웁니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
# 여기서 force_reinit은 파라미터가 이전에 이미 초기화되었더라도
# 새로 초기화되도록 합니다
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)
net[0].weight.data()[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def init_normal(module):
    if type(module) == nn.Linear:
        nn.init.normal_(module.weight, mean=0, std=0.01)
        nn.init.zeros_(module.bias)

net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(
        4, activation=tf.nn.relu,
        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),
        bias_initializer=tf.zeros_initializer()),
    tf.keras.layers.Dense(1)])

net(X)
net.weights[0], net.weights[1]
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
weight_init = nn.initializers.normal(0.01)
bias_init = nn.initializers.zeros

net = nn.Sequential([nn.Dense(8, kernel_init=weight_init, bias_init=bias_init),
                     nn.relu,
                     nn.Dense(1, kernel_init=weight_init, bias_init=bias_init)])

params = net.init(jax.random.PRNGKey(d2l.get_seed()), X)
layer_0 = params['params']['layers_0']
layer_0['kernel'][:, 0], layer_0['bias'][0]
</code></pre>
<p>또한 모든 파라미터를 주어진 상수 값(예: 1)으로 초기화할 수도 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net.initialize(init=init.Constant(1), force_reinit=True)
net[0].weight.data()[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def init_constant(module):
    if type(module) == nn.Linear:
        nn.init.constant_(module.weight, 1)
        nn.init.zeros_(module.bias)

net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(
        4, activation=tf.nn.relu,
        kernel_initializer=tf.keras.initializers.Constant(1),
        bias_initializer=tf.zeros_initializer()),
    tf.keras.layers.Dense(1),
])

net(X)
net.weights[0], net.weights[1]
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
weight_init = nn.initializers.constant(1)

net = nn.Sequential([nn.Dense(8, kernel_init=weight_init, bias_init=bias_init),
                     nn.relu,
                     nn.Dense(1, kernel_init=weight_init, bias_init=bias_init)])

params = net.init(jax.random.PRNGKey(d2l.get_seed()), X)
layer_0 = params['params']['layers_0']
layer_0['kernel'][:, 0], layer_0['bias'][0]
</code></pre>
<p>[<strong>특정 블록에 대해 다른 초기화 생성기를 적용할 수도 있습니다.</strong>]
예를 들어, 아래에서는 첫 번째 레이어를 Xavier 초기화 생성기로 초기화하고
두 번째 레이어를 상수 값 42로 초기화합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net[0].weight.initialize(init=init.Xavier(), force_reinit=True)
net[1].initialize(init=init.Constant(42), force_reinit=True)
print(net[0].weight.data()[0])
print(net[1].weight.data())
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def init_xavier(module):
    if type(module) == nn.Linear:
        nn.init.xavier_uniform_(module.weight)

def init_42(module):
    if type(module) == nn.Linear:
        nn.init.constant_(module.weight, 42)

net[0].apply(init_xavier)
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(
        4,
        activation=tf.nn.relu,
        kernel_initializer=tf.keras.initializers.GlorotUniform()),
    tf.keras.layers.Dense(
        1, kernel_initializer=tf.keras.initializers.Constant(42)),
])

net(X)
print(net.layers[1].weights[0])
print(net.layers[2].weights[0])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = nn.Sequential([nn.Dense(8, kernel_init=nn.initializers.xavier_uniform(),
                              bias_init=bias_init),
                     nn.relu,
                     nn.Dense(1, kernel_init=nn.initializers.constant(42),
                              bias_init=bias_init)])

params = net.init(jax.random.PRNGKey(d2l.get_seed()), X)
params['params']['layers_0']['kernel'][:, 0], params['params']['layers_2']['kernel']
</code></pre>
<h3 id="사용자-정의-초기화-custom-initialization"><a class="header" href="#사용자-정의-초기화-custom-initialization">[<strong>사용자 정의 초기화 (Custom Initialization)</strong>]</a></h3>
<p>때때로 우리가 필요한 초기화 방법이 딥러닝 프레임워크에서 제공되지 않을 수 있습니다.
아래 예제에서는 다음과 같은 이상한 분포를 사용하여 가중치 파라미터 $w$에 대한 초기화 생성기를 정의합니다:</p>
<p>$$
\begin{aligned}
w \sim \begin{cases}
U(5, 10) &amp; \textrm{ 확률 } \frac{1}{4} \
0    &amp; \textrm{ 확률 } \frac{1}{2} \
U(-10, -5) &amp; \textrm{ 확률 } \frac{1}{4}
\end{cases}
\end{aligned}
$$</p>
<p>:begin_tab:<code>mxnet</code>
여기서는 <code>Initializer</code> 클래스의 서브클래스를 정의합니다.
일반적으로 텐서 인수(<code>data</code>)를 받아 원하는 초기화 값을 할당하는 <code>_init_weight</code> 함수만 구현하면 됩니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
다시 한번, <code>net</code>에 적용할 <code>my_init</code> 함수를 구현합니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
여기서는 <code>Initializer</code>의 서브클래스를 정의하고 모양과 데이터 유형이 주어졌을 때 원하는 텐서를 반환하는 <code>__call__</code> 함수를 구현합니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
Jax 초기화 함수는 <code>PRNGKey</code>, <code>shape</code>, <code>dtype</code>을 인수로 받습니다. 여기서는 모양과 데이터 유형이 주어졌을 때 원하는 텐서를 반환하는 <code>my_init</code> 함수를 구현합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class MyInit(init.Initializer):
    def _init_weight(self, name, data):
        print('초기화', name, data.shape)
        data[:] = np.random.uniform(-10, 10, data.shape)
        data *= np.abs(data) &gt;= 5

net.initialize(MyInit(), force_reinit=True)
net[0].weight.data()[:2]
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def my_init(module):
    if type(module) == nn.Linear:
        print("초기화", *[(name, param.shape)
                        for name, param in module.named_parameters()][0])
        nn.init.uniform_(module.weight, -10, 10)
        module.weight.data *= module.weight.data.abs() &gt;= 5

net.apply(my_init)
net[0].weight[:2]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class MyInit(tf.keras.initializers.Initializer):
    def __call__(self, shape, dtype=None):
        data=tf.random.uniform(shape, -10, 10, dtype=dtype)
        factor=(tf.abs(data) &gt;= 5)
        factor=tf.cast(factor, tf.float32)
        return data * factor

net = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(
        4,
        activation=tf.nn.relu,
        kernel_initializer=MyInit()),
    tf.keras.layers.Dense(1),
])

net(X)
print(net.layers[1].weights[0])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def my_init(key, shape, dtype=jnp.float_):
    data = jax.random.uniform(key, shape, minval=-10, maxval=10)
    return data * (jnp.abs(data) &gt;= 5)

net = nn.Sequential([nn.Dense(8, kernel_init=my_init), nn.relu, nn.Dense(1)])
params = net.init(d2l.get_key(), X)
print(params['params']['layers_0']['kernel'][:, :2])
</code></pre>
<p>:begin_tab:<code>mxnet, pytorch, tensorflow</code>
파라미터를 직접 설정하는 옵션은 언제나 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
JAX와 Flax에서 파라미터를 초기화할 때 반환되는 파라미터 딕셔너리는 <code>flax.core.frozen_dict.FrozenDict</code> 유형을 갖습니다. Jax 생태계에서는 배열의 값을 직접 변경하는 것이 권장되지 않으므로, 데이터 유형은 일반적으로 불변입니다. 변경하려면 <code>params.unfreeze()</code>를 사용할 수 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net[0].weight.data()[:] += 1
net[0].weight.data()[0, 0] = 42
net[0].weight.data()[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
net[0].weight.data[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)
net.layers[1].weights[0][0, 0].assign(42)
net.layers[1].weights[0]
</code></pre>
<h2 id="요약-summary-20"><a class="header" href="#요약-summary-20">요약 (Summary)</a></h2>
<p>내장 및 사용자 정의 초기화 생성기를 사용하여 파라미터를 초기화할 수 있습니다.</p>
<h2 id="연습-문제-exercises-23"><a class="header" href="#연습-문제-exercises-23">연습 문제 (Exercises)</a></h2>
<p>더 많은 내장 초기화 생성기에 대해 온라인 문서를 찾아보십시오.</p>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/8089">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/8090">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/8091">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17991">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="지연-초기화-lazy-initialization"><a class="header" href="#지연-초기화-lazy-initialization">지연 초기화 (Lazy Initialization)</a></h1>
<p>:label:<code>sec_lazy_init</code></p>
<p>지금까지 우리는 네트워크를 설정하는 데 있어 다소 엉성했던 것처럼 보일 수 있습니다.
구체적으로, 다음과 같은 직관적이지 않고 작동하지 않아야 할 것 같은 일들을 했습니다:</p>
<ul>
<li>입력 차원을 지정하지 않고 네트워크 아키텍처를 정의했습니다.</li>
<li>이전 레이어의 출력 차원을 지정하지 않고 레이어를 추가했습니다.</li>
<li>모델에 몇 개의 파라미터가 포함되어야 하는지 결정하기에 충분한 정보를 제공하기도 전에 파라미터를 "초기화"했습니다.</li>
</ul>
<p>코드가 실행된다는 것 자체가 놀라울 수 있습니다.
결국 딥러닝 프레임워크가 네트워크의 입력 차원이 무엇인지 알 수 있는 방법은 없습니다.
여기서 트릭은 프레임워크가 *초기화를 지연(defers initialization)*하여, 모델을 통해 데이터를 처음 전달할 때까지 기다렸다가 즉석에서 각 레이어의 크기를 추론한다는 것입니다.</p>
<p>나중에 합성곱 신경망을 다룰 때, 이 기술은 더욱 편리해질 것입니다.
입력 차원(예: 이미지의 해상도)이 후속 각 레이어의 차원에 영향을 미치기 때문입니다.
따라서 코드를 작성할 때 차원 값을 알 필요 없이 파라미터를 설정할 수 있는 능력은 모델을 지정하고 나중에 수정하는 작업을 크게 단순화할 수 있습니다.
다음으로 초기화 메커니즘을 더 깊이 살펴봅니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<p>시작하기 위해 MLP를 인스턴스화해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(10),
])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(10)])
</code></pre>
<p>이 시점에서 네트워크는 입력 레이어 가중치의 차원을 알 수 없습니다.
입력 차원이 아직 알려지지 않았기 때문입니다.</p>
<p>:begin_tab:<code>mxnet, pytorch, tensorflow</code>
결과적으로 프레임워크는 아직 어떤 파라미터도 초기화하지 않았습니다.
아래에서 파라미터에 액세스하려고 시도하여 이를 확인합니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
:numref:<code>subsec_param-access</code>에서 언급했듯이 Jax와 Flax에서는 파라미터와 네트워크 정의가 분리되어 있으며 사용자가 둘 다 수동으로 처리합니다. Flax 모델은 상태 비저장(stateless)이므로 <code>parameters</code> 속성이 없습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
print(net.collect_params)
print(net.collect_params())
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net[0].weight
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
[net.layers[i].get_weights() for i in range(len(net.layers))]
</code></pre>
<p>:begin_tab:<code>mxnet</code>
파라미터 객체는 존재하지만 각 레이어에 대한 입력 차원이 -1로 나열되어 있음에 유의하십시오.
MXNet은 파라미터 차원이 아직 알려지지 않았음을 나타내기 위해 특수 값 -1을 사용합니다.
이 시점에서 <code>net[0].weight.data()</code>에 액세스하려고 시도하면 파라미터에 액세스하기 전에 네트워크를 초기화해야 한다는 런타임 오류가 발생합니다.
이제 <code>initialize</code> 메서드를 통해 파라미터를 초기화하려고 시도하면 어떻게 되는지 봅시다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
각 레이어 객체는 존재하지만 가중치는 비어 있다는 점에 유의하십시오.
가중치가 아직 초기화되지 않았으므로 <code>net.get_weights()</code>를 사용하면 오류가 발생합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net.initialize()
net.collect_params()
</code></pre>
<p>:begin_tab:<code>mxnet</code>
보시다시피 아무것도 바뀌지 않았습니다.
입력 차원을 알 수 없을 때 initialize 호출은 실제로 파라미터를 초기화하지 않습니다.
대신 이 호출은 파라미터를 초기화하고 싶다는 의사를 (선택적으로 어떤 분포에 따라) MXNet에 등록합니다.
:end_tab:</p>
<p>이제 네트워크를 통해 데이터를 전달하여 프레임워크가 마침내 파라미터를 초기화하도록 해봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
X = np.random.uniform(size=(2, 20))
net(X)

net.collect_params()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
X = torch.rand(2, 20)
net(X)

net[0].weight.shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X = tf.random.uniform((2, 20))
net(X)
[w.shape for w in net.get_weights()]
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
params = net.init(d2l.get_key(), jnp.zeros((2, 20)))
jax.tree_util.tree_map(lambda x: x.shape, params).tree_flatten_with_keys()
</code></pre>
<p>입력 차원이 20이라는 것을 알게 되자마자, 프레임워크는 20 값을 대입하여 첫 번째 레이어의 가중치 행렬 모양을 식별할 수 있습니다.
첫 번째 레이어의 모양을 인식한 후, 프레임워크는 두 번째 레이어로, 그리고 계산 그래프를 통해 모든 모양이 알려질 때까지 계속 진행합니다.
이 경우 첫 번째 레이어만 지연 초기화가 필요하지만, 프레임워크는 순차적으로 초기화합니다.
모든 파라미터 모양이 알려지면 프레임워크는 마침내 파라미터를 초기화할 수 있습니다.</p>
<p>:begin_tab:<code>pytorch</code>
다음 메서드는 모든 파라미터 모양을 추론하기 위해 네트워크를 통해 더미 입력을 전달하여 예행 연습을 하고,
그 후 파라미터를 초기화합니다.
기본 무작위 초기화가 필요하지 않을 때 나중에 사용될 것입니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
Flax의 파라미터 초기화는 항상 수동으로 수행되며 사용자가 처리합니다.
다음 메서드는 더미 입력과 키 딕셔너리를 인수로 받습니다.
이 키 딕셔너리에는 모델 파라미터를 초기화하기 위한 rng와 드롭아웃 레이어가 있는 모델의 드롭아웃 마스크를 생성하기 위한 드롭아웃 rng가 있습니다. 드롭아웃에 대한 자세한 내용은 나중에 :numref:<code>sec_dropout</code>에서 다룰 것입니다.
결국 메서드는 모델을 초기화하고 파라미터를 반환합니다.
우리는 이전 섹션에서도 내부적으로 이를 사용해 왔습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch
@d2l.add_to_class(d2l.Module)  #@save
def apply_init(self, inputs, init=None):
    self.forward(*inputs)
    if init is not None:
        self.net.apply(init)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(d2l.Module)  #@save
def apply_init(self, dummy_input, key):
    params = self.init(key, *dummy_input)  # dummy_input 튜플 언팩됨
    return params
</code></pre>
<h2 id="요약-summary-21"><a class="header" href="#요약-summary-21">요약 (Summary)</a></h2>
<p>지연 초기화는 편리할 수 있으며, 프레임워크가 파라미터 모양을 자동으로 추론하도록 하여 아키텍처를 쉽게 수정하고 일반적인 오류 원인을 제거할 수 있습니다.
모델을 통해 데이터를 전달하여 프레임워크가 최종적으로 파라미터를 초기화하도록 할 수 있습니다.</p>
<h2 id="연습-문제-exercises-24"><a class="header" href="#연습-문제-exercises-24">연습 문제 (Exercises)</a></h2>
<ol>
<li>첫 번째 레이어에만 입력 차원을 지정하고 후속 레이어에는 지정하지 않으면 어떻게 됩니까? 즉시 초기화됩니까?</li>
<li>일치하지 않는 차원을 지정하면 어떻게 됩니까?</li>
<li>입력 차원이 다양한 경우 어떻게 해야 합니까? 힌트: 파라미터 묶기를 살펴보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/280">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/8092">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/281">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17992">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="사용자-정의-레이어-custom-layers"><a class="header" href="#사용자-정의-레이어-custom-layers">사용자 정의 레이어 (Custom Layers)</a></h1>
<p>딥러닝의 성공 요인 중 하나는 다양한 작업에 적합한 아키텍처를 설계하기 위해 창의적인 방식으로 구성할 수 있는 광범위한 레이어를 사용할 수 있다는 점입니다.
예를 들어 연구자들은 이미지, 텍스트를 처리하고 순차 데이터를 반복하며 동적 프로그래밍을 수행하기 위한 레이어를 발명했습니다.
조만간 딥러닝 프레임워크에 아직 존재하지 않는 레이어가 필요하게 될 것입니다.
이러한 경우 사용자 정의 레이어를 구축해야 합니다.
이 섹션에서는 그 방법을 보여드립니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="파라미터-없는-레이어-layers-without-parameters"><a class="header" href="#파라미터-없는-레이어-layers-without-parameters">(<strong>파라미터 없는 레이어 (Layers without Parameters)</strong>)</a></h2>
<p>시작하기 위해 자체 파라미터가 없는 사용자 정의 레이어를 구성해 보겠습니다.
:numref:<code>sec_model_construction</code>의 모듈 소개를 기억한다면 익숙해 보일 것입니다.
다음 <code>CenteredLayer</code> 클래스는 단순히 입력에서 평균을 뺍니다.
이를 구축하려면 기본 레이어 클래스를 상속하고 순전파 함수를 구현하기만 하면 됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class CenteredLayer(nn.Block):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def forward(self, X):
        return X - X.mean()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class CenteredLayer(tf.keras.Model):
    def __init__(self):
        super().__init__()

    def call(self, X):
        return X - tf.reduce_mean(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class CenteredLayer(nn.Module):
    def __call__(self, X):
        return X - X.mean()
</code></pre>
<p>데이터를 공급하여 레이어가 의도한 대로 작동하는지 확인해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
layer = CenteredLayer()
layer(d2l.tensor([1.0, 2, 3, 4, 5]))
</code></pre>
<p>이제 [<strong>더 복잡한 모델을 구성하는 데 우리 레이어를 구성 요소로 통합</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = nn.Sequential()
net.add(nn.Dense(128), CenteredLayer())
net.initialize()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.Sequential([tf.keras.layers.Dense(128), CenteredLayer()])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = nn.Sequential([nn.Dense(128), CenteredLayer()])
</code></pre>
<p>추가적인 정상성 확인으로, 네트워크를 통해 무작위 데이터를 보내고 평균이 실제로 0인지 확인할 수 있습니다.
부동 소수점 숫자를 다루고 있기 때문에 양자화로 인해 매우 작은 0이 아닌 숫자가 보일 수 있습니다.</p>
<p>:begin_tab:<code>jax</code>
여기서는 네트워크의 출력과 파라미터를 모두 반환하는 <code>init_with_output</code> 메서드를 활용합니다. 이 경우 우리는 출력에만 집중합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet
Y = net(d2l.rand(4, 8))
Y.mean()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
Y = net(tf.random.uniform((4, 8)))
tf.reduce_mean(Y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
Y, _ = net.init_with_output(d2l.get_key(), jax.random.uniform(d2l.get_key(),
                                                              (4, 8)))
Y.mean()
</code></pre>
<h2 id="파라미터가-있는-레이어-layers-with-parameters"><a class="header" href="#파라미터가-있는-레이어-layers-with-parameters">[<strong>파라미터가 있는 레이어 (Layers with Parameters)</strong>]</a></h2>
<p>이제 단순한 레이어를 정의하는 방법을 알았으니, 훈련을 통해 조정할 수 있는 파라미터가 있는 레이어를 정의하는 것으로 넘어가겠습니다.
기본적인 관리 기능을 제공하는 내장 함수를 사용하여 파라미터를 생성할 수 있습니다.
특히 모델 파라미터의 액세스, 초기화, 공유, 저장 및 로드를 관리합니다.
이렇게 하면 다른 이점들 중에서도 모든 사용자 정의 레이어에 대해 사용자 정의 직렬화 루틴을 작성할 필요가 없습니다.</p>
<p>이제 완전 연결 레이어의 자체 버전을 구현해 봅시다.
이 레이어에는 가중치를 나타내는 파라미터 하나와 편향을 위한 파라미터 하나, 총 두 개의 파라미터가 필요합니다.
이 구현에서는 기본적으로 ReLU 활성화를 포함합니다.
이 레이어는 <code>in_units</code>와 <code>units</code>라는 두 개의 입력 인수가 필요하며, 각각 입력 및 출력 수를 나타냅니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class MyDense(nn.Block):
    def __init__(self, units, in_units, **kwargs):
        super().__init__(**kwargs)
        self.weight = self.params.get('weight', shape=(in_units, units))
        self.bias = self.params.get('bias', shape=(units,))

    def forward(self, x):
        linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(
            ctx=x.ctx)
        return npx.relu(linear)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
        
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class MyDense(tf.keras.Model):
    def __init__(self, units):
        super().__init__()
        self.units = units

    def build(self, X_shape):
        self.weight = self.add_weight(name='weight',
            shape=[X_shape[-1], self.units],
            initializer=tf.random_normal_initializer())
        self.bias = self.add_weight(
            name='bias', shape=[self.units],
            initializer=tf.zeros_initializer())

    def call(self, X):
        linear = tf.matmul(X, self.weight) + self.bias
        return tf.nn.relu(linear)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class MyDense(nn.Module):
    in_units: int
    units: int

    def setup(self):
        self.weight = self.param('weight', nn.initializers.normal(stddev=1),
                                 (self.in_units, self.units))
        self.bias = self.param('bias', nn.initializers.zeros, self.units)

    def __call__(self, X):
        linear = jnp.matmul(X, self.weight) + self.bias
        return nn.relu(linear)
</code></pre>
<p>:begin_tab:<code>mxnet, tensorflow, jax</code>
다음으로 <code>MyDense</code> 클래스를 인스턴스화하고 모델 파라미터에 액세스합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
다음으로 <code>MyLinear</code> 클래스를 인스턴스화하고 모델 파라미터에 액세스합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
dense = MyDense(units=3, in_units=5)
dense.params
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
linear = MyLinear(5, 3)
linear.weight
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
dense = MyDense(3)
dense(tf.random.uniform((2, 5)))
dense.get_weights()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
dense = MyDense(5, 3)
params = dense.init(d2l.get_key(), jnp.zeros((3, 5)))
params
</code></pre>
<p>우리는 [<strong>사용자 정의 레이어를 사용하여 순전파 계산을 직접 수행</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
dense.initialize()
dense(np.random.uniform(size=(2, 5)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
linear(torch.rand(2, 5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
dense(tf.random.uniform((2, 5)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
dense.apply(params, jax.random.uniform(d2l.get_key(),
                                       (2, 5)))
</code></pre>
<p>또한 (<strong>사용자 정의 레이어를 사용하여 모델을 구성</strong>)할 수도 있습니다.
일단 가지고 있으면 내장 완전 연결 레이어처럼 사용할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = nn.Sequential()
net.add(MyDense(8, in_units=64),
        MyDense(1, in_units=8))
net.initialize()
net(np.random.uniform(size=(2, 64)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(2, 64))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net = tf.keras.models.Sequential([MyDense(8), MyDense(1)])
net(tf.random.uniform((2, 64)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = nn.Sequential([MyDense(64, 8), MyDense(8, 1)])
Y, _ = net.init_with_output(d2l.get_key(), jax.random.uniform(d2l.get_key(),
                                                              (2, 64)))
Y
</code></pre>
<h2 id="요약-summary-22"><a class="header" href="#요약-summary-22">요약 (Summary)</a></h2>
<p>기본 레이어 클래스를 통해 사용자 정의 레이어를 설계할 수 있습니다. 이를 통해 라이브러리의 기존 레이어와 다르게 동작하는 유연한 새 레이어를 정의할 수 있습니다.
일단 정의되면 사용자 정의 레이어는 임의의 맥락과 아키텍처에서 호출될 수 있습니다.
레이어는 로컬 파라미터를 가질 수 있으며, 이는 내장 함수를 통해 생성될 수 있습니다.</p>
<h2 id="연습-문제-exercises-25"><a class="header" href="#연습-문제-exercises-25">연습 문제 (Exercises)</a></h2>
<ol>
<li>입력을 받아 텐서 축소를 계산하는 레이어를 설계하십시오. 즉, $y_k = \sum_{i, j} W_{ijk} x_i x_j$를 반환합니다.</li>
<li>데이터의 푸리에 계수의 앞쪽 절반을 반환하는 레이어를 설계하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/58">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/59">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/279">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17993">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="파일-io-file-io"><a class="header" href="#파일-io-file-io">파일 I/O (File I/O)</a></h1>
<p>지금까지 데이터 처리 방법과 딥러닝 모델 구축, 훈련, 테스트 방법에 대해 논의했습니다.
하지만 언젠가는 학습된 모델에 충분히 만족하여 다양한 맥락에서 나중에 사용하기 위해 결과를 저장하고 싶을 것입니다(배포 시 예측을 수행하기 위해).
또한 긴 훈련 프로세스를 실행할 때, 서버의 전원 코드를 건드려 며칠 동안의 계산을 잃지 않도록 중간 결과를 주기적으로 저장(체크포인트)하는 것이 가장 좋습니다.
따라서 개별 가중치 벡터와 전체 모델을 모두 로드하고 저장하는 방법을 배워야 할 때입니다.
이 섹션에서는 두 가지 문제를 모두 다룹니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
import numpy as np
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
import flax
from flax import linen as nn
from flax.training import checkpoints
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="텐서-로드-및-저장-loading-and-saving-tensors"><a class="header" href="#텐서-로드-및-저장-loading-and-saving-tensors">(<strong>텐서 로드 및 저장 (Loading and Saving Tensors)</strong>)</a></h2>
<p>개별 텐서의 경우 <code>load</code> 및 <code>save</code> 함수를 직접 호출하여 읽고 쓸 수 있습니다.
두 함수 모두 이름을 제공해야 하며, <code>save</code>는 저장할 변수를 입력으로 요구합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(4)
npx.save('x-file', x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(4)
torch.save(x, 'x-file')
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.range(4)
np.save('x-file.npy', x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(4)
jnp.save('x-file.npy', x)
</code></pre>
<p>이제 저장된 파일에서 데이터를 다시 메모리로 읽어올 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x2 = npx.load('x-file')
x2
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x2 = torch.load('x-file')
x2
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x2 = np.load('x-file.npy', allow_pickle=True)
x2
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x2 = jnp.load('x-file.npy', allow_pickle=True)
x2
</code></pre>
<p>우리는 [<strong>텐서 리스트를 저장하고 다시 메모리로 읽어올 수 있습니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y = np.zeros(4)
npx.save('x-files', [x, y])
x2, y2 = npx.load('x-files')
(x2, y2)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y = torch.zeros(4)
torch.save([x, y],'x-files')
x2, y2 = torch.load('x-files')
(x2, y2)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
y = tf.zeros(4)
np.save('xy-files.npy', [x, y])
x2, y2 = np.load('xy-files.npy', allow_pickle=True)
(x2, y2)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = jnp.zeros(4)
jnp.save('xy-files.npy', [x, y])
x2, y2 = jnp.load('xy-files.npy', allow_pickle=True)
(x2, y2)
</code></pre>
<p>심지어 [<strong>문자열에서 텐서로 매핑하는 딕셔너리를 쓰고 읽을 수도 있습니다.</strong>]
이는 모델의 모든 가중치를 읽거나 쓰고 싶을 때 편리합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
mydict = {'x': x, 'y': y}
npx.save('mydict', mydict)
mydict2 = npx.load('mydict')
mydict2
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
mydict = {'x': x, 'y': y}
np.save('mydict.npy', mydict)
mydict2 = np.load('mydict.npy', allow_pickle=True)
mydict2
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
mydict = {'x': x, 'y': y}
jnp.save('mydict.npy', mydict)
mydict2 = jnp.load('mydict.npy', allow_pickle=True)
mydict2
</code></pre>
<h2 id="모델-파라미터-로드-및-저장-loading-and-saving-model-parameters"><a class="header" href="#모델-파라미터-로드-및-저장-loading-and-saving-model-parameters">[<strong>모델 파라미터 로드 및 저장 (Loading and Saving Model Parameters)</strong>]</a></h2>
<p>개별 가중치 벡터(또는 다른 텐서)를 저장하는 것은 유용하지만, 전체 모델을 저장(하고 나중에 로드)하려는 경우 매우 지루해집니다.
결국 수백 개의 파라미터 그룹이 곳곳에 흩어져 있을 수 있습니다.
이러한 이유로 딥러닝 프레임워크는 전체 네트워크를 로드하고 저장하는 내장 기능을 제공합니다.
주목해야 할 중요한 세부 사항은 이것이 전체 모델이 아닌 모델 <em>파라미터</em>를 저장한다는 것입니다.
예를 들어 3개 레이어의 MLP가 있는 경우 아키텍처를 별도로 지정해야 합니다.
그 이유는 모델 자체에 임의의 코드가 포함될 수 있어 자연스럽게 직렬화할 수 없기 때문입니다.
따라서 모델을 복원하려면 코드에서 아키텍처를 생성한 다음 디스크에서 파라미터를 로드해야 합니다.
(<strong>익숙한 MLP로 시작해 봅시다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class MLP(nn.Block):
    def __init__(self, **kwargs):
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Dense(256, activation='relu')
        self.output = nn.Dense(10)

    def forward(self, x):
        return self.output(self.hidden(x))

net = MLP()
net.initialize()
X = np.random.uniform(size=(2, 20))
Y = net(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.LazyLinear(256)
        self.output = nn.LazyLinear(10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class MLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.flatten = tf.keras.layers.Flatten()
        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)
        self.out = tf.keras.layers.Dense(units=10)

    def call(self, inputs):
        x = self.flatten(inputs)
        x = self.hidden(x)
        return self.out(x)

net = MLP()
X = tf.random.uniform((2, 20))
Y = net(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class MLP(nn.Module):
    def setup(self):
        self.hidden = nn.Dense(256)
        self.output = nn.Dense(10)

    def __call__(self, x):
        return self.output(nn.relu(self.hidden(x)))

net = MLP()
X = jax.random.normal(jax.random.PRNGKey(d2l.get_seed()), (2, 20))
Y, params = net.init_with_output(jax.random.PRNGKey(d2l.get_seed()), X)
</code></pre>
<p>다음으로, "mlp.params"라는 이름으로 [<strong>모델의 파라미터를 파일로 저장</strong>]합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net.save_parameters('mlp.params')
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.save(net.state_dict(), 'mlp.params')
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net.save_weights('mlp.params')
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
checkpoints.save_checkpoint('ckpt_dir', params, step=1, overwrite=True)
</code></pre>
<p>모델을 복구하기 위해 원래 MLP 모델의 클론을 인스턴스화합니다.
모델 파라미터를 무작위로 초기화하는 대신, [<strong>파일에 저장된 파라미터를 직접 읽습니다</strong>].</p>
<pre><code class="language-{.python .input}">%%tab mxnet
clone = MLP()
clone.load_parameters('mlp.params')
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
clone = MLP()
clone.load_weights('mlp.params')
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
clone = MLP()
cloned_params = flax.core.freeze(checkpoints.restore_checkpoint('ckpt_dir',
                                                                target=None))
</code></pre>
<p>두 인스턴스 모두 동일한 모델 파라미터를 가지므로 동일한 입력 <code>X</code>의 계산 결과는 같아야 합니다.
이것을 확인해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
Y_clone = clone(X)
Y_clone == Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
Y_clone = clone.apply(cloned_params, X)
Y_clone == Y
</code></pre>
<h2 id="요약-summary-23"><a class="header" href="#요약-summary-23">요약 (Summary)</a></h2>
<p><code>save</code> 및 <code>load</code> 함수는 텐서 객체에 대한 파일 I/O를 수행하는 데 사용할 수 있습니다.
파라미터 딕셔너리를 통해 네트워크의 전체 파라미터 세트를 저장하고 로드할 수 있습니다.
아키텍처 저장은 파라미터가 아닌 코드로 수행해야 합니다.</p>
<h2 id="연습-문제-exercises-26"><a class="header" href="#연습-문제-exercises-26">연습 문제 (Exercises)</a></h2>
<ol>
<li>훈련된 모델을 다른 장치에 배포할 필요가 없더라도 모델 파라미터를 저장하는 것의 실질적인 이점은 무엇입니까?</li>
<li>다른 아키텍처를 가진 네트워크에 통합하기 위해 네트워크의 일부만 재사용하고 싶다고 가정해 봅시다. 이전 네트워크의 처음 두 레이어를 새 네트워크에서 사용하려면 어떻게 해야 합니까?</li>
<li>네트워크 아키텍처와 파라미터를 어떻게 저장하시겠습니까? 아키텍처에 어떤 제약을 두겠습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/60">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/61">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/327">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17994">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="gpu-gpus"><a class="header" href="#gpu-gpus">GPU (GPUs)</a></h1>
<p>:label:<code>sec_use_gpu</code></p>
<p>:numref:<code>tab_intro_decade</code>에서 우리는 지난 20년 동안의 급격한 계산 성장을 설명했습니다.
간단히 말해서 GPU 성능은 2000년 이후 매 10년마다 1000배씩 증가했습니다.
이것은 엄청난 기회를 제공하지만 동시에 그러한 성능에 대한 상당한 수요가 있었음을 시사합니다.</p>
<p>이 섹션에서는 연구를 위해 이러한 계산 성능을 활용하는 방법에 대해 논의하기 시작합니다.
먼저 단일 GPU를 사용하는 방법을 다루고, 나중에 다중 GPU와 다중 서버(다중 GPU 포함)를 사용하는 방법을 다룹니다.</p>
<p>구체적으로 단일 NVIDIA GPU를 계산에 사용하는 방법을 논의합니다.
먼저 NVIDIA GPU가 하나 이상 설치되어 있는지 확인하십시오.
그런 다음 <a href="https://developer.nvidia.com/cuda-downloads">NVIDIA 드라이버 및 CUDA</a>를 다운로드하고 프롬프트에 따라 적절한 경로를 설정하십시오.
이러한 준비가 완료되면 <code>nvidia-smi</code> 명령을 사용하여 (<strong>그래픽 카드 정보를 볼 수 있습니다</strong>).</p>
<p>:begin_tab:<code>mxnet</code>
MXNet 텐서가 NumPy <code>ndarray</code>와 거의 똑같아 보인다는 것을 눈치챘을 것입니다.
하지만 몇 가지 중요한 차이점이 있습니다.
NumPy와 MXNet을 구별하는 주요 특징 중 하나는 다양한 하드웨어 장치 지원입니다.</p>
<p>MXNet에서 모든 배열에는 컨텍스트(context)가 있습니다.
지금까지는 기본적으로 모든 변수와 관련 계산이 CPU에 할당되었습니다.
일반적으로 다른 컨텍스트는 다양한 GPU일 수 있습니다.
여러 서버에 작업을 배포할 때 상황은 더욱 복잡해질 수 있습니다.
배열을 컨텍스트에 지능적으로 할당함으로써 장치 간 데이터 전송에 소요되는 시간을 최소화할 수 있습니다.
예를 들어 GPU가 있는 서버에서 신경망을 훈련할 때 일반적으로 모델의 파라미터가 GPU에 상주하는 것을 선호합니다.</p>
<p>다음으로 MXNet의 GPU 버전이 설치되어 있는지 확인해야 합니다.
CPU 버전의 MXNet이 이미 설치되어 있는 경우 먼저 제거해야 합니다.
예를 들어 <code>pip uninstall mxnet</code> 명령을 사용한 다음 CUDA 버전에 따라 해당 MXNet 버전을 설치하십시오.
CUDA 10.0이 설치되어 있다고 가정하면 <code>pip install mxnet-cu100</code>을 통해 CUDA 10.0을 지원하는 MXNet 버전을 설치할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
PyTorch에서 모든 배열에는 장치(device)가 있습니다. 우리는 종종 이를 *컨텍스트(context)*라고 부릅니다.
지금까지는 기본적으로 모든 변수와 관련 계산이 CPU에 할당되었습니다.
일반적으로 다른 컨텍스트는 다양한 GPU일 수 있습니다.
여러 서버에 작업을 배포할 때 상황은 더욱 복잡해질 수 있습니다.
배열을 컨텍스트에 지능적으로 할당함으로써 장치 간 데이터 전송에 소요되는 시간을 최소화할 수 있습니다.
예를 들어 GPU가 있는 서버에서 신경망을 훈련할 때 일반적으로 모델의 파라미터가 GPU에 상주하는 것을 선호합니다.
:end_tab:</p>
<p>이 섹션의 프로그램을 실행하려면 최소 두 개의 GPU가 필요합니다.
대부분의 데스크톱 컴퓨터에는 과도할 수 있지만 클라우드(예: AWS EC2 멀티 GPU 인스턴스 사용)에서는 쉽게 사용할 수 있습니다.
거의 모든 다른 섹션은 다중 GPU를 <em>요구하지 않지만</em>, 여기서는 단순히 장치 간 데이터 흐름을 설명하고자 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="컴퓨팅-장치-computing-devices"><a class="header" href="#컴퓨팅-장치-computing-devices">[<strong>컴퓨팅 장치 (Computing Devices)</strong>]</a></h2>
<p>저장 및 계산을 위해 CPU 및 GPU와 같은 장치를 지정할 수 있습니다.
기본적으로 텐서는 메인 메모리에 생성된 다음 계산에 CPU를 사용합니다.</p>
<p>:begin_tab:<code>mxnet</code>
MXNet에서 CPU와 GPU는 <code>cpu()</code>와 <code>gpu()</code>로 나타낼 수 있습니다.
<code>cpu()</code>(또는 괄호 안의 정수)는 모든 물리적 CPU와 메모리를 의미한다는 점에 유의해야 합니다.
이는 MXNet의 계산이 모든 CPU 코어를 사용하려고 시도한다는 것을 의미합니다.
그러나 <code>gpu()</code>는 하나의 카드와 해당 메모리만 나타냅니다.
GPU가 여러 개 있는 경우 <code>gpu(i)</code>를 사용하여 $i$번째 GPU를 나타냅니다($i$는 0부터 시작).
또한 <code>gpu(0)</code>과 <code>gpu()</code>는 동일합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
PyTorch에서 CPU와 GPU는 <code>torch.device('cpu')</code>와 <code>torch.device('cuda')</code>로 나타낼 수 있습니다.
<code>cpu</code> 장치는 모든 물리적 CPU와 메모리를 의미한다는 점에 유의해야 합니다.
이는 PyTorch의 계산이 모든 CPU 코어를 사용하려고 시도한다는 것을 의미합니다.
그러나 <code>gpu</code> 장치는 하나의 카드와 해당 메모리만 나타냅니다.
GPU가 여러 개 있는 경우 <code>torch.device(f'cuda:{i}')</code>를 사용하여 $i$번째 GPU를 나타냅니다($i$는 0부터 시작).
또한 <code>gpu:0</code>과 <code>gpu</code>는 동일합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch
def cpu():  #@save
    """CPU 장치를 가져옵니다."""
    return torch.device('cpu')

def gpu(i=0):  #@save
    """GPU 장치를 가져옵니다."""
    return torch.device(f'cuda:{i}')

cpu(), gpu(), gpu(1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow, jax
def cpu():  #@save
    """CPU 장치를 가져옵니다."""
    if tab.selected('mxnet'):
        return npx.cpu()
    if tab.selected('tensorflow'):
        return tf.device('/CPU:0')
    if tab.selected('jax'):
        return jax.devices('cpu')[0]

def gpu(i=0):  #@save
    """GPU 장치를 가져옵니다."""
    if tab.selected('mxnet'):
        return npx.gpu(i)
    if tab.selected('tensorflow'):
        return tf.device(f'/GPU:{i}')
    if tab.selected('jax'):
        return jax.devices('gpu')[i]

cpu(), gpu(), gpu(1)
</code></pre>
<p>우리는 (<strong>사용 가능한 GPU 수를 쿼리</strong>)할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
def num_gpus():  #@save
    """사용 가능한 GPU 수를 가져옵니다."""
    return torch.cuda.device_count()

num_gpus()
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow, jax
def num_gpus():  #@save
    """사용 가능한 GPU 수를 가져옵니다."""
    if tab.selected('mxnet'):
        return npx.num_gpus()
    if tab.selected('tensorflow'):
        return len(tf.config.experimental.list_physical_devices('GPU'))
    if tab.selected('jax'):
        try:
            return jax.device_count('gpu')
        except:
            return 0  # GPU 백엔드를 찾을 수 없음

num_gpus()
</code></pre>
<p>이제 [<strong>요청한 GPU가 존재하지 않더라도 코드를 실행할 수 있게 해주는 두 가지 편리한 함수를 정의합니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab all
def try_gpu(i=0):  #@save
    """존재하면 gpu(i)를, 그렇지 않으면 cpu()를 반환합니다."""
    if num_gpus() &gt;= i + 1:
        return gpu(i)
    return cpu()

def try_all_gpus():  #@save
    """모든 사용 가능한 GPU를 반환하거나, GPU가 없으면 [cpu(),]를 반환합니다."""
    return [gpu(i) for i in range(num_gpus())]

try_gpu(), try_gpu(10), try_all_gpus()
</code></pre>
<h2 id="텐서와-gpu-tensors-and-gpus"><a class="header" href="#텐서와-gpu-tensors-and-gpus">텐서와 GPU (Tensors and GPUs)</a></h2>
<p>:begin_tab:<code>pytorch</code>
기본적으로 텐서는 CPU에 생성됩니다.
우리는 [<strong>텐서가 위치한 장치를 쿼리</strong>]할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>mxnet</code>
기본적으로 텐서는 CPU에 생성됩니다.
우리는 [<strong>텐서가 위치한 장치를 쿼리</strong>]할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow, jax</code>
기본적으로 텐서는 GPU/TPU를 사용할 수 있으면 생성되고, 그렇지 않으면 CPU가 사용됩니다.
우리는 [<strong>텐서가 위치한 장치를 쿼리</strong>]할 수 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.array([1, 2, 3])
x.ctx
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.tensor([1, 2, 3])
x.device
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.constant([1, 2, 3])
x.device
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.array([1, 2, 3])
x.device()
</code></pre>
<p>여러 항에 대해 연산하고자 할 때마다, 그것들이 동일한 장치에 있어야 한다는 점을 기억하는 것이 중요합니다.
예를 들어 두 텐서를 더하는 경우, 두 인수가 동일한 장치에 있는지 확인해야 합니다. 그렇지 않으면 프레임워크는 결과를 어디에 저장해야 할지, 심지어 어디서 계산을 수행해야 할지 결정할 수 없습니다.</p>
<h3 id="gpu에-저장하기-storage-on-the-gpu"><a class="header" href="#gpu에-저장하기-storage-on-the-gpu">GPU에 저장하기 (Storage on the GPU)</a></h3>
<p>[<strong>텐서를 GPU에 저장</strong>]하는 방법에는 여러 가지가 있습니다.
예를 들어 텐서를 생성할 때 저장 장치를 지정할 수 있습니다.
다음으로 첫 번째 <code>gpu</code>에 텐서 변수 <code>X</code>를 생성합니다.
GPU에 생성된 텐서는 해당 GPU의 메모리만 소비합니다.
<code>nvidia-smi</code> 명령을 사용하여 GPU 메모리 사용량을 볼 수 있습니다.
일반적으로 GPU 메모리 한도를 초과하는 데이터를 생성하지 않도록 해야 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
X = np.ones((2, 3), ctx=try_gpu())
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
X = torch.ones(2, 3, device=try_gpu())
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with try_gpu():
    X = tf.ones((2, 3))
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# 기본적으로 JAX는 사용 가능한 경우 배열을 GPU 또는 TPU에 넣습니다
X = jax.device_put(jnp.ones((2, 3)), try_gpu())
X
</code></pre>
<p>최소 두 개의 GPU가 있다고 가정하면, 다음 코드는 (<strong>두 번째 GPU에 무작위 텐서 <code>Y</code>를 생성합니다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab mxnet
Y = np.random.uniform(size=(2, 3), ctx=try_gpu(1))
Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
Y = torch.rand(2, 3, device=try_gpu(1))
Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with try_gpu(1):
    Y = tf.random.uniform((2, 3))
Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
Y = jax.device_put(jax.random.uniform(jax.random.PRNGKey(0), (2, 3)),
                   try_gpu(1))
Y
</code></pre>
<h3 id="복사하기-copying"><a class="header" href="#복사하기-copying">복사하기 (Copying)</a></h3>
<p>[<strong><code>X + Y</code>를 계산하려면, 이 연산을 어디서 수행할지 결정해야 합니다.</strong>]
예를 들어 :numref:<code>fig_copyto</code>에 표시된 것처럼 <code>X</code>를 두 번째 GPU로 전송하여 거기서 연산을 수행할 수 있습니다.
단순히 <code>X</code>와 <code>Y</code>를 더하지 <em>마십시오</em>. 예외가 발생할 것입니다.
런타임 엔진은 무엇을 해야 할지 모릅니다: 동일한 장치에서 데이터를 찾을 수 없어 실패합니다.
<code>Y</code>가 두 번째 GPU에 있으므로 두 개를 더하기 전에 <code>X</code>를 거기로 옮겨야 합니다.</p>
<p><img src="chapter_builders-guide/../img/copyto.svg" alt="데이터를 복사하여 동일한 장치에서 연산을 수행합니다." />
:label:<code>fig_copyto</code></p>
<pre><code class="language-{.python .input}">%%tab mxnet
Z = X.copyto(try_gpu(1))
print(X)
print(Z)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
Z = X.cuda(1)
print(X)
print(Z)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with try_gpu(1):
    Z = X
print(X)
print(Z)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
Z = jax.device_put(X, try_gpu(1))
print(X)
print(Z)
</code></pre>
<p>이제 [<strong>데이터(<code>Z</code>와 <code>Y</code> 모두)가 동일한 GPU에 있으므로, 더할 수 있습니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab all
Y + Z
</code></pre>
<p>:begin_tab:<code>mxnet</code>
변수 <code>Z</code>가 이미 두 번째 GPU에 있다고 상상해 보십시오.
여전히 <code>Z.copyto(gpu(1))</code>을 호출하면 어떻게 될까요?
변수가 이미 원하는 장치에 있음에도 불구하고 복사본을 만들고 새 메모리를 할당할 것입니다.
코드가 실행되는 환경에 따라 두 변수가 이미 동일한 장치에 있을 수 있는 경우가 있습니다.
따라서 변수가 현재 다른 장치에 있는 경우에만 복사본을 만들고 싶습니다.
이러한 경우 <code>as_in_ctx</code>를 호출할 수 있습니다.
변수가 이미 지정된 장치에 있다면 아무 작업도 수행하지 않습니다.
특별히 복사본을 만들고 싶지 않다면 <code>as_in_ctx</code>가 선택할 수 있는 방법입니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
하지만 변수 <code>Z</code>가 이미 두 번째 GPU에 있었다면 어떨까요?
여전히 <code>Z.cuda(1)</code>을 호출하면 어떻게 될까요?
복사본을 만들고 새 메모리를 할당하는 대신 <code>Z</code>를 반환할 것입니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
변수 <code>Z</code>가 이미 두 번째 GPU에 있다고 상상해 보십시오.
동일한 장치 범위에서 <code>Z2 = Z</code>를 호출하면 어떻게 될까요?
복사본을 만들고 새 메모리를 할당하는 대신 <code>Z</code>를 반환할 것입니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
변수 <code>Z</code>가 이미 두 번째 GPU에 있다고 상상해 보십시오.
동일한 장치 범위에서 <code>Z2 = Z</code>를 호출하면 어떻게 될까요?
복사본을 만들고 새 메모리를 할당하는 대신 <code>Z</code>를 반환할 것입니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
Z.as_in_ctx(try_gpu(1)) is Z
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
Z.cuda(1) is Z
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with try_gpu(1):
    Z2 = Z
Z2 is Z
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
Z2 = jax.device_put(Z, try_gpu(1))
Z2 is Z
</code></pre>
<h3 id="부가적인-참고-사항-side-notes"><a class="header" href="#부가적인-참고-사항-side-notes">부가적인 참고 사항 (Side Notes)</a></h3>
<p>사람들은 빠를 것이라고 기대하기 때문에 GPU를 사용하여 머신러닝을 수행합니다.
하지만 장치 간에 변수를 전송하는 것은 느립니다: 계산보다 훨씬 느립니다.
따라서 우리는 여러분이 느린 작업을 수행하기를 원한다는 것을 100% 확신하기를 바랍니다.
딥러닝 프레임워크가 충돌 없이 자동으로 복사를 수행했다면 여러분은 느린 코드를 작성했다는 것을 깨닫지 못했을 수 있습니다.</p>
<p>데이터 전송은 느릴 뿐만 아니라 병렬화도 훨씬 어렵게 만듭니다. 더 많은 작업을 진행하기 전에 데이터가 전송될 때까지(또는 수신될 때까지) 기다려야 하기 때문입니다.
이것이 복사 작업에 각별한 주의를 기울여야 하는 이유입니다.
경험 법칙에 따르면, 작은 작업 여러 개는 하나의 큰 작업보다 훨씬 나쁩니다.
또한 여러분이 무엇을 하고 있는지 알지 못한다면, 코드에 흩어져 있는 많은 단일 작업보다 한 번에 여러 작업을 수행하는 것이 훨씬 낫습니다.
한 장치가 다른 장치를 기다려야 다른 작업을 수행할 수 있는 경우 이러한 작업이 차단될 수 있기 때문입니다.
마치 줄을 서서 커피를 주문하는 것보다 전화로 미리 주문하고 갔을 때 준비되어 있는 것을 확인하는 것과 비슷합니다.</p>
<p>마지막으로, 텐서를 인쇄하거나 텐서를 NumPy 형식으로 변환할 때 데이터가 메인 메모리에 없으면 프레임워크는 먼저 메인 메모리로 복사하여 추가적인 전송 오버헤드를 발생시킵니다.
설상가상으로 이제 Python이 완료될 때까지 모든 것을 기다리게 만드는 무시무시한 전역 인터프리터 록(Global Interpreter Lock)의 적용을 받습니다.</p>
<h2 id="신경망과-gpu-neural-networks-and-gpus"><a class="header" href="#신경망과-gpu-neural-networks-and-gpus">[<strong>신경망과 GPU (Neural Networks and GPUs)</strong>]</a></h2>
<p>마찬가지로 신경망 모델도 장치를 지정할 수 있습니다.
다음 코드는 모델 파라미터를 GPU에 넣습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(ctx=try_gpu())
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net = nn.Sequential(nn.LazyLinear(1))
net = net.to(device=try_gpu())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    net = tf.keras.models.Sequential([
        tf.keras.layers.Dense(1)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net = nn.Sequential([nn.Dense(1)])

key1, key2 = jax.random.split(jax.random.PRNGKey(0))
x = jax.random.normal(key1, (10,))  # 더미 입력
params = net.init(key2, x)  # 초기화 호출
</code></pre>
<p>다음 장에서 GPU에서 모델을 실행하는 더 많은 예를 볼 수 있을 것입니다. 단순히 모델이 계산적으로 좀 더 집약적이 될 것이기 때문입니다.</p>
<p>예를 들어 입력이 GPU에 있는 텐서인 경우, 모델은 동일한 GPU에서 결과를 계산합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, tensorflow
net(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
net.apply(params, x)
</code></pre>
<p>(<strong>모델 파라미터가 동일한 GPU에 저장되어 있는지 확인</strong>)해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
net[0].weight.data().ctx
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
net[0].weight.data.device
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
net.layers[0].weights[0].device, net.layers[0].weights[1].device
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
print(jax.tree_util.tree_map(lambda x: x.device(), params))
</code></pre>
<p>트레이너가 GPU를 지원하도록 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
@d2l.add_to_class(d2l.Module)  #@save
def set_scratch_params_device(self, device):
    for attr in dir(self):
        a = getattr(self, attr)
        if isinstance(a, np.ndarray):
            with autograd.record():
                setattr(self, attr, a.as_in_ctx(device))
            getattr(self, attr).attach_grad()
        if isinstance(a, d2l.Module):
            a.set_scratch_params_device(device)
        if isinstance(a, list):
            for elem in a:
                elem.set_scratch_params_device(device)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
@d2l.add_to_class(d2l.Trainer)  #@save
def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):
    self.save_hyperparameters()
    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]

@d2l.add_to_class(d2l.Trainer)  #@save
def prepare_batch(self, batch):
    if self.gpus:
        batch = [d2l.to(a, self.gpus[0]) for a in batch]
    return batch

@d2l.add_to_class(d2l.Trainer)  #@save
def prepare_model(self, model):
    model.trainer = self
    model.board.xlim = [0, self.max_epochs]
    if self.gpus:
        if tab.selected('mxnet'):
            model.collect_params().reset_ctx(self.gpus[0])
            model.set_scratch_params_device(self.gpus[0])
        if tab.selected('pytorch'):
            model.to(self.gpus[0])
    self.model = model
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(d2l.Trainer)  #@save
def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):
    self.save_hyperparameters()
    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]

@d2l.add_to_class(d2l.Trainer)  #@save
def prepare_batch(self, batch):
    if self.gpus:
        batch = [d2l.to(a, self.gpus[0]) for a in batch]
    return batch
</code></pre>
<p>간단히 말해서 모든 데이터와 파라미터가 동일한 장치에 있는 한 모델을 효율적으로 학습할 수 있습니다. 다음 장에서 몇 가지 그러한 예를 볼 것입니다.</p>
<h2 id="요약-summary-24"><a class="header" href="#요약-summary-24">요약 (Summary)</a></h2>
<p>CPU 또는 GPU와 같은 저장 및 계산용 장치를 지정할 수 있습니다.
기본적으로 데이터는 메인 메모리에 생성된 다음 계산을 위해 CPU를 사용합니다.
딥러닝 프레임워크는 계산을 위한 모든 입력 데이터가 CPU이든 동일한 GPU이든 동일한 장치에 있어야 합니다.
주의 없이 데이터를 이동하면 상당한 성능 저하가 발생할 수 있습니다.
전형적인 실수는 다음과 같습니다: GPU에서 모든 미니배치에 대한 손실을 계산하고 명령줄에서 사용자에게 다시 보고(또는 NumPy <code>ndarray</code>에 기록)하면 모든 GPU를 멈추게 하는 전역 인터프리터 록이 트리거됩니다.
GPU 내부에 로깅을 위한 메모리를 할당하고 더 큰 로그만 이동하는 것이 훨씬 낫습니다.</p>
<h2 id="연습-문제-exercises-27"><a class="header" href="#연습-문제-exercises-27">연습 문제 (Exercises)</a></h2>
<ol>
<li>큰 행렬의 곱셈과 같은 더 큰 계산 작업을 시도해보고 CPU와 GPU 사이의 속도 차이를 확인하십시오. 계산 횟수가 적은 작업은 어떻습니까?</li>
<li>GPU에서 모델 파라미터를 어떻게 읽고 써야 합니까?</li>
<li>$100 	imes 100$ 행렬의 행렬-행렬 곱셈 1000개를 계산하는 데 걸리는 시간을 측정하고, 한 번에 하나씩 결과 출력 행렬의 프로베니우스 노름을 기록하십시오. GPU에 로그를 유지하고 최종 결과만 전송하는 것과 비교하십시오.</li>
<li>동시에 두 개의 GPU에서 두 개의 행렬-행렬 곱셈을 수행하는 데 걸리는 시간을 측정하십시오. 하나의 GPU에서 순차적으로 계산하는 것과 비교하십시오. 힌트: 거의 선형적인 확장을 볼 수 있어야 합니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/62">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/63">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/270">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17995">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="합성곱-신경망-convolutional-neural-networks"><a class="header" href="#합성곱-신경망-convolutional-neural-networks">합성곱 신경망 (Convolutional Neural Networks)</a></h1>
<p>:label:<code>chap_cnn</code></p>
<p>이미지 데이터는 흑백이든 컬러이든 픽셀의 2차원 그리드로 표현됩니다.
이에 따라 각 픽셀은 각각 하나 또는 여러 개의 수치 값에 해당합니다.
지금까지 우리는 이러한 풍부한 구조를 무시하고 픽셀 간의 공간적 관계를 고려하지 않고 이미지를 *평탄화(flattening)*하여 숫자 벡터로 취급했습니다.
이러한 매우 불만족스러운 접근 방식은 결과로 나오는 1차원 벡터를 완전 연결 MLP에 공급하기 위해 필요했습니다.</p>
<p>이러한 네트워크는 특성의 순서에 불변하기 때문에, 픽셀의 공간 구조에 해당하는 순서를 보존하든 MLP의 파라미터를 맞추기 전에 설계 행렬의 열을 치환하든 비슷한 결과를 얻을 수 있습니다.
이상적으로는 인접한 픽셀이 일반적으로 서로 관련되어 있다는 사전 지식을 활용하여 이미지 데이터로부터 학습하기 위한 효율적인 모델을 구축할 것입니다.</p>
<p>이 장에서는 바로 이 목적을 위해 설계된 강력한 신경망 제품군인 *합성곱 신경망(convolutional neural networks, CNN)*을 소개합니다 :cite:<code>LeCun.Jackel.Bottou.ea.1995</code>.
CNN 기반 아키텍처는 이제 컴퓨터 비전 분야에서 어디에나 있습니다.
예를 들어 Imagnet 컬렉션 :cite:<code>Deng.Dong.Socher.ea.2009</code>에서 상당한 성능 향상을 제공한 것은 오직 합성곱 신경망(줄여서 ConvNet)의 사용뿐이었습니다 :cite:<code>Krizhevsky.Sutskever.Hinton.2012</code>.</p>
<p>구어체로 불리는 현대 CNN은 생물학, 그룹 이론, 그리고 건전한 실험적 팅커링(tinkering)에서 영감을 얻어 설계되었습니다.
정확한 모델을 달성하는 데 있어 샘플 효율성 외에도, CNN은 완전 연결 아키텍처보다 더 적은 파라미터를 필요로 하고 합성곱이 GPU 코어 전체에 걸쳐 병렬화하기 쉽기 때문에 계산 효율적인 경향이 있습니다 :cite:<code>Chetlur.Woolley.Vandermersch.ea.2014</code>.
결과적으로 실무자들은 가능할 때마다 CNN을 적용하며, 오디오 :cite:<code>Abdel-Hamid.Mohamed.Jiang.ea.2014</code>, 텍스트 :cite:<code>Kalchbrenner.Grefenstette.Blunsom.2014</code>, 시계열 분석 :cite:<code>LeCun.Bengio.ea.1995</code>과 같이 전통적으로 순환 신경망이 사용되는 1차원 시퀀스 구조를 가진 작업에서도 신뢰할 수 있는 경쟁자로 점점 더 부상하고 있습니다.
CNN의 일부 영리한 적응은 그래프 구조 데이터 :cite:<code>Kipf.Welling.2016</code>와 추천 시스템에도 적용되었습니다.</p>
<p>먼저, 합성곱 신경망에 대한 동기를 더 깊이 파고들 것입니다.
이어서 모든 합성곱 네트워크의 중추를 구성하는 기본 연산을 살펴봅니다.
여기에는 합성곱 레이어 자체, 패딩 및 스트라이드를 포함한 세부 사항, 인접한 공간 영역에 걸쳐 정보를 집계하는 데 사용되는 풀링 레이어, 각 레이어에서 다중 채널의 사용, 현대 아키텍처의 구조에 대한 신중한 논의가 포함됩니다.
우리는 현대 딥러닝이 부상하기 훨씬 전에 성공적으로 배포된 최초의 합성곱 네트워크인 LeNet의 완전한 작동 예제로 장을 마칠 것입니다.
다음 장에서는 현대 실무자들이 일반적으로 사용하는 기술의 대부분을 대표하는 인기 있고 비교적 최신인 CNN 아키텍처의 전체 구현에 대해 알아볼 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

why-conv
conv-layer
padding-and-strides
channels
pooling
lenet
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="완전-연결-레이어에서-합성곱으로-from-fully-connected-layers-to-convolutions"><a class="header" href="#완전-연결-레이어에서-합성곱으로-from-fully-connected-layers-to-convolutions">완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></h1>
<p>:label:<code>sec_why-conv</code></p>
<p>오늘날까지도, 지금까지 논의한 모델들은 표 형식 데이터를 다룰 때 여전히 적절한 옵션입니다.
표 형식이라 함은 데이터가 예제에 해당하는 행과 특성에 해당하는 열로 구성되어 있음을 의미합니다.
표 형식 데이터의 경우, 우리가 찾는 패턴이 특성 간의 상호 작용을 포함할 수 있다고 예상할 수는 있지만, 특성이 어떻게 상호 작용하는지에 대한 어떠한 구조도 <em>사전적으로</em> 가정하지 않습니다.</p>
<p>때로는 더 화려한 아키텍처 구성을 안내할 수 있는 지식이 정말 부족할 때도 있습니다.
이러한 경우 MLP가 최선일 수 있습니다.
그러나 고차원 지각 데이터의 경우, 그러한 구조 없는 네트워크는 다루기 어려워질 수 있습니다.</p>
<p>예를 들어, 고양이와 개를 구별하는 실행 예제로 돌아가 봅시다.
데이터 수집을 철저히 하여 1메가픽셀 사진으로 구성된 주석이 달린 데이터셋을 수집했다고 가정해 봅시다.
이는 네트워크에 대한 각 입력이 100만 차원을 가지고 있음을 의미합니다.
은닉 차원을 1,000개로 과감하게 줄이더라도 $10^6 \times 10^3 = 10^9$개의 파라미터로 특징지어지는 완전 연결 레이어가 필요합니다.
GPU가 많고 분산 최적화에 재능이 있으며 엄청난 인내심이 없다면, 이 네트워크의 파라미터를 학습하는 것은 불가능할 수 있습니다.</p>
<p>주의 깊은 독자는 1메가픽셀 해상도가 필요하지 않을 수 있다는 근거로 이 주장에 반대할 수 있습니다.
그러나 10만 픽셀로 해결할 수 있다 하더라도, 크기 1000의 은닉층은 이미지의 좋은 표현을 학습하는 데 필요한 은닉 유닛의 수를 크게 과소평가하므로, 실용적인 시스템은 여전히 수십억 개의 파라미터를 필요로 할 것입니다.
게다가 그렇게 많은 파라미터를 맞춰 분류기를 학습하려면 엄청난 데이터셋을 수집해야 할 수도 있습니다.
그럼에도 불구하고 오늘날 인간과 컴퓨터 모두 고양이와 개를 아주 잘 구별할 수 있어, 이러한 직관과 모순되는 것처럼 보입니다.
그 이유는 이미지가 풍부한 구조를 보여주며, 인간과 머신러닝 모델 모두 이를 활용할 수 있기 때문입니다.
합성곱 신경망(CNN)은 머신러닝이 자연 이미지의 알려진 구조 중 일부를 활용하기 위해 받아들인 창의적인 방법 중 하나입니다.</p>
<h2 id="불변성-invariance"><a class="header" href="#불변성-invariance">불변성 (Invariance)</a></h2>
<p>이미지에서 객체를 감지하고 싶다고 상상해 보십시오.
우리가 객체를 인식하기 위해 사용하는 방법이 무엇이든 이미지 내 객체의 정확한 위치에 지나치게 신경 쓰지 않는 것이 합리적으로 보입니다.
이상적으로는 우리 시스템이 이 지식을 활용해야 합니다.
돼지는 보통 날지 않고 비행기는 보통 수영하지 않습니다.
그럼에도 불구하고 돼지가 이미지 상단에 나타나더라도 우리는 여전히 돼지를 인식해야 합니다.
여기서 어린이 게임 "월리를 찾아라(Where's Waldo)"에서 영감을 얻을 수 있습니다
(이 게임 자체도 :numref:<code>img_waldo</code>에 묘사된 것과 같은 많은 실제 모방작에 영감을 주었습니다).
게임은 활동으로 가득 찬 수많은 혼란스러운 장면들로 구성됩니다.
월도는 각각의 어딘가에 나타나며, 일반적으로 엉뚱한 위치에 숨어 있습니다.
독자의 목표는 그를 찾는 것입니다.
그의 특징적인 복장에도 불구하고, 많은 방해 요소들 때문에 이것은 놀랍게도 어려울 수 있습니다.
하지만 <em>월도가 어떻게 생겼는지</em>는 <em>월도가 어디에 있는지</em>에 달려 있지 않습니다.
우리는 각 패치에 점수를 할당하여 패치에 월도가 포함될 가능성을 나타내는 월도 감지기로 이미지를 훑을 수 있습니다.
실제로 많은 객체 감지 및 분할 알고리즘이 이 접근 방식에 기반을 두고 있습니다 :cite:<code>Long.Shelhamer.Darrell.2015</code>.
CNN은 *공간 불변성(spatial invariance)*이라는 이 아이디어를 체계화하여 더 적은 파라미터로 유용한 표현을 학습하는 데 활용합니다.</p>
<p><img src="chapter_convolutional-neural-networks/../img/waldo-football.jpg" alt="월도를 찾을 수 있나요 (이미지 제공: William Murphy (Infomatique))?" />
:width:<code>400px</code>
:label:<code>img_waldo</code></p>
<p>우리는 이제 컴퓨터 비전에 적합한 신경망 아키텍처 설계를 안내하기 위해 몇 가지 요구 사항을 열거함으로써 이러한 직관을 더 구체화할 수 있습니다:</p>
<ol>
<li>초기 레이어에서 우리 네트워크는 이미지가 어디에 나타나든 동일한 패치에 유사하게 반응해야 합니다. 이 원리를 <em>평행 이동 불변성(translation invariance)</em> (또는 <em>평행 이동 등변성(translation equivariance)</em>)이라고 합니다.</li>
<li>네트워크의 초기 레이어는 먼 영역의 이미지 내용에 관계없이 국소 영역에 초점을 맞춰야 합니다. 이것이 <em>지역성(locality)</em> 원칙입니다. 결국 이러한 국소 표현은 전체 이미지 수준에서 예측을 수행하기 위해 집계될 수 있습니다.</li>
<li>진행함에 따라, 더 깊은 레이어는 자연의 고수준 비전과 유사한 방식으로 이미지의 더 장거리 특징을 포착할 수 있어야 합니다.</li>
</ol>
<p>이것이 수학으로 어떻게 변환되는지 살펴봅시다.</p>
<h2 id="mlp-제약하기-constraining-the-mlp"><a class="header" href="#mlp-제약하기-constraining-the-mlp">MLP 제약하기 (Constraining the MLP)</a></h2>
<p>시작하기 위해, 2차원 이미지 $\mathbf{X}$를 입력으로 하고 그 즉각적인 은닉 표현 $\mathbf{H}$가 유사하게 행렬로 표현되는(코드에서는 2차원 텐서) MLP를 고려할 수 있습니다. 여기서 $\mathbf{X}$와 $\mathbf{H}$는 동일한 모양을 갖습니다.
잠시 생각해 봅시다.
우리는 이제 입력뿐만 아니라 은닉 표현도 공간 구조를 가지고 있다고 상상합니다.</p>
<p>$[⁡\mathbf{X}]<em>{i, j}$와 $[⁡\mathbf{H}]</em>{i, j}$가 각각 입력 이미지와 은닉 표현의 위치 $(i,j)$에 있는 픽셀을 나타낸다고 합시다.
결과적으로, 각 은닉 유닛이 각 입력 픽셀로부터 입력을 받도록 하려면, (이전에 MLP에서 했던 것처럼) 가중치 행렬을 사용하는 것에서 파라미터를 4차 가중치 텐서 $\mathsf{W}$로 표현하는 것으로 전환해야 합니다.
$⁡\mathbf{U}$가 편향을 포함한다고 가정하면, 완전 연결 레이어를 공식적으로 다음과 같이 표현할 수 있습니다.</p>
<p>$$\begin{aligned} \left[\mathbf{H}\right]<em>{i, j} &amp;= [\mathbf{U}]</em>{i, j} + \sum_k \sum_l[\mathsf{W}]<em>{i, j, k, l}  [\mathbf{X}]</em>{k, l}\ &amp;=  [\mathbf{U}]<em>{i, j} + \&amp;\quad \sum_a \sum_b [\mathsf{V}]</em>{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}$$</p>
<p>$⁡\mathsf{W}$에서 $⁡\mathsf{V}$로의 전환은 지금으로서는 전적으로 외관상의 변화입니다. 두 4차 텐서의 계수 사이에 일대일 대응이 있기 때문입니다.
우리는 단순히 $k = i+a$ 및 $l = j+b$가 되도록 아래첨자 $(k, l)$을 다시 인덱싱합니다.
즉, $[⁡\mathsf{V}]<em>{i, j, a, b} = [\mathsf{W}]</em>{i, j, i+a, j+b}$로 설정합니다.
인덱스 $a$와 $b$는 양수 및 음수 오프셋 모두에 대해 실행되어 전체 이미지를 커버합니다.
은닉 표현 $[⁡\mathbf{H}]<em>{i, j}$의 임의의 주어진 위치 ($i$, $j$)에 대해, 우리는 $(i, j)$를 중심으로 하고 $[⁡\mathsf{V}]</em>{i, j, a, b}$로 가중치가 부여된 $x$의 픽셀에 대해 합산하여 그 값을 계산합니다. 계속하기 전에, 이 파라미터화에서 <em>단일</em> 레이어에 필요한 총 파라미터 수를 고려해 봅시다: $1000 \times 1000$ 이미지(1메가픽셀)가 $1000 \times 1000$ 은닉 표현으로 매핑됩니다. 이를 위해서는 $10^{12}$개의 파라미터가 필요하며, 이는 현재 컴퓨터가 처리할 수 있는 수준을 훨씬 넘어섭니다.</p>
<h3 id="평행-이동-불변성-translation-invariance"><a class="header" href="#평행-이동-불변성-translation-invariance">평행 이동 불변성 (Translation Invariance)</a></h3>
<p>이제 위에서 확립한 첫 번째 원칙인 평행 이동 불변성을 호출해 봅시다 :cite:<code>Zhang.ea.1988</code>.
이는 입력 $⁡\mathbf{X}$의 이동이 단순히 은닉 표현 $⁡\mathbf{H}$의 이동으로 이어져야 함을 의미합니다.
이것은 $⁡\mathsf{V}$와 $⁡\mathbf{U}$가 실제로 $(i, j)$에 의존하지 않는 경우에만 가능합니다. 따라서,
우리는 $[⁡\mathsf{V}]<em>{i, j, a, b} = [\mathbf{V}]</em>{a, b}$를 갖고 $⁡\mathbf{U}$는 상수, 예를 들어 $u$입니다.
결과적으로 $⁡\mathbf{H}$에 대한 정의를 단순화할 수 있습니다:</p>
<p>$$[\mathbf{H}]<em>{i, j} = u + \sum_a\sum_b [\mathbf{V}]</em>{a, b}  [\mathbf{X}]_{i+a, j+b}.$$</p>
<p>이것은 *합성곱(convolution)*입니다!
우리는 효과적으로 위치 $(i, j)$ 근처의 $(i+a, j+b)$에 있는 픽셀에 계수 $[⁡\mathbf{V}]<em>{a, b}$로 가중치를 부여하여 값 $[⁡\mathbf{H}]</em>{i, j}$를 얻습니다.
$[⁡\mathbf{V}]<em>{a, b}$는 더 이상 이미지 내 위치에 의존하지 않기 때문에 $[⁡\mathsf{V}]</em>{i, j, a, b}$보다 훨씬 적은 계수가 필요하다는 점에 유의하십시오. 결과적으로 필요한 파라미터 수는 더 이상 $10^{12}$가 아니라 훨씬 더 합리적인 $4 \times 10^6$입니다: 우리는 여전히 $a, b \in (-1000, 1000)$에 대한 의존성을 가지고 있습니다. 요컨대 우리는 상당한 진전을 이루었습니다. 시간 지연 신경망(TDNN)은 이 아이디어를 활용한 첫 번째 예 중 일부입니다 :cite:<code>Waibel.Hanazawa.Hinton.ea.1989</code>.</p>
<h3 id="지역성-locality"><a class="header" href="#지역성-locality">지역성 (Locality)</a></h3>
<p>이제 두 번째 원칙인 지역성을 호출해 봅시다.
위에서 동기 부여된 바와 같이, 우리는 $[⁡\mathbf{H}]<em>{i, j}$에서 무슨 일이 일어나고 있는지 평가하기 위해 관련 정보를 수집하기 위해 위치 $(i, j)$에서 아주 멀리 볼 필요가 없다고 믿습니다.
이는 어떤 범위 $|a|&gt; \Delta$ 또는 $|b| &gt; \Delta$ 밖에서는 $[⁡\mathbf{V}]</em>{a, b} = 0$으로 설정해야 함을 의미합니다.
동등하게, 우리는 $[⁡\mathbf{H}]_{i, j}$를 다음과 같이 다시 쓸 수 있습니다.</p>
<p>$$[\mathbf{H}]<em>{i, j} = u + \sum</em>{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]<em>{a, b}  [\mathbf{X}]</em>{i+a, j+b}.$$
:eqlabel:<code>eq_conv-layer</code></p>
<p>이로써 파라미터 수는 $4 \times 10^6$에서 $4 \Delta^2$로 줄어듭니다. 여기서 $\Delta$는 일반적으로 10보다 작습니다. 따라서 파라미터 수를 또다시 4자리수만큼 줄였습니다. :eqref:<code>eq_conv-layer</code>는 간단히 말해서 *합성곱 레이어(convolutional layer)*라고 불리는 것입니다.
*합성곱 신경망(Convolutional neural networks, CNN)*은 합성곱 레이어를 포함하는 신경망의 특수 제품군입니다.
딥러닝 연구 커뮤니티에서 $⁡\mathbf{V}$는 <em>합성곱 커널(convolution kernel)</em>, <em>필터(filter)</em>, 또는 단순히 학습 가능한 파라미터인 레이어의 *가중치(weights)*라고 합니다.</p>
<p>이전에는 이미지 처리 네트워크의 단일 레이어만 표현하는 데 수십억 개의 파라미터가 필요했을 수 있지만, 이제는 입력이나 은닉 표현의 차원을 변경하지 않고도 일반적으로 수백 개만 있으면 됩니다.
파라미터의 이러한 급격한 감소에 대해 치러야 할 대가는 우리의 특성이 이제 평행 이동 불변이며, 각 은닉 활성화의 값을 결정할 때 우리 레이어가 지역 정보만 통합할 수 있다는 것입니다.
모든 학습은 귀납적 편향을 부과하는 것에 달려 있습니다.
그 편향이 현실과 일치할 때, 우리는 보지 못한 데이터에 잘 일반화되는 샘플 효율적인 모델을 얻습니다.
물론 그러한 편향이 현실과 일치하지 않는다면, 예를 들어 이미지가 평행 이동 불변이 아닌 것으로 판명된다면, 우리 모델은 훈련 데이터를 맞추는 데에도 어려움을 겪을 수 있습니다.</p>
<p>파라미터의 이러한 극적인 감소는 우리의 마지막 요구 사항, 즉 더 깊은 레이어가 이미지의 더 크고 복잡한 측면을 나타내야 한다는 점으로 우리를 이끕니다. 이는 비선형성과 합성곱 레이어를 반복적으로 인터리빙하여 달성할 수 있습니다.</p>
<h2 id="합성곱-convolutions"><a class="header" href="#합성곱-convolutions">합성곱 (Convolutions)</a></h2>
<p>:eqref:<code>eq_conv-layer</code>가 왜 합성곱이라고 불리는지 간단히 검토해 봅시다.
수학에서 두 함수 사이의 <em>합성곱</em> :cite:<code>Rudin.1973</code>, 예를 들어 $f, g: \mathbb{R}^d \to \mathbb{R}$은 다음과 같이 정의됩니다.</p>
<p>$$(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.$$</p>
<p>즉, 우리는 한 함수가 "뒤집히고" $\mathbf{x}$만큼 이동할 때 $f$와 $g$ 사이의 겹침을 측정합니다.
이산 객체가 있을 때마다 적분은 합으로 바뀝니다.
예를 들어, 인덱스가 $\mathbb{Z}$에 걸쳐 실행되는 제곱 합 가능한 무한 차원 벡터 집합의 벡터에 대해 다음 정의를 얻습니다:</p>
<p>$$(f * g)(i) = \sum_a f(a) g(i-a).$$</p>
<p>2차원 텐서의 경우, $f$에 대해서는 인덱스 $(a, b)$, $g$에 대해서는 $(i-a, j-b)$를 갖는 해당 합을 얻습니다:</p>
<p>$$(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).$$
:eqlabel:<code>eq_2d-conv-discrete</code></p>
<p>이것은 :eqref:<code>eq_conv-layer</code>와 유사해 보이지만 한 가지 주요 차이점이 있습니다.
$(i+a, j+b)$를 사용하는 대신 차이를 사용하고 있습니다.
하지만 우리는 언제나 :eqref:<code>eq_conv-layer</code>와 :eqref:<code>eq_2d-conv-discrete</code> 사이의 표기법을 일치시킬 수 있으므로 이 구분은 대부분 외관상의 문제입니다.
:eqref:<code>eq_conv-layer</code>의 원래 정의는 *상호 상관(cross-correlation)*을 더 적절하게 설명합니다.
우리는 다음 섹션에서 이 문제로 돌아올 것입니다.</p>
<h2 id="채널-channels"><a class="header" href="#채널-channels">채널 (Channels)</a></h2>
<p>:label:<code>subsec_why-conv-channels</code></p>
<p>월도 감지기로 돌아가서 이것이 어떻게 생겼는지 봅시다.
합성곱 레이어는 주어진 크기의 윈도우를 선택하고 :numref:<code>fig_waldo_mask</code>에 시연된 것처럼 필터 $⁡\mathsf{V}$에 따라 강도에 가중치를 부여합니다.
우리는 "월도스러움(waldoness)"이 가장 높은 곳마다 은닉층 표현에서 피크를 찾아야 하도록 모델을 학습하는 것을 목표로 할 수 있습니다.</p>
<p><img src="chapter_convolutional-neural-networks/../img/waldo-mask.jpg" alt="월도 감지하기 (이미지 제공: William Murphy (Infomatique))." />
:width:<code>400px</code>
:label:<code>fig_waldo_mask</code></p>
<p>이 접근 방식에는 한 가지 문제만 있습니다.
지금까지 우리는 이미지가 빨강, 초록, 파랑의 3개 채널로 구성되어 있다는 사실을 행복하게 무시했습니다.
요컨대 이미지는 2차원 객체가 아니라 높이, 너비, 채널, 예: $1024 \times 1024 \times 3$ 픽셀 모양으로 특징지어지는 3차 텐서입니다.
이 축 중 처음 두 개는 공간 관계와 관련이 있지만, 세 번째 축은 각 픽셀 위치에 다차원 표현을 할당하는 것으로 간주될 수 있습니다.
따라서 우리는 $⁡\mathsf{X}$를 $[⁡\mathsf{X}]<em>{i, j, k}$로 인덱싱합니다.
합성곱 필터는 그에 따라 적응해야 합니다.
$[⁡\mathbf{V}]</em>{a,b}$ 대신 이제 $[⁡\mathsf{V}]_{a,b,c}$를 갖습니다.</p>
<p>더욱이 입력이 3차 텐서로 구성된 것과 마찬가지로, 은닉 표현도 비슷하게 3차 텐서 $⁡\mathsf{H}$로 공식화하는 것이 좋은 아이디어라는 것이 밝혀졌습니다.
다시 말해, 각 공간 위치에 해당하는 단일 은닉 표현만 갖는 것이 아니라, 각 공간 위치에 해당하는 전체 은닉 표현 벡터를 원합니다.
은닉 표현을 서로 위에 쌓인 2차원 그리드 수로 생각할 수 있습니다.
입력과 마찬가지로 이들은 때때로 *채널(channels)*이라고 불립니다.
각각이 후속 레이어를 위한 공간화된 학습된 특성 세트를 제공하기 때문에 *특성 맵(feature maps)*이라고도 불립니다.
직관적으로 입력에 더 가까운 하위 레이어에서 일부 채널은 가장자리를 인식하도록 전문화되고 다른 채널은 텍스처를 인식할 수 있다고 상상할 수 있습니다.</p>
<p>입력($⁡\mathsf{X}$)과 은닉 표현($⁡\mathsf{H}$) 모두에서 다중 채널을 지원하기 위해, 우리는 $⁡\mathsf{V}$에 네 번째 좌표를 추가할 수 있습니다: $[⁡\mathsf{V}]_{a, b, c, d}$.
모든 것을 합치면 다음과 같습니다:</p>
<p>$$[\mathsf{H}]<em>{i,j,d} = \sum</em>{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]<em>{a, b, c, d} [\mathsf{X}]</em>{i+a, j+b, c},$$
:eqlabel:<code>eq_conv-layer-channels</code></p>
<p>여기서 $d$는 은닉 표현 $⁡\mathsf{H}$의 출력 채널을 인덱싱합니다. 후속 합성곱 레이어는 3차 텐서 $⁡\mathsf{H}$를 입력으로 취하게 됩니다.
우리는 일반성 때문에 :eqref:<code>eq_conv-layer-channels</code>를 다중 채널에 대한 합성곱 레이어의 정의로 취합니다. 여기서 $⁡\mathsf{V}$는 레이어의 커널 또는 필터입니다.</p>
<p>우리가 다루어야 할 연산이 아직 많이 남아 있습니다.
예를 들어 모든 은닉 표현을 단일 출력으로 결합하는 방법, 예: 이미지 <em>어딘가에</em> 월도가 있는지 여부를 알아내야 합니다.
또한 효율적으로 계산하는 방법, 다중 레이어를 결합하는 방법, 적절한 활성화 함수, 그리고 실제로 효과적인 네트워크를 산출하기 위한 합리적인 설계 선택을 하는 방법을 결정해야 합니다.
우리는 이 장의 나머지 부분에서 이러한 문제들을 다룹니다.</p>
<h2 id="요약-및-토론-summary-and-discussion-3"><a class="header" href="#요약-및-토론-summary-and-discussion-3">요약 및 토론 (Summary and Discussion)</a></h2>
<p>이 섹션에서 우리는 첫 번째 원칙에서 합성곱 신경망의 구조를 도출했습니다.
이것이 CNN의 발명으로 이어진 경로인지는 불분명하지만, 이미지 처리 및 컴퓨터 비전 알고리즘이 적어도 하위 수준에서 어떻게 작동해야 하는지에 대한 합리적인 원칙을 적용할 때 이것이 <em>올바른</em> 선택임을 아는 것은 만족스럽습니다.
특히 이미지의 평행 이동 불변성은 이미지의 모든 패치가 동일한 방식으로 처리될 것임을 의미합니다.
지역성은 픽셀의 작은 이웃만 해당 은닉 표현을 계산하는 데 사용됨을 의미합니다.
CNN에 대한 초기 언급 중 일부는 Neocognitron 형태입니다 :cite:<code>Fukushima.1982</code>.</p>
<p>A second principle that we encountered in our reasoning is how to reduce the number of parameters in a function class without limiting its expressive power, at least, whenever certain assumptions on the model hold. We saw a dramatic reduction of complexity as a result of this restriction, turning computationally and statistically infeasible problems into tractable models.</p>
<p>Adding channels allowed us to bring back some of the complexity that was lost due to the restrictions imposed on the convolutional kernel by locality and translation invariance. Note that it is quite natural to add channels other than just red, green, and blue. Many satellite
images, in particular for agriculture and meteorology, have tens to hundreds of channels,
generating hyperspectral images instead. They report data on many different wavelengths. In the following we will see how to use convolutions effectively to manipulate the dimensionality of the images they operate on, how to move from location-based to channel-based representations, and how to deal with large numbers of categories efficiently.</p>
<h2 id="연습-문제-exercises-28"><a class="header" href="#연습-문제-exercises-28">연습 문제 (Exercises)</a></h2>
<ol>
<li>합성곱 커널의 크기가 $\Delta = 0$이라고 가정합니다.
이 경우 합성곱 커널이 각 채널 세트에 대해 독립적으로 MLP를 구현함을 보이십시오.
이것은 Network in Network 아키텍처로 이어집니다 :cite:<code>Lin.Chen.Yan.2013</code>.</li>
<li>오디오 데이터는 종종 1차원 시퀀스로 표현됩니다.
<ol>
<li>언제 오디오에 대해 지역성과 평행 이동 불변성을 부과하고 싶을까요?</li>
<li>오디오에 대한 합성곱 연산을 유도하십시오.</li>
<li>컴퓨터 비전과 동일한 도구를 사용하여 오디오를 처리할 수 있습니까? 힌트: 스펙트로그램을 사용하십시오.</li>
</ol>
</li>
<li>평행 이동 불변성이 결국 좋은 아이디어가 아닌 이유는 무엇일까요? 예를 들어 보십시오.</li>
<li>합성곱 레이어가 텍스트 데이터에도 적용될 수 있다고 생각하십니까? 언어에서 어떤 문제에 부딪힐 수 있습니까?</li>
<li>객체가 이미지 경계에 있을 때 합성곱은 어떻게 됩니까?</li>
<li>합성곱이 대칭임을 증명하십시오. 즉, $f * g = g * f$.</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/64">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="이미지를-위한-합성곱-convolutions-for-images"><a class="header" href="#이미지를-위한-합성곱-convolutions-for-images">이미지를 위한 합성곱 (Convolutions for Images)</a></h1>
<p>:label:<code>sec_conv_layer</code></p>
<p>이제 이론적으로 합성곱 레이어가 어떻게 작동하는지 이해했으므로, 실제로 어떻게 작동하는지 볼 준비가 되었습니다.
이미지 데이터에서 구조를 탐색하기 위한 효율적인 아키텍처로서의 합성곱 신경망에 대한 동기를 바탕으로, 우리는 실행 예제로 이미지를 고수합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<h2 id="상호-상관-연산-the-cross-correlation-operation"><a class="header" href="#상호-상관-연산-the-cross-correlation-operation">상호 상관 연산 (The Cross-Correlation Operation)</a></h2>
<p>엄밀히 말하면 합성곱 레이어라는 이름은 잘못된 것입니다. 그들이 표현하는 연산은 상호 상관으로 더 정확하게 설명되기 때문입니다.
:numref:<code>sec_why-conv</code>의 합성곱 레이어 설명에 따르면, 그러한 레이어에서 입력 텐서와 커널 텐서는 (<strong>상호 상관 연산</strong>)을 통해 결합되어 출력 텐서를 생성합니다.</p>
<p>지금은 채널을 무시하고 2차원 데이터와 은닉 표현으로 이것이 어떻게 작동하는지 봅시다.
:numref:<code>fig_correlation</code>에서 입력은 높이 3, 너비 3인 2차원 텐서입니다.
우리는 텐서의 모양을 $3 \times 3$ 또는 ($3$, $3$)으로 표시합니다.
커널의 높이와 너비는 모두 2입니다.
<em>커널 윈도우</em> (또는 <em>합성곱 윈도우</em>)의 모양은 커널의 높이와 너비로 주어집니다(여기서는 $2 \times 2$).</p>
<p><img src="chapter_convolutional-neural-networks/../img/correlation.svg" alt="2차원 상호 상관 연산. 음영 처리된 부분은 첫 번째 출력 요소와 출력 계산에 사용된 입력 및 커널 텐서 요소입니다: $0\times0+1\times1+3\times2+4\times3=19$." />
:label:<code>fig_correlation</code></p>
<p>2차원 상호 상관 연산에서는 입력 텐서의 왼쪽 상단 모서리에 위치한 합성곱 윈도우로 시작하여 왼쪽에서 오른쪽으로, 위에서 아래로 입력 텐서를 가로질러 밉니다.
합성곱 윈도우가 특정 위치로 미끄러질 때, 해당 윈도우에 포함된 입력 하위 텐서와 커널 텐서가 요소별로 곱해지고 결과 텐서가 합산되어 단일 스칼라 값을 산출합니다.
이 결과는 해당 위치에서 출력 텐서의 값을 제공합니다.
여기서 출력 텐서는 높이 2, 너비 2를 가지며 4개의 요소는 2차원 상호 상관 연산에서 파생됩니다:</p>
<p>$$
0\times0+1\times1+3\times2+4\times3=19,<br />
1\times0+2\times1+4\times2+5\times3=25,<br />
3\times0+4\times1+6\times2+7\times3=37,<br />
4\times0+5\times1+7\times2+8\times3=43.
$$</p>
<p>각 축을 따라 출력 크기는 입력 크기보다 약간 작습니다.
커널의 너비와 높이가 $1$보다 크기 때문에, 커널이 이미지 내에 완전히 들어맞는 위치에 대해서만 상호 상관을 적절하게 계산할 수 있습니다.
출력 크기는 입력 크기 $n_\textrm{h} \times n_\textrm{w}$에서 합성곱 커널 크기 $k_\textrm{h} \times k_\textrm{w}$를 뺀 값으로 다음과 같이 주어집니다.</p>
<p>$$(n_\textrm{h}-k_\textrm{h}+1) \times (n_\textrm{w}-k_\textrm{w}+1).$$</p>
<p>이는 이미지를 가로질러 합성곱 커널을 "이동"할 충분한 공간이 필요하기 때문입니다.
나중에 커널을 이동할 충분한 공간이 있도록 경계 주위에 0으로 이미지를 패딩하여 크기를 변경하지 않고 유지하는 방법을 볼 것입니다.
다음으로 입력 텐서 <code>X</code>와 커널 텐서 <code>K</code>를 받아 출력 텐서 <code>Y</code>를 반환하는 <code>corr2d</code> 함수로 이 과정을 구현합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def corr2d(X, K):  #@save
    """2D 상호 상관을 계산합니다."""
    h, w = K.shape
    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))
    return Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def corr2d(X, K):  #@save
    """2D 상호 상관을 계산합니다."""
    h, w = K.shape
    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))
    return Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def corr2d(X, K):  #@save
    """2D 상호 상관을 계산합니다."""
    h, w = K.shape
    Y = jnp.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y = Y.at[i, j].set((X[i:i + h, j:j + w] * K).sum())
    return Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def corr2d(X, K):  #@save
    """2D 상호 상관을 계산합니다."""
    h, w = K.shape
    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j].assign(tf.reduce_sum(
                X[i: i + h, j: j + w] * K))
    return Y
</code></pre>
<p>:numref:<code>fig_correlation</code>의 입력 텐서 <code>X</code>와 커널 텐서 <code>K</code>를 구성하여 2차원 상호 상관 연산의 [<strong>위 구현 출력을 검증</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
K = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])
corr2d(X, K)
</code></pre>
<h2 id="합성곱-레이어-convolutional-layers"><a class="header" href="#합성곱-레이어-convolutional-layers">합성곱 레이어 (Convolutional Layers)</a></h2>
<p>합성곱 레이어는 입력과 커널을 상호 상관시키고 스칼라 편향을 더하여 출력을 생성합니다.
합성곱 레이어의 두 파라미터는 커널과 스칼라 편향입니다.
합성곱 레이어를 기반으로 모델을 훈련할 때, 일반적으로 완전 연결 레이어와 마찬가지로 커널을 무작위로 초기화합니다.</p>
<p>이제 위에서 정의한 <code>corr2d</code> 함수를 기반으로 [<strong>2차원 합성곱 레이어를 구현</strong>]할 준비가 되었습니다.
<code>__init__</code> 생성자 메서드에서 <code>weight</code>와 <code>bias</code>를 두 모델 파라미터로 선언합니다.
순전파 메서드는 <code>corr2d</code> 함수를 호출하고 편향을 더합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class Conv2D(nn.Block):
    def __init__(self, kernel_size, **kwargs):
        super().__init__(**kwargs)
        self.weight = self.params.get('weight', shape=kernel_size)
        self.bias = self.params.get('bias', shape=(1,))

    def forward(self, x):
        return corr2d(x, self.weight.data()) + self.bias.data()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class Conv2D(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def build(self, kernel_size):
        initializer = tf.random_normal_initializer()
        self.weight = self.add_weight(name='w', shape=kernel_size,
                                      initializer=initializer)
        self.bias = self.add_weight(name='b', shape=(1, ),
                                    initializer=initializer)

    def call(self, inputs):
        return corr2d(inputs, self.weight) + self.bias
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Conv2D(nn.Module):
    kernel_size: int

    def setup(self):
        self.weight = nn.param('w', nn.initializers.uniform, self.kernel_size)
        self.bias = nn.param('b', nn.initializers.zeros, 1)

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
</code></pre>
<p>$h \times w$ 합성곱 또는 $h \times w$ 합성곱 커널에서, 합성곱 커널의 높이와 너비는 각각 $h$와 $w$입니다.
우리는 또한 $h \times w$ 합성곱 커널을 가진 합성곱 레이어를 단순히 $h \times w$ 합성곱 레이어라고 부릅니다.</p>
<h2 id="이미지의-객체-가장자리-감지-object-edge-detection-in-images"><a class="header" href="#이미지의-객체-가장자리-감지-object-edge-detection-in-images">이미지의 객체 가장자리 감지 (Object Edge Detection in Images)</a></h2>
<p>픽셀 변화의 위치를 찾아 [<strong>합성곱 레이어의 간단한 응용: 이미지의 객체 가장자리 감지</strong>]를 분석해 봅시다.
먼저 $6\times 8$ 픽셀의 "이미지"를 구성합니다.
가운데 네 열은 검은색($0$)이고 나머지는 흰색($1$)입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
X = d2l.ones((6, 8))
X[:, 2:6] = 0
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X = tf.Variable(tf.ones((6, 8)))
X[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X = jnp.ones((6, 8))
X = X.at[:, 2:6].set(0)
X
</code></pre>
<p>다음으로 높이가 1이고 너비가 2인 커널 <code>K</code>를 구성합니다.
입력과 상호 상관 연산을 수행할 때, 수평으로 인접한 요소가 같으면 출력은 0입니다. 그렇지 않으면 출력은 0이 아닙니다.
이 커널은 유한 차분 연산자의 특수한 경우라는 점에 유의하십시오. 위치 $(i,j)$에서 $x_{i,j} - x_{(i+1),j}$를 계산합니다. 즉, 수평으로 인접한 픽셀 값의 차이를 계산합니다. 이것은 수평 방향의 1계 도함수의 이산 근사입니다. 결국 함수 $f(i,j)$에 대해 그 도함수는 $-\partial_i f(i,j) = \lim_{\epsilon \to 0} \frac{f(i,j) - f(i+\epsilon,j)}{\epsilon}$입니다. 이것이 실제로 어떻게 작동하는지 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
K = d2l.tensor([[1.0, -1.0]])
</code></pre>
<p>인수 <code>X</code>(입력)와 <code>K</code>(커널)로 상호 상관 연산을 수행할 준비가 되었습니다.
보시다시피, [<strong>흰색에서 검은색으로 변하는 가장자리는 $1$로, 검은색에서 흰색으로 변하는 가장자리는 $-1$로 감지합니다.</strong>]
다른 모든 출력은 값 $0$을 취합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
Y = corr2d(X, K)
Y
</code></pre>
<p>이제 전치된 이미지에 커널을 적용할 수 있습니다.
예상대로 사라집니다. [<strong>커널 <code>K</code>는 수직 가장자리만 감지합니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab all
corr2d(d2l.transpose(X), K)
</code></pre>
<h2 id="커널-학습하기-learning-a-kernel"><a class="header" href="#커널-학습하기-learning-a-kernel">커널 학습하기 (Learning a Kernel)</a></h2>
<p>이것이 우리가 찾고 있는 것이 정확히 무엇인지 안다면 유한 차분 <code>[1, -1]</code>로 가장자리 감지기를 설계하는 것은 깔끔합니다.
하지만 더 큰 커널을 보고 연속적인 합성곱 레이어를 고려할 때, 각 필터가 무엇을 해야 하는지 수동으로 정확하게 지정하는 것은 불가능할 수 있습니다.</p>
<p>이제 입력-출력 쌍만 보고 [<strong><code>X</code>에서 <code>Y</code>를 생성한 커널을 학습할 수 있는지</strong>] 봅시다.
먼저 합성곱 레이어를 구성하고 커널을 무작위 텐서로 초기화합니다.
다음으로 각 반복에서 제곱 오차를 사용하여 <code>Y</code>를 합성곱 레이어의 출력과 비교합니다.
그런 다음 기울기를 계산하여 커널을 업데이트할 수 있습니다.
단순함을 위해 다음에서는 2차원 합성곱 레이어에 대한 내장 클래스를 사용하고 편향을 무시합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
# 1개의 출력 채널과 모양 (1, 2)의 커널을 가진 2차원 합성곱 레이어를 구성합니다.
# 단순함을 위해 여기서는 편향을 무시합니다
conv2d = nn.Conv2D(1, kernel_size=(1, 2), use_bias=False)
conv2d.initialize()

# 2차원 합성곱 레이어는 (예제, 채널, 높이, 너비) 형식의 4차원 입력 및 출력을 사용합니다.
# 여기서 배치 크기(배치의 예제 수)와 채널 수는 모두 1입니다
X = X.reshape(1, 1, 6, 8)
Y = Y.reshape(1, 1, 6, 7)
lr = 3e-2  # 학습률

for i in range(10):
    with autograd.record():
        Y_hat = conv2d(X)
        l = (Y_hat - Y) ** 2
    l.backward()
    # 커널 업데이트
    conv2d.weight.data()[:] -= lr * conv2d.weight.grad()
    if (i + 1) % 2 == 0:
        print(f'epoch {i + 1}, loss {float(l.sum()):.3f}')
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 1개의 출력 채널과 모양 (1, 2)의 커널을 가진 2차원 합성곱 레이어를 구성합니다.
# 단순함을 위해 여기서는 편향을 무시합니다
conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)

# 2차원 합성곱 레이어는 (예제, 채널, 높이, 너비) 형식의 4차원 입력 및 출력을 사용합니다.
# 여기서 배치 크기(배치의 예제 수)와 채널 수는 모두 1입니다
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # 학습률

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # 커널 업데이트
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i + 1}, loss {l.sum():.3f}')
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# 1개의 출력 채널과 모양 (1, 2)의 커널을 가진 2차원 합성곱 레이어를 구성합니다.
# 단순함을 위해 여기서는 편향을 무시합니다
conv2d = tf.keras.layers.Conv2D(1, (1, 2), use_bias=False)

# 2차원 합성곱 레이어는 (예제, 높이, 너비, 채널) 형식의 4차원 입력 및 출력을 사용합니다.
# 여기서 배치 크기(배치의 예제 수)와 채널 수는 모두 1입니다
X = tf.reshape(X, (1, 6, 8, 1))
Y = tf.reshape(Y, (1, 6, 7, 1))
lr = 3e-2  # 학습률

Y_hat = conv2d(X)
for i in range(10):
    with tf.GradientTape(watch_accessed_variables=False) as g:
        g.watch(conv2d.weights[0])
        Y_hat = conv2d(X)
        l = (abs(Y_hat - Y)) ** 2
        # 커널 업데이트
        update = tf.multiply(lr, g.gradient(l, conv2d.weights[0]))
        weights = conv2d.get_weights()
        weights[0] = conv2d.weights[0] - update
        conv2d.set_weights(weights)
        if (i + 1) % 2 == 0:
            print(f'epoch {i + 1}, loss {tf.reduce_sum(l):.3f}')
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# 1개의 출력 채널과 모양 (1, 2)의 커널을 가진 2차원 합성곱 레이어를 구성합니다.
# 단순함을 위해 여기서는 편향을 무시합니다
conv2d = nn.Conv(1, kernel_size=(1, 2), use_bias=False, padding='VALID')

# 2차원 합성곱 레이어는 (예제, 높이, 너비, 채널) 형식의 4차원 입력 및 출력을 사용합니다.
# 여기서 배치 크기(배치의 예제 수)와 채널 수는 모두 1입니다
X = X.reshape((1, 6, 8, 1))
Y = Y.reshape((1, 6, 7, 1))
lr = 3e-2  # 학습률

params = conv2d.init(jax.random.PRNGKey(d2l.get_seed()), X)

def loss(params, X, Y):
    Y_hat = conv2d.apply(params, X)
    return ((Y_hat - Y) ** 2).sum()

for i in range(10):
    l, grads = jax.value_and_grad(loss)(params, X, Y)
    # 커널 업데이트
    params = jax.tree_map(lambda p, g: p - lr * g, params, grads)
    if (i + 1) % 2 == 0:
        print(f'epoch {i + 1}, loss {l:.3f}')
</code></pre>
<p>10번 반복 후 오차가 작은 값으로 떨어졌음에 유의하십시오. 이제 [<strong>우리가 학습한 커널 텐서를 살펴봅시다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
d2l.reshape(conv2d.weight.data(), (1, 2))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
d2l.reshape(conv2d.weight.data, (1, 2))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
d2l.reshape(conv2d.get_weights()[0], (1, 2))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
params['params']['kernel'].reshape((1, 2))
</code></pre>
<p>실제로 학습된 커널 텐서는 우리가 앞서 정의한 커널 텐서 <code>K</code>와 놀랍도록 가깝습니다.</p>
<h2 id="상호-상관과-합성곱-cross-correlation-and-convolution"><a class="header" href="#상호-상관과-합성곱-cross-correlation-and-convolution">상호 상관과 합성곱 (Cross-Correlation and Convolution)</a></h2>
<p>:numref:<code>sec_why-conv</code>에서 상호 상관 연산과 합성곱 연산 사이의 대응 관계에 대한 관찰을 상기해 보십시오.
여기서 2차원 합성곱 레이어를 계속 고려해 봅시다.
만약 그러한 레이어가 상호 상관 대신 :eqref:<code>eq_2d-conv-discrete</code>에 정의된 엄격한 합성곱 연산을 수행한다면 어떨까요?
엄격한 <em>합성곱</em> 연산의 출력을 얻으려면, 2차원 커널 텐서를 수평 및 수직으로 모두 뒤집은 다음 입력 텐서와 <em>상호 상관</em> 연산을 수행하기만 하면 됩니다.</p>
<p>딥러닝에서는 데이터로부터 커널을 학습하기 때문에, 그러한 레이어가 엄격한 합성곱 연산을 수행하든 상호 상관 연산을 수행하든 합성곱 레이어의 출력은 영향을 받지 않는다는 점에 주목할 가치가 있습니다.</p>
<p>이를 설명하기 위해, 합성곱 레이어가 <em>상호 상관</em>을 수행하고 :numref:<code>fig_correlation</code>의 커널을 학습한다고 가정합시다. 여기서는 행렬 $\mathbf{K}$로 표시됩니다.
다른 조건이 변하지 않는다고 가정할 때, 이 레이어가 대신 엄격한 <em>합성곱</em>을 수행한다면, 학습된 커널 $\mathbf{K}'$는 $\mathbf{K}'$가 수평 및 수직으로 모두 뒤집힌 후 $\mathbf{K}$와 같아질 것입니다.
즉, 합성곱 레이어가 :numref:<code>fig_correlation</code>의 입력과 $\mathbf{K}'$에 대해 엄격한 <em>합성곱</em>을 수행할 때, :numref:<code>fig_correlation</code>의 동일한 출력(입력과 $\mathbf{K}$의 상호 상관)을 얻게 됩니다.</p>
<p>딥러닝 문헌의 표준 용어를 따르기 위해, 엄밀히 말하면 약간 다르지만 상호 상관 연산을 계속해서 합성곱이라고 부를 것입니다.
또한 레이어 표현이나 합성곱 커널을 나타내는 텐서의 항목(또는 구성 요소)을 지칭하기 위해 <em>요소</em>라는 용어를 사용합니다.</p>
<h2 id="특성-맵과-수용-영역-feature-map-and-receptive-field"><a class="header" href="#특성-맵과-수용-영역-feature-map-and-receptive-field">특성 맵과 수용 영역 (Feature Map and Receptive Field)</a></h2>
<p>:numref:<code>subsec_why-conv-channels</code>에서 설명한 바와 같이, :numref:<code>fig_correlation</code>의 합성곱 레이어 출력은 때때로 *특성 맵(feature map)*이라고 불립니다. 후속 레이어에 대한 공간 차원(예: 너비 및 높이)의 학습된 표현(특성)으로 간주될 수 있기 때문입니다.
CNN에서 어떤 레이어의 요소 $x$에 대해, 그 *수용 영역(receptive field)*은 순전파 동안 $x$의 계산에 영향을 줄 수 있는 (모든 이전 레이어의) 모든 요소를 말합니다.
수용 영역은 입력의 실제 크기보다 클 수 있다는 점에 유의하십시오.</p>
<p>:numref:<code>fig_correlation</code>을 계속 사용하여 수용 영역을 설명해 봅시다.
$2 \times 2$ 합성곱 커널이 주어졌을 때, 음영 처리된 출력 요소(값 $19$)의 수용 영역은 입력의 음영 처리된 부분에 있는 4개의 요소입니다.
이제 $2 \times 2$ 출력을 $\mathbf{Y}$로 표시하고, $\mathbf{Y}$를 입력으로 받아 단일 요소 $z$를 출력하는 추가적인 $2 \times 2$ 합성곱 레이어가 있는 더 깊은 CNN을 고려해 봅시다.
이 경우, $\mathbf{Y}$에 대한 $z$의 수용 영역은 $\mathbf{Y}$의 4개 요소를 모두 포함하는 반면, 입력에 대한 수용 영역은 9개의 입력 요소를 모두 포함합니다.
따라서 특성 맵의 어떤 요소가 더 넓은 영역의 입력 특성을 감지하기 위해 더 큰 수용 영역이 필요한 경우, 우리는 더 깊은 네트워크를 구축할 수 있습니다.</p>
<p>수용 영역이라는 이름은 신경생리학에서 유래했습니다.
다양한 자극을 사용하여 다양한 동물에 대해 수행된 일련의 실험들 :cite:<code>Hubel.Wiesel.1959,Hubel.Wiesel.1962,Hubel.Wiesel.1968</code>은 소위 시각 피질이 해당 자극에 반응하는 것을 탐구했습니다.
대체로 그들은 낮은 수준이 가장자리 및 관련 모양에 반응한다는 것을 발견했습니다.
나중에 :citet:<code>Field.1987</code>은 합성곱 커널이라고밖에 부를 수 없는 것으로 자연 이미지에 대한 이 효과를 설명했습니다.
우리는 놀라운 유사성을 설명하기 위해 :numref:<code>field_visual</code>에 핵심 그림을 다시 인쇄합니다.</p>
<p><img src="chapter_convolutional-neural-networks/../img/field-visual.png" alt=" :citet:Field.1987에서 가져온 그림 및 캡션: 6개의 다른 채널로 코딩하는 예. (왼쪽) 각 채널과 관련된 6가지 유형의 센서 예. (오른쪽) (가운데) 이미지를 (왼쪽)에 표시된 6개 센서로 합성곱. 개별 센서의 응답은 센서 크기에 비례하는 거리(점으로 표시됨)에서 이러한 필터링된 이미지를 샘플링하여 결정됩니다. 이 다이어그램은 짝수 대칭 센서의 응답만 보여줍니다." />
:label:<code>field_visual</code></p>
<p>밝혀진 바로는, 이 관계는 예를 들어 :citet:<code>Kuzovkin.Vicente.Petton.ea.2018</code>에서 입증된 바와 같이 이미지 분류 작업에 대해 훈련된 네트워크의 더 깊은 레이어에 의해 계산된 특성에도 적용됩니다.
합성곱은 생물학과 코드 모두에서 컴퓨터 비전을 위한 믿을 수 없을 정도로 강력한 도구임이 입증되었다고 말하는 것으로 충분합니다.
따라서 (지나고 나서 보면) 그것들이 딥러닝의 최근 성공을 예고했다는 것은 놀라운 일이 아닙니다.</p>
<h2 id="요약-summary-25"><a class="header" href="#요약-summary-25">요약 (Summary)</a></h2>
<p>합성곱 레이어에 필요한 핵심 계산은 상호 상관 연산입니다. 우리는 간단한 중첩 for-루프만으로 그 값을 계산할 수 있음을 보았습니다. 다중 입력 및 다중 출력 채널이 있는 경우, 우리는 채널 간에 행렬-행렬 연산을 수행합니다. 보시다시피 계산은 간단하며, 가장 중요한 것은 고도로 <em>국소적</em>이라는 점입니다. 이는 상당한 하드웨어 최적화를 제공하며 컴퓨터 비전의 많은 최근 결과는 그 덕분에 가능했습니다. 결국 칩 설계자가 합성곱 최적화와 관련하여 메모리보다는 빠른 계산에 투자할 수 있다는 것을 의미합니다. 이것이 다른 응용 분야에 대한 최적의 설계로 이어지지는 않을지라도, 유비쿼터스하고 저렴한 컴퓨터 비전의 문을 엽니다.</p>
<p>합성곱 자체의 측면에서 보면, 가장자리 및 선 감지, 이미지 흐리기 또는 선명하게 하기 등 다양한 목적으로 사용될 수 있습니다.
가장 중요한 것은 통계학자(또는 엔지니어)가 적절한 필터를 발명할 필요가 없다는 것입니다.
대신 데이터로부터 간단히 <em>학습</em>할 수 있습니다.
이것은 특성 엔지니어링 휴리스틱을 증거 기반 통계로 대체합니다.
마지막으로, 아주 기쁘게도 이러한 필터는 심층 네트워크를 구축하는 데 유리할 뿐만 아니라 뇌의 수용 영역 및 특성 맵과도 일치합니다.
이것은 우리가 올바른 길을 가고 있다는 확신을 줍니다.</p>
<h2 id="연습-문제-exercises-29"><a class="header" href="#연습-문제-exercises-29">연습 문제 (Exercises)</a></h2>
<ol>
<li>대각선 가장자리가 있는 이미지 <code>X</code>를 구성하십시오.
<ol>
<li>이 섹션의 커널 <code>K</code>를 적용하면 어떻게 됩니까?</li>
<li><code>X</code>를 전치하면 어떻게 됩니까?</li>
<li><code>K</code>를 전치하면 어떻게 됩니까?</li>
</ol>
</li>
<li>커널을 수동으로 설계해 보십시오.
<ol>
<li>방향 벡터 $\mathbf{v} = (v_1, v_2)$가 주어졌을 때, $\mathbf{v}$에 직교하는 가장자리, 즉 $(v_2, -v_1)$ 방향의 가장자리를 감지하는 가장자리 감지 커널을 유도하십시오.</li>
<li>2계 도함수에 대한 유한 차분 연산자를 유도하십시오. 이와 관련된 합성곱 커널의 최소 크기는 얼마입니까? 이미지의 어떤 구조가 가장 강하게 반응합니까?</li>
<li>흐림(blur) 커널을 어떻게 설계하시겠습니까? 왜 그런 커널을 사용하고 싶을까요?</li>
<li>차수 $d$의 도함수를 얻기 위한 커널의 최소 크기는 얼마입니까?</li>
</ol>
</li>
<li>우리가 만든 <code>Conv2D</code> 클래스에 대한 기울기를 자동으로 찾으려고 할 때 어떤 종류의 오류 메시지가 표시됩니까?</li>
<li>입력 및 커널 텐서를 변경하여 상호 상관 연산을 행렬 곱셈으로 어떻게 표현합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/65">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/66">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/271">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17996">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="패딩과-스트라이드-padding-and-stride"><a class="header" href="#패딩과-스트라이드-padding-and-stride">패딩과 스트라이드 (Padding and Stride)</a></h1>
<p>:label:<code>sec_padding</code></p>
<p>:numref:<code>fig_correlation</code>의 합성곱 예제를 상기해 보십시오.
입력은 높이와 너비가 모두 3이었고 합성곱 커널은 높이와 너비가 모두 2였으며, $2\times2$ 차원의 출력 표현을 산출했습니다.
입력 모양이 $n_\textrm{h}\times n_\textrm{w}$이고 합성곱 커널 모양이 $k_\textrm{h}\times k_\textrm{w}$라고 가정하면,
출력 모양은 $(n_\textrm{h}-k_\textrm{h}+1) \times (n_\textrm{w}-k_\textrm{w}+1)$이 됩니다:
합성곱을 적용할 픽셀이 떨어질 때까지만 합성곱 커널을 이동할 수 있습니다.</p>
<p>다음에서는 출력 크기에 대한 더 많은 제어권을 제공하는 패딩 및 스트라이드 합성곱을 포함한 여러 기술을 살펴볼 것입니다.
동기를 부여하자면, 커널은 일반적으로 $1$보다 큰 너비와 높이를 가지므로, 많은 연속적인 합성곱을 적용한 후에는 입력보다 상당히 작은 출력으로 끝나는 경향이 있습니다.
$240 \times 240$ 픽셀 이미지로 시작하는 경우, 10개의 $5 \times 5$ 합성곱 레이어는 이미지를 $200 \times 200$ 픽셀로 줄여 이미지의 $30 %$를 잘라내고 원본 이미지 경계에 있는 흥미로운 정보를 없애버립니다.
*패딩(padding)*은 이 문제를 처리하는 가장 인기 있는 도구입니다.
다른 경우에는 차원을 대폭 줄이고 싶을 수도 있습니다. 예를 들어 원래 입력 해상도가 다루기 힘들다고 생각되는 경우입니다.
*스트라이드 합성곱(strided convolutions)*은 이러한 경우에 도움이 될 수 있는 인기 있는 기술입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="패딩-padding"><a class="header" href="#패딩-padding">패딩 (Padding)</a></h2>
<p>위에서 설명한 바와 같이, 합성곱 레이어를 적용할 때 한 가지 까다로운 문제는 이미지 주변의 픽셀을 잃어버리는 경향이 있다는 것입니다. 합성곱 커널 크기와 이미지 내 위치의 함수로서 픽셀 활용도를 나타내는 :numref:<code>img_conv_reuse</code>를 고려해 보십시오. 모서리의 픽셀은 거의 사용되지 않습니다.</p>
<p><img src="chapter_convolutional-neural-networks/../img/conv-reuse.svg" alt="각각 $1 \times 1$, $2 \times 2$, $3 \times 3$ 크기의 합성곱에 대한 픽셀 활용도." />
:label:<code>img_conv_reuse</code></p>
<p>우리는 일반적으로 작은 커널을 사용하므로 주어진 합성곱에 대해 몇 개의 픽셀만 잃을 수 있지만, 많은 연속적인 합성곱 레이어를 적용함에 따라 이것이 누적될 수 있습니다.
이 문제에 대한 한 가지 간단한 해결책은 입력 이미지의 경계 주위에 채우기 픽셀을 추가하여 이미지의 유효 크기를 늘리는 것입니다.
일반적으로 추가 픽셀의 값을 0으로 설정합니다.
:numref:<code>img_conv_pad</code>에서는 $3 \times 3$ 입력을 패딩하여 크기를 $5 \times 5$로 늘립니다.
해당 출력은 $4 \times 4$ 행렬로 증가합니다.
음영 처리된 부분은 첫 번째 출력 요소와 출력 계산에 사용된 입력 및 커널 텐서 요소입니다: $0\times0+0\times1+0\times2+0\times3=0$.</p>
<p><img src="chapter_convolutional-neural-networks/../img/conv-pad.svg" alt="패딩이 있는 2차원 상호 상관." />
:label:<code>img_conv_pad</code></p>
<p>일반적으로 총 $p_\textrm{h}$ 행의 패딩(대략 위쪽에 절반, 아래쪽에 절반)과 총 $p_\textrm{w}$ 열의 패딩(대략 왼쪽에 절반, 오른쪽에 절반)을 추가하면 출력 모양은 다음과 같습니다.</p>
<p>$$(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+1)\times(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+1).$$</p>
<p>즉, 출력의 높이와 너비가 각각 $p_\textrm{h}$와 $p_\textrm{w}$만큼 증가합니다.</p>
<p>많은 경우, 입력과 출력의 높이와 너비를 같게 만들기 위해 $p_\textrm{h}=k_\textrm{h}-1$ 및 $p_\textrm{w}=k_\textrm{w}-1$로 설정하고 싶을 것입니다.
이렇게 하면 네트워크를 구성할 때 각 레이어의 출력 모양을 예측하기가 더 쉬워집니다.
여기서 $k_\textrm{h}$가 홀수라고 가정하면, 높이의 양쪽에 $p_\textrm{h}/2$ 행을 패딩합니다.
$k_\textrm{h}$가 짝수인 경우, 한 가지 가능성은 입력의 위쪽에 $\lceil p_\textrm{h}/2\rceil$ 행을 패딩하고 아래쪽에 $\lfloor p_\textrm{h}/2\rfloor$ 행을 패딩하는 것입니다.
너비의 양쪽도 같은 방식으로 패딩합니다.</p>
<p>CNN은 일반적으로 1, 3, 5 또는 7과 같이 홀수 높이 및 너비 값을 가진 합성곱 커널을 사용합니다.
홀수 커널 크기를 선택하면 위쪽과 아래쪽에 같은 수의 행을, 왼쪽과 오른쪽에 같은 수의 열을 패딩하면서 차원을 보존할 수 있다는 이점이 있습니다.</p>
<p>더욱이 홀수 커널을 사용하고 차원을 정확하게 보존하기 위해 패딩하는 이 관행은 사무적인 이점을 제공합니다.
모든 2차원 텐서 <code>X</code>에 대해, 커널 크기가 홀수이고 모든 측면의 패딩 행과 열 수가 동일하여 입력과 동일한 높이와 너비를 가진 출력을 생성할 때,
우리는 출력 <code>Y[i, j]</code>가 <code>X[i, j]</code>를 중심으로 하는 윈도우와 입력 및 합성곱 커널의 상호 상관에 의해 계산된다는 것을 알고 있습니다.</p>
<p>다음 예제에서는 높이와 너비가 3인 2차원 합성곱 레이어를 생성하고 (<strong>모든 면에 1픽셀의 패딩을 적용합니다.</strong>)
높이와 너비가 8인 입력이 주어지면 출력의 높이와 너비도 8임을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
# 합성곱을 계산하기 위한 도우미 함수를 정의합니다.
# 합성곱 레이어 가중치를 초기화하고 입력 및 출력에 대해 해당하는 차원 상승 및 축소를 수행합니다.
def comp_conv2d(conv2d, X):
    conv2d.initialize()
    # (1, 1)은 배치 크기와 채널 수가 모두 1임을 나타냅니다
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # 처음 두 차원을 제거합니다: 예제 및 채널
    return Y.reshape(Y.shape[2:])

# 양쪽에 1행과 1열이 패딩되므로 총 2개의 행 또는 열이 추가됩니다
conv2d = nn.Conv2D(1, kernel_size=3, padding=1)
X = np.random.uniform(size=(8, 8))
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 합성곱을 계산하기 위한 도우미 함수를 정의합니다.
# 합성곱 레이어 가중치를 초기화하고 입력 및 출력에 대해 해당하는 차원 상승 및 축소를 수행합니다.
def comp_conv2d(conv2d, X):
    # (1, 1)은 배치 크기와 채널 수가 모두 1임을 나타냅니다
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # 처음 두 차원을 제거합니다: 예제 및 채널
    return Y.reshape(Y.shape[2:])

# 양쪽에 1행과 1열이 패딩되므로 총 2개의 행 또는 열이 추가됩니다
conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# 합성곱을 계산하기 위한 도우미 함수를 정의합니다.
# 합성곱 레이어 가중치를 초기화하고 입력 및 출력에 대해 해당하는 차원 상승 및 축소를 수행합니다.
def comp_conv2d(conv2d, X):
    # (1, 1)은 배치 크기와 채널 수가 모두 1임을 나타냅니다
    X = tf.reshape(X, (1, ) + X.shape + (1, ))
    Y = conv2d(X)
    # 처음 두 차원을 제거합니다: 예제 및 채널
    return tf.reshape(Y, Y.shape[1:3])
# 양쪽에 1행과 1열이 패딩되므로 총 2개의 행 또는 열이 추가됩니다
conv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding='same')
X = tf.random.uniform(shape=(8, 8))
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# 합성곱을 계산하기 위한 도우미 함수를 정의합니다.
# 합성곱 레이어 가중치를 초기화하고 입력 및 출력에 대해 해당하는 차원 상승 및 축소를 수행합니다.
def comp_conv2d(conv2d, X):
    # (1, X.shape, 1)은 배치 크기와 채널 수가 모두 1임을 나타냅니다
    key = jax.random.PRNGKey(d2l.get_seed())
    X = X.reshape((1,) + X.shape + (1,))
    Y, _ = conv2d.init_with_output(key, X)
    # 차원을 제거합니다: 예제 및 채널
    return Y.reshape(Y.shape[1:3])
# 양쪽에 1행과 1열이 패딩되므로 총 2개의 행 또는 열이 추가됩니다
conv2d = nn.Conv(1, kernel_size=(3, 3), padding='SAME')
X = jax.random.uniform(jax.random.PRNGKey(d2l.get_seed()), shape=(8, 8))
comp_conv2d(conv2d, X).shape
</code></pre>
<p>합성곱 커널의 높이와 너비가 다른 경우, [<strong>높이와 너비에 대해 다른 패딩 숫자를 설정</strong>]하여 출력과 입력이 동일한 높이와 너비를 갖도록 만들 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
# 높이 5, 너비 3인 합성곱 커널을 사용합니다.
# 높이와 너비의 양쪽 패딩은 각각 2와 1입니다
conv2d = nn.Conv2D(1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 높이 5, 너비 3인 합성곱 커널을 사용합니다.
# 높이와 너비의 양쪽 패딩은 각각 2와 1입니다
conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# 높이 5, 너비 3인 합성곱 커널을 사용합니다.
# 높이와 너비의 양쪽 패딩은 각각 2와 1입니다
conv2d = tf.keras.layers.Conv2D(1, kernel_size=(5, 3), padding='same')
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# 높이 5, 너비 3인 합성곱 커널을 사용합니다.
# 높이와 너비의 양쪽 패딩은 각각 2와 1입니다
conv2d = nn.Conv(1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
</code></pre>
<h2 id="스트라이드-stride"><a class="header" href="#스트라이드-stride">스트라이드 (Stride)</a></h2>
<p>상호 상관을 계산할 때, 입력 텐서의 왼쪽 상단 모서리에서 합성곱 윈도우를 시작한 다음, 아래쪽과 오른쪽의 모든 위치로 밉니다.
이전 예제에서는 기본적으로 한 번에 한 요소씩 이동했습니다.
그러나 때로는 계산 효율성을 위해 또는 다운샘플링을 원하기 때문에 중간 위치를 건너뛰고 윈도우를 한 번에 두 개 이상의 요소로 이동합니다. 이는 합성곱 커널이 클 경우 기본 이미지의 넓은 영역을 캡처하므로 특히 유용합니다.</p>
<p>우리는 슬라이드당 가로지르는 행과 열의 수를 *스트라이드(stride)*라고 합니다.
지금까지는 높이와 너비 모두에 대해 1의 스트라이드를 사용했습니다.
때로는 더 큰 스트라이드를 사용하고 싶을 수 있습니다.
:numref:<code>img_conv_stride</code>는 세로로 3, 가로로 2의 스트라이드를 갖는 2차원 상호 상관 연산을 보여줍니다.
음영 처리된 부분은 출력 요소와 출력 계산에 사용된 입력 및 커널 텐서 요소입니다: $0\times0+0\times1+1\times2+2\times3=8$, $0\times0+6\times1+0\times2+0\times3=6$.
첫 번째 열의 두 번째 요소가 생성될 때 합성곱 윈도우가 아래로 세 행 이동함을 알 수 있습니다.
첫 번째 행의 두 번째 요소가 생성될 때 합성곱 윈도우가 오른쪽으로 두 열 이동합니다.
합성곱 윈도우가 입력에서 오른쪽으로 두 열 더 이동하면, 입력 요소가 윈도우를 채울 수 없으므로 출력이 없습니다(다른 패딩 열을 추가하지 않는 한).</p>
<p><img src="chapter_convolutional-neural-networks/../img/conv-stride.svg" alt="높이와 너비에 대해 각각 3과 2의 스트라이드를 갖는 상호 상관." />
:label:<code>img_conv_stride</code></p>
<p>일반적으로 높이에 대한 스트라이드가 $s_\textrm{h}$이고 너비에 대한 스트라이드가 $s_\textrm{w}$일 때 출력 모양은 다음과 같습니다.</p>
<p>$$\lfloor(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+s_\textrm{h})/s_\textrm{h}\rfloor \times \lfloor(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+s_\textrm{w})/s_\textrm{w}\rfloor.$$</p>
<p>$p_\textrm{h}=k_\textrm{h}-1$ 및 $p_\textrm{w}=k_\textrm{w}-1$로 설정하면 출력 모양은 $\lfloor(n_\textrm{h}+s_\textrm{h}-1)/s_\textrm{h}\rfloor \times \lfloor(n_\textrm{w}+s_\textrm{w}-1)/s_\textrm{w}\rfloor$로 단순화될 수 있습니다.
한 단계 더 나아가 입력 높이와 너비가 높이와 너비의 스트라이드로 나누어떨어지면 출력 모양은 $(n_\textrm{h}/s_\textrm{h}) \times (n_\textrm{w}/s_\textrm{w})$가 됩니다.</p>
<p>아래에서는 [<strong>높이와 너비 모두의 스트라이드를 2로 설정</strong>]하여 입력 높이와 너비를 반으로 줄입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
conv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding='same', strides=2)
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
conv2d = nn.Conv(1, kernel_size=(3, 3), padding=1, strides=2)
comp_conv2d(conv2d, X).shape
</code></pre>
<p>(<strong>약간 더 복잡한 예제</strong>)를 살펴봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
conv2d = nn.Conv2D(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
conv2d = tf.keras.layers.Conv2D(1, kernel_size=(3,5), padding='valid',
                                strides=(3, 4))
comp_conv2d(conv2d, X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
conv2d = nn.Conv(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))
comp_conv2d(conv2d, X).shape
</code></pre>
<h2 id="요약-및-토론-summary-and-discussion-4"><a class="header" href="#요약-및-토론-summary-and-discussion-4">요약 및 토론 (Summary and Discussion)</a></h2>
<p>패딩은 출력의 높이와 너비를 늘릴 수 있습니다. 이는 종종 출력의 바람직하지 않은 축소를 피하기 위해 출력에 입력과 동일한 높이와 너비를 제공하는 데 사용됩니다. 또한 모든 픽셀이 똑같이 자주 사용되도록 합니다. 일반적으로 입력 높이와 너비의 양쪽에 대칭 패딩을 선택합니다. 이 경우 $(p_\textrm{h}, p_\textrm{w})$ 패딩이라고 합니다. 가장 일반적으로 $p_\textrm{h} = p_\textrm{w}$로 설정하며, 이 경우 단순히 패딩 $p$를 선택한다고 말합니다.</p>
<p>비슷한 관례가 스트라이드에도 적용됩니다. 수평 스트라이드 $s_\textrm{h}$와 수직 스트라이드 $s_\textrm{w}$가 일치하면 단순히 스트라이드 $s$라고 합니다. 스트라이드는 출력의 해상도를 줄일 수 있습니다. 예를 들어 $n &gt; 1$인 경우 출력의 높이와 너비를 입력 높이와 너비의 $1/n$로 줄입니다. 기본적으로 패딩은 0이고 스트라이드는 1입니다.</p>
<p>지금까지 논의한 모든 패딩은 단순히 이미지를 0으로 확장했습니다. 이것은 달성하기 쉽기 때문에 상당한 계산상의 이점이 있습니다. 더욱이 연산자는 추가 메모리를 할당할 필요 없이 이 패딩을 암시적으로 활용하도록 엔지니어링될 수 있습니다. 동시에, 단순히 "공백"이 어디에 있는지 학습함으로써 CNN이 이미지 내의 암시적 위치 정보를 인코딩할 수 있게 합니다. 제로 패딩 외에도 많은 대안이 있습니다. :citet:<code>Alsallakh.Kokhlikyan.Miglani.ea.2020</code>는 이에 대한 광범위한 개요를 제공했습니다(아티팩트가 발생하지 않는 한 0이 아닌 패딩을 사용해야 하는 명확한 경우는 없지만).</p>
<h2 id="연습-문제-exercises-30"><a class="header" href="#연습-문제-exercises-30">연습 문제 (Exercises)</a></h2>
<ol>
<li>커널 크기 $(3, 5)$, 패딩 $(0, 1)$, 스트라이드 $(3, 4)$인 이 섹션의 마지막 코드 예제를 감안할 때,
출력 모양을 계산하여 실험 결과와 일치하는지 확인하십시오.</li>
<li>오디오 신호의 경우 스트라이드 2는 무엇에 해당합니까?</li>
<li>미러 패딩, 즉 경계 값을 단순히 미러링하여 텐서를 확장하는 패딩을 구현하십시오.</li>
<li>1보다 큰 스트라이드의 계산상의 이점은 무엇입니까?</li>
<li>1보다 큰 스트라이드의 통계적 이점은 무엇일까요?</li>
<li>$\frac{1}{2}$의 스트라이드를 어떻게 구현하시겠습니까? 이것은 무엇에 해당합니까? 언제 유용할까요?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/67">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/68">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/272">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17997">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="다중-입력-및-다중-출력-채널-multiple-input-and-multiple-output-channels"><a class="header" href="#다중-입력-및-다중-출력-채널-multiple-input-and-multiple-output-channels">다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></h1>
<p>:label:<code>sec_channels</code></p>
<p>우리는 각 이미지를 구성하는 다중 채널(예: 컬러 이미지는 빨강, 초록, 파랑의 양을 나타내는 표준 RGB 채널을 가짐)과 :numref:<code>subsec_why-conv-channels</code>에서 다중 채널을 위한 합성곱 레이어를 설명했지만,
지금까지 모든 수치 예제를 단일 입력 및 단일 출력 채널로 작업하여 단순화했습니다.
이를 통해 입력, 합성곱 커널, 출력을 각각 2차원 텐서로 생각할 수 있었습니다.</p>
<p>채널을 혼합에 추가하면 입력과 은닉 표현 모두 3차원 텐서가 됩니다.
예를 들어 각 RGB 입력 이미지는 $3\times h\times w$ 모양을 갖습니다.
크기가 3인 이 축을 <em>채널(channel)</em> 차원이라고 합니다. 채널의 개념은 CNN 자체만큼이나 오래되었습니다. 예를 들어 LeNet-5 :cite:<code>LeCun.Jackel.Bottou.ea.1995</code>는 채널을 사용합니다.
이 섹션에서는 다중 입력 및 다중 출력 채널이 있는 합성곱 커널을 더 깊이 살펴봅니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
import jax
from jax import numpy as jnp
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<h2 id="다중-입력-채널-multiple-input-channels"><a class="header" href="#다중-입력-채널-multiple-input-channels">다중 입력 채널 (Multiple Input Channels)</a></h2>
<p>입력 데이터에 다중 채널이 포함된 경우, 입력 데이터와 상호 상관을 수행할 수 있도록 입력 데이터와 동일한 수의 입력 채널을 가진 합성곱 커널을 구성해야 합니다.
입력 데이터의 채널 수가 $c_\textrm{i}$라고 가정하면, 합성곱 커널의 입력 채널 수도 $c_\textrm{i}$여야 합니다. 합성곱 커널의 윈도우 모양이 $k_\textrm{h}\times k_\textrm{w}$인 경우, $c_\textrm{i}=1$일 때 합성곱 커널을 $k_\textrm{h}\times k_\textrm{w}$ 모양의 2차원 텐서로 생각할 수 있습니다.</p>
<p>그러나 $c_\textrm{i}&gt;1$인 경우, <em>모든</em> 입력 채널에 대해 $k_\textrm{h}\times k_\textrm{w}$ 모양의 텐서를 포함하는 커널이 필요합니다. 이 $c_\textrm{i}$ 텐서들을 함께 연결하면 $c_\textrm{i}\times k_\textrm{h}\times k_\textrm{w}$ 모양의 합성곱 커널이 생성됩니다.
입력과 합성곱 커널 각각에 $c_\textrm{i}$ 채널이 있으므로, 각 채널에 대해 입력의 2차원 텐서와 합성곱 커널의 2차원 텐서에 대해 상호 상관 연산을 수행하고, $c_\textrm{i}$ 결과를 함께 더하여(채널에 대해 합산) 2차원 텐서를 산출할 수 있습니다.
이것이 다중 채널 입력과 다중 입력 채널 합성곱 커널 간의 2차원 상호 상관 결과입니다.</p>
<p>:numref:<code>fig_conv_multi_in</code>은 두 개의 입력 채널이 있는 2차원 상호 상관의 예를 제공합니다.
음영 처리된 부분은 첫 번째 출력 요소뿐만 아니라 출력 계산에 사용된 입력 및 커널 텐서 요소입니다:
$(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56$.</p>
<p><img src="chapter_convolutional-neural-networks/../img/conv-multi-in.svg" alt="두 개의 입력 채널을 사용한 상호 상관 계산." />
:label:<code>fig_conv_multi_in</code></p>
<p>여기서 무슨 일이 일어나고 있는지 정말로 이해했는지 확인하기 위해, 우리는 (<strong>다중 입력 채널을 사용한 상호 상관 연산을 직접 구현</strong>)할 수 있습니다.
우리가 하는 일은 채널당 상호 상관 연산을 수행한 다음 결과를 더하는 것뿐이라는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
def corr2d_multi_in(X, K):
    # K의 0번째 차원(채널)을 먼저 반복한 다음 더합니다
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def corr2d_multi_in(X, K):
    # K의 0번째 차원(채널)을 먼저 반복한 다음 더합니다
    return tf.reduce_sum([d2l.corr2d(x, k) for x, k in zip(X, K)], axis=0)
</code></pre>
<p>:numref:<code>fig_conv_multi_in</code>의 값에 해당하는 입력 텐서 <code>X</code>와 커널 텐서 <code>K</code>를 구성하여 상호 상관 연산의 (<strong>출력을 검증</strong>)할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
X = d2l.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])
K = d2l.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])

corr2d_multi_in(X, K)
</code></pre>
<h2 id="다중-출력-채널-multiple-output-channels"><a class="header" href="#다중-출력-채널-multiple-output-channels">다중 출력 채널 (Multiple Output Channels)</a></h2>
<p>:label:<code>subsec_multi-output-channels</code></p>
<p>입력 채널 수에 관계없이 지금까지 우리는 항상 하나의 출력 채널로 끝났습니다.
그러나 :numref:<code>subsec_why-conv-channels</code>에서 논의했듯이 각 레이어에 다중 채널을 갖는 것이 필수적임이 밝혀졌습니다.
가장 인기 있는 신경망 아키텍처에서는 실제로 신경망 깊이 들어갈수록 채널 차원을 늘리는데, 일반적으로 공간 해상도를 더 큰 <em>채널 깊이</em>와 교환하기 위해 다운샘플링합니다.
직관적으로 각 채널이 다른 특성 세트에 반응한다고 생각할 수 있습니다.
현실은 이것보다 조금 더 복잡합니다. 순진한 해석은 표현이 픽셀당 또는 채널당 독립적으로 학습됨을 시사할 것입니다.
대신 채널은 공동으로 유용하도록 최적화됩니다.
이는 단일 채널을 가장자리 감지기에 매핑하는 대신 채널 공간의 어떤 방향이 가장자리 감지에 해당할 수 있음을 의미합니다.</p>
<p>입력 및 출력 채널 수를 각각 $c_\textrm{i}$ 및 $c_\textrm{o}$로, 커널의 높이와 너비를 $k_\textrm{h}$ 및 $k_\textrm{w}$로 표시합니다.
다중 채널 출력을 얻기 위해, <em>모든</em> 출력 채널에 대해 $c_\textrm{i}\times k_\textrm{h}\times k_\textrm{w}$ 모양의 커널 텐서를 생성할 수 있습니다.
출력 채널 차원에서 이들을 연결하여 합성곱 커널의 모양이 $c_\textrm{o}\times c_\textrm{i}\times k_\textrm{h}\times k_\textrm{w}$가 되도록 합니다.
상호 상관 연산에서 각 출력 채널의 결과는 해당 출력 채널에 해당하는 합성곱 커널에서 계산되며 입력 텐서의 모든 채널에서 입력을 받습니다.</p>
<p>아래와 같이 [<strong>다중 채널의 출력을 계산</strong>]하는 상호 상관 함수를 구현합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def corr2d_multi_in_out(X, K):
    # K의 0번째 차원을 반복하고 매번 입력 X와 상호 상관 연산을 수행합니다.
    # 모든 결과가 함께 쌓입니다
    return d2l.stack([corr2d_multi_in(X, k) for k in K], 0)
</code></pre>
<p><code>K</code>에 대한 커널 텐서를 <code>K+1</code> 및 <code>K+2</code>와 연결하여 3개의 출력 채널이 있는 간단한 합성곱 커널을 구성합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
K = d2l.stack((K, K + 1, K + 2), 0)
K.shape
</code></pre>
<p>아래에서는 입력 텐서 <code>X</code>와 커널 텐서 <code>K</code>에 대해 상호 상관 연산을 수행합니다.
이제 출력에는 3개의 채널이 포함됩니다.
첫 번째 채널의 결과는 이전 입력 텐서 <code>X</code>와 다중 입력 채널, 단일 출력 채널 커널의 결과와 일치합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
corr2d_multi_in_out(X, K)
</code></pre>
<h2 id="1times-1-합성곱-레이어-1times-1-convolutional-layer"><a class="header" href="#1times-1-합성곱-레이어-1times-1-convolutional-layer">$1\times 1$ 합성곱 레이어 ($1\times 1$ Convolutional Layer)</a></h2>
<p>:label:<code>subsec_1x1</code></p>
<p>처음에는 [<strong>$1 \times 1$ 합성곱</strong>], 즉 $k_\textrm{h} = k_\textrm{w} = 1$이 별로 의미가 없어 보일 수 있습니다.
결국 합성곱은 인접한 픽셀을 상관시킵니다.
$1 \times 1$ 합성곱은 분명히 그렇지 않습니다.
그럼에도 불구하고 이들은 복잡한 심층 네트워크 설계에 때때로 포함되는 인기 있는 연산입니다 :cite:<code>Lin.Chen.Yan.2013,Szegedy.Ioffe.Vanhoucke.ea.2017</code>.
이것이 실제로 무엇을 하는지 자세히 살펴봅시다.</p>
<p>최소 윈도우가 사용되기 때문에 $1\times 1$ 합성곱은 높이 및 너비 차원에서 인접 요소 간의 상호 작용으로 구성된 패턴을 인식하는 더 큰 합성곱 레이어의 능력을 잃습니다.
$1\times 1$ 합성곱의 유일한 계산은 채널 차원에서 발생합니다.</p>
<p>:numref:<code>fig_conv_1x1</code>은 3개의 입력 채널과 2개의 출력 채널을 가진 $1\times 1$ 합성곱 커널을 사용한 상호 상관 계산을 보여줍니다.
입력과 출력의 높이와 너비가 동일하다는 점에 유의하십시오.
출력의 각 요소는 입력 이미지의 <em>동일한 위치</em>에 있는 요소들의 선형 결합에서 파생됩니다.
$1\times 1$ 합성곱 레이어를 모든 단일 픽셀 위치에 적용되는 완전 연결 레이어로 생각하여 $c_\textrm{i}$개의 해당 입력 값을 $c_\textrm{o}$개의 출력 값으로 변환하는 것으로 생각할 수 있습니다.
이것은 여전히 합성곱 레이어이므로 가중치는 픽셀 위치 전체에 묶여 있습니다.
따라서 $1\times 1$ 합성곱 레이어에는 $c_\textrm{o}\times c_\textrm{i}$ 가중치(더하기 편향)가 필요합니다. 또한 합성곱 레이어 뒤에는 일반적으로 비선형성이 뒤따릅니다. 이렇게 하면 $1 \times 1$ 합성곱이 단순히 다른 합성곱으로 접혀 들어가는 것을 방지할 수 있습니다.</p>
<p><img src="chapter_convolutional-neural-networks/../img/conv-1x1.svg" alt="3개의 입력 채널과 2개의 출력 채널이 있는 $1\times 1$ 합성곱 커널을 사용하는 상호 상관 계산. 입력과 출력은 높이와 너비가 같습니다." />
:label:<code>fig_conv_1x1</code></p>
<p>이것이 실제로 작동하는지 확인해 봅시다: 완전 연결 레이어를 사용하여 $1 \times 1$ 합성곱을 구현합니다.
유일한 것은 행렬 곱셈 전후에 데이터 모양을 약간 조정해야 한다는 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = d2l.reshape(X, (c_i, h * w))
    K = d2l.reshape(K, (c_o, c_i))
    # 완전 연결 레이어에서의 행렬 곱셈
    Y = d2l.matmul(K, X)
    return d2l.reshape(Y, (c_o, h, w))
</code></pre>
<p>$1\times 1$ 합성곱을 수행할 때, 위의 함수는 이전에 구현된 상호 상관 함수 <code>corr2d_multi_in_out</code>과 동일합니다.
샘플 데이터로 이것을 확인해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
X = d2l.normal(0, 1, (3, 3, 3))
K = d2l.normal(0, 1, (2, 3, 1, 1))
Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
assert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) &lt; 1e-6
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X = d2l.normal((3, 3, 3), 0, 1)
K = d2l.normal((2, 3, 1, 1), 0, 1)
Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
assert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) &lt; 1e-6
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X = jax.random.normal(jax.random.PRNGKey(d2l.get_seed()), (3, 3, 3)) + 0 * 1
K = jax.random.normal(jax.random.PRNGKey(d2l.get_seed()), (2, 3, 1, 1)) + 0 * 1
Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
assert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) &lt; 1e-6
</code></pre>
<h2 id="토론-discussion"><a class="header" href="#토론-discussion">토론 (Discussion)</a></h2>
<p>채널을 사용하면 두 가지 장점을 결합할 수 있습니다: 상당한 비선형성을 허용하는 MLP와 특성의 <em>국소적</em> 분석을 허용하는 합성곱입니다. 특히 채널을 통해 CNN은 가장자리 및 모양 감지기와 같은 여러 특성을 동시에 추론할 수 있습니다. 또한 평행 이동 불변성 및 지역성에서 발생하는 급격한 파라미터 감소와 컴퓨터 비전에서 표현력 있고 다양한 모델의 필요성 사이의 실용적인 절충안을 제공합니다.</p>
<p>하지만 이 유연성에는 대가가 따릅니다. 크기 $(h \times w)$의 이미지가 주어졌을 때, $k \times k$ 합성곱을 계산하는 비용은 $\mathcal{O}(h \cdot w \cdot k^2)$입니다. $c_\textrm{i}$ 및 $c_\textrm{o}$ 입력 및 출력 채널의 경우 이는 각각 $\mathcal{O}(h \cdot w \cdot k^2 \cdot c_\textrm{i} \cdot c_\textrm{o})$로 증가합니다. $5 \times 5$ 커널과 $128$개의 입력 및 출력 채널이 있는 $256 \times 256$ 픽셀 이미지의 경우 이는 530억 개 이상의 연산에 해당합니다(곱셈과 덧셈을 별도로 계산). 나중에 우리는 채널별 연산이 블록 대각선이어야 한다는 요구 사항을 통해 비용을 줄이는 효과적인 전략을 만날 것입니다. 이는 ResNeXt :cite:<code>Xie.Girshick.Dollar.ea.2017</code>와 같은 아키텍처로 이어집니다.</p>
<h2 id="연습-문제-exercises-31"><a class="header" href="#연습-문제-exercises-31">연습 문제 (Exercises)</a></h2>
<ol>
<li>각각 크기 $k_1$과 $k_2$인 두 개의 합성곱 커널이 있다고 가정합니다(중간에 비선형성 없음).
<ol>
<li>연산 결과가 단일 합성곱으로 표현될 수 있음을 증명하십시오.</li>
<li>동등한 단일 합성곱의 차원은 무엇입니까?</li>
<li>역이 성립합니까? 즉, 항상 합성곱을 두 개의 더 작은 합성곱으로 분해할 수 있습니까?</li>
</ol>
</li>
<li>모양 $c_\textrm{i}\times h\times w$의 입력과 모양 $c_\textrm{o}\times c_\textrm{i}\times k_\textrm{h}\times k_\textrm{w}$의 합성곱 커널, 패딩 $(p_\textrm{h}, p_\textrm{w})$, 스트라이드 $(s_\textrm{h}, s_\textrm{w})$를 가정합니다.
<ol>
<li>순전파에 대한 계산 비용(곱셈 및 덧셈)은 얼마입니까?</li>
<li>메모리 사용량은 얼마입니까?</li>
<li>역방향 계산에 대한 메모리 사용량은 얼마입니까?</li>
<li>역전파에 대한 계산 비용은 얼마입니까?</li>
</ol>
</li>
<li>입력 채널 수 $c_\textrm{i}$와 출력 채널 수 $c_\textrm{o}$를 모두 두 배로 늘리면 계산 횟수는 몇 배로 증가합니까? 패딩을 두 배로 늘리면 어떻게 됩니까?</li>
<li>이 섹션의 마지막 예제에 있는 변수 <code>Y1</code>과 <code>Y2</code>는 정확히 같습니까? 그 이유는 무엇입니까?</li>
<li>합성곱 윈도우가 $1 \times 1$이 아니더라도 합성곱을 행렬 곱셈으로 표현하십시오.</li>
<li>여러분의 임무는 $k \times k$ 커널로 빠른 합성곱을 구현하는 것입니다. 알고리즘 후보 중 하나는 소스를 수평으로 스캔하여 $k$ 너비 스트립을 읽고 한 번에 하나의 값을 $1$ 너비 출력 스트립으로 계산하는 것입니다. 대안은 $k + \Delta$ 너비 스트립을 읽고 $\Delta$ 너비 출력 스트립을 계산하는 것입니다. 후자가 더 바람직한 이유는 무엇입니까? $\Delta$를 얼마나 크게 선택해야 하는지에 대한 제한이 있습니까?</li>
<li>$c \times c$ 행렬이 있다고 가정합니다.
<ol>
<li>행렬이 $b$개의 블록으로 나뉘어 있는 경우 블록 대각 행렬과 곱하는 것이 얼마나 더 빠릅니까?</li>
<li>$b$개의 블록을 갖는 것의 단점은 무엇입니까? 어떻게 고칠 수 있습니까(적어도 부분적으로)?</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/69">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/70">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/273">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17998">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="풀링-pooling"><a class="header" href="#풀링-pooling">풀링 (Pooling)</a></h1>
<p>:label:<code>sec_pooling</code></p>
<p>많은 경우 우리의 궁극적인 작업은 이미지에 대한 전역적인 질문을 던집니다.
예를 들어 <em>고양이가 포함되어 있습니까?</em> 결과적으로 우리 최종 레이어의 유닛은 전체 입력에 민감해야 합니다.
점차적으로 정보를 집계하여 점점 더 거친 맵을 생성함으로써,
우리는 처리의 중간 레이어에서 합성곱 레이어의 모든 이점을 유지하면서 궁극적으로 전역 표현을 학습한다는 목표를 달성합니다.
네트워크에서 더 깊이 들어갈수록 각 은닉 노드가 민감한 수용 영역(입력에 비해)이 커집니다. 공간 해상도를 줄이면 합성곱 커널이 더 큰 유효 영역을 커버하기 때문에 이 프로세스가 가속화됩니다.</p>
<p>더욱이 가장자리와 같은 하위 수준 특성을 감지할 때(:numref:<code>sec_conv_layer</code>에서 논의됨),
우리는 종종 표현이 평행 이동에 어느 정도 불변하기를 원합니다.
예를 들어 흑백의 뚜렷한 경계가 있는 이미지 <code>X</code>를 가져와 전체 이미지를 오른쪽으로 한 픽셀 이동하면, 즉 <code>Z[i, j] = X[i, j + 1]</code>이면,
새 이미지 <code>Z</code>에 대한 출력은 상당히 다를 수 있습니다.
가장자리가 한 픽셀 이동했을 것입니다.
실제로 객체는 정확히 같은 위치에 거의 나타나지 않습니다.
사실 삼각대와 정지된 물체가 있어도 셔터의 움직임으로 인한 카메라의 진동으로 모든 것이 픽셀 정도 이동할 수 있습니다
(고급 카메라는 이 문제를 해결하기 위한 특수 기능을 갖추고 있습니다).</p>
<p>이 섹션에서는 합성곱 레이어의 위치에 대한 민감도를 완화하고 공간적으로 표현을 다운샘플링하는 두 가지 목적을 수행하는 *풀링 레이어(pooling layers)*를 소개합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="최대-풀링과-평균-풀링-maximum-pooling-and-average-pooling"><a class="header" href="#최대-풀링과-평균-풀링-maximum-pooling-and-average-pooling">최대 풀링과 평균 풀링 (Maximum Pooling and Average Pooling)</a></h2>
<p>합성곱 레이어와 마찬가지로 <em>풀링</em> 연산자는 고정된 모양의 윈도우로 구성되며, 스트라이드에 따라 입력의 모든 영역 위로 미끄러지며 고정 모양 윈도우(때로는 <em>풀링 윈도우</em>라고도 함)가 지나가는 각 위치에 대해 단일 출력을 계산합니다.
그러나 합성곱 레이어의 입력 및 커널의 상호 상관 계산과 달리, 풀링 레이어에는 파라미터가 없습니다(<em>커널</em>이 없음).
대신 풀링 연산자는 결정론적이며, 일반적으로 풀링 윈도우에 있는 요소의 최댓값 또는 평균값을 계산합니다.
이러한 연산을 각각 <em>최대 풀링(maximum pooling)</em> (줄여서 <em>max-pooling</em>) 및 *평균 풀링(average pooling)*이라고 합니다.</p>
<p><em>평균 풀링</em>은 본질적으로 CNN만큼이나 오래되었습니다. 아이디어는 이미지를 다운샘플링하는 것과 유사합니다. 저해상도 이미지를 위해 두 번째(또는 세 번째) 픽셀마다 값을 취하는 대신, 인접한 픽셀에 대해 평균을 내어 더 나은 신호 대 잡음비를 가진 이미지를 얻을 수 있습니다. 우리는 여러 인접 픽셀의 정보를 결합하고 있기 때문입니다. <em>최대 풀링</em>은 :citet:<code>Riesenhuber.Poggio.1999</code>에서 인지 신경과학의 맥락에서 객체 인식을 위해 정보 집계가 계층적으로 집계될 수 있는 방법을 설명하기 위해 도입되었습니다. 음성 인식에는 이미 이전 버전이 있었습니다 :cite:<code>Yamaguchi.Sakamoto.Akabane.ea.1990</code>. 거의 모든 경우에 최대 풀링이 평균 풀링보다 선호됩니다.</p>
<p>두 경우 모두 상호 상관 연산자와 마찬가지로 풀링 윈도우가 입력 텐서의 왼쪽 상단에서 시작하여 왼쪽에서 오른쪽으로, 위에서 아래로 미끄러지는 것으로 생각할 수 있습니다.
풀링 윈도우가 닿는 각 위치에서 최대 풀링이 사용되는지 평균 풀링이 사용되는지에 따라 윈도우에 있는 입력 하위 텐서의 최댓값 또는 평균값을 계산합니다.</p>
<p><img src="chapter_convolutional-neural-networks/../img/pooling.svg" alt="풀링 윈도우 모양이 $2\times 2$인 최대 풀링. 음영 처리된 부분은 첫 번째 출력 요소와 출력 계산에 사용된 입력 텐서 요소입니다: $\max(0, 1, 3, 4)=4$." />
:label:<code>fig_pooling</code></p>
<p>:numref:<code>fig_pooling</code>의 출력 텐서는 높이 2, 너비 2를 갖습니다.
4개의 요소는 각 풀링 윈도우의 최댓값에서 파생됩니다:</p>
<p>$$
\max(0, 1, 3, 4)=4,<br />
\max(1, 2, 4, 5)=5,<br />
\max(3, 4, 6, 7)=7,<br />
\max(4, 5, 7, 8)=8.<br />
$$</p>
<p>더 일반적으로 해당 크기의 영역에 대해 집계하여 $p \times q$ 풀링 레이어를 정의할 수 있습니다. 가장자리 감지 문제로 돌아가서, 합성곱 레이어의 출력을 $2\times 2$ 최대 풀링의 입력으로 사용합니다.
<code>X</code>를 합성곱 레이어 입력의 입력으로, <code>Y</code>를 풀링 레이어 출력으로 표시합니다.
<code>X[i, j]</code>, <code>X[i, j + 1]</code>, <code>X[i+1, j]</code>, <code>X[i+1, j + 1]</code>의 값이 다른지 여부에 관계없이 풀링 레이어는 항상 <code>Y[i, j] = 1</code>을 출력합니다.
즉, $2\times 2$ 최대 풀링 레이어를 사용하면 합성곱 레이어가 인식한 패턴이 높이 또는 너비에서 한 요소 이상 움직이지 않는 경우 여전히 감지할 수 있습니다.</p>
<p>아래 코드에서는 <code>pool2d</code> 함수에서 (<strong>풀링 레이어의 순전파를 구현</strong>)합니다.
이 함수는 :numref:<code>sec_conv_layer</code>의 <code>corr2d</code> 함수와 유사합니다.
그러나 커널이 필요하지 않으며 입력의 각 영역에 대한 최댓값 또는 평균으로 출력을 계산합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = d2l.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = jnp.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y = Y.at[i, j].set(X[i: i + p_h, j: j + p_w].max())
            elif mode == 'avg':
                Y = Y.at[i, j].set(X[i: i + p_h, j: j + p_w].mean())
    return Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf

def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = tf.Variable(tf.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w +1)))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j].assign(tf.reduce_max(X[i: i + p_h, j: j + p_w]))
            elif mode =='avg':
                Y[i, j].assign(tf.reduce_mean(X[i: i + p_h, j: j + p_w]))
    return Y
</code></pre>
<p>:numref:<code>fig_pooling</code>의 입력 텐서 <code>X</code>를 구성하여 [<strong>2차원 최대 풀링 레이어의 출력을 검증</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
pool2d(X, (2, 2))
</code></pre>
<p>또한 (<strong>평균 풀링 레이어</strong>)를 실험해 볼 수도 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
pool2d(X, (2, 2), 'avg')
</code></pre>
<h2 id="패딩과-스트라이드-padding-and-stride-1"><a class="header" href="#패딩과-스트라이드-padding-and-stride-1">[<strong>패딩과 스트라이드 (Padding and Stride)</strong>]</a></h2>
<p>합성곱 레이어와 마찬가지로 풀링 레이어는 출력 모양을 변경합니다.
그리고 이전과 마찬가지로 입력을 패딩하고 스트라이드를 조정하여 원하는 출력 모양을 얻도록 작업을 조정할 수 있습니다.
딥러닝 프레임워크의 내장 2차원 최대 풀링 레이어를 통해 풀링 레이어에서 패딩과 스트라이드의 사용을 시연할 수 있습니다.
먼저 모양이 4차원인 입력 텐서 <code>X</code>를 구성합니다. 여기서 예제 수(배치 크기)와 채널 수는 모두 1입니다.</p>
<p>:begin_tab:<code>tensorflow</code>
다른 프레임워크와 달리 TensorFlow는 <em>채널-마지막(channels-last)</em> 입력을 선호하고 이에 최적화되어 있음에 유의하십시오.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 1, 4, 4))
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow, jax
X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 4, 4, 1))
X
</code></pre>
<p>풀링은 영역의 정보를 집계하므로 (<strong>딥러닝 프레임워크는 기본적으로 풀링 윈도우 크기와 스트라이드를 일치시킵니다.</strong>) 예를 들어 <code>(3, 3)</code> 모양의 풀링 윈도우를 사용하면 기본적으로 <code>(3, 3)</code>의 스트라이드 모양을 얻습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
pool2d = nn.MaxPool2D(3)
# 풀링에는 모델 파라미터가 없으므로 초기화가 필요하지 않습니다
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
pool2d = nn.MaxPool2d(3)
# 풀링에는 모델 파라미터가 없으므로 초기화가 필요하지 않습니다
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
pool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3])
# 풀링에는 모델 파라미터가 없으므로 초기화가 필요하지 않습니다
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# 풀링에는 모델 파라미터가 없으므로 초기화가 필요하지 않습니다
nn.max_pool(X, window_shape=(3, 3), strides=(3, 3))
</code></pre>
<p>말할 필요도 없이, 필요한 경우 프레임워크 기본값을 재정의하기 위해 [<strong>스트라이드와 패딩을 수동으로 지정할 수 있습니다</strong>].</p>
<pre><code class="language-{.python .input}">%%tab mxnet
pool2d = nn.MaxPool2D(3, padding=1, strides=2)
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
paddings = tf.constant([[0, 0], [1,0], [1,0], [0,0]])
X_padded = tf.pad(X, paddings, "CONSTANT")
pool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3], padding='valid',
                                   strides=2)
pool2d(X_padded)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X_padded = jnp.pad(X, ((0, 0), (1, 0), (1, 0), (0, 0)), mode='constant')
nn.max_pool(X_padded, window_shape=(3, 3), padding='VALID', strides=(2, 2))
</code></pre>
<p>물론 아래 예제와 같이 임의의 높이와 너비를 가진 임의의 직사각형 풀링 윈도우를 지정할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
pool2d = nn.MaxPool2D((2, 3), padding=(0, 1), strides=(2, 3))
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
paddings = tf.constant([[0, 0], [0, 0], [1, 1], [0, 0]])
X_padded = tf.pad(X, paddings, "CONSTANT")

pool2d = tf.keras.layers.MaxPool2D(pool_size=[2, 3], padding='valid',
                                   strides=(2, 3))
pool2d(X_padded)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax

X_padded = jnp.pad(X, ((0, 0), (0, 0), (1, 1), (0, 0)), mode='constant')
nn.max_pool(X_padded, window_shape=(2, 3), strides=(2, 3), padding='VALID')
</code></pre>
<h2 id="다중-채널-multiple-channels"><a class="header" href="#다중-채널-multiple-channels">다중 채널 (Multiple Channels)</a></h2>
<p>다중 채널 입력 데이터를 처리할 때,
[<strong>풀링 레이어는</strong>] 합성곱 레이어처럼 채널에 대해 입력을 합산하는 대신 [<strong>각 입력 채널을 개별적으로 풀링합니다</strong>].
이는 풀링 레이어의 출력 채널 수가 입력 채널 수와 같다는 것을 의미합니다.
아래에서 채널 차원에서 텐서 <code>X</code>와 <code>X + 1</code>을 연결하여 두 개의 채널이 있는 입력을 구성합니다.</p>
<p>:begin_tab:<code>tensorflow</code>
TensorFlow는 채널-마지막 구문으로 인해 마지막 차원을 따라 연결해야 한다는 점에 유의하십시오.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
X = d2l.concat((X, X + 1), 1)
X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow, jax
# 채널-마지막 구문으로 인해 `dim=3`을 따라 연결
X = d2l.concat([X, X + 1], 3)
X
</code></pre>
<p>보시다시피 풀링 후에도 출력 채널 수는 여전히 2개입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
pool2d = nn.MaxPool2D(3, padding=1, strides=2)
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
paddings = tf.constant([[0, 0], [1,0], [1,0], [0,0]])
X_padded = tf.pad(X, paddings, "CONSTANT")
pool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3], padding='valid',
                                   strides=2)
pool2d(X_padded)

</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X_padded = jnp.pad(X, ((0, 0), (1, 0), (1, 0), (0, 0)), mode='constant')
nn.max_pool(X_padded, window_shape=(3, 3), padding='VALID', strides=(2, 2))
</code></pre>
<p>:begin_tab:<code>tensorflow</code>
TensorFlow 풀링의 출력이 언뜻 보기에 다르게 보일 수 있지만, 수치적으로는 MXNet 및 PyTorch와 동일한 결과가 제시된다는 점에 유의하십시오.
차이점은 차원성에 있으며, 출력을 수직으로 읽으면 다른 구현과 동일한 출력을 산출합니다.
:end_tab:</p>
<h2 id="요약-summary-26"><a class="header" href="#요약-summary-26">요약 (Summary)</a></h2>
<p>풀링은 매우 간단한 연산입니다. 이름이 나타내는 대로 정확히 값의 윈도우에 대한 결과를 집계합니다. 스트라이드 및 패딩과 같은 모든 합성곱 의미론은 이전과 동일한 방식으로 적용됩니다. 풀링은 채널에 무관심합니다. 즉, 채널 수를 변경하지 않고 각 채널에 개별적으로 적용됩니다. 마지막으로, 두 가지 인기 있는 풀링 선택 중 최대 풀링이 평균 풀링보다 선호되는데, 이는 출력에 어느 정도의 불변성을 부여하기 때문입니다. 인기 있는 선택은 공간 해상도를 1/4로 줄이기 위해 $2 \times 2$의 풀링 윈도우 크기를 선택하는 것입니다.</p>
<p>풀링 외에도 해상도를 줄이는 방법은 더 많이 있습니다. 예를 들어 확률적 풀링 :cite:<code>Zeiler.Fergus.2013</code> 및 부분 최대 풀링 :cite:<code>Graham.2014</code>에서 집계는 무작위화와 결합됩니다. 이는 일부 경우에 정확도를 약간 향상시킬 수 있습니다. 마지막으로, 나중에 주의 메커니즘에서 볼 수 있듯이, 쿼리와 표현 벡터 간의 정렬을 사용하는 등 출력에 대해 집계하는 더 세련된 방법이 있습니다.</p>
<h2 id="연습-문제-exercises-32"><a class="header" href="#연습-문제-exercises-32">연습 문제 (Exercises)</a></h2>
<ol>
<li>합성곱을 통해 평균 풀링을 구현하십시오.</li>
<li>최대 풀링은 합성곱만으로 구현될 수 없음을 증명하십시오.</li>
<li>최대 풀링은 ReLU 연산, 즉 $\textrm{ReLU}(x) = \max(0, x)$을 사용하여 달성할 수 있습니다.
<ol>
<li>ReLU 연산만 사용하여 $\max (a, b)$를 표현하십시오.</li>
<li>이를 사용하여 합성곱 및 ReLU 레이어를 통해 최대 풀링을 구현하십시오.</li>
<li>$2 \times 2$ 합성곱에는 몇 개의 채널과 레이어가 필요합니까? $3 \times 3$ 합성곱에는 몇 개가 필요합니까?</li>
</ol>
</li>
<li>풀링 레이어의 계산 비용은 얼마입니까? 풀링 레이어에 대한 입력 크기가 $c\times h\times w$이고, 풀링 윈도우 모양이 $p_\textrm{h}\times p_\textrm{w}$이며 패딩 $(p_\textrm{h}, p_\textrm{w})$와 스트라이드 $(s_\textrm{h}, s_\textrm{w})$를 갖는다고 가정합니다.</li>
<li>최대 풀링과 평균 풀링이 다르게 작동할 것으로 예상하는 이유는 무엇입니까?</li>
<li>별도의 최소 풀링 레이어가 필요합니까? 다른 연산으로 대체할 수 있습니까?</li>
<li>풀링에 소프트맥스 연산을 사용할 수 있습니다. 왜 인기가 없을까요?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/71">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/72">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/274">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17999">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="합성곱-신경망-lenet-convolutional-neural-networks-lenet"><a class="header" href="#합성곱-신경망-lenet-convolutional-neural-networks-lenet">합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></h1>
<p>:label:<code>sec_lenet</code></p>
<p>우리는 이제 완전히 기능하는 CNN을 조립하는 데 필요한 모든 재료를 갖추었습니다.
이전의 이미지 데이터와의 만남에서, 우리는 Fashion-MNIST 데이터셋의 의류 사진에 선형 모델과 소프트맥스 회귀(:numref:<code>sec_softmax_scratch</code>) 및 MLP(:numref:<code>sec_mlp-implementation</code>)를 적용했습니다.
이러한 데이터를 처리하기 위해 우리는 먼저 각 이미지를 $28\times28$ 행렬에서 고정 길이 $784$차원 벡터로 평탄화한 다음, 완전 연결 레이어에서 처리했습니다.
이제 합성곱 레이어를 다룰 수 있게 되었으므로, 이미지의 공간 구조를 유지할 수 있습니다.
완전 연결 레이어를 합성곱 레이어로 대체하는 추가적인 이점으로, 훨씬 적은 파라미터를 필요로 하는 더 간결한 모델을 누릴 수 있습니다.</p>
<p>이 섹션에서는 컴퓨터 비전 작업에서의 성능으로 폭넓은 주목을 받은 최초의 발표된 CNN 중 하나인 <em>LeNet</em>을 소개합니다.
이 모델은 당시 AT&amp;T 벨 연구소의 연구원이었던 얀 르쿤(Yann LeCun)이 이미지 내 손글씨 숫자를 인식하기 위해 소개했습니다(그리고 그의 이름을 따서 명명되었습니다) :cite:<code>LeCun.Bottou.Bengio.ea.1998</code>.
이 작업은 기술을 개발해 온 10년 연구의 정점을 나타냈습니다.
르쿤의 팀은 역전파를 통해 CNN을 성공적으로 훈련한 최초의 연구를 발표했습니다 :cite:<code>LeCun.Boser.Denker.ea.1989</code>.</p>
<p>당시 LeNet은 지도 학습의 지배적인 접근 방식이었던 서포트 벡터 머신의 성능과 일치하는 뛰어난 결과를 달성했으며, 숫자당 1% 미만의 오류율을 달성했습니다.
LeNet은 결국 ATM 기계에서 예금을 처리하기 위해 숫자를 인식하도록 조정되었습니다.
오늘날까지도 일부 ATM은 1990년대에 얀 르쿤과 그의 동료 레옹 보투(Leon Bottou)가 작성한 코드를 실행하고 있습니다!</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
from d2l import tensorflow as d2l
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
from types import FunctionType
</code></pre>
<h2 id="lenet"><a class="header" href="#lenet">LeNet</a></h2>
<p>높은 수준에서 볼 때, (<strong>LeNet (LeNet-5)은 두 부분으로 구성됩니다:
(i) 두 개의 합성곱 레이어로 구성된 합성곱 인코더; 그리고
(ii) 세 개의 완전 연결 레이어로 구성된 밀집 블록(dense block)</strong>).
아키텍처는 :numref:<code>img_lenet</code>에 요약되어 있습니다.</p>
<p><img src="chapter_convolutional-neural-networks/../img/lenet.svg" alt="LeNet의 데이터 흐름. 입력은 손글씨 숫자이고 출력은 10가지 가능한 결과에 대한 확률입니다." />
:label:<code>img_lenet</code></p>
<p>각 합성곱 블록의 기본 단위는 합성곱 레이어, 시그모이드 활성화 함수, 그리고 후속 평균 풀링 연산입니다.
ReLU와 최대 풀링이 더 잘 작동하지만, 당시에는 아직 발견되지 않았다는 점에 유의하십시오.
각 합성곱 레이어는 $5\times 5$ 커널과 시그모이드 활성화 함수를 사용합니다.
이 레이어들은 공간적으로 배열된 입력을 다수의 2차원 특성 맵으로 매핑하며, 일반적으로 채널 수를 늘립니다.
첫 번째 합성곱 레이어는 6개의 출력 채널을 갖고, 두 번째는 16개를 갖습니다.
각 $2\times2$ 풀링 연산(스트라이드 2)은 공간 다운샘플링을 통해 차원을 $4$배로 줄입니다.
합성곱 블록은 (배치 크기, 채널 수, 높이, 너비)로 주어진 모양의 출력을 방출합니다.</p>
<p>합성곱 블록의 출력을 밀집 블록으로 전달하려면, 미니배치의 각 예제를 평탄화해야 합니다.
즉, 이 4차원 입력을 완전 연결 레이어가 예상하는 2차원 입력으로 변환합니다:
상기시키자면, 우리가 원하는 2차원 표현은 첫 번째 차원을 사용하여 미니배치의 예제를 인덱싱하고 두 번째 차원을 사용하여 각 예제의 평면 벡터 표현을 제공합니다.
LeNet의 밀집 블록에는 각각 120, 84, 10개의 출력을 가진 세 개의 완전 연결 레이어가 있습니다.
여전히 분류를 수행하고 있으므로 10차원 출력 레이어는 가능한 출력 클래스 수에 해당합니다.</p>
<p>LeNet 내부에서 무슨 일이 일어나고 있는지 진정으로 이해하는 데까지는 약간의 노력이 필요했을 수 있지만, 다음 코드 스니펫이 현대 딥러닝 프레임워크로 이러한 모델을 구현하는 것이 놀랍도록 간단하다는 것을 확신시켜 주기를 바랍니다.
우리는 <code>Sequential</code> 블록을 인스턴스화하고 적절한 레이어를 함께 연결하기만 하면 됩니다.
:numref:<code>subsec_xavier</code>에서 소개된 Xavier 초기화를 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
def init_cnn(module):  #@save
    """CNN 가중치 초기화."""
    if type(module) == nn.Linear or type(module) == nn.Conv2d:
        nn.init.xavier_uniform_(module.weight)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class LeNet(d2l.Classifier):  #@save
    """The LeNet-5 모델."""
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.net = nn.Sequential()
            self.net.add(
                nn.Conv2D(channels=6, kernel_size=5, padding=2,
                          activation='sigmoid'),
                nn.AvgPool2D(pool_size=2, strides=2),
                nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),
                nn.AvgPool2D(pool_size=2, strides=2),
                nn.Dense(120, activation='sigmoid'),
                nn.Dense(84, activation='sigmoid'),
                nn.Dense(num_classes))
            self.net.initialize(init.Xavier())
        if tab.selected('pytorch'):
            self.net = nn.Sequential(
                nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),
                nn.AvgPool2d(kernel_size=2, stride=2),
                nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),
                nn.AvgPool2d(kernel_size=2, stride=2),
                nn.Flatten(),
                nn.LazyLinear(120), nn.Sigmoid(),
                nn.LazyLinear(84), nn.Sigmoid(),
                nn.LazyLinear(num_classes))
        if tab.selected('tensorflow'):
            self.net = tf.keras.models.Sequential([
                tf.keras.layers.Conv2D(filters=6, kernel_size=5,
                                       activation='sigmoid', padding='same'),
                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),
                tf.keras.layers.Conv2D(filters=16, kernel_size=5,
                                       activation='sigmoid'),
                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(120, activation='sigmoid'),
                tf.keras.layers.Dense(84, activation='sigmoid'),
                tf.keras.layers.Dense(num_classes)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class LeNet(d2l.Classifier):  #@save
    """The LeNet-5 모델."""
    lr: float = 0.1
    num_classes: int = 10
    kernel_init: FunctionType = nn.initializers.xavier_uniform

    def setup(self):
        self.net = nn.Sequential([
            nn.Conv(features=6, kernel_size=(5, 5), padding='SAME',
                    kernel_init=self.kernel_init()),
            nn.sigmoid,
            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),
            nn.Conv(features=16, kernel_size=(5, 5), padding='VALID',
                    kernel_init=self.kernel_init()),
            nn.sigmoid,
            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),
            lambda x: x.reshape((x.shape[0], -1)),  # 평탄화
            nn.Dense(features=120, kernel_init=self.kernel_init()),
            nn.sigmoid,
            nn.Dense(features=84, kernel_init=self.kernel_init()),
            nn.sigmoid,
            nn.Dense(features=self.num_classes, kernel_init=self.kernel_init())
        ])
</code></pre>
<p>우리는 가우스 활성화 레이어를 소프트맥스 레이어로 대체한다는 점에서 LeNet 재현에 약간의 자유를 취했습니다.
이는 무엇보다도 가우스 디코더가 오늘날 거의 사용되지 않는다는 사실 때문에 구현을 크게 단순화합니다.
그 외에는 이 네트워크가 원래 LeNet-5 아키텍처와 일치합니다.</p>
<p>:begin_tab:<code>pytorch, mxnet, tensorflow</code>
네트워크 내부에서 어떤 일이 일어나는지 봅시다.
단일 채널(흑백) $28 \times 28$ 이미지를 네트워크에 통과시키고 각 레이어에서 출력 모양을 인쇄함으로써,
[<strong>모델을 검사</strong>]하여 그 연산이 :numref:<code>img_lenet_vert</code>에서 기대하는 것과 일치하는지 확인할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
네트워크 내부에서 어떤 일이 일어나는지 봅시다.
단일 채널(흑백) $28 \times 28$ 이미지를 네트워크에 통과시키고 각 레이어에서 출력 모양을 인쇄함으로써,
[<strong>모델을 검사</strong>]하여 그 연산이 :numref:<code>img_lenet_vert</code>에서 기대하는 것과 일치하는지 확인할 수 있습니다.
Flax는 네트워크의 레이어와 파라미터를 요약하는 멋진 메서드인 <code>nn.tabulate</code>를 제공합니다.
여기서는 <code>bind</code> 메서드를 사용하여 바운드 모델을 생성합니다.
변수들은 이제 <code>d2l.Module</code> 클래스에 바인딩됩니다. 즉, 이 바운드 모델은 상태 저장(stateful) 객체가 되어 <code>Sequential</code> 객체 속성 <code>net</code>과 그 안의 <code>layers</code>에 액세스하는 데 사용할 수 있습니다.
<code>bind</code> 메서드는 대화형 실험에만 사용해야 하며 <code>apply</code> 메서드를 직접 대체하는 것은 아님에 유의하십시오.
:end_tab:</p>
<p><img src="chapter_convolutional-neural-networks/../img/lenet-vert.svg" alt="LeNet-5를 위한 압축 표기법." />
:label:<code>img_lenet_vert</code></p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
@d2l.add_to_class(d2l.Classifier)  #@save
def layer_summary(self, X_shape):
    X = d2l.randn(*X_shape)
    for layer in self.net:
        X = layer(X)
        print(layer.__class__.__name__, 'output shape:\t', X.shape)
        
model = LeNet()
model.layer_summary((1, 1, 28, 28))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
@d2l.add_to_class(d2l.Classifier)  #@save
def layer_summary(self, X_shape):
    X = d2l.normal(X_shape)
    for layer in self.net.layers:
        X = layer(X)
        print(layer.__class__.__name__, 'output shape:\t', X.shape)

model = LeNet()
model.layer_summary((1, 28, 28, 1))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(d2l.Classifier)  #@save
def layer_summary(self, X_shape, key=d2l.get_key()):
    X = jnp.zeros(X_shape)
    params = self.init(key, X)
    bound_model = self.clone().bind(params, mutable=['batch_stats'])
    _ = bound_model(X)
    for layer in bound_model.net.layers:
        X = layer(X)
        print(layer.__class__.__name__, 'output shape:\t', X.shape)

model = LeNet()
model.layer_summary((1, 28, 28, 1))
</code></pre>
<p>합성곱 블록 전체의 각 레이어에서 표현의 높이와 너비가 (이전 레이어에 비해) 줄어든다는 점에 유의하십시오.
첫 번째 합성곱 레이어는 $5 \times 5$ 커널을 사용하여 발생할 수 있는 높이와 너비의 감소를 보상하기 위해 2픽셀의 패딩을 사용합니다.
참고로 원래 MNIST OCR 데이터셋의 $28 \times 28$ 픽셀 이미지 크기는 $32 \times 32$ 픽셀 크기의 원본 스캔에서 2픽셀 행(및 열)을 <em>다듬은(trimming)</em> 결과입니다.
이는 주로 메가바이트가 중요했던 시절에 공간을 절약(30% 감소)하기 위해 수행되었습니다.</p>
<p>대조적으로, 두 번째 합성곱 레이어는 패딩을 생략하므로 높이와 너비가 모두 4픽셀씩 줄어듭니다.
레이어 스택을 올라가면서 채널 수는 입력의 1개에서 첫 번째 합성곱 레이어 후 6개, 두 번째 합성곱 레이어 후 16개로 레이어마다 증가합니다.
그러나 각 풀링 레이어는 높이와 너비를 반으로 줄입니다.
마지막으로 각 완전 연결 레이어는 차원 수를 줄여 최종적으로 클래스 수와 일치하는 차원의 출력을 방출합니다.</p>
<h2 id="훈련-training-8"><a class="header" href="#훈련-training-8">훈련 (Training)</a></h2>
<p>이제 모델을 구현했으므로, [<strong>LeNet-5 모델이 Fashion-MNIST에서 어떻게 수행되는지 확인하기 위해 실험을 실행</strong>]해 보겠습니다.</p>
<p>CNN은 파라미터가 더 적지만, 각 파라미터가 훨씬 더 많은 곱셈에 참여하기 때문에 비슷하게 깊은 MLP보다 계산 비용이 더 많이 들 수 있습니다.
GPU에 액세스할 수 있다면 지금이 훈련 속도를 높이기 위해 실행에 옮길 좋은 때입니다.
<code>d2l.Trainer</code> 클래스가 모든 세부 사항을 처리한다는 점에 유의하십시오.
기본적으로 사용 가능한 장치에서 모델 파라미터를 초기화합니다.
MLP와 마찬가지로 손실 함수는 교차 엔트로피이며 미니배치 확률적 경사 하강법을 통해 최소화합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, jax
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128)
model = LeNet(lr=0.1)
if tab.selected('pytorch'):
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128)
with d2l.try_gpu():
    model = LeNet(lr=0.1)
    trainer.fit(model, data)
</code></pre>
<h2 id="요약-summary-27"><a class="header" href="#요약-summary-27">요약 (Summary)</a></h2>
<p>우리는 이 장에서 상당한 진전을 이루었습니다. 1980년대의 MLP에서 1990년대와 2000년대 초반의 CNN으로 이동했습니다. 제안된 아키텍처, 예를 들어 LeNet-5 형태는 오늘날까지도 의미가 있습니다. Fashion-MNIST에서 LeNet-5로 달성할 수 있는 오류율을 MLP로 가능한 최고의 오류율(:numref:<code>sec_mlp-implementation</code>) 및 ResNet(:numref:<code>sec_resnet</code>)과 같은 훨씬 더 진보된 아키텍처의 오류율과 비교해 볼 가치가 있습니다. LeNet은 전자보다는 후자와 훨씬 더 유사합니다. 우리가 보게 될 주요 차이점 중 하나는 더 많은 양의 계산이 훨씬 더 복잡한 아키텍처를 가능하게 했다는 것입니다.</p>
<p>두 번째 차이점은 우리가 LeNet을 구현할 수 있었던 상대적인 용이성입니다. 예전에는 몇 달 간의 C++ 및 어셈블리 코드 가치가 있는 엔지니어링 과제였고, 초기 Lisp 기반 딥러닝 도구인 SN :cite:<code>Bottou.Le-Cun.1988</code>을 개선하기 위한 엔지니어링, 그리고 마지막으로 모델 실험이 이제는 몇 분 만에 달성될 수 있습니다. 딥러닝 모델 개발을 엄청나게 민주화한 것은 바로 이러한 놀라운 생산성 향상입니다. 다음 장에서는 이 토끼굴을 따라 내려가 어디로 가는지 볼 것입니다.</p>
<h2 id="연습-문제-exercises-33"><a class="header" href="#연습-문제-exercises-33">연습 문제 (Exercises)</a></h2>
<ol>
<li>LeNet을 현대화해 봅시다. 다음 변경 사항을 구현하고 테스트하십시오:
<ol>
<li>평균 풀링을 최대 풀링으로 대체하십시오.</li>
<li>소프트맥스 레이어를 ReLU로 대체하십시오.</li>
</ol>
</li>
<li>최대 풀링 및 ReLU 외에도 정확도를 향상시키기 위해 LeNet 스타일 네트워크의 크기를 변경해 보십시오.
<ol>
<li>합성곱 윈도우 크기를 조정하십시오.</li>
<li>출력 채널 수를 조정하십시오.</li>
<li>합성곱 레이어 수를 조정하십시오.</li>
<li>완전 연결 레이어 수를 조정하십시오.</li>
<li>학습률 및 기타 훈련 세부 사항(예: 초기화 및 에폭 수)을 조정하십시오.</li>
</ol>
</li>
<li>개선된 네트워크를 원본 MNIST 데이터셋에서 사용해 보십시오.</li>
<li>다른 입력(예: 스웨터 및 코트)에 대한 LeNet의 첫 번째 및 두 번째 레이어의 활성화를 표시하십시오.</li>
<li>네트워크에 상당히 다른 이미지(예: 고양이, 자동차, 심지어 무작위 노이즈)를 공급하면 활성화에 어떤 일이 발생합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/73">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/74">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/275">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18000">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="현대-합성곱-신경망-modern-convolutional-neural-networks"><a class="header" href="#현대-합성곱-신경망-modern-convolutional-neural-networks">현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a></h1>
<p>:label:<code>chap_modern_cnn</code></p>
<p>이제 CNN을 함께 연결하는 기본 사항을 이해했으므로 현대 CNN 아키텍처를 둘러보겠습니다.
흥미진진한 새로운 디자인이 엄청나게 추가되고 있기 때문에 이 둘러보기는 필연적으로 불완전합니다.
이들의 중요성은 비전 작업에 직접 사용될 수 있을 뿐만 아니라 추적 :cite:<code>Zhang.Sun.Jiang.ea.2021</code>, 분할 :cite:<code>Long.Shelhamer.Darrell.2015</code>, 객체 감지 :cite:<code>Redmon.Farhadi.2018</code>, 스타일 변환 :cite:<code>Gatys.Ecker.Bethge.2016</code>과 같은 고급 작업을 위한 기본 특성 생성기 역할을 한다는 사실에서 비롯됩니다.
이 장의 대부분 섹션은 한때(또는 현재) 많은 연구 프로젝트와 배포된 시스템이 구축된 기본 모델이었던 중요한 CNN 아키텍처에 해당합니다.
이러한 네트워크 각각은 잠시 동안 지배적인 아키텍처였으며 많은 네트워크가 2010년 이후 컴퓨터 비전의 지도 학습 진행 상황을 가늠하는 척도 역할을 해온 <a href="https://www.image-net.org/challenges/LSVRC/">ImageNet 대회</a>의 우승자 또는 준우승자였습니다.
최근에야 :citet:<code>Dosovitskiy.Beyer.Kolesnikov.ea.2021</code>를 시작으로 Swin Transformer :cite:<code>liu2021swin</code>가 그 뒤를 이으면서 Transformer가 CNN을 대체하기 시작했습니다.
이 개발 내용은 나중에 :numref:<code>chap_attention-and-transformers</code>에서 다룰 것입니다.</p>
<p><em>심층(deep)</em> 신경망의 아이디어는 매우 간단하지만(여러 레이어를 함께 쌓음), 성능은 아키텍처와 하이퍼파라미터 선택에 따라 크게 달라질 수 있습니다.
이 장에서 설명하는 신경망은 직관, 몇 가지 수학적 통찰력, 그리고 많은 시행착오의 산물입니다.
우리는 이 모델들을 시간 순서대로 제시하는데, 부분적으로는 역사의 흐름을 전달하여 여러분이 이 분야가 어디로 향하고 있는지에 대한 직관을 형성하고 아마도 여러분만의 아키텍처를 개발할 수 있도록 하기 위함입니다.
예를 들어, 이 장에서 설명하는 배치 정규화와 잔차 연결은 심층 모델을 훈련하고 설계하기 위한 두 가지 인기 있는 아이디어를 제공했으며, 두 가지 모두 이후 컴퓨터 비전을 넘어선 아키텍처에도 적용되었습니다.</p>
<p>대규모 비전 챌린지에서 기존 컴퓨터 비전 방법을 이기기 위해 배포된 최초의 대규모 네트워크인 AlexNet :cite:<code>Krizhevsky.Sutskever.Hinton.2012</code>으로 현대 CNN 둘러보기를 시작합니다;
반복되는 요소 블록을 사용하는 VGG 네트워크 :cite:<code>Simonyan.Zisserman.2014</code>;
입력에 대해 전체 신경망을 패치 단위로 합성곱하는 네트워크 인 네트워크(NiN) :cite:<code>Lin.Chen.Yan.2013</code>;
다중 분기 합성곱을 사용하는 네트워크를 사용하는 GoogLeNet :cite:<code>Szegedy.Liu.Jia.ea.2015</code>;
컴퓨터 비전에서 가장 인기 있는 기성 아키텍처 중 하나로 남아 있는 잔차 네트워크(ResNet) :cite:<code>He.Zhang.Ren.ea.2016</code>;
더 희소한 연결을 위한 ResNeXt 블록 :cite:<code>Xie.Girshick.Dollar.ea.2017</code>;
그리고 잔차 아키텍처의 일반화를 위한 DenseNet :cite:<code>Huang.Liu.Van-Der-Maaten.ea.2017</code>이 있습니다.
시간이 지남에 따라 좌표 이동(ShiftNet) :cite:<code>wu2018shift</code>과 같은 효율적인 네트워크를 위한 많은 특수 최적화가 개발되었습니다.
이는 MobileNet v3 :cite:<code>Howard.Sandler.Chu.ea.2019</code>와 같은 효율적인 아키텍처에 대한 자동 검색으로 정점을 찍었습니다.
또한 이 장의 뒷부분에서 논의할 RegNetX/Y로 이어진 :citet:<code>Radosavovic.Kosaraju.Girshick.ea.2020</code>의 반자동 설계 탐색도 포함됩니다.
이 작업은 효율적인 설계 공간을 탐색하는 데 있어 무차별 대입 계산과 실험자의 독창성을 결합하는 경로를 제공한다는 점에서 교훈적입니다.
주목할 만한 것은 훈련 기술(예: 최적화기, 데이터 증강, 정규화)이 정확도 향상에 중추적인 역할을 한다는 것을 보여주는 :citet:<code>liu2022convnet</code>의 연구입니다.
또한 계산과 데이터의 증가를 감안할 때 합성곱 윈도우 크기와 같은 오래된 가정을 재검토해야 할 수도 있음을 보여줍니다.
이 장 전체에서 적절한 때에 이 문제와 더 많은 질문을 다룰 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

alexnet
vgg
nin
googlenet
batch-norm
resnet
densenet
cnn-design
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="심층-합성곱-신경망-alexnet-deep-convolutional-neural-networks-alexnet"><a class="header" href="#심층-합성곱-신경망-alexnet-deep-convolutional-neural-networks-alexnet">심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></h1>
<p>:label:<code>sec_alexnet</code></p>
<p>LeNet :cite:<code>LeCun.Jackel.Bottou.ea.1995</code>이 소개된 후 컴퓨터 비전 및 머신러닝 커뮤니티에 CNN이 잘 알려지기는 했지만,
이 분야를 즉시 지배하지는 못했습니다.
LeNet은 초기 소규모 데이터셋에서 좋은 결과를 얻었지만, 더 크고 현실적인 데이터셋에 대한 CNN의 성능과 훈련 가능성은 아직 확립되지 않았습니다.
실제로 1990년대 초반과 2012년의 분수령이 되는 결과 :cite:<code>Krizhevsky.Sutskever.Hinton.2012</code> 사이의 시간 동안,
신경망은 커널 방법 :cite:<code>Scholkopf.Smola.2002</code>, 앙상블 방법 :cite:<code>Freund.Schapire.ea.1996</code>, 구조적 추정 :cite:<code>Taskar.Guestrin.Koller.2004</code>과 같은 다른 머신러닝 방법에 의해 종종 추월당했습니다.</p>
<p>컴퓨터 비전의 경우 이 비교가 전적으로 정확하지 않을 수 있습니다.
즉, 합성곱 네트워크에 대한 입력은 원시 또는 가볍게 처리된(예: 중심 맞추기) 픽셀 값으로 구성되지만, 실무자들은 전통적인 모델에 원시 픽셀을 공급하지 않았습니다.
대신 일반적인 컴퓨터 비전 파이프라인은 SIFT :cite:<code>Lowe.2004</code>, SURF :cite:<code>Bay.Tuytelaars.Van-Gool.2006</code>, 시각적 단어 가방(bags of visual words) :cite:<code>Sivic.Zisserman.2003</code>과 같은 수동으로 엔지니어링된 특성 추출 파이프라인으로 구성되었습니다.
특성을 <em>학습</em>하기보다는 특성을 <em>만들었습니다</em>.
대부분의 진전은 한편으로는 특성 추출에 대한 더 기발한 아이디어와 다른 한편으로는 기하학에 대한 깊은 통찰력 :cite:<code>Hartley.Zisserman.2000</code>에서 비롯되었습니다. 학습 알고리즘은 종종 나중에 생각하는 것으로 간주되었습니다.</p>
<p>1990년대에 일부 신경망 가속기를 사용할 수 있었지만,
많은 수의 파라미터를 가진 깊은 다중 채널, 다층 CNN을 만들기에는 아직 충분히 강력하지 않았습니다.
예를 들어 1999년 NVIDIA의 GeForce 256은 게임 이외의 작업에 대한 의미 있는 프로그래밍 프레임워크 없이 초당 최대 4억 8천만 개의 부동 소수점 연산(덧셈 및 곱셈 등, MFLOPS)만 처리할 수 있었습니다.
오늘날의 가속기는 장치당 1000 TFLOPs를 초과하는 성능을 발휘할 수 있습니다.
게다가 데이터셋은 여전히 상대적으로 작았습니다: 60,000개의 저해상도 $28 \times 28$ 픽셀 이미지에 대한 OCR은 매우 어려운 작업으로 간주되었습니다.
이러한 장애물에 더해 파라미터 초기화 휴리스틱 :cite:<code>Glorot.Bengio.2010</code>, 확률적 경사 하강법의 영리한 변형 :cite:<code>Kingma.Ba.2014</code>, 비스쿼싱(non-squashing) 활성화 함수 :cite:<code>Nair.Hinton.2010</code>, 효과적인 정규화 기술 :cite:<code>Srivastava.Hinton.Krizhevsky.ea.2014</code>을 포함한 신경망 훈련을 위한 핵심 트릭이 여전히 누락되었습니다.</p>
<p>따라서 <em>엔드 투 엔드</em>(픽셀에서 분류까지) 시스템을 훈련하는 대신 고전적인 파이프라인은 다음과 같았습니다:</p>
<ol>
<li>흥미로운 데이터셋을 얻습니다. 초기에는 이러한 데이터셋에 값비싼 센서가 필요했습니다. 예를 들어 1994년 <a href="https://en.wikipedia.org/wiki/Apple_QuickTake">Apple QuickTake 100</a>은 무려 0.3메가픽셀(VGA) 해상도를 자랑했으며 최대 8개의 이미지를 저장할 수 있었고 가격은 1000달러였습니다.</li>
<li>광학, 기하학, 기타 분석 도구에 대한 지식, 그리고 때로는 운 좋은 대학원생의 우연한 발견을 바탕으로 수작업으로 만든 특성으로 데이터셋을 전처리합니다.</li>
<li>SIFT(scale-invariant feature transform) :cite:<code>Lowe.2004</code>, SURF(speeded up robust features) :cite:<code>Bay.Tuytelaars.Van-Gool.2006</code> 또는 기타 수동으로 튜닝된 파이프라인과 같은 표준 특성 추출기 세트를 통해 데이터를 공급합니다. OpenCV는 오늘날에도 여전히 SIFT 추출기를 제공합니다!</li>
<li>결과 표현을 선형 모델이나 커널 방법과 같은 선호하는 분류기에 덤프하여 분류기를 훈련합니다.</li>
</ol>
<p>머신러닝 연구자들과 이야기했다면, 그들은 머신러닝이 중요하고 아름답다고 대답했을 것입니다.
우아한 이론은 다양한 분류기의 속성을 증명했고 :cite:<code>boucheron2005theory</code> 볼록 최적화 :cite:<code>Boyd.Vandenberghe.2004</code>는 이를 얻기 위한 주류가 되었습니다.
머신러닝 분야는 번성하고 엄격하며 매우 유용했습니다. 하지만,
컴퓨터 비전 연구자와 이야기했다면 매우 다른 이야기를 들었을 것입니다.
이미지 인식의 더러운 진실은 새로운 학습 알고리즘이 아니라 특성, 기하학 :cite:<code>Hartley.Zisserman.2000,hartley2009global</code> 및 엔지니어링이 발전을 주도했다는 것이라고 그들은 말할 것입니다.
컴퓨터 비전 연구자들은 약간 더 크거나 깨끗한 데이터셋 또는 약간 개선된 특성 추출 파이프라인이 어떤 학습 알고리즘보다 최종 정확도에 훨씬 더 중요하다고 정당하게 믿었습니다.</p>
<pre><code class="language-{.python .input  n=2}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, init, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=3}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input  n=4}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="표현-학습-representation-learning"><a class="header" href="#표현-학습-representation-learning">표현 학습 (Representation Learning)</a></h2>
<p>상황을 설명하는 또 다른 방법은 파이프라인의 가장 중요한 부분이 표현이었다는 것입니다.
그리고 2012년까지 표현은 대부분 기계적으로 계산되었습니다.
사실 새로운 특성 함수 세트를 엔지니어링하고 결과를 개선하고 방법을 작성하는 것이 모두 논문에서 두드러지게 다루어졌습니다.
SIFT :cite:<code>Lowe.2004</code>, SURF :cite:<code>Bay.Tuytelaars.Van-Gool.2006</code>, HOG(histograms of oriented gradient) :cite:<code>Dalal.Triggs.2005</code>, 시각적 단어 가방 :cite:<code>Sivic.Zisserman.2003</code> 및 유사한 특성 추출기들이 주도권을 잡았습니다.</p>
<p>Yann LeCun, Geoff Hinton, Yoshua Bengio, Andrew Ng, Shun-ichi Amari, Juergen Schmidhuber를 포함한 또 다른 연구자 그룹은 다른 계획을 가지고 있었습니다.
그들은 특성 자체가 학습되어야 한다고 믿었습니다.
더욱이 그들은 합리적으로 복잡해지기 위해서는 특성이 학습 가능한 파라미터를 가진 여러 공동 학습 레이어로 계층적으로 구성되어야 한다고 믿었습니다.
이미지의 경우, 동물의 시각 시스템이 입력을 처리하는 방식과 유사하게 가장 낮은 레이어는 가장자리, 색상 및 텍스처를 감지할 수 있습니다. 특히 희소 코딩 :cite:<code>olshausen1996emergence</code>으로 얻은 것과 같은 시각적 특성의 자동 설계는 현대 CNN이 도래할 때까지 열린 과제로 남아 있었습니다.
:citet:<code>Dean.Corrado.Monga.ea.2012,le2013building</code>에 이르러서야 이미지 데이터에서 자동으로 특성을 생성한다는 아이디어가 상당한 견인력을 얻었습니다.</p>
<p>최초의 현대식 CNN :cite:<code>Krizhevsky.Sutskever.Hinton.2012</code>은 발명가 중 한 명인 Alex Krizhevsky의 이름을 따서 <em>AlexNet</em>이라고 명명되었으며, 대체로 LeNet의 진화적 개선입니다. 2012년 ImageNet 챌린지에서 뛰어난 성능을 달성했습니다.</p>
<p><img src="chapter_convolutional-modern/../img/filters.png" alt="AlexNet의 첫 번째 레이어에서 학습한 이미지 필터. 복제 허가: :citet:Krizhevsky.Sutskever.Hinton.2012." />
:width:<code>400px</code>
:label:<code>fig_filters</code></p>
<p>흥미롭게도 네트워크의 가장 낮은 레이어에서 모델은 일부 전통적인 필터와 유사한 특성 추출기를 학습했습니다.
:numref:<code>fig_filters</code>는 하위 수준 이미지 기술자를 보여줍니다.
네트워크의 상위 레이어는 이러한 표현을 기반으로 눈, 코, 풀잎 등과 같은 더 큰 구조를 나타낼 수 있습니다.
더 높은 레이어는 사람, 비행기, 개 또는 프리스비와 같은 전체 객체를 나타낼 수 있습니다.
궁극적으로 최종 은닉 상태는 다른 범주에 속하는 데이터를 쉽게 분리할 수 있도록 콘텐츠를 요약하는 이미지의 압축된 표현을 학습합니다.</p>
<p>AlexNet(2012)과 그 전신인 LeNet(1995)은 많은 아키텍처 요소를 공유합니다. 그렇다면 왜 그렇게 오래 걸렸을까요?
주요 차이점은 지난 20년 동안 데이터 양과 사용 가능한 컴퓨팅 성능이 크게 증가했다는 것입니다. 따라서 AlexNet은 훨씬 더 컸습니다: 1995년에 사용할 수 있었던 CPU에 비해 훨씬 더 빠른 GPU에서 훨씬 더 많은 데이터로 훈련되었습니다.</p>
<h3 id="누락된-요소-데이터-missing-ingredient-data"><a class="header" href="#누락된-요소-데이터-missing-ingredient-data">누락된 요소: 데이터 (Missing Ingredient: Data)</a></h3>
<p>많은 레이어가 있는 심층 모델은 볼록 최적화에 기반한 전통적인 방법(예: 선형 및 커널 방법)을 훨씬 능가하는 영역에 진입하기 위해 많은 양의 데이터를 필요로 합니다.
그러나 컴퓨터의 제한된 저장 용량, (이미징) 센서의 상대적인 비용, 1990년대의 비교적 부족한 연구 예산으로 인해 대부분의 연구는 작은 데이터셋에 의존했습니다.
수많은 논문이 UCI 데이터셋 모음에 의존했는데, 그중 다수는 저해상도와 종종 인위적으로 깨끗한 배경으로 캡처된 수백 또는 (몇) 수천 개의 이미지만 포함하고 있었습니다.</p>
<p>2009년에 ImageNet 데이터셋이 공개되어 :cite:<code>Deng.Dong.Socher.ea.2009</code> 연구자들에게 1,000개의 고유한 객체 범주에서 각각 1,000개씩 100만 개의 예제로 모델을 학습하도록 도전했습니다. 범주 자체는 WordNet :cite:<code>Miller.1995</code>의 가장 인기 있는 명사 노드를 기반으로 했습니다.
ImageNet 팀은 Google 이미지 검색을 사용하여 각 범주에 대한 대규모 후보 세트를 사전 필터링하고 Amazon Mechanical Turk 크라우드소싱 파이프라인을 사용하여 각 이미지가 관련 범주에 속하는지 확인했습니다.
이 규모는 전례가 없었으며 다른 것들을 한 자리수 이상 초과했습니다(예: CIFAR-100은 60,000개의 이미지를 가짐). 또 다른 측면은 이미지가 $32 \times 32$ 픽셀 썸네일로 구성된 8천만 크기의 TinyImages 데이터셋 :cite:<code>Torralba.Fergus.Freeman.2008</code>과 달리 $224 \times 224$ 픽셀의 비교적 고해상도라는 점이었습니다.
이를 통해 더 높은 수준의 특성을 형성할 수 있었습니다.
ImageNet 대규모 시각 인식 챌린지(ImageNet Large Scale Visual Recognition Challenge) :cite:<code>russakovsky2015imagenet</code>라고 불리는 관련 대회는 컴퓨터 비전 및 머신러닝 연구를 발전시켜 연구자들이 이전에 학계에서 고려했던 것보다 더 큰 규모에서 어떤 모델이 가장 잘 수행되는지 식별하도록 도전했습니다. LAION-5B :cite:<code>schuhmann2022laion</code>와 같은 가장 큰 비전 데이터셋에는 추가 메타데이터가 있는 수십억 개의 이미지가 포함되어 있습니다.</p>
<h3 id="누락된-요소-하드웨어-missing-ingredient-hardware"><a class="header" href="#누락된-요소-하드웨어-missing-ingredient-hardware">누락된 요소: 하드웨어 (Missing Ingredient: Hardware)</a></h3>
<p>딥러닝 모델은 컴퓨팅 사이클의 엄청난 소비자입니다.
훈련에는 수백 에폭이 걸릴 수 있으며, 각 반복은 계산 비용이 많이 드는 선형 대수 연산의 많은 레이어를 통해 데이터를 전달해야 합니다.
이것이 1990년대와 2000년대 초반에 보다 효율적으로 최적화된 볼록 목적 함수에 기반한 간단한 알고리즘이 선호되었던 주된 이유 중 하나입니다.</p>
<p><em>그래픽 처리 장치</em>(GPU)는 딥러닝을 실현 가능하게 만드는 게임 체임저로 입증되었습니다.
이 칩들은 이전에 컴퓨터 게임에 도움이 되도록 그래픽 처리를 가속화하기 위해 개발되었습니다.
특히 많은 컴퓨터 그래픽 작업에 필요한 높은 처리량의 $4 \times 4$ 행렬-벡터 곱에 최적화되었습니다.
다행히도 이 수학은 합성곱 레이어를 계산하는 데 필요한 것과 놀랍도록 유사합니다.
그 무렵 NVIDIA와 ATI는 범용 컴퓨팅 작업을 위해 GPU를 최적화하기 시작했으며 :cite:<code>Fernando.2004</code>, <em>범용 GPU</em>(GPGPU)로 마케팅하기까지 했습니다.</p>
<p>직관을 제공하기 위해 최신 마이크로프로세서(CPU)의 코어를 고려해 보십시오.
각 코어는 높은 클록 주파수에서 실행되고 대형 캐시(최대 수 메가바이트의 L3)를 자랑하는 상당히 강력한 성능을 제공합니다.
각 코어는 분기 예측기, 깊은 파이프라인, 특수 실행 유닛, 투기적 실행 및 정교한 제어 흐름을 가진 다양한 프로그램을 실행할 수 있는 기타 많은 부가 기능을 통해 광범위한 명령을 실행하는 데 적합합니다.
그러나 이 명백한 강점은 아킬레스건이기도 합니다. 범용 코어는 구축하는 데 비용이 많이 듭니다. 그들은 제어 흐름이 많은 범용 코드에 탁월합니다.
이를 위해서는 계산이 일어나는 실제 ALU(산술 논리 장치)뿐만 아니라 앞서 언급한 모든 부가 기능, 메모리 인터페이스, 코어 간 캐싱 로직, 고속 상호 연결 등을 위한 많은 칩 면적이 필요합니다. CPU는 전용 하드웨어와 비교할 때 단일 작업에 상대적으로 나쁩니다.
최신 노트북에는 4~8개의 코어가 있으며 하이엔드 서버조차도 소켓당 64개 코어를 거의 초과하지 않습니다. 단순히 비용 효율적이지 않기 때문입니다.</p>
<p>이에 비해 GPU는 수천 개의 작은 처리 요소(NIVIDA의 최신 Ampere 칩에는 최대 6912개의 CUDA 코어가 있음)로 구성될 수 있으며 종종 더 큰 그룹(NVIDIA는 워프라고 부름)으로 그룹화됩니다.
세부 사항은 NVIDIA, AMD, ARM 및 기타 칩 공급업체마다 다소 다릅니다. 각 코어는 약 1GHz 클록 주파수에서 실행되어 상대적으로 약하지만, GPU를 CPU보다 몇 자릿수 더 빠르게 만드는 것은 그러한 코어의 총수입니다.
예를 들어 NVIDIA의 최근 Ampere A100 GPU는 특수 16비트 정밀도(BFLOAT16) 행렬-행렬 곱셈에 대해 칩당 300 TFLOPs 이상을 제공하고 보다 범용적인 부동 소수점 연산(FP32)에 대해 최대 20 TFLOPs를 제공합니다.
동시에 CPU의 부동 소수점 성능은 1 TFLOPs를 거의 초과하지 않습니다. 예를 들어 Amazon의 Graviton 3은 16비트 정밀도 연산에 대해 2 TFLOPs 피크 성능에 도달하는데, 이는 Apple M1 프로세서의 GPU 성능과 비슷한 수치입니다.</p>
<p>FLOPs 측면에서 GPU가 CPU보다 훨씬 빠른 데는 여러 가지 이유가 있습니다.
첫째, 전력 소비는 클록 주파수에 따라 <em>이차적으로</em> 증가하는 경향이 있습니다.
따라서 4배 더 빠르게 실행되는 CPU 코어의 전력 예산(일반적인 수치)으로 $\frac{1}{4}$ 속도의 GPU 코어 16개를 사용할 수 있으며, 이는 $16 \times \frac{1}{4} = 4$배의 성능을 산출합니다.
둘째, GPU 코어는 훨씬 단순하여(사실 오랫동안 범용 코드를 실행할 수조차 <em>없었음</em>) 에너지 효율이 더 높습니다. 예를 들어 (i) 투기적 평가를 지원하지 않는 경향이 있고, (ii) 일반적으로 각 처리 요소를 개별적으로 프로그래밍할 수 없으며, (iii) 코어당 캐시가 훨씬 작습니다.
마지막으로, 딥러닝의 많은 작업에는 높은 메모리 대역폭이 필요합니다.
다시 말하지만, GPU는 많은 CPU보다 적어도 10배 더 넓은 버스로 여기서 빛납니다.</p>
<p>2012년으로 돌아가 봅시다. Alex Krizhevsky와 Ilya Sutskever가 GPU에서 실행할 수 있는 심층 CNN을 구현했을 때 주요 돌파구가 마련되었습니다.
그들은 CNN의 계산 병목 현상인 합성곱과 행렬 곱셈이 모두 하드웨어에서 병렬화할 수 있는 연산이라는 것을 깨달았습니다.
각각 1.5 TFLOPs(10년 후 대부분의 CPU에게도 여전히 도전적인 수치)의 성능을 가진 3GB 메모리의 NVIDIA GTX 580 2개를 사용하여 빠른 합성곱을 구현했습니다.
<a href="https://code.google.com/archive/p/cuda-convnet/">cuda-convnet</a> 코드는 몇 년 동안 업계 표준이 되어 딥러닝 붐의 첫 몇 년을 이끌 만큼 훌륭했습니다.</p>
<h2 id="alexnet"><a class="header" href="#alexnet">AlexNet</a></h2>
<p>8레이어 CNN을 채용한 AlexNet은 2012 ImageNet 대규모 시각 인식 챌린지에서 큰 격차로 우승했습니다 :cite:<code>Russakovsky.Deng.Huang.ea.2013</code>.
이 네트워크는 학습을 통해 얻은 특성이 수동으로 설계된 특성을 초월할 수 있음을 처음으로 보여주며 컴퓨터 비전의 이전 패러다임을 깨뜨렸습니다.</p>
<p><img src="chapter_convolutional-modern/../img/alexnet.svg" alt="LeNet(왼쪽)에서 AlexNet(오른쪽)으로." />
:label:<code>fig_alexnet</code></p>
<p>AlexNet과 LeNet 사이에는 중요한 차이점도 있습니다.
첫째, AlexNet은 비교적 작은 LeNet-5보다 훨씬 깊습니다.
AlexNet은 8개 레이어로 구성됩니다: 5개의 합성곱 레이어, 2개의 완전 연결 은닉 레이어, 1개의 완전 연결 출력 레이어입니다.
둘째, AlexNet은 활성화 함수로 시그모이드 대신 ReLU를 사용했습니다. 아래에서 세부 사항을 살펴봅시다.</p>
<h3 id="아키텍처-architecture"><a class="header" href="#아키텍처-architecture">아키텍처 (Architecture)</a></h3>
<p>AlexNet의 첫 번째 레이어에서 합성곱 윈도우 모양은 $11\times11$입니다.
ImageNet의 이미지는 MNIST 이미지보다 높이와 너비가 8배 더 크기 때문에, ImageNet 데이터의 객체는 더 많은 시각적 세부 정보와 함께 더 많은 픽셀을 차지하는 경향이 있습니다.
결과적으로 객체를 포착하려면 더 큰 합성곱 윈도우가 필요합니다.
두 번째 레이어의 합성곱 윈도우 모양은 $5\times5$로 줄어들고, 그다음에는 $3\times3$이 이어집니다.
또한 첫 번째, 두 번째, 다섯 번째 합성곱 레이어 뒤에 네트워크는 윈도우 모양이 $3\times3$이고 스트라이드가 2인 최대 풀링 레이어를 추가합니다.
게다가 AlexNet은 LeNet보다 10배 더 많은 합성곱 채널을 가지고 있습니다.</p>
<p>마지막 합성곱 레이어 뒤에는 4096개의 출력을 가진 두 개의 거대한 완전 연결 레이어가 있습니다.
이 레이어들은 거의 1GB의 모델 파라미터를 필요로 합니다.
초기 GPU의 메모리 제한 때문에 원래 AlexNet은 이중 데이터 스트림 설계를 사용하여 두 개의 GPU 각각이 모델의 절반만 저장하고 계산하는 것을 담당하도록 했습니다.
다행히 지금은 GPU 메모리가 비교적 풍부하므로 요즘에는 모델을 GPU에 걸쳐 분할해야 하는 경우가 드뭅니다(우리의 AlexNet 모델 버전은 이 측면에서 원래 논문과 다릅니다).</p>
<h3 id="활성화-함수-activation-functions-1"><a class="header" href="#활성화-함수-activation-functions-1">활성화 함수 (Activation Functions)</a></h3>
<p>또한 AlexNet은 시그모이드 활성화 함수를 더 간단한 ReLU 활성화 함수로 변경했습니다. 한편으로 ReLU 활성화 함수의 계산은 더 간단합니다. 예를 들어 시그모이드 활성화 함수에서 볼 수 있는 지수 연산이 없습니다.
다른 한편으로 ReLU 활성화 함수는 다른 파라미터 초기화 방법을 사용할 때 모델 훈련을 더 쉽게 만듭니다. 이는 시그모이드 활성화 함수의 출력이 0 또는 1에 매우 가까울 때 이 영역의 기울기가 거의 0이 되어 역전파가 일부 모델 파라미터를 계속 업데이트할 수 없기 때문입니다. 대조적으로 양의 구간에서 ReLU 활성화 함수의 기울기는 항상 1입니다(:numref:<code>subsec_activation-functions</code>). 따라서 모델 파라미터가 적절하게 초기화되지 않으면 시그모이드 함수는 양의 구간에서 거의 0인 기울기를 얻어 모델을 효과적으로 훈련할 수 없게 될 수 있습니다.</p>
<h3 id="용량-제어-및-전처리-capacity-control-and-preprocessing"><a class="header" href="#용량-제어-및-전처리-capacity-control-and-preprocessing">용량 제어 및 전처리 (Capacity Control and Preprocessing)</a></h3>
<p>AlexNet은 드롭아웃(:numref:<code>sec_dropout</code>)으로 완전 연결 레이어의 모델 복잡도를 제어하는 반면, LeNet은 가중치 감쇠만 사용합니다.
데이터를 더욱 증강하기 위해 AlexNet의 훈련 루프는 뒤집기(flipping), 자르기(clipping), 색상 변경과 같은 많은 이미지 증강을 추가했습니다.
이는 모델을 더 견고하게 만들고 더 큰 샘플 크기는 과대적합을 효과적으로 줄입니다.
이러한 전처리 단계에 대한 심층적인 검토는 :citet:<code>Buslaev.Iglovikov.Khvedchenya.ea.2020</code>를 참조하십시오.</p>
<pre><code class="language-{.python .input  n=5}">%%tab pytorch, mxnet, tensorflow
class AlexNet(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.net = nn.Sequential()
            self.net.add(
                nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),
                nn.MaxPool2D(pool_size=3, strides=2),
                nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),
                nn.MaxPool2D(pool_size=3, strides=2),
                nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
                nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
                nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),
                nn.MaxPool2D(pool_size=3, strides=2),
                nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
                nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
                nn.Dense(num_classes))
            self.net.initialize(init.Xavier())
        if tab.selected('pytorch'):
            self.net = nn.Sequential(
                nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),
                nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),
                nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2),
                nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),
                nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),
                nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),
                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),
                nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),
                nn.LazyLinear(num_classes))
            self.net.apply(d2l.init_cnn)
        if tab.selected('tensorflow'):
            self.net = tf.keras.models.Sequential([
                tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,
                                       activation='relu'),
                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),
                tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same',
                                       activation='relu'),
                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),
                tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',
                                       activation='relu'),
                tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',
                                       activation='relu'),
                tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same',
                                       activation='relu'),
                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(4096, activation='relu'),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(4096, activation='relu'),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(num_classes)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class AlexNet(d2l.Classifier):
    lr: float = 0.1
    num_classes: int = 10
    training: bool = True

    def setup(self):
        self.net = nn.Sequential([
            nn.Conv(features=96, kernel_size=(11, 11), strides=4, padding=1),
            nn.relu,
            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2)),
            nn.Conv(features=256, kernel_size=(5, 5)),
            nn.relu,
            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2)),
            nn.Conv(features=384, kernel_size=(3, 3)), nn.relu,
            nn.Conv(features=384, kernel_size=(3, 3)), nn.relu,
            nn.Conv(features=256, kernel_size=(3, 3)), nn.relu,
            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2)),
            lambda x: x.reshape((x.shape[0], -1)),  # flatten
            nn.Dense(features=4096),
            nn.relu,
            nn.Dropout(0.5, deterministic=not self.training),
            nn.Dense(features=4096),
            nn.relu,
            nn.Dropout(0.5, deterministic=not self.training),
            nn.Dense(features=self.num_classes)
        ])
</code></pre>
<p>우리는 높이와 너비가 224인 [<strong>단일 채널 데이터 예제를 구성</strong>]하여 (<strong>각 레이어의 출력 모양을 관찰</strong>)합니다. 이는 :numref:<code>fig_alexnet</code>의 AlexNet 아키텍처와 일치합니다.</p>
<pre><code class="language-{.python .input  n=6}">%%tab pytorch, mxnet
AlexNet().layer_summary((1, 1, 224, 224))
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab tensorflow
AlexNet().layer_summary((1, 224, 224, 1))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
AlexNet(training=False).layer_summary((1, 224, 224, 1))
</code></pre>
<h2 id="훈련-training-9"><a class="header" href="#훈련-training-9">훈련 (Training)</a></h2>
<p>AlexNet은 :citet:<code>Krizhevsky.Sutskever.Hinton.2012</code>에서 ImageNet으로 훈련되었지만, 여기서는 Fashion-MNIST를 사용합니다.
ImageNet 모델을 수렴할 때까지 훈련하는 것은 최신 GPU에서도 몇 시간 또는 며칠이 걸릴 수 있기 때문입니다.
[<strong>Fashion-MNIST</strong>]에 AlexNet을 직접 적용할 때의 문제 중 하나는 (<strong>이미지가 ImageNet 이미지보다</strong>) (<strong>해상도가 낮다는 것</strong>)(28 $\times$ 28 픽셀)입니다.
제대로 작동하게 하기 위해, (<strong>우리는 이를 $224 \times 224$로 업샘플링합니다</strong>).
이는 단순히 정보를 추가하지 않고 계산 복잡성을 증가시키기 때문에 일반적으로 현명한 관행은 아닙니다. 그럼에도 불구하고 우리는 AlexNet 아키텍처에 충실하기 위해 여기서 이렇게 합니다.
<code>d2l.FashionMNIST</code> 생성자의 <code>resize</code> 인수를 사용하여 이 크기 조정을 수행합니다.</p>
<p>이제 [<strong>AlexNet 훈련을 시작할 수 있습니다.</strong>]
:numref:<code>sec_lenet</code>의 LeNet과 비교할 때, 주요 변경 사항은 더 깊고 넓은 네트워크, 더 높은 이미지 해상도, 더 많은 비용이 드는 합성곱으로 인해 더 작은 학습률을 사용하고 훈련이 훨씬 느리다는 것입니다.</p>
<pre><code class="language-{.python .input  n=8}">%%tab pytorch, mxnet, jax
model = AlexNet(lr=0.01)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
with d2l.try_gpu():
    model = AlexNet(lr=0.01)
    trainer.fit(model, data)
</code></pre>
<h2 id="토론-discussion-1"><a class="header" href="#토론-discussion-1">토론 (Discussion)</a></h2>
<p>AlexNet의 구조는 정확도(드롭아웃)와 훈련 용이성(ReLU) 모두에 대한 여러 가지 중요한 개선 사항과 함께 LeNet과 놀랍도록 유사합니다. 마찬가지로 놀라운 것은 딥러닝 툴링 측면에서 이루어진 진전의 양입니다. 2012년에 몇 달이 걸렸던 작업은 이제 모든 최신 프레임워크를 사용하여 12줄의 코드로 수행할 수 있습니다.</p>
<p>아키텍처를 검토해 보면 AlexNet은 효율성 측면에서 아킬레스건이 있습니다: 마지막 두 은닉층에는 각각 $6400 \times 4096$ 및 $4096 \times 4096$ 크기의 행렬이 필요합니다. 이는 164MB의 메모리와 81 MFLOPs의 계산에 해당하며, 둘 다 특히 휴대전화와 같은 소형 장치에서는 사소하지 않은 지출입니다. 이것이 AlexNet이 다음 섹션에서 다룰 훨씬 더 효과적인 아키텍처에 의해 추월당한 이유 중 하나입니다. 그럼에도 불구하고 이는 오늘날 사용되는 얕은 네트워크에서 깊은 네트워크로 가는 핵심 단계입니다. 실험에서 파라미터 수가 훈련 데이터 양을 훨씬 초과하더라도(마지막 두 레이어에는 6만 개의 이미지 데이터셋에서 훈련된 4천만 개 이상의 파라미터가 있음) 과대적합이 거의 없습니다: 훈련 및 검증 손실은 훈련 내내 거의 동일합니다. 이는 현대 심층 네트워크 설계에 내재된 드롭아웃과 같은 개선된 정규화 때문입니다.</p>
<p>AlexNet 구현이 LeNet보다 몇 줄 더 많은 것처럼 보이지만, 학계가 이 개념적 변화를 받아들이고 뛰어난 실험 결과를 활용하는 데는 수년이 걸렸습니다. 이는 또한 효율적인 계산 도구의 부족 때문이기도 했습니다. 당시에는 DistBelief :cite:<code>Dean.Corrado.Monga.ea.2012</code>나 Caffe :cite:<code>Jia.Shelhamer.Donahue.ea.2014</code>가 존재하지 않았고 Theano :cite:<code>Bergstra.Breuleux.Bastien.ea.2010</code>는 여전히 많은 특징적인 기능이 부족했습니다. 상황을 극적으로 바꾼 것은 TensorFlow :cite:<code>Abadi.Barham.Chen.ea.2016</code>의 가용성이었습니다.</p>
<h2 id="연습-문제-exercises-34"><a class="header" href="#연습-문제-exercises-34">연습 문제 (Exercises)</a></h2>
<ol>
<li>위 논의에 이어 AlexNet의 계산적 특성을 분석하십시오.
<ol>
<li>합성곱과 완전 연결 레이어의 메모리 사용량을 각각 계산하십시오. 어느 것이 지배적입니까?</li>
<li>합성곱과 완전 연결 레이어의 계산 비용을 계산하십시오.</li>
<li>메모리(읽기 및 쓰기 대역폭, 대기 시간, 크기)가 계산에 어떤 영향을 미칩니까? 훈련과 추론에 미치는 영향에 차이가 있습니까?</li>
</ol>
</li>
<li>당신은 칩 설계자이고 계산과 메모리 대역폭을 절충해야 합니다. 예를 들어 더 빠른 칩은 더 많은 전력과 아마도 더 큰 칩 면적을 필요로 합니다. 더 많은 메모리 대역폭은 더 많은 핀과 제어 로직을 필요로 하므로 더 많은 면적을 필요로 합니다. 어떻게 최적화합니까?</li>
<li>엔지니어들이 더 이상 AlexNet에 대한 성능 벤치마크를 보고하지 않는 이유는 무엇입니까?</li>
<li>AlexNet을 훈련할 때 에폭 수를 늘려 보십시오. LeNet과 비교할 때 결과가 어떻게 다릅니까? 그 이유는 무엇입니까?</li>
<li>AlexNet은 Fashion-MNIST 데이터셋, 특히 초기 이미지의 저해상도 때문에 너무 복잡할 수 있습니다.
<ol>
<li>정확도가 크게 떨어지지 않도록 하면서 훈련 속도를 높이도록 모델을 단순화해 보십시오.</li>
<li>$28 \times 28$ 이미지에서 직접 작동하는 더 나은 모델을 설계하십시오.</li>
</ol>
</li>
<li>배치 크기를 수정하고 처리량(이미지/초), 정확도 및 GPU 메모리의 변화를 관찰하십시오.</li>
<li>LeNet-5에 드롭아웃과 ReLU를 적용하십시오. 개선됩니까? 이미지에 내재된 불변성을 활용하기 위해 전처리를 통해 상황을 더 개선할 수 있습니까?</li>
<li>AlexNet을 과대적합하게 만들 수 있습니까? 훈련을 중단시키려면 어떤 특성을 제거하거나 변경해야 합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/75">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/76">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/276">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18001">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="블록을-사용하는-네트워크-vgg-networks-using-blocks-vgg"><a class="header" href="#블록을-사용하는-네트워크-vgg-networks-using-blocks-vgg">블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></h1>
<p>:label:<code>sec_vgg</code></p>
<p>AlexNet은 심층 CNN이 좋은 결과를 얻을 수 있다는 경험적 증거를 제공했지만, 후속 연구자들이 새로운 네트워크를 설계하는 데 지침이 될 일반적인 템플릿을 제공하지는 않았습니다.
다음 섹션에서는 심층 네트워크를 설계하는 데 일반적으로 사용되는 몇 가지 휴리스틱 개념을 소개합니다.</p>
<p>이 분야의 발전은 칩 설계의 VLSI(초고밀도 집적 회로) 발전과 유사합니다.
엔지니어들은 트랜지스터 배치에서 논리 요소, 논리 블록으로 이동했습니다 :cite:<code>Mead.1980</code>.
마찬가지로 신경망 아키텍처의 설계는 점점 더 추상적으로 성장하여, 연구자들은 개별 뉴런 측면에서 생각하는 것에서 전체 레이어로, 그리고 이제는 레이어의 반복 패턴인 블록으로 이동했습니다. 10년 후, 이제는 연구자들이 전체 훈련된 모델을 사용하여 관련성은 있지만 다른 작업을 위해 용도 변경하는 것으로 발전했습니다. 이러한 대규모 사전 훈련된 모델을 일반적으로 *파운데이션 모델(foundation models)*이라고 부릅니다 :cite:<code>bommasi2021opportunities</code>.</p>
<p>네트워크 설계로 돌아갑시다. 블록을 사용한다는 아이디어는 옥스퍼드 대학의 VGG(Visual Geometry Group)에서 그들의 이름을 딴 <em>VGG</em> 네트워크로 처음 등장했습니다 :cite:<code>Simonyan.Zisserman.2014</code>.
루프와 서브루틴을 사용하여 최신 딥러닝 프레임워크로 코드에서 이러한 반복 구조를 쉽게 구현할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx, init
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
from d2l import tensorflow as d2l
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
</code></pre>
<h2 id="vgg-블록"><a class="header" href="#vgg-블록">(<strong>VGG 블록</strong>)</a></h2>
<p>:label:<code>subsec_vgg-blocks</code></p>
<p>CNN의 기본 빌딩 블록은 다음의 시퀀스입니다:
(i) 해상도를 유지하기 위한 패딩이 있는 합성곱 레이어,
(ii) ReLU와 같은 비선형성,
(iii) 해상도를 줄이기 위한 최대 풀링과 같은 풀링 레이어.
이 접근 방식의 문제 중 하나는 공간 해상도가 매우 빠르게 감소한다는 것입니다. 특히,
이것은 모든 차원($d$)이 소진되기 전에 네트워크에 $\log_2 d$ 합성곱 레이어라는 엄격한 제한을 부과합니다. 예를 들어 ImageNet의 경우, 이 방법으로는 8개 이상의 합성곱 레이어를 가질 수 없습니다.</p>
<p>:citet:<code>Simonyan.Zisserman.2014</code>의 핵심 아이디어는 블록 형태의 최대 풀링을 통한 다운샘플링 사이에 <em>여러</em> 합성곱을 사용하는 것이었습니다. 그들은 주로 깊은 네트워크와 넓은 네트워크 중 어느 것이 더 잘 수행되는지에 관심이 있었습니다. 예를 들어 두 번의 $3 \times 3$ 합성곱을 연속적으로 적용하면 단일 $5 \times 5$ 합성곱과 동일한 픽셀을 터치합니다. 동시에 후자는 세 번의 $3 \times 3$ 합성곱($3 \cdot 9 \cdot c^2$)과 거의 같은 수의 파라미터($25 \cdot c^2$)를 사용합니다.
상당히 상세한 분석에서 그들은 깊고 좁은 네트워크가 얕은 네트워크보다 훨씬 성능이 뛰어나다는 것을 보여주었습니다. 이로 인해 딥러닝은 일반적인 응용 프로그램을 위해 100개 이상의 레이어가 있는 더 깊은 네트워크를 추구하게 되었습니다.
$3 \times 3$ 합성곱을 쌓는 것은 나중의 심층 네트워크에서 금본위제가 되었습니다(최근 :citet:<code>liu2022convnet</code>에 의해 재검토된 설계 결정). 결과적으로 작은 합성곱을 위한 빠른 구현은 GPU의 필수 요소가 되었습니다 :cite:<code>lavin2016fast</code>.</p>
<p>VGG로 돌아가서: VGG 블록은 패딩 1(높이와 너비 유지)이 있는 $3\times3$ 커널을 가진 합성곱 <em>시퀀스</em>와 스트라이드 2인 $2 \times 2$ 최대 풀링 레이어(각 블록 후 높이와 너비 절반으로 줄임)로 구성됩니다.
아래 코드에서는 하나의 VGG 블록을 구현하기 위해 <code>vgg_block</code>이라는 함수를 정의합니다.</p>
<p>아래 함수는 합성곱 레이어 수 <code>num_convs</code>와 출력 채널 수 <code>num_channels</code>에 해당하는 두 개의 인수를 취합니다.</p>
<pre><code class="language-{.python .input  n=2}">%%tab mxnet
def vgg_block(num_convs, num_channels):
    blk = nn.Sequential()
    for _ in range(num_convs):
        blk.add(nn.Conv2D(num_channels, kernel_size=3,
                          padding=1, activation='relu'))
    blk.add(nn.MaxPool2D(pool_size=2, strides=2))
    return blk
</code></pre>
<pre><code class="language-{.python .input  n=3}">%%tab pytorch
def vgg_block(num_convs, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))
        layers.append(nn.ReLU())
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)
</code></pre>
<pre><code class="language-{.python .input  n=4}">%%tab tensorflow
def vgg_block(num_convs, num_channels):
    blk = tf.keras.models.Sequential()
    for _ in range(num_convs):
        blk.add(
            tf.keras.layers.Conv2D(num_channels, kernel_size=3,
                                   padding='same', activation='relu'))
    blk.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
    return blk
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def vgg_block(num_convs, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv(out_channels, kernel_size=(3, 3), padding=(1, 1)))
        layers.append(nn.relu)
    layers.append(lambda x: nn.max_pool(x, window_shape=(2, 2), strides=(2, 2)))
    return nn.Sequential(layers)
</code></pre>
<h2 id="vgg-네트워크"><a class="header" href="#vgg-네트워크">[<strong>VGG 네트워크</strong>]</a></h2>
<p>:label:<code>subsec_vgg-network</code></p>
<p>AlexNet 및 LeNet과 마찬가지로, VGG 네트워크는 두 부분으로 나눌 수 있습니다:
첫 번째는 대부분 합성곱 및 풀링 레이어로 구성되고, 두 번째는 AlexNet과 동일한 완전 연결 레이어로 구성됩니다.
주요 차이점은 합성곱 레이어가 차원을 변경하지 않는 비선형 변환으로 그룹화되고, 그 뒤에 :numref:<code>fig_vgg</code>에 묘사된 대로 해상도 감소 단계가 따른다는 것입니다.</p>
<p><img src="chapter_convolutional-modern/../img/vgg.svg" alt="AlexNet에서 VGG로. 주요 차이점은 VGG는 레이어 블록으로 구성되는 반면 AlexNet의 레이어는 모두 개별적으로 설계되었다는 점입니다." />
:width:<code>400px</code>
:label:<code>fig_vgg</code></p>
<p>네트워크의 합성곱 부분은 :numref:<code>fig_vgg</code>의 여러 VGG 블록(<code>vgg_block</code> 함수에도 정의됨)을 연속적으로 연결합니다.
이러한 합성곱 그룹화는 지난 10년 동안 거의 변하지 않은 패턴이지만, 구체적인 연산 선택은 상당한 수정을 거쳤습니다.
변수 <code>arch</code>는 튜플 목록(블록당 하나)으로 구성되며, 각 튜플에는 합성곱 레이어 수와 출력 채널 수라는 두 개의 값이 포함되어 있습니다. 이는 <code>vgg_block</code> 함수를 호출하는 데 필요한 인수와 정확히 일치합니다. 따라서 VGG는 특정 구현보다는 네트워크 <em>패밀리</em>를 정의합니다. 특정 네트워크를 구축하기 위해 우리는 단순히 <code>arch</code>를 반복하여 블록을 구성합니다.</p>
<pre><code class="language-{.python .input  n=5}">%%tab pytorch, mxnet, tensorflow
class VGG(d2l.Classifier):
    def __init__(self, arch, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.net = nn.Sequential()
            for (num_convs, num_channels) in arch:
                self.net.add(vgg_block(num_convs, num_channels))
            self.net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
                         nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
                         nn.Dense(num_classes))
            self.net.initialize(init.Xavier())
        if tab.selected('pytorch'):
            conv_blks = []
            for (num_convs, out_channels) in arch:
                conv_blks.append(vgg_block(num_convs, out_channels))
            self.net = nn.Sequential(
                *conv_blks, nn.Flatten(),
                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
                nn.LazyLinear(num_classes))
            self.net.apply(d2l.init_cnn)
        if tab.selected('tensorflow'):
            self.net = tf.keras.models.Sequential()
            for (num_convs, num_channels) in arch:
                self.net.add(vgg_block(num_convs, num_channels))
            self.net.add(
                tf.keras.models.Sequential([
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(4096, activation='relu'),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(4096, activation='relu'),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(num_classes)]))
</code></pre>
<pre><code class="language-{.python .input  n=5}">%%tab jax
class VGG(d2l.Classifier):
    arch: list
    lr: float = 0.1
    num_classes: int = 10
    training: bool = True

    def setup(self):
        conv_blks = []
        for (num_convs, out_channels) in self.arch:
            conv_blks.append(vgg_block(num_convs, out_channels))

        self.net = nn.Sequential([
            *conv_blks,
            lambda x: x.reshape((x.shape[0], -1)),  # flatten
            nn.Dense(4096), nn.relu,
            nn.Dropout(0.5, deterministic=not self.training),
            nn.Dense(4096), nn.relu,
            nn.Dropout(0.5, deterministic=not self.training),
            nn.Dense(self.num_classes)])
</code></pre>
<p>원래 VGG 네트워크에는 5개의 합성곱 블록이 있었으며, 그중 처음 두 개는 각각 하나의 합성곱 레이어를 갖고 나중 세 개는 각각 두 개의 합성곱 레이어를 포함합니다.
첫 번째 블록에는 64개의 출력 채널이 있으며, 각 후속 블록은 출력 채널 수가 512에 도달할 때까지 두 배로 늘립니다.
이 네트워크는 8개의 합성곱 레이어와 3개의 완전 연결 레이어를 사용하므로 종종 VGG-11이라고 불립니다.</p>
<pre><code class="language-{.python .input  n=6}">%%tab pytorch, mxnet
VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(
    (1, 1, 224, 224))
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab tensorflow
VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(
    (1, 224, 224, 1))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)),
    training=False).layer_summary((1, 224, 224, 1))
</code></pre>
<p>보시다시피 각 블록에서 높이와 너비를 반으로 줄여, 네트워크의 완전 연결 부분에서 처리하기 위해 표현을 평탄화하기 전에 최종적으로 높이와 너비 7에 도달합니다.
:citet:<code>Simonyan.Zisserman.2014</code>는 VGG의 여러 다른 변형을 설명했습니다.
사실 새로운 아키텍처를 도입할 때 속도-정확도 트레이드오프가 다른 네트워크 <em>패밀리</em>를 제안하는 것이 표준이 되었습니다.</p>
<h2 id="훈련-training-10"><a class="header" href="#훈련-training-10">훈련 (Training)</a></h2>
<p>[<strong>VGG-11은 AlexNet보다 계산적으로 더 까다로으므로 더 적은 수의 채널을 가진 네트워크를 구성합니다.</strong>]
이는 Fashion-MNIST에서 훈련하기에 충분합니다.
[<strong>모델 훈련</strong>] 과정은 :numref:<code>sec_alexnet</code>의 AlexNet과 유사합니다.
다시 검증 손실과 훈련 손실이 밀접하게 일치하여 과대적합이 적다는 것을 관찰하십시오.</p>
<pre><code class="language-{.python .input  n=8}">%%tab mxnet, pytorch, jax
model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
if tab.selected('pytorch'):
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
with d2l.try_gpu():
    model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)
    trainer.fit(model, data)
</code></pre>
<h2 id="요약-summary-28"><a class="header" href="#요약-summary-28">요약 (Summary)</a></h2>
<p>VGG가 최초의 진정한 현대적 합성곱 신경망이라고 주장할 수 있습니다. AlexNet이 딥러닝을 대규모로 효과적으로 만드는 구성 요소 중 많은 것을 도입했지만, 다중 합성곱 블록과 깊고 좁은 네트워크에 대한 선호도와 같은 핵심 속성을 도입한 것은 틀림없이 VGG입니다. 또한 실제로 유사하게 파라미터화된 모델의 전체 패밀리인 첫 번째 네트워크로, 실무자에게 복잡성과 속도 간의 충분한 트레이드오프를 제공합니다. 이곳은 또한 현대 딥러닝 프레임워크가 빛을 발하는 곳이기도 합니다. 네트워크를 지정하기 위해 XML 구성 파일을 생성할 필요가 없으며, 간단한 Python 코드를 통해 해당 네트워크를 조립할 수 있습니다.</p>
<p>최근에는 ParNet :cite:<code>Goyal.Bochkovskiy.Deng.ea.2021</code>이 대규모 병렬 계산을 통해 훨씬 더 얕은 아키텍처를 사용하여 경쟁력 있는 성능을 달성할 수 있음을 입증했습니다. 이는 흥미로운 발전이며 미래의 아키텍처 설계에 영향을 미칠 것이라는 희망이 있습니다. 하지만 이 장의 나머지 부분에서는 지난 10년 동안의 과학적 진보의 길을 따를 것입니다.</p>
<h2 id="연습-문제-exercises-35"><a class="header" href="#연습-문제-exercises-35">연습 문제 (Exercises)</a></h2>
<ol>
<li>AlexNet과 비교할 때 VGG는 계산 측면에서 훨씬 느리고 GPU 메모리도 더 많이 필요합니다.
<ol>
<li>AlexNet과 VGG에 필요한 파라미터 수를 비교하십시오.</li>
<li>합성곱 레이어와 완전 연결 레이어에서 사용되는 부동 소수점 연산 수를 비교하십시오.</li>
<li>완전 연결 레이어로 인해 발생하는 계산 비용을 어떻게 줄일 수 있습니까?</li>
</ol>
</li>
<li>네트워크의 다양한 레이어와 관련된 차원을 표시할 때, 네트워크에 11개의 레이어가 있음에도 불구하고 8개의 블록(및 일부 보조 변환)과 관련된 정보만 표시됩니다. 나머지 3개의 레이어는 어디로 갔습니까?</li>
<li>VGG 논문 :cite:<code>Simonyan.Zisserman.2014</code>의 표 1을 사용하여 VGG-16 또는 VGG-19와 같은 다른 일반적인 모델을 구성하십시오.</li>
<li>Fashion-MNIST의 해상도를 $28 \times 28$에서 $224 \times 224$ 차원으로 8배 업샘플링하는 것은 매우 낭비적입니다. 대신 입력을 위해 네트워크 아키텍처와 해상도 변환을 수정해 보십시오(예: 56 또는 84 차원으로). 네트워크의 정확도를 떨어뜨리지 않고 할 수 있습니까? 다운샘플링 전에 더 많은 비선형성을 추가하는 아이디어에 대해서는 VGG 논문 :cite:<code>Simonyan.Zisserman.2014</code>을 참조하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/77">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/78">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/277">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18002">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="네트워크-인-네트워크-nin-network-in-network-nin"><a class="header" href="#네트워크-인-네트워크-nin-network-in-network-nin">네트워크 인 네트워크 (NiN) (Network in Network (NiN))</a></h1>
<p>:label:<code>sec_nin</code></p>
<p>LeNet, AlexNet, VGG는 모두 공통적인 설계 패턴을 공유합니다:
일련의 합성곱 및 풀링 레이어를 통해 <em>공간</em> 구조를 활용하여 특성을 추출하고 완전 연결 레이어를 통해 표현을 후처리합니다.
AlexNet과 VGG가 LeNet보다 개선된 점은 주로 이 후반부 네트워크가 이 두 모듈을 어떻게 넓히고 깊게 만들었는지에 있습니다.</p>
<p>이 설계는 두 가지 주요 과제를 제기합니다.
첫째, 아키텍처 끝의 완전 연결 레이어는 엄청난 수의 파라미터를 소비합니다. 예를 들어 VGG-11과 같은 단순한 모델조차도 단일 정밀도(FP32)에서 거의 400MB의 RAM을 차지하는 괴물 같은 행렬을 필요로 합니다. 이는 특히 모바일 및 임베디드 장치에서 계산에 상당한 장애물이 됩니다. 결국 최고급 휴대폰조차도 8GB 이상의 RAM을 자랑하지 않습니다. VGG가 발명되었을 당시에는 이것이 10배나 적었습니다(iPhone 4S는 512MB였습니다). 따라서 이미지 분류기에 메모리의 대부분을 소비하는 것을 정당화하기 어려웠을 것입니다.</p>
<p>둘째, 비선형성 정도를 높이기 위해 네트워크 초기에 완전 연결 레이어를 추가하는 것도 마찬가지로 불가능합니다. 그렇게 하면 공간 구조가 파괴되고 잠재적으로 훨씬 더 많은 메모리가 필요하기 때문입니다.</p>
<p><em>네트워크 인 네트워크</em> (<em>NiN</em>) 블록 :cite:<code>Lin.Chen.Yan.2013</code>은 하나의 간단한 전략으로 두 문제를 모두 해결할 수 있는 대안을 제공합니다.
그것들은 매우 간단한 통찰력을 바탕으로 제안되었습니다: (i) $1 \times 1$ 합성곱을 사용하여 채널 활성화에 국소 비선형성을 추가하고 (ii) 전역 평균 풀링을 사용하여 마지막 표현 레이어의 모든 위치에 걸쳐 통합합니다. 전역 평균 풀링은 추가된 비선형성이 없었다면 효과적이지 않았을 것입니다. 이에 대해 자세히 알아봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx, init
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
from d2l import tensorflow as d2l
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="nin-블록"><a class="header" href="#nin-블록">(<strong>NiN 블록</strong>)</a></h2>
<p>:numref:<code>subsec_1x1</code>을 상기하십시오. 거기서 우리는 합성곱 레이어의 입력과 출력이 예제, 채널, 높이, 너비에 해당하는 축을 가진 4차원 텐서로 구성된다고 말했습니다.
또한 완전 연결 레이어의 입력과 출력은 일반적으로 예제와 특성에 해당하는 2차원 텐서임을 상기하십시오.
NiN 뒤에 있는 아이디어는 각 픽셀 위치(각 높이와 너비에 대해)에 완전 연결 레이어를 적용하는 것입니다.
결과로 나오는 $1 \times 1$ 합성곱은 각 픽셀 위치에서 독립적으로 작동하는 완전 연결 레이어로 생각할 수 있습니다.</p>
<p>:numref:<code>fig_nin</code>은 VGG와 NiN, 그리고 그들의 블록 간의 주요 구조적 차이점을 보여줍니다.
NiN 블록의 차이점(초기 합성곱 뒤에 $1 \times 1$ 합성곱이 이어지지만 VGG는 $3 \times 3$ 합성곱을 유지함)과 더 이상 거대한 완전 연결 레이어가 필요하지 않은 끝부분의 차이점에 유의하십시오.</p>
<p><img src="chapter_convolutional-modern/../img/nin.svg" alt="VGG와 NiN, 그리고 그들의 블록 아키텍처 비교." />
:width:<code>600px</code>
:label:<code>fig_nin</code></p>
<pre><code class="language-{.python .input}">%%tab mxnet
def nin_block(num_channels, kernel_size, strides, padding):
    blk = nn.Sequential()
    blk.add(nn.Conv2D(num_channels, kernel_size, strides, padding,
                      activation='relu'),
            nn.Conv2D(num_channels, kernel_size=1, activation='relu'),
            nn.Conv2D(num_channels, kernel_size=1, activation='relu'))
    return blk
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def nin_block(out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),
        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),
        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def nin_block(out_channels, kernel_size, strides, padding):
    return tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(out_channels, kernel_size, strides=strides,
                           padding=padding),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Conv2D(out_channels, 1),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Conv2D(out_channels, 1),
    tf.keras.layers.Activation('relu')])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def nin_block(out_channels, kernel_size, strides, padding):
    return nn.Sequential([
        nn.Conv(out_channels, kernel_size, strides, padding),
        nn.relu,
        nn.Conv(out_channels, kernel_size=(1, 1)), nn.relu,
        nn.Conv(out_channels, kernel_size=(1, 1)), nn.relu])
</code></pre>
<h2 id="nin-모델"><a class="header" href="#nin-모델">[<strong>NiN 모델</strong>]</a></h2>
<p>NiN은 AlexNet과 동일한 초기 합성곱 크기를 사용합니다(그 직후에 제안되었습니다).
커널 크기는 각각 $11\times 11$, $5\times 5$, $3\times 3$이며 출력 채널 수는 AlexNet과 일치합니다. 각 NiN 블록 뒤에는 스트라이드 2와 윈도우 모양 $3\times 3$인 최대 풀링 레이어가 이어집니다.</p>
<p>NiN과 AlexNet 및 VGG의 두 번째 중요한 차이점은 NiN이 완전 연결 레이어를 완전히 피한다는 것입니다.
대신 NiN은 레이블 클래스 수와 동일한 출력 채널 수를 가진 NiN 블록을 사용하고 <em>전역</em> 평균 풀링 레이어가 이어져 로짓 벡터를 산출합니다.
이 설계는 잠재적으로 훈련 시간이 증가하는 대신 필요한 모델 파라미터 수를 크게 줄입니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class NiN(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.net = nn.Sequential()
            self.net.add(
                nin_block(96, kernel_size=11, strides=4, padding=0),
                nn.MaxPool2D(pool_size=3, strides=2),
                nin_block(256, kernel_size=5, strides=1, padding=2),
                nn.MaxPool2D(pool_size=3, strides=2),
                nin_block(384, kernel_size=3, strides=1, padding=1),
                nn.MaxPool2D(pool_size=3, strides=2),
                nn.Dropout(0.5),
                nin_block(num_classes, kernel_size=3, strides=1, padding=1),
                nn.GlobalAvgPool2D(),
                nn.Flatten())
            self.net.initialize(init.Xavier())
        if tab.selected('pytorch'):
            self.net = nn.Sequential(
                nin_block(96, kernel_size=11, strides=4, padding=0),
                nn.MaxPool2d(3, stride=2),
                nin_block(256, kernel_size=5, strides=1, padding=2),
                nn.MaxPool2d(3, stride=2),
                nin_block(384, kernel_size=3, strides=1, padding=1),
                nn.MaxPool2d(3, stride=2),
                nn.Dropout(0.5),
                nin_block(num_classes, kernel_size=3, strides=1, padding=1),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten())
            self.net.apply(d2l.init_cnn)
        if tab.selected('tensorflow'):
            self.net = tf.keras.models.Sequential([
                nin_block(96, kernel_size=11, strides=4, padding='valid'),
                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),
                nin_block(256, kernel_size=5, strides=1, padding='same'),
                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),
                nin_block(384, kernel_size=3, strides=1, padding='same'),
                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),
                tf.keras.layers.Dropout(0.5),
                nin_block(num_classes, kernel_size=3, strides=1, padding='same'),
                tf.keras.layers.GlobalAvgPool2D(),
                tf.keras.layers.Flatten()])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class NiN(d2l.Classifier):
    lr: float = 0.1
    num_classes = 10
    training: bool = True

    def setup(self):
        self.net = nn.Sequential([
            nin_block(96, kernel_size=(11, 11), strides=(4, 4), padding=(0, 0)),
            lambda x: nn.max_pool(x, (3, 3), strides=(2, 2)),
            nin_block(256, kernel_size=(5, 5), strides=(1, 1), padding=(2, 2)),
            lambda x: nn.max_pool(x, (3, 3), strides=(2, 2)),
            nin_block(384, kernel_size=(3, 3), strides=(1, 1), padding=(1, 1)),
            lambda x: nn.max_pool(x, (3, 3), strides=(2, 2)),
            nn.Dropout(0.5, deterministic=not self.training),
            nin_block(self.num_classes, kernel_size=(3, 3), strides=1, padding=(1, 1)),
            lambda x: nn.avg_pool(x, (5, 5)),  # global avg pooling
            lambda x: x.reshape((x.shape[0], -1))  # flatten
        ])
</code></pre>
<p>우리는 [<strong>각 블록의 출력 모양</strong>]을 보기 위해 데이터 예제를 생성합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
NiN().layer_summary((1, 1, 224, 224))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
NiN().layer_summary((1, 224, 224, 1))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
NiN(training=False).layer_summary((1, 224, 224, 1))
</code></pre>
<h2 id="훈련-training-11"><a class="header" href="#훈련-training-11">[<strong>훈련 (Training)</strong>]</a></h2>
<p>이전과 마찬가지로 Fashion-MNIST를 사용하여 AlexNet 및 VGG에 사용했던 것과 동일한 최적화기로 모델을 훈련합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
model = NiN(lr=0.05)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
if tab.selected('pytorch'):
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
with d2l.try_gpu():
    model = NiN(lr=0.05)
    trainer.fit(model, data)
</code></pre>
<h2 id="요약-summary-29"><a class="header" href="#요약-summary-29">요약 (Summary)</a></h2>
<p>NiN은 AlexNet 및 VGG보다 파라미터 수가 훨씬 적습니다. 이는 주로 거대한 완전 연결 레이어가 필요하지 않기 때문입니다. 대신 네트워크 본문의 마지막 단계 이후 모든 이미지 위치에 걸쳐 집계하기 위해 전역 평균 풀링을 사용합니다. 이것은 비용이 많이 드는 (학습된) 축소 연산의 필요성을 없애고 단순 평균으로 대체합니다. 당시 연구자들을 놀라게 했던 것은 이 평균화 연산이 정확도에 해를 끼치지 않는다는 사실이었습니다. (많은 채널을 가진) 저해상도 표현에 걸친 평균화는 네트워크가 처리할 수 있는 평행 이동 불변성의 양도 추가한다는 점에 유의하십시오.</p>
<p>넓은 커널을 가진 더 적은 합성곱을 선택하고 이를 $1 \times 1$ 합성곱으로 대체하는 것은 더 적은 파라미터를 향한 탐구를 더욱 돕습니다. 주어진 위치 내의 채널 전반에 걸쳐 상당한 양의 비선형성을 제공할 수 있습니다. $1 \times 1$ 합성곱과 전역 평균 풀링 모두 후속 CNN 설계에 큰 영향을 미쳤습니다.</p>
<h2 id="연습-문제-exercises-36"><a class="header" href="#연습-문제-exercises-36">연습 문제 (Exercises)</a></h2>
<ol>
<li>NiN 블록당 두 개의 $1\times 1$ 합성곱 레이어가 있는 이유는 무엇입니까? 그 수를 3개로 늘리십시오. 1개로 줄이십시오. 무엇이 변경됩니까?</li>
<li>$1 \times 1$ 합성곱을 $3 \times 3$ 합성곱으로 대체하면 어떻게 변경됩니까?</li>
<li>전역 평균 풀링을 완전 연결 레이어로 대체하면 어떻게 됩니까(속도, 정확도, 파라미터 수)?</li>
<li>NiN의 리소스 사용량을 계산하십시오.
<ol>
<li>파라미터 수는 얼마입니까?</li>
<li>계산량은 얼마입니까?</li>
<li>훈련 중 필요한 메모리 양은 얼마입니까?</li>
<li>예측 중 필요한 메모리 양은 얼마입니까?</li>
</ol>
</li>
<li>$384 \times 5 \times 5$ 표현을 $10 \times 5 \times 5$ 표현으로 한 번에 줄이는 것의 가능한 문제는 무엇입니까?</li>
<li>VGG-11, VGG-16, VGG-19로 이어진 VGG의 구조적 설계 결정을 사용하여 NiN과 유사한 네트워크 패밀리를 설계하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/79">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/80">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18003">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="다중-분기-네트워크-googlenet-multi-branch-networks-googlenet"><a class="header" href="#다중-분기-네트워크-googlenet-multi-branch-networks-googlenet">다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></h1>
<p>:label:<code>sec_googlenet</code></p>
<p>2014년, <em>GoogLeNet</em>은 NiN :cite:<code>Lin.Chen.Yan.2013</code>의 강점, 반복 블록 :cite:<code>Simonyan.Zisserman.2014</code>, 합성곱 커널의 칵테일을 결합한 구조를 사용하여 ImageNet 챌린지에서 우승했습니다 :cite:<code>Szegedy.Liu.Jia.ea.2015</code>.
이것은 틀림없이 CNN에서 스템(데이터 수집), 바디(데이터 처리), 헤드(예측) 간의 명확한 구분을 보인 최초의 네트워크이기도 했습니다.
이 디자인 패턴은 이후 딥 네트워크 설계에서 지속되었습니다: *스템(stem)*은 이미지에 작동하는 처음 두세 개의 합성곱으로 주어집니다. 그들은 기본 이미지에서 하위 수준 특성을 추출합니다. 그 뒤를 이어 합성곱 블록의 *바디(body)*가 나옵니다. 마지막으로 *헤드(head)*는 지금까지 얻은 특성을 당면한 필수 분류, 분할, 감지 또는 추적 문제로 매핑합니다.</p>
<p>GoogLeNet의 주요 기여는 네트워크 바디의 설계였습니다.
그것은 독창적인 방식으로 합성곱 커널 선택 문제를 해결했습니다.
다른 연구들은 $1 \times 1$에서 $11 \times 11$까지 어떤 합성곱이 가장 좋을지 식별하려고 시도했지만, 이것은 단순히 다중 분기 합성곱을 *연결(concatenated)*했습니다.
다음에서는 약간 단순화된 버전의 GoogLeNet을 소개합니다: 원래 설계에는 네트워크의 여러 레이어에 적용된 중간 손실 함수를 통해 훈련을 안정화하기 위한 여러 가지 트릭이 포함되어 있었습니다.
향상된 훈련 알고리즘의 가용성으로 인해 더 이상 필요하지 않습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx, init
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
from d2l import tensorflow as d2l
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
import jax
</code></pre>
<h2 id="inception-블록"><a class="header" href="#inception-블록">(<strong>Inception 블록</strong>)</a></h2>
<p>GoogLeNet의 기본 합성곱 블록은 영화 *인셉션(Inception)*의 밈 "우리는 더 깊이 들어가야 해(we need to go deeper)"에서 유래한 <em>Inception 블록</em>이라고 합니다.</p>
<p><img src="chapter_convolutional-modern/../img/inception.svg" alt="Inception 블록의 구조." />
:label:<code>fig_inception</code></p>
<p>:numref:<code>fig_inception</code>에 묘사된 것처럼, Inception 블록은 4개의 병렬 분기로 구성됩니다.
처음 세 분기는 $1\times 1$, $3\times 3$, $5\times 5$의 윈도우 크기를 가진 합성곱 레이어를 사용하여 다양한 공간 크기에서 정보를 추출합니다.
가운데 두 분기는 입력의 $1\times 1$ 합성곱도 추가하여 채널 수를 줄여 모델의 복잡성을 줄입니다.
네 번째 분기는 $3\times 3$ 최대 풀링 레이어를 사용하고, 채널 수를 변경하기 위해 $1\times 1$ 합성곱 레이어가 뒤따릅니다.
4개의 분기는 모두 입력과 출력의 높이와 너비가 같도록 적절한 패딩을 사용합니다.
마지막으로 각 분기의 출력은 채널 차원을 따라 연결되어 블록의 출력을 구성합니다.
Inception 블록의 일반적으로 조정되는 하이퍼파라미터는 레이어당 출력 채널 수, 즉 다른 크기의 합성곱 간에 용량을 할당하는 방법입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class Inception(nn.Block):
    # c1--c4는 각 분기의 출력 채널 수입니다
    def __init__(self, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 분기 1
        self.b1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')
        # 분기 2
        self.b2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')
        self.b2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,
                              activation='relu')
        # 분기 3
        self.b3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')
        self.b3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,
                              activation='relu')
        # 분기 4
        self.b4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)
        self.b4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')

    def forward(self, x):
        b1 = self.b1_1(x)
        b2 = self.b2_2(self.b2_1(x))
        b3 = self.b3_2(self.b3_1(x))
        b4 = self.b4_2(self.b4_1(x))
        return np.concatenate((b1, b2, b3, b4), axis=1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class Inception(nn.Module):
    # c1--c4는 각 분기의 출력 채널 수입니다
    def __init__(self, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 분기 1
        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)
        # 분기 2
        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)
        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)
        # 분기 3
        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)
        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)
        # 분기 4
        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)

    def forward(self, x):
        b1 = F.relu(self.b1_1(x))
        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))
        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))
        b4 = F.relu(self.b4_2(self.b4_1(x)))
        return torch.cat((b1, b2, b3, b4), dim=1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class Inception(tf.keras.Model):
    # c1--c4는 각 분기의 출력 채널 수입니다
    def __init__(self, c1, c2, c3, c4):
        super().__init__()
        self.b1_1 = tf.keras.layers.Conv2D(c1, 1, activation='relu')
        self.b2_1 = tf.keras.layers.Conv2D(c2[0], 1, activation='relu')
        self.b2_2 = tf.keras.layers.Conv2D(c2[1], 3, padding='same',
                                           activation='relu')
        self.b3_1 = tf.keras.layers.Conv2D(c3[0], 1, activation='relu')
        self.b3_2 = tf.keras.layers.Conv2D(c3[1], 5, padding='same',
                                           activation='relu')
        self.b4_1 = tf.keras.layers.MaxPool2D(3, 1, padding='same')
        self.b4_2 = tf.keras.layers.Conv2D(c4, 1, activation='relu')

    def call(self, x):
        b1 = self.b1_1(x)
        b2 = self.b2_2(self.b2_1(x))
        b3 = self.b3_2(self.b3_1(x))
        b4 = self.b4_2(self.b4_1(x))
        return tf.keras.layers.Concatenate()([b1, b2, b3, b4])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Inception(nn.Module):
    # `c1`--`c4`는 각 분기의 출력 채널 수입니다
    c1: int
    c2: tuple
    c3: tuple
    c4: int

    def setup(self):
        # 분기 1
        self.b1_1 = nn.Conv(self.c1, kernel_size=(1, 1))
        # 분기 2
        self.b2_1 = nn.Conv(self.c2[0], kernel_size=(1, 1))
        self.b2_2 = nn.Conv(self.c2[1], kernel_size=(3, 3), padding='same')
        # 분기 3
        self.b3_1 = nn.Conv(self.c3[0], kernel_size=(1, 1))
        self.b3_2 = nn.Conv(self.c3[1], kernel_size=(5, 5), padding='same')
        # 분기 4
        self.b4_1 = lambda x: nn.max_pool(x, window_shape=(3, 3),
                                          strides=(1, 1), padding='same')
        self.b4_2 = nn.Conv(self.c4, kernel_size=(1, 1))

    def __call__(self, x):
        b1 = nn.relu(self.b1_1(x))
        b2 = nn.relu(self.b2_2(nn.relu(self.b2_1(x))))
        b3 = nn.relu(self.b3_2(nn.relu(self.b3_1(x))))
        b4 = nn.relu(self.b4_2(self.b4_1(x)))
        return jnp.concatenate((b1, b2, b3, b4), axis=-1)
</code></pre>
<p>이 네트워크가 왜 그렇게 잘 작동하는지에 대한 직관을 얻으려면 필터의 조합을 고려하십시오.
그들은 다양한 필터 크기에서 이미지를 탐색합니다.
이는 다른 범위의 세부 사항이 다른 크기의 필터에 의해 효율적으로 인식될 수 있음을 의미합니다.
동시에 우리는 다른 필터에 대해 다른 양의 파라미터를 할당할 수 있습니다.</p>
<h2 id="googlenet-모델"><a class="header" href="#googlenet-모델">[<strong>GoogLeNet 모델</strong>]</a></h2>
<p>:numref:<code>fig_inception_full</code>에 표시된 것처럼, GoogLeNet은 총 9개의 Inception 블록 스택을 사용하며, 그 사이에 최대 풀링이 있는 세 그룹으로 배열되고, 추정치를 생성하기 위해 헤드에 전역 평균 풀링이 있습니다.
Inception 블록 사이의 최대 풀링은 차원을 줄입니다.
스템의 첫 번째 모듈은 AlexNet 및 LeNet과 유사합니다.</p>
<p><img src="chapter_convolutional-modern/../img/inception-full-90.svg" alt="GoogLeNet 아키텍처." />
:label:<code>fig_inception_full</code></p>
<p>이제 GoogLeNet을 하나씩 구현할 수 있습니다. 스템부터 시작해 봅시다.
첫 번째 모듈은 64채널 $7\times 7$ 합성곱 레이어를 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class GoogleNet(d2l.Classifier):
    def b1(self):
        if tab.selected('mxnet'):
            net = nn.Sequential()
            net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3,
                              activation='relu'),
                    nn.MaxPool2D(pool_size=3, strides=2, padding=1))
            return net
        if tab.selected('pytorch'):
            return nn.Sequential(
                nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
                nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        if tab.selected('tensorflow'):
            return tf.keras.models.Sequential([
                tf.keras.layers.Conv2D(64, 7, strides=2, padding='same',
                                       activation='relu'),
                tf.keras.layers.MaxPool2D(pool_size=3, strides=2,
                                          padding='same')])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class GoogleNet(d2l.Classifier):
    lr: float = 0.1
    num_classes: int = 10

    def setup(self):
        self.net = nn.Sequential([self.b1(), self.b2(), self.b3(), self.b4(),
                                  self.b5(), nn.Dense(self.num_classes)])

    def b1(self):
        return nn.Sequential([
                nn.Conv(64, kernel_size=(7, 7), strides=(2, 2), padding='same'),
                nn.relu,
                lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2),
                                      padding='same')])
</code></pre>
<p>두 번째 모듈은 두 개의 합성곱 레이어를 사용합니다:
먼저, 64채널 $1\times 1$ 합성곱 레이어,
그다음 채널 수를 세 배로 늘리는 $3\times 3$ 합성곱 레이어입니다. 이것은 Inception 블록의 두 번째 분기에 해당하며 바디 설계를 마칩니다. 이 시점에서 우리는 192개의 채널을 갖게 됩니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(GoogleNet)
def b2(self):
    if tab.selected('mxnet'):
        net = nn.Sequential()
        net.add(nn.Conv2D(64, kernel_size=1, activation='relu'),
               nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),
               nn.MaxPool2D(pool_size=3, strides=2, padding=1))
        return net
    if tab.selected('pytorch'):
        return nn.Sequential(
            nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),
            nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
    if tab.selected('tensorflow'):
        return tf.keras.Sequential([
            tf.keras.layers.Conv2D(64, 1, activation='relu'),
            tf.keras.layers.Conv2D(192, 3, padding='same', activation='relu'),
            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])
    if tab.selected('jax'):
        return nn.Sequential([nn.Conv(64, kernel_size=(1, 1)),
                              nn.relu,
                              nn.Conv(192, kernel_size=(3, 3), padding='same'),
                              nn.relu,
                              lambda x: nn.max_pool(x, window_shape=(3, 3),
                                                    strides=(2, 2),
                                                    padding='same')])
</code></pre>
<p>세 번째 모듈은 두 개의 완전한 Inception 블록을 직렬로 연결합니다.
첫 번째 Inception 블록의 출력 채널 수는 $64+128+32+32=256$입니다.
이는 네 분기 간의 출력 채널 수 비율이 $2:4:1:1$임을 의미합니다. 이를 달성하기 위해 두 번째와 세 번째 분기에서 입력 차원을 각각 $rac{1}{2}$과 $rac{1}{12}$로 줄여 각각 $96 = 192/2$ 및 $16 = 192/12$ 채널에 도달합니다.</p>
<p>두 번째 Inception 블록의 출력 채널 수는 $128+192+96+64=480$으로 증가하여 $128:192:96:64 = 4:6:3:2$의 비율을 산출합니다. 이전과 마찬가지로
두 번째와 세 번째 채널의 중간 차원 수를 줄여야 합니다.
각각 $rac{1}{2}$과 $rac{1}{8}$의 스케일로 충분하며, 각각 $128$과 $32$ 채널을 산출합니다. 이것은 다음 <code>Inception</code> 블록 생성자의 인수에 의해 캡처됩니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(GoogleNet)
def b3(self):
    if tab.selected('mxnet'):
        net = nn.Sequential()
        net.add(Inception(64, (96, 128), (16, 32), 32),
               Inception(128, (128, 192), (32, 96), 64),
               nn.MaxPool2D(pool_size=3, strides=2, padding=1))
        return net
    if tab.selected('pytorch'):
        return nn.Sequential(Inception(64, (96, 128), (16, 32), 32),
                             Inception(128, (128, 192), (32, 96), 64),
                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
    if tab.selected('tensorflow'):
        return tf.keras.models.Sequential([
            Inception(64, (96, 128), (16, 32), 32),
            Inception(128, (128, 192), (32, 96), 64),
            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])
    if tab.selected('jax'):
        return nn.Sequential([Inception(64, (96, 128), (16, 32), 32),
                              Inception(128, (128, 192), (32, 96), 64),
                              lambda x: nn.max_pool(x, window_shape=(3, 3),
                                                    strides=(2, 2),
                                                    padding='same')])
</code></pre>
<p>네 번째 모듈은 더 복잡합니다.
5개의 Inception 블록을 직렬로 연결하며, 각각 $192+208+48+64=512$, $160+224+64+64=512$, $128+256+64+64=512$, $112+288+64+64=528$, $256+320+128+128=832$ 출력 채널을 갖습니다.
이 분기들에 할당된 채널 수는 세 번째 모듈의 것과 유사합니다:
$3\times 3$ 합성곱 레이어가 있는 두 번째 분기가 가장 많은 수의 채널을 출력하고,
$1\times 1$ 합성곱 레이어만 있는 첫 번째 분기,
$5\times 5$ 합성곱 레이어가 있는 세 번째 분기,
$3\times 3$ 최대 풀링 레이어가 있는 네 번째 분기가 그 뒤를 따릅니다.
두 번째와 세 번째 분기는 비율에 따라 채널 수를 먼저 줄입니다.
이 비율은 다른 Inception 블록에서 약간 다릅니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(GoogleNet)
def b4(self):
    if tab.selected('mxnet'):
        net = nn.Sequential()
        net.add(Inception(192, (96, 208), (16, 48), 64),
                Inception(160, (112, 224), (24, 64), 64),
                Inception(128, (128, 256), (24, 64), 64),
                Inception(112, (144, 288), (32, 64), 64),
                Inception(256, (160, 320), (32, 128), 128),
                nn.MaxPool2D(pool_size=3, strides=2, padding=1))
        return net
    if tab.selected('pytorch'):
        return nn.Sequential(Inception(192, (96, 208), (16, 48), 64),
                             Inception(160, (112, 224), (24, 64), 64),
                             Inception(128, (128, 256), (24, 64), 64),
                             Inception(112, (144, 288), (32, 64), 64),
                             Inception(256, (160, 320), (32, 128), 128),
                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
    if tab.selected('tensorflow'):
        return tf.keras.Sequential([
            Inception(192, (96, 208), (16, 48), 64),
            Inception(160, (112, 224), (24, 64), 64),
            Inception(128, (128, 256), (24, 64), 64),
            Inception(112, (144, 288), (32, 64), 64),
            Inception(256, (160, 320), (32, 128), 128),
            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])
    if tab.selected('jax'):
        return nn.Sequential([Inception(192, (96, 208), (16, 48), 64),
                              Inception(160, (112, 224), (24, 64), 64),
                              Inception(128, (128, 256), (24, 64), 64),
                              Inception(112, (144, 288), (32, 64), 64),
                              Inception(256, (160, 320), (32, 128), 128),
                              lambda x: nn.max_pool(x, window_shape=(3, 3),
                                                    strides=(2, 2),
                                                    padding='same')])
</code></pre>
<p>다섯 번째 모듈은 $256+320+128+128=832$ 및 $384+384+128+128=1024$ 출력 채널을 가진 두 개의 Inception 블록을 갖습니다.
각 분기에 할당된 채널 수는 세 번째 및 네 번째 모듈의 채널 수와 동일하지만 특정 값은 다릅니다.
다섯 번째 블록 뒤에는 출력 레이어가 뒤따른다는 점에 유의해야 합니다.
이 블록은 NiN에서와 같이 전역 평균 풀링 레이어를 사용하여 각 채널의 높이와 너비를 1로 변경합니다.
마지막으로 우리는 출력을 2차원 배열로 바꾼 다음 출력 수가 레이블 클래스 수인 완전 연결 레이어가 이어집니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(GoogleNet)
def b5(self):
    if tab.selected('mxnet'):
        net = nn.Sequential()
        net.add(Inception(256, (160, 320), (32, 128), 128),
                Inception(384, (192, 384), (48, 128), 128),
                nn.GlobalAvgPool2D())
        return net
    if tab.selected('pytorch'):
        return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),
                             Inception(384, (192, 384), (48, 128), 128),
                             nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())
    if tab.selected('tensorflow'):
        return tf.keras.Sequential([
            Inception(256, (160, 320), (32, 128), 128),
            Inception(384, (192, 384), (48, 128), 128),
            tf.keras.layers.GlobalAvgPool2D(),
            tf.keras.layers.Flatten()])
    if tab.selected('jax'):
        return nn.Sequential([Inception(256, (160, 320), (32, 128), 128),
                              Inception(384, (192, 384), (48, 128), 128),
                              # Flax는 GlobalAvgPool2D 레이어를 제공하지 않습니다
                              lambda x: nn.avg_pool(x,
                                                    window_shape=x.shape[1:3],
                                                    strides=x.shape[1:3],
                                                    padding='valid'),
                              lambda x: x.reshape((x.shape[0], -1))])
</code></pre>
<p>이제 모든 블록 <code>b1</code>부터 <code>b5</code>까지 정의했으므로, 이들을 전체 네트워크로 조립하는 일만 남았습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(GoogleNet)
def __init__(self, lr=0.1, num_classes=10):
    super(GoogleNet, self).__init__()
    self.save_hyperparameters()
    if tab.selected('mxnet'):
        self.net = nn.Sequential()
        self.net.add(self.b1(), self.b2(), self.b3(), self.b4(), self.b5(),
                     nn.Dense(num_classes))
        self.net.initialize(init.Xavier())
    if tab.selected('pytorch'):
        self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),
                                 self.b5(), nn.LazyLinear(num_classes))
        self.net.apply(d2l.init_cnn)
    if tab.selected('tensorflow'):
        self.net = tf.keras.Sequential([
            self.b1(), self.b2(), self.b3(), self.b4(), self.b5(),
            tf.keras.layers.Dense(num_classes)])
</code></pre>
<p>GoogLeNet 모델은 계산적으로 복잡합니다.
선택된 채널 수, 차원 축소 전의 블록 수, 채널 전체의 상대적 용량 분할 등의 측면에서 비교적 임의적인 하이퍼파라미터가 많다는 점에 유의하십시오. 대부분은 GoogLeNet이 도입될 당시에는 네트워크 정의나 설계 탐색을 위한 자동 도구를 아직 사용할 수 없었다는 사실 때문입니다. 예를 들어, 이제 우리는 유능한 딥러닝 프레임워크가 입력 텐서의 차원을 자동으로 추론할 수 있다는 것을 당연하게 여깁니다. 당시에는 이러한 많은 구성을 실험자가 명시적으로 지정해야 했기 때문에 활발한 실험이 느려지는 경우가 많았습니다. 더욱이 자동 탐색에 필요한 도구는 여전히 유동적이었고 초기 실험은 주로 비용이 많이 드는 무차별 대입 탐색, 유전 알고리즘 및 유사한 전략에 해당했습니다.</p>
<p>지금은 [<strong>Fashion-MNIST에서 합리적인 훈련 시간을 갖기 위해 입력 높이와 너비를 224에서 96으로 줄이는</strong>] 수정만 수행할 것입니다.
이것은 계산을 단순화합니다. 다양한 모듈 간의 출력 모양 변화를 살펴봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
model = GoogleNet().layer_summary((1, 1, 96, 96))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow, jax
model = GoogleNet().layer_summary((1, 96, 96, 1))
</code></pre>
<h2 id="훈련-training-12"><a class="header" href="#훈련-training-12">[<strong>훈련 (Training)</strong>]</a></h2>
<p>이전과 마찬가지로 Fashion-MNIST 데이터셋을 사용하여 모델을 훈련합니다.
훈련 절차를 호출하기 전에 $96 \times 96$ 픽셀 해상도로 변환합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
model = GoogleNet(lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
if tab.selected('pytorch'):
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
with d2l.try_gpu():
    model = GoogleNet(lr=0.01)
    trainer.fit(model, data)
</code></pre>
<h2 id="토론-discussion-2"><a class="header" href="#토론-discussion-2">토론 (Discussion)</a></h2>
<p>GoogLeNet의 주요 특징은 이전 모델보다 계산 비용이 <em>저렴</em>하면서도 동시에 향상된 정확도를 제공한다는 것입니다. 이것은 오류 감소와 네트워크 평가 비용을 절충하는 훨씬 더 신중한 네트워크 설계의 시작을 알립니다. 또한 당시에는 완전히 수동적이었지만 네트워크 설계 하이퍼파라미터를 사용하여 블록 수준에서 실험을 시작한 것을 의미합니다. 우리는 네트워크 구조 탐색 전략을 논의할 때 :numref:<code>sec_cnn-design</code>에서 이 주제를 다시 다룰 것입니다.</p>
<p>다음 섹션에서는 네트워크를 크게 개선할 수 있는 여러 가지 설계 선택(예: 배치 정규화, 잔차 연결, 채널 그룹화)을 접하게 될 것입니다. 지금으로서는 틀림없이 최초의 진정한 현대적 CNN을 구현했다는 사실에 자부심을 가질 수 있습니다.</p>
<h2 id="연습-문제-exercises-37"><a class="header" href="#연습-문제-exercises-37">연습 문제 (Exercises)</a></h2>
<ol>
<li>GoogLeNet은 매우 성공적이어서 속도와 정확도를 점진적으로 개선하는 여러 번의 반복을 거쳤습니다. 그중 일부를 구현하고 실행해 보십시오. 여기에는 다음이 포함됩니다:
<ol>
<li>나중에 :numref:<code>sec_batch_norm</code>에서 설명하는 대로 배치 정규화 레이어 :cite:<code>Ioffe.Szegedy.2015</code>를 추가합니다.</li>
<li>:citet:<code>Szegedy.Vanhoucke.Ioffe.ea.2016</code>에 설명된 대로 Inception 블록(너비, 합성곱의 선택 및 순서)을 조정합니다.</li>
<li>:citet:<code>Szegedy.Vanhoucke.Ioffe.ea.2016</code>에 설명된 대로 모델 정규화를 위해 레이블 스무딩(label smoothing)을 사용합니다.</li>
<li>나중에 :numref:<code>sec_resnet</code>에서 설명하는 대로 잔차 연결 :cite:<code>Szegedy.Ioffe.Vanhoucke.ea.2017</code>을 추가하여 Inception 블록을 추가로 조정합니다.</li>
</ol>
</li>
<li>GoogLeNet이 작동하는 데 필요한 최소 이미지 크기는 얼마입니까?</li>
<li>Fashion-MNIST의 기본 해상도인 $28 \times 28$ 픽셀에서 작동하는 GoogLeNet 변형을 설계할 수 있습니까? 네트워크의 스템, 바디, 헤드를 변경해야 한다면 어떻게 변경해야 합니까?</li>
<li>AlexNet, VGG, NiN, GoogLeNet의 모델 파라미터 크기를 비교하십시오. 후자의 두 네트워크 아키텍처는 어떻게 모델 파라미터 크기를 크게 줄입니까?</li>
<li>GoogLeNet과 AlexNet에 필요한 계산량을 비교하십시오. 이것이 가속기 칩 설계(예: 메모리 크기, 메모리 대역폭, 캐시 크기, 계산량, 특수 연산의 이점 측면에서)에 어떤 영향을 미칩니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/81">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/82">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/316">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18004">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="배치-정규화-batch-normalization"><a class="header" href="#배치-정규화-batch-normalization">배치 정규화 (Batch Normalization)</a></h1>
<p>:label:<code>sec_batch_norm</code></p>
<p>심층 신경망을 훈련하는 것은 어렵습니다.
합리적인 시간 내에 수렴하도록 만드는 것은 까다로울 수 있습니다.
이 섹션에서는 심층 네트워크의 수렴을 일관되게 가속화하는 인기 있고 효과적인 기술인 *배치 정규화(batch normalization)*를 설명합니다 :cite:<code>Ioffe.Szegedy.2015</code>.
나중에 :numref:<code>sec_resnet</code>에서 다룰 잔차 블록과 함께 배치 정규화는 실무자들이 100개 이상의 레이어를 가진 네트워크를 일상적으로 훈련할 수 있게 만들었습니다.
배치 정규화의 부차적인(우연한) 이점은 고유한 정규화(regularization)에 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, init
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from functools import partial
from jax import numpy as jnp
import jax
import optax
</code></pre>
<h2 id="심층-네트워크-훈련-training-deep-networks"><a class="header" href="#심층-네트워크-훈련-training-deep-networks">심층 네트워크 훈련 (Training Deep Networks)</a></h2>
<p>데이터로 작업할 때 우리는 종종 훈련 전에 전처리를 합니다.
데이터 전처리에 관한 선택은 종종 최종 결과에 엄청난 차이를 만듭니다.
주택 가격 예측(:numref:<code>sec_kaggle_house</code>)에 MLP를 적용했던 것을 상기해 보십시오.
실제 데이터로 작업할 때 첫 번째 단계는 여러 관찰에 걸쳐 입력 특성을 평균이 0($\boldsymbol{\mu} = 0$)이고 분산이 1($\boldsymbol{\Sigma} = \boldsymbol{1}$)이 되도록 표준화하는 것이었습니다 :cite:<code>friedman1987exploratory</code>. 대각선이 1이 되도록, 즉 $\Sigma_{ii} = 1$이 되도록 후자를 재조정하는 경우가 많습니다.
또 다른 전략은 벡터를 단위 길이로, 가능하면 <em>관찰당</em> 평균이 0이 되도록 재조정하는 것입니다.
이것은 공간 센서 데이터 등에 잘 작동할 수 있습니다. 이러한 전처리 기술과 다른 많은 기술들은 추정 문제를 잘 통제하는 데 유익합니다.
특성 선택 및 추출에 대한 검토는 예를 들어 :citet:<code>guyon2008feature</code>의 기사를 참조하십시오.
벡터를 표준화하는 것은 그것에 작용하는 함수의 함수 복잡도를 제한하는 좋은 부작용도 있습니다. 예를 들어 서포트 벡터 머신의 유명한 반지름-마진 경계(radius-margin bound) :cite:<code>Vapnik95</code>와 퍼셉트론 수렴 정리(Perceptron Convergence Theorem) :cite:<code>Novikoff62</code>는 경계가 있는 노름의 입력에 의존합니다.</p>
<p>직관적으로 이 표준화는 파라미터를 <em>사전적으로</em> 비슷한 스케일에 놓기 때문에 최적화기와 잘 어울립니다.
따라서 심층 네트워크 <em>내부</em>에 해당하는 정규화 단계가 유익하지 않을까 묻는 것은 자연스러운 일입니다. 이것이 배치 정규화 :cite:<code>Ioffe.Szegedy.2015</code>의 발명으로 이어진 추론은 아니지만, 통일된 프레임워크 내에서 그것과 사촌 격인 레이어 정규화 :cite:<code>Ba.Kiros.Hinton.2016</code>를 이해하는 유용한 방법입니다.</p>
<p>둘째, 일반적인 MLP나 CNN의 경우 훈련함에 따라 중간 레이어의 변수(예: MLP의 아핀 변환 출력)는
입력에서 출력까지의 레이어를 따라, 동일한 레이어의 유닛 전체에 걸쳐, 그리고 모델 파라미터 업데이트로 인해 시간이 지남에 따라 크기가 매우 다양할 수 있습니다.
배치 정규화의 발명가들은 이러한 변수 분포의 이동이 네트워크의 수렴을 방해할 수 있다고 비공식적으로 가정했습니다.
직관적으로 한 레이어의 변수 활성화가 다른 레이어의 100배라면 학습률을 보상적으로 조정해야 할 수 있다고 추측할 수 있습니다. AdaGrad :cite:<code>Duchi.Hazan.Singer.2011</code>, Adam :cite:<code>Kingma.Ba.2014</code>, Yogi :cite:<code>Zaheer.Reddi.Sachan.ea.2018</code> 또는 Distributed Shampoo :cite:<code>anil2020scalable</code>와 같은 적응형 솔버는 최적화 관점에서 이를 해결하려고 합니다(예: 2차 방법의 측면을 추가하여).
대안은 단순히 적응형 정규화를 통해 문제가 발생하는 것을 방지하는 것입니다.</p>
<p>셋째, 더 깊은 네트워크는 복잡하고 과대적합되기 쉬운 경향이 있습니다.
이는 정규화(regularization)가 더 중요해진다는 것을 의미합니다. 정규화를 위한 일반적인 기술은 노이즈 주입입니다. 이것은 오랫동안 알려져 왔습니다(예: 입력에 대한 노이즈 주입 :cite:<code>Bishop.1995</code>). 또한 :numref:<code>sec_dropout</code>의 드롭아웃의 기초를 형성합니다. 아주 우연히도 배치 정규화는 전처리, 수치적 안정성, 정규화라는 세 가지 이점을 모두 전달합니다.</p>
<p>배치 정규화는 개별 레이어 또는 선택적으로 모든 레이어에 적용됩니다:
각 훈련 반복에서 우리는 먼저 (배치 정규화의) 입력의 평균을 빼고 표준 편차로 나누어 정규화합니다.
여기서 둘 다 현재 미니배치의 통계를 기반으로 추정됩니다.
다음으로 손실된 자유도를 복구하기 위해 스케일 계수와 오프셋을 적용합니다. <em>배치</em> 통계를 기반으로 한 이 <em>정규화</em> 때문에 <em>배치 정규화</em>라는 이름이 유래되었습니다.</p>
<p>크기 1의 미니배치로 배치 정규화를 적용하려고 하면 아무것도 배울 수 없다는 점에 유의하십시오.
평균을 뺀 후 각 은닉 유닛이 값 0을 취하기 때문입니다.
짐작하시겠지만, 우리가 배치 정규화에 전체 섹션을 할애하고 있으므로 충분히 큰 미니배치에서는 이 접근 방식이 효과적이고 안정적인 것으로 입증되었습니다.
여기서 얻을 수 있는 한 가지 교훈은 배치 정규화를 적용할 때 배치 크기의 선택이 배치 정규화가 없을 때보다 훨씬 더 중요하다는 것입니다. 또는 적어도 배치 크기를 조정할 때 적절한 보정이 필요합니다.</p>
<p>$\mathcal{B}$를 미니배치라고 하고 $\mathbf{x} \in \mathcal{B}$를 배치 정규화($\textrm{BN}$)에 대한 입력이라고 합시다. 이 경우 배치 정규화는 다음과 같이 정의됩니다:</p>
<p>$$\textrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}<em>\mathcal{B}}{\hat{\boldsymbol{\sigma}}</em>\mathcal{B}} + \boldsymbol{\beta}.$$:eqlabel:<code>eq_batchnorm</code></p>
<p>:eqref:<code>eq_batchnorm</code>에서 $\hat{\boldsymbol{\mu}}<em>\mathcal{B}$는 표본 평균이고 $\hat{\boldsymbol{\sigma}}</em>\mathcal{B}$는 미니배치 $\mathcal{B}$의 표본 표준 편차입니다.
표준화를 적용한 후 결과 미니배치는 평균 0과 단위 분산을 갖습니다.
단위 분산(다른 마법의 숫자 대신)의 선택은 임의적입니다. 우리는 $\mathbf{x}$와 동일한 모양을 가진 요소별 <em>스케일 파라미터</em> $\boldsymbol{\gamma}$와 <em>시프트 파라미터</em> $\boldsymbol{\beta}$를 포함하여 이 자유도를 복구합니다. 둘 다 모델 훈련의 일부로 학습해야 하는 파라미터입니다.</p>
<p>중간 레이어의 변수 크기는 훈련 중에 발산할 수 없습니다. 배치 정규화가 주어진 평균과 크기로 적극적으로 중심을 맞추고 재조정하기 때문입니다($\hat{\boldsymbol{\mu}}<em>\mathcal{B}$ 및 ${\hat{\boldsymbol{\sigma}}</em>\mathcal{B}}$를 통해).
실제 경험에 따르면 특성 재조정에 대해 논의할 때 암시했듯이 배치 정규화는 더 공격적인 학습률을 허용하는 것 같습니다.
우리는 :eqref:<code>eq_batchnorm</code>에서 $\hat{\boldsymbol{\mu}}<em>\mathcal{B}$와 ${\hat{\boldsymbol{\sigma}}</em>\mathcal{B}}$를 다음과 같이 계산합니다:</p>
<p>$$\hat{\boldsymbol{\mu}}<em>\mathcal{B} = \frac{1}{|\mathcal{B}|} \sum</em>{\mathbf{x} \in \mathcal{B}} \mathbf{x}
\textrm{ 그리고 }
\hat{\boldsymbol{\sigma}}<em>\mathcal{B}^2 = \frac{1}{|\mathcal{B}|} \sum</em>{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_\{\mathcal{B}})^2 + \epsilon.$$</p>
<p>경험적 분산 추정치가 매우 작거나 사라질 수 있는 경우에도 0으로 나누는 것을 방지하기 위해 분산 추정치에 작은 상수 $\epsilon &gt; 0$을 더한다는 점에 유의하십시오.
추정치 $\hat{\boldsymbol{\mu}}<em>\mathcal{B}$와 ${\hat{\boldsymbol{\sigma}}</em>\mathcal{B}}$는 평균과 분산의 잡음이 있는 추정치를 사용하여 스케일링 문제에 대응합니다.
이 잡음이 문제라고 생각할 수 있습니다.
반대로 실제로는 유익합니다.</p>
<p>이는 딥러닝에서 반복되는 주제인 것으로 밝혀졌습니다.
아직 이론적으로 잘 특성화되지 않은 이유로, 최적화에서 다양한 노이즈 소스는 종종 더 빠른 훈련과 더 적은 과대적합으로 이어집니다:
이 변동은 일종의 정규화 역할을 하는 것으로 보입니다.
:citet:<code>Teye.Azizpour.Smith.2018</code>와 :citet:<code>Luo.Wang.Shao.ea.2018</code>는 배치 정규화의 속성을 각각 베이지안 사전 확률 및 페널티와 관련시켰습니다.
특히 이것은 배치 정규화가 50-100 범위의 적당한 미니배치 크기에서 가장 잘 작동하는 이유에 대한 수수께끼를 어느 정도 밝혀줍니다.
이 특정 크기의 미니배치는 $\hat{\boldsymbol{\sigma}}$를 통한 스케일 측면과 $\hat{\boldsymbol{\mu}}$를 통한 오프셋 측면 모두에서 레이어당 "적절한 양"의 노이즈를 주입하는 것으로 보입니다: 더 큰 미니배치는 더 안정적인 추정치로 인해 덜 정규화하는 반면, 아주 작은 미니배치는 높은 분산으로 인해 유용한 신호를 파괴합니다. 이 방향을 더 탐구하여 대안적인 유형의 전처리 및 필터링을 고려하면 다른 효과적인 유형의 정규화로 이어질 수 있습니다.</p>
<p>훈련된 모델을 고정하면 전체 데이터셋을 사용하여 평균과 분산을 추정하는 것을 선호할 것이라고 생각할 수 있습니다.
훈련이 완료되면 동일한 이미지가 속해 있는 배치에 따라 다르게 분류되는 것을 원치 않기 때문입니다.
훈련 중에는 모든 데이터 예제에 대한 중간 변수가 모델을 업데이트할 때마다 변경되기 때문에 이러한 정확한 계산은 불가능합니다.
그러나 모델이 훈련되면 전체 데이터셋을 기반으로 각 레이어 변수의 평균과 분산을 계산할 수 있습니다.
실제로 이것은 배치 정규화를 사용하는 모델의 표준 관행입니다;
따라서 배치 정규화 레이어는 <em>훈련 모드</em>(미니배치 통계로 정규화)와 <em>예측 모드</em>(데이터셋 통계로 정규화)에서 다르게 작동합니다.
이 형태에서는 노이즈가 훈련 중에만 주입되는 :numref:<code>sec_dropout</code>의 드롭아웃 정규화 동작과 매우 유사합니다.</p>
<h2 id="배치-정규화-레이어-batch-normalization-layers"><a class="header" href="#배치-정규화-레이어-batch-normalization-layers">배치 정규화 레이어 (Batch Normalization Layers)</a></h2>
<p>완전 연결 레이어와 합성곱 레이어에 대한 배치 정규화 구현은 약간 다릅니다.
배치 정규화와 다른 레이어 간의 한 가지 주요 차이점은 전자가 한 번에 전체 미니배치에서 작동하기 때문에 다른 레이어를 소개할 때처럼 배치 차원을 무시할 수 없다는 것입니다.</p>
<h3 id="완전-연결-레이어-fully-connected-layers"><a class="header" href="#완전-연결-레이어-fully-connected-layers">완전 연결 레이어 (Fully Connected Layers)</a></h3>
<p>완전 연결 레이어에 배치 정규화를 적용할 때 :citet:<code>Ioffe.Szegedy.2015</code>는 원래 논문에서 아핀 변환 후 비선형 활성화 함수 <em>전</em>에 배치 정규화를 삽입했습니다. 나중의 응용 프로그램에서는 활성화 함수 바로 <em>뒤</em>에 배치 정규화를 삽입하는 실험을 했습니다.
완전 연결 레이어에 대한 입력을 $\mathbf{x}$, 아핀 변환을 $\mathbf{W}\mathbf{x} + \mathbf{b}$(가중치 파라미터 $\mathbf{W}$ 및 편향 파라미터 $\mathbf{b}$ 포함), 활성화 함수를 $\phi$로 나타내면, 배치 정규화가 활성화된 완전 연결 레이어 출력 $\mathbf{h}$의 계산을 다음과 같이 표현할 수 있습니다:</p>
<p>$$\mathbf{h} = \phi(\textrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}) ).$$</p>
<p>평균과 분산은 변환이 적용되는 <em>동일한</em> 미니배치에서 계산된다는 것을 기억하십시오.</p>
<h3 id="합성곱-레이어-convolutional-layers-1"><a class="header" href="#합성곱-레이어-convolutional-layers-1">합성곱 레이어 (Convolutional Layers)</a></h3>
<p>마찬가지로 합성곱 레이어에서도 합성곱 후 비선형 활성화 함수 전에 배치 정규화를 적용할 수 있습니다. 완전 연결 레이어의 배치 정규화와의 주요 차이점은 <em>모든 위치에 걸쳐</em> 채널별로 연산을 적용한다는 것입니다. 이것은 합성곱으로 이어진 평행 이동 불변성 가정과 호환됩니다: 우리는 이미지 내 패턴의 특정 위치가 이해 목적에 중요하지 않다고 가정했습니다.</p>
<p>미니배치에 $m$개의 예제가 있고 각 채널에 대해 합성곱의 출력이 높이 $p$와 너비 $q$를 갖는다고 가정합니다.
합성곱 레이어의 경우 출력 채널당 $m \cdot p \cdot q$ 요소에 대해 동시에 각 배치 정규화를 수행합니다.
따라서 평균과 분산을 계산할 때 모든 공간 위치에 걸쳐 값을 수집하고, 결과적으로 주어진 채널 내에서 동일한 평균과 분산을 적용하여 각 공간 위치의 값을 정규화합니다.
각 채널에는 고유한 스케일 및 시프트 파라미터가 있으며 둘 다 스칼라입니다.</p>
<h3 id="레이어-정규화-layer-normalization"><a class="header" href="#레이어-정규화-layer-normalization">레이어 정규화 (Layer Normalization)</a></h3>
<p>:label:<code>subsec_layer-normalization-in-bn</code></p>
<p>합성곱의 맥락에서 배치 정규화는 크기 1의 미니배치에 대해서도 잘 정의되어 있다는 점에 유의하십시오: 결국 평균을 낼 이미지 전체의 모든 위치가 있기 때문입니다. 결과적으로 단일 관찰 내일지라도 평균과 분산이 잘 정의됩니다.
이러한 고려 사항은 :citet:<code>Ba.Kiros.Hinton.2016</code>가 <em>레이어 정규화</em> 개념을 도입하게 했습니다. 이것은 배치 정규화처럼 작동하지만 한 번에 하나의 관찰에 적용된다는 점만 다릅니다. 결과적으로 오프셋과 스케일링 계수 모두 스칼라입니다. $n$차원 벡터 $\mathbf{x}$에 대해 레이어 정규화는 다음과 같이 주어집니다.</p>
<p>$$\mathbf{x} \rightarrow \textrm{LN}(\mathbf{x}) =  \frac{\mathbf{x} - \hat{\mu}}{\hat{\sigma}},$$</p>
<p>여기서 스케일링과 오프셋은 계수별로 적용되며 다음과 같이 주어집니다.</p>
<p>$$\hat{\mu} \stackrel{\textrm{def}}{=} \frac{1}{n} \sum_{i=1}^n x_i \textrm{ 그리고 }\n\hat{\sigma}^2 \stackrel{\textrm{def}}{=} \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2 + \epsilon.$$</p>
<p>이전과 마찬가지로 0으로 나누는 것을 방지하기 위해 작은 오프셋 $\epsilon &gt; 0$을 더합니다. 레이어 정규화를 사용하는 주요 이점 중 하나는 발산을 방지한다는 것입니다. 결국 $\epsilon$을 무시하면 레이어 정규화의 출력은 스케일에 독립적입니다. 즉, $\alpha \neq 0$인 어떤 선택에 대해서도 $\textrm{LN}(\mathbf{x}) \approx \textrm{LN}(\alpha \mathbf{x})$를 갖습니다. 이것은 $|\alpha| \to \infty$일 때 등식이 됩니다(근사 등식은 분산에 대한 오프셋 $\epsilon$ 때문입니다).</p>
<p>레이어 정규화의 또 다른 장점은 미니배치 크기에 의존하지 않는다는 것입니다. 또한 훈련 중인지 테스트 중인지 여부와도 무관합니다. 즉, 단순히 활성화를 주어진 스케일로 표준화하는 결정론적 변환입니다. 이는 최적화에서 발산을 방지하는 데 매우 유익할 수 있습니다. 자세한 내용은 생략하고 관심 있는 독자는 원본 논문을 참조할 것을 권장합니다.</p>
<h3 id="예측-중-배치-정규화-batch-normalization-during-prediction"><a class="header" href="#예측-중-배치-정규화-batch-normalization-during-prediction">예측 중 배치 정규화 (Batch Normalization During Prediction)</a></h3>
<p>앞서 언급했듯이 배치 정규화는 일반적으로 훈련 모드와 예측 모드에서 다르게 작동합니다.
첫째, 미니배치에서 각각을 추정하는 데서 발생하는 표본 평균과 표본 분산의 노이즈는 모델을 훈련한 후에는 더 이상 바람직하지 않습니다.
둘째, 우리는 배치별 정규화 통계를 계산할 여유가 없을 수 있습니다.
예를 들어 한 번에 하나의 예측을 수행하기 위해 모델을 적용해야 할 수도 있습니다.</p>
<p>일반적으로 훈련 후에는 전체 데이터셋을 사용하여 변수 통계의 안정적인 추정치를 계산한 다음 예측 시에 고정합니다.
따라서 배치 정규화는 훈련 중과 테스트 시에 다르게 동작합니다.
드롭아웃도 이 특성을 보인다는 것을 기억하십시오.</p>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-3"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-3">(<strong>밑바닥부터 구현하기 (Implementation from Scratch)</strong>)</a></h2>
<p>배치 정규화가 실제로 어떻게 작동하는지 보기 위해 아래에서 밑바닥부터 구현합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # autograd를 사용하여 훈련 모드인지 확인
    if not autograd.is_training():
        # 예측 모드에서는 이동 평균으로 얻은 평균과 분산 사용
        X_hat = (X - moving_mean) / np.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 완전 연결 레이어를 사용할 때 특성 차원에서 평균과 분산 계산
            mean = X.mean(axis=0)
            var = ((X - mean) ** 2).mean(axis=0)
        else:
            # 2차원 합성곱 레이어를 사용할 때 채널 차원(axis=1)에서 평균과 분산 계산.
            # 나중에 브로드캐스팅 연산을 수행할 수 있도록 X의 모양을 유지해야 합니다
            mean = X.mean(axis=(0, 2, 3), keepdims=True)
            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)
        # 훈련 모드에서는 현재 평균과 분산 사용
        X_hat = (X - mean) / np.sqrt(var + eps)
        # 이동 평균을 사용하여 평균과 분산 업데이트
        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean
        moving_var = (1.0 - momentum) * moving_var + momentum * var
    Y = gamma * X_hat + beta  # 스케일 및 시프트
    return Y, moving_mean, moving_var
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # is_grad_enabled를 사용하여 훈련 모드인지 확인
    if not torch.is_grad_enabled():
        # 예측 모드에서는 이동 평균으로 얻은 평균과 분산 사용
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 완전 연결 레이어를 사용할 때 특성 차원에서 평균과 분산 계산
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 2차원 합성곱 레이어를 사용할 때 채널 차원(axis=1)에서 평균과 분산 계산.
            # 나중에 브로드캐스팅 연산을 수행할 수 있도록 X의 모양을 유지해야 합니다
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # 훈련 모드에서는 현재 평균과 분산 사용
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 이동 평균을 사용하여 평균과 분산 업데이트
        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean
        moving_var = (1.0 - momentum) * moving_var + momentum * var
    Y = gamma * X_hat + beta  # 스케일 및 시프트
    return Y, moving_mean.data, moving_var.data
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps):
    # 이동 분산의 제곱근 역수를 요소별로 계산
    inv = tf.cast(tf.math.rsqrt(moving_var + eps), X.dtype)
    # 스케일 및 시프트
    inv *= gamma
    Y = X * inv + (beta - moving_mean * inv)
    return Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def batch_norm(X, deterministic, gamma, beta, moving_mean, moving_var, eps,
               momentum):
    # `deterministic`을 사용하여 현재 모드가 훈련 모드인지 예측 모드인지 확인
    if deterministic:
        # 예측 모드에서는 이동 평균으로 얻은 평균과 분산 사용
        # `linen.Module.variables`는 배열을 포함하는 `value` 속성을 가집니다
        X_hat = (X - moving_mean.value) / jnp.sqrt(moving_var.value + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 완전 연결 레이어를 사용할 때 특성 차원에서 평균과 분산 계산
            mean = X.mean(axis=0)
            var = ((X - mean) ** 2).mean(axis=0)
        else:
            # 2차원 합성곱 레이어를 사용할 때 채널 차원(axis=1)에서 평균과 분산 계산.
            # 나중에 브로드캐스팅 연산을 수행할 수 있도록 `X`의 모양을 유지해야 합니다
            mean = X.mean(axis=(0, 2, 3), keepdims=True)
            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)
        # 훈련 모드에서는 현재 평균과 분산 사용
        X_hat = (X - mean) / jnp.sqrt(var + eps)
        # 이동 평균을 사용하여 평균과 분산 업데이트
        moving_mean.value = momentum * moving_mean.value + (1.0 - momentum) * mean
        moving_var.value = momentum * moving_var.value + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # 스케일 및 시프트
    return Y
</code></pre>
<p>이제 [<strong>적절한 <code>BatchNorm</code> 레이어를 생성</strong>]할 수 있습니다.
우리 레이어는 스케일 <code>gamma</code>와 시프트 <code>beta</code>에 대한 적절한 파라미터를 유지하며, 둘 다 훈련 과정에서 업데이트됩니다.
또한 우리 레이어는 이후 모델 예측 시 사용하기 위해 평균과 분산의 이동 평균을 유지합니다.</p>
<p>알고리즘 세부 사항은 제쳐두고 우리 레이어 구현의 기저에 있는 디자인 패턴에 주목하십시오.
일반적으로 우리는 <code>batch_norm</code>과 같은 별도의 함수에 수학을 정의합니다.
그런 다음 이 기능을 사용자 정의 레이어에 통합합니다. 이 레이어의 코드는 주로 올바른 장치 컨텍스트로 데이터 이동, 필요한 변수 할당 및 초기화, 이동 평균 추적(여기서는 평균 및 분산) 등과 같은 부기(bookkeeping) 문제를 처리합니다.
이 패턴은 수학과 상용구 코드의 깔끔한 분리를 가능하게 합니다.
또한 편의를 위해 여기서는 입력 모양을 자동으로 추론하는 것에 대해 걱정하지 않았습니다.
따라서 전체적으로 특성 수를 지정해야 합니다.
지금까지 모든 최신 딥러닝 프레임워크는 고수준 배치 정규화 API에서 크기 및 모양의 자동 감지를 제공합니다(실제로는 대신 이것을 사용할 것입니다).</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class BatchNorm(nn.Block):
    # `num_features`: 완전 연결 레이어의 출력 수 또는 합성곱 레이어의 출력 채널 수.
    # `num_dims`: 완전 연결 레이어의 경우 2, 합성곱 레이어의 경우 4
    def __init__(self, num_features, num_dims, **kwargs):
        super().__init__(**kwargs)
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 스케일 파라미터와 시프트 파라미터(모델 파라미터)는 각각 1과 0으로 초기화됩니다
        self.gamma = self.params.get('gamma', shape=shape, init=init.One())
        self.beta = self.params.get('beta', shape=shape, init=init.Zero())
        # 모델 파라미터가 아닌 변수는 0과 1로 초기화됩니다
        self.moving_mean = np.zeros(shape)
        self.moving_var = np.ones(shape)

    def forward(self, X):
        # `X`가 메인 메모리에 없으면 `moving_mean`과 `moving_var`를 `X`가 있는 장치로 복사합니다
        if self.moving_mean.ctx != X.ctx:
            self.moving_mean = self.moving_mean.copyto(X.ctx)
            self.moving_var = self.moving_var.copyto(X.ctx)
        # 업데이트된 `moving_mean`과 `moving_var` 저장
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma.data(), self.beta.data(), self.moving_mean,
            self.moving_var, eps=1e-12, momentum=0.1)
        return Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class BatchNorm(nn.Module):
    # num_features: 완전 연결 레이어의 출력 수 또는 합성곱 레이어의 출력 채널 수.
    # num_dims: 완전 연결 레이어의 경우 2, 합성곱 레이어의 경우 4
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 스케일 파라미터와 시프트 파라미터(모델 파라미터)는 각각 1과 0으로 초기화됩니다
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # 모델 파라미터가 아닌 변수는 0과 1로 초기화됩니다
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # X가 메인 메모리에 없으면 moving_mean과 moving_var를 X가 있는 장치로 복사합니다
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 업데이트된 moving_mean과 moving_var 저장
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.1)
        return Y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class BatchNorm(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(BatchNorm, self).__init__(**kwargs)

    def build(self, input_shape):
        weight_shape = [input_shape[-1], ]
        # 스케일 파라미터와 시프트 파라미터(모델 파라미터)는 각각 1과 0으로 초기화됩니다
        self.gamma = self.add_weight(name='gamma', shape=weight_shape,
            initializer=tf.initializers.ones, trainable=True)
        self.beta = self.add_weight(name='beta', shape=weight_shape,
            initializer=tf.initializers.zeros, trainable=True)
        # 모델 파라미터가 아닌 변수는 0으로 초기화됩니다
        self.moving_mean = self.add_weight(name='moving_mean',
            shape=weight_shape, initializer=tf.initializers.zeros,
            trainable=False)
        self.moving_variance = self.add_weight(name='moving_variance',
            shape=weight_shape, initializer=tf.initializers.ones,
            trainable=False)
        super(BatchNorm, self).build(input_shape)

    def assign_moving_average(self, variable, value):
        momentum = 0.1
        delta = (1.0 - momentum) * variable + momentum * value
        return variable.assign(delta)

    @tf.function
    def call(self, inputs, training):
        if training:
            axes = list(range(len(inputs.shape) - 1))
            batch_mean = tf.reduce_mean(inputs, axes, keepdims=True)
            batch_variance = tf.reduce_mean(tf.math.squared_difference(
                inputs, tf.stop_gradient(batch_mean)), axes, keepdims=True)
            batch_mean = tf.squeeze(batch_mean, axes)
            batch_variance = tf.squeeze(batch_variance, axes)
            mean_update = self.assign_moving_average(
                self.moving_mean, batch_mean)
            variance_update = self.assign_moving_average(
                self.moving_variance, batch_variance)
            self.add_update(mean_update)
            self.add_update(variance_update)
            mean, variance = batch_mean, batch_variance
        else:
            mean, variance = self.moving_mean, self.moving_variance
        output = batch_norm(inputs, moving_mean=mean, moving_var=variance,
            beta=self.beta, gamma=self.gamma, eps=1e-5)
        return output
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class BatchNorm(nn.Module):
    # `num_features`: 완전 연결 레이어의 출력 수 또는 합성곱 레이어의 출력 채널 수.
    # `num_dims`: 완전 연결 레이어의 경우 2, 합성곱 레이어의 경우 4
    # `deterministic`을 사용하여 현재 모드가 훈련 모드인지 예측 모드인지 결정
    num_features: int
    num_dims: int
    deterministic: bool = False

    @nn.compact
    def __call__(self, X):
        if self.num_dims == 2:
            shape = (1, self.num_features)
        else:
            shape = (1, 1, 1, self.num_features)

        # 스케일 파라미터와 시프트 파라미터(모델 파라미터)는 각각 1과 0으로 초기화됩니다
        gamma = self.param('gamma', jax.nn.initializers.ones, shape)
        beta = self.param('beta', jax.nn.initializers.zeros, shape)

        # 모델 파라미터가 아닌 변수는 0과 1로 초기화됩니다. 'batch_stats' 컬렉션에 저장합니다
        moving_mean = self.variable('batch_stats', 'moving_mean', jnp.zeros, shape)
        moving_var = self.variable('batch_stats', 'moving_var', jnp.ones, shape)
        Y = batch_norm(X, self.deterministic, gamma, beta,
                       moving_mean, moving_var, eps=1e-5, momentum=0.9)

        return Y
</code></pre>
<p>과거 평균과 분산 추정치에 대한 집계를 제어하기 위해 <code>momentum</code>을 사용했습니다. 이는 최적화의 <em>모멘텀</em> 항과 전혀 관련이 없으므로 약간 잘못된 이름입니다. 그럼에도 불구하고 이 항에 대해 일반적으로 채택된 이름이며 API 명명 관례에 따라 코드에서 동일한 변수 이름을 사용합니다.</p>
<h2 id="배치-정규화가-있는-lenet"><a class="header" href="#배치-정규화가-있는-lenet">[<strong>배치 정규화가 있는 LeNet</strong>]</a></h2>
<p>문맥에서 <code>BatchNorm</code>을 적용하는 방법을 보기 위해, 아래에서는 이를 기존 LeNet 모델(:numref:<code>sec_lenet</code>)에 적용합니다.
배치 정규화는 합성곱 레이어 또는 완전 연결 레이어 다음, 해당 활성화 함수 이전에 적용됨을 기억하십시오.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class BNLeNetScratch(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.net = nn.Sequential()
            self.net.add(
                nn.Conv2D(6, kernel_size=5), BatchNorm(6, num_dims=4),
                nn.Activation('sigmoid'),
                nn.AvgPool2D(pool_size=2, strides=2),
                nn.Conv2D(16, kernel_size=5), BatchNorm(16, num_dims=4),
                nn.Activation('sigmoid'),
                nn.AvgPool2D(pool_size=2, strides=2), nn.Dense(120),
                BatchNorm(120, num_dims=2), nn.Activation('sigmoid'),
                nn.Dense(84), BatchNorm(84, num_dims=2),
                nn.Activation('sigmoid'), nn.Dense(num_classes))
            self.initialize()
        if tab.selected('pytorch'):
            self.net = nn.Sequential(
                nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),
                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
                nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),
                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
                nn.Flatten(), nn.LazyLinear(120),
                BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),
                BatchNorm(84, num_dims=2), nn.Sigmoid(),
                nn.LazyLinear(num_classes))
        if tab.selected('tensorflow'):
            self.net = tf.keras.models.Sequential([
                tf.keras.layers.Conv2D(filters=6, kernel_size=5,
                                       input_shape=(28, 28, 1)),
                BatchNorm(), tf.keras.layers.Activation('sigmoid'),
                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),
                tf.keras.layers.Conv2D(filters=16, kernel_size=5),
                BatchNorm(), tf.keras.layers.Activation('sigmoid'),
                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),
                tf.keras.layers.Flatten(), tf.keras.layers.Dense(120),
                BatchNorm(), tf.keras.layers.Activation('sigmoid'),
                tf.keras.layers.Dense(84), BatchNorm(),
                tf.keras.layers.Activation('sigmoid'),
                tf.keras.layers.Dense(num_classes)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class BNLeNetScratch(d2l.Classifier):
    lr: float = 0.1
    num_classes: int = 10
    training: bool = True

    def setup(self):
        self.net = nn.Sequential([
            nn.Conv(6, kernel_size=(5, 5)),
            BatchNorm(6, num_dims=4, deterministic=not self.training),
            nn.sigmoid,
            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),
            nn.Conv(16, kernel_size=(5, 5)),
            BatchNorm(16, num_dims=4, deterministic=not self.training),
            nn.sigmoid,
            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),
            lambda x: x.reshape((x.shape[0], -1)),
            nn.Dense(120),
            BatchNorm(120, num_dims=2, deterministic=not self.training),
            nn.sigmoid,
            nn.Dense(84),
            BatchNorm(84, num_dims=2, deterministic=not self.training),
            nn.sigmoid,
            nn.Dense(self.num_classes)])
</code></pre>
<p>:begin_tab:<code>jax</code>
<code>BatchNorm</code> 레이어는 배치 통계(평균 및 분산)를 계산해야 하므로, Flax는 <code>batch_stats</code> 딕셔너리를 추적하고 모든 미니배치마다 업데이트합니다. <code>batch_stats</code>와 같은 컬렉션은 <code>TrainState</code> 객체(:numref:<code>oo-design-training</code>에 정의된 <code>d2l.Trainer</code> 클래스에 있음)에 속성으로 저장될 수 있으며 모델의 순전파 동안 <code>mutable</code> 인수에 전달되어야 Flax가 변경된 변수를 반환합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(d2l.Classifier)  #@save
@partial(jax.jit, static_argnums=(0, 5))
def loss(self, params, X, Y, state, averaged=True):
    Y_hat, updates = state.apply_fn({'params': params,
                                     'batch_stats': state.batch_stats},
                                    *X, mutable=['batch_stats'],
                                    rngs={'dropout': state.dropout_rng})
    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))
    Y = d2l.reshape(Y, (-1,))
    fn = optax.softmax_cross_entropy_with_integer_labels
    return (fn(Y_hat, Y).mean(), updates) if averaged else (fn(Y_hat, Y), updates)
</code></pre>
<p>이전과 마찬가지로 [<strong>Fashion-MNIST 데이터셋에서 네트워크를 훈련</strong>]합니다.
이 코드는 LeNet을 처음 훈련했을 때와 거의 동일합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128)
model = BNLeNetScratch(lr=0.1)
if tab.selected('pytorch'):
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128)
with d2l.try_gpu():
    model = BNLeNetScratch(lr=0.5)
    trainer.fit(model, data)
</code></pre>
<p>첫 번째 배치 정규화 레이어에서 학습된 [<strong>스케일 파라미터 <code>gamma</code>와 시프트 파라미터 <code>beta</code>를 살펴봅시다</strong>].</p>
<pre><code class="language-{.python .input}">%%tab mxnet
model.net[1].gamma.data().reshape(-1,), model.net[1].beta.data().reshape(-1,)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
model.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reshape(model.net.layers[1].gamma, (-1,)), tf.reshape(
    model.net.layers[1].beta, (-1,))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
trainer.state.params['net']['layers_1']['gamma'].reshape((-1,)), \
trainer.state.params['net']['layers_1']['beta'].reshape((-1,))
</code></pre>
<h2 id="간결한-구현-2"><a class="header" href="#간결한-구현-2">[<strong>간결한 구현</strong>]</a></h2>
<p>방금 직접 정의한 <code>BatchNorm</code> 클래스와 비교하여, 딥러닝 프레임워크의 고수준 API에 정의된 <code>BatchNorm</code> 클래스를 직접 사용할 수 있습니다.
코드는 위의 구현과 거의 동일해 보이지만 차원을 올바르게 맞추기 위해 추가 인수를 제공할 필요가 없다는 점이 다릅니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, tensorflow, mxnet
class BNLeNet(d2l.Classifier):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.net = nn.Sequential()
            self.net.add(
                nn.Conv2D(6, kernel_size=5), nn.BatchNorm(),
                nn.Activation('sigmoid'),
                nn.AvgPool2D(pool_size=2, strides=2),
                nn.Conv2D(16, kernel_size=5), nn.BatchNorm(),
                nn.Activation('sigmoid'),
                nn.AvgPool2D(pool_size=2, strides=2),
                nn.Dense(120), nn.BatchNorm(), nn.Activation('sigmoid'),
                nn.Dense(84), nn.BatchNorm(), nn.Activation('sigmoid'),
                nn.Dense(num_classes))
            self.initialize()
        if tab.selected('pytorch'):
            self.net = nn.Sequential(
                nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),
                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
                nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),
                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
                nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),
                nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),
                nn.Sigmoid(), nn.LazyLinear(num_classes))
        if tab.selected('tensorflow'):
            self.net = tf.keras.models.Sequential([
                tf.keras.layers.Conv2D(filters=6, kernel_size=5,
                                       input_shape=(28, 28, 1)),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Activation('sigmoid'),
                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),
                tf.keras.layers.Conv2D(filters=16, kernel_size=5),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Activation('sigmoid'),
                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),
                tf.keras.layers.Flatten(), tf.keras.layers.Dense(120),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Activation('sigmoid'),
                tf.keras.layers.Dense(84),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Activation('sigmoid'),
                tf.keras.layers.Dense(num_classes)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class BNLeNet(d2l.Classifier):
    lr: float = 0.1
    num_classes: int = 10
    training: bool = True

    def setup(self):
        self.net = nn.Sequential([
            nn.Conv(6, kernel_size=(5, 5)),
            nn.BatchNorm(not self.training),
            nn.sigmoid,
            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),
            nn.Conv(16, kernel_size=(5, 5)),
            nn.BatchNorm(not self.training),
            nn.sigmoid,
            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),
            lambda x: x.reshape((x.shape[0], -1)),
            nn.Dense(120),
            nn.BatchNorm(not self.training),
            nn.sigmoid,
            nn.Dense(84),
            nn.BatchNorm(not self.training),
            nn.sigmoid,
            nn.Dense(self.num_classes)])
</code></pre>
<p>아래에서는 [<strong>동일한 하이퍼파라미터를 사용하여 모델을 훈련</strong>]합니다.
평소와 같이 고수준 API 변형이 훨씬 더 빠르게 실행된다는 점에 유의하십시오.
코드가 C++ 또는 CUDA로 컴파일된 반면 사용자 정의 구현은 Python으로 해석되어야 하기 때문입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128)
model = BNLeNet(lr=0.1)
if tab.selected('pytorch'):
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128)
with d2l.try_gpu():
    model = BNLeNet(lr=0.5)
    trainer.fit(model, data)
</code></pre>
<h2 id="토론-discussion-3"><a class="header" href="#토론-discussion-3">토론 (Discussion)</a></h2>
<p>직관적으로 배치 정규화는 최적화 지형을 더 매끄럽게 만든다고 생각됩니다.
하지만 심층 모델을 훈련할 때 관찰하는 현상에 대한 투기적 직관과 실제 설명을 구별하도록 주의해야 합니다.
우리는 더 단순한 심층 신경망(MLP 및 기존 CNN)이 애초에 왜 잘 일반화되는지조차 모른다는 것을 상기하십시오.
드롭아웃과 가중치 감쇠를 사용하더라도, 보지 못한 데이터로 일반화할 수 있는 능력이 너무 유연해서 훨씬 더 정제된 학습 이론적 일반화 보장이 필요할 가능성이 높습니다.</p>
<p>배치 정규화를 제안한 원본 논문 :cite:<code>Ioffe.Szegedy.2015</code>은 강력하고 유용한 도구를 도입했을 뿐만 아니라 *내부 공변량 이동(internal covariate shift)*을 줄임으로써 작동한다고 설명했습니다.
아마도 <em>내부 공변량 이동</em>이란 훈련 과정에서 변수 값의 분포가 변경된다는 개념과 같은 직관을 의미했을 것입니다.
그러나 이 설명에는 두 가지 문제가 있었습니다:
i) 이 드리프트는 <em>공변량 이동</em>과 매우 다르므로 이름이 잘못되었습니다. 굳이 말하자면 개념 드리프트(concept drift)에 더 가깝습니다.
ii) 설명은 명시되지 않은 직관을 제공하지만 <em>이 기술이 정확히 왜 작동하는지</em>에 대한 질문은 엄격한 설명을 기다리는 열린 문제로 남겨 둡니다.
이 책 전반에 걸쳐 우리는 실무자들이 심층 신경망 개발을 안내하는 데 사용하는 직관을 전달하는 것을 목표로 합니다.
그러나 우리는 이러한 지침이 되는 직관을 확립된 과학적 사실과 분리하는 것이 중요하다고 믿습니다.
결국 이 자료를 마스터하고 자신의 연구 논문을 작성하기 시작할 때 기술적 주장과 예감을 명확히 구분하고 싶을 것입니다.</p>
<p>배치 정규화의 성공에 이어, <em>내부 공변량 이동</em> 측면에서의 설명은 기술 문헌의 토론과 머신러닝 연구를 발표하는 방법에 대한 광범위한 담론에서 반복적으로 표면화되었습니다.
2017 NeurIPS 컨퍼런스에서 Test of Time Award를 수상하면서 한 기억에 남는 연설에서 Ali Rahimi는 현대 딥러닝 관행을 연금술에 비유하는 주장의 초점으로 <em>내부 공변량 이동</em>을 사용했습니다.
그 후 이 예제는 머신러닝의 골치 아픈 추세를 개괄하는 입장 논문에서 자세히 재검토되었습니다 :cite:<code>Lipton.Steinhardt.2018</code>.
다른 저자들은 배치 정규화의 성공에 대한 대안적 설명을 제안했으며, 일부 :cite:<code>Santurkar.Tsipras.Ilyas.ea.2018</code>는 배치 정규화의 성공이 원본 논문에서 주장한 것과 어떤 면에서는 반대되는 행동을 보임에도 불구하고 온다고 주장했습니다.</p>
<p>우리는 <em>내부 공변량 이동</em>이 매년 기술 머신러닝 문헌에서 만들어지는 수천 개의 유사하게 모호한 주장보다 더 비판받을 가치가 없다는 점에 주목합니다.
이 논쟁의 초점으로서의 공명은 대상 청중에게 널리 인식되기 때문일 것입니다.
배치 정규화는 거의 모든 배포된 이미지 분류기에 적용되는 필수 불가결한 방법임이 입증되었으며, 이 기술을 소개한 논문은 수만 건의 인용을 얻었습니다. 하지만 노이즈 주입을 통한 정규화, 재조정을 통한 가속화, 마지막으로 전처리라는 기본 원칙이 미래에 레이어와 기술의 추가 발명으로 이어질 수 있다고 추측합니다.</p>
<p>좀 더 실용적인 측면에서 배치 정규화에 대해 기억해야 할 몇 가지 사항이 있습니다:</p>
<ul>
<li>모델 훈련 중 배치 정규화는 미니배치의 평균과 표준 편차를 활용하여 네트워크의 중간 출력을 지속적으로 조정하므로 신경망 전체의 각 레이어에서 중간 출력 값이 더 안정적입니다.</li>
<li>배치 정규화는 완전 연결 레이어와 합성곱 레이어에 대해 약간 다릅니다. 사실 합성곱 레이어의 경우 레이어 정규화가 때때로 대안으로 사용될 수 있습니다.</li>
<li>드롭아웃 레이어와 마찬가지로 배치 정규화 레이어는 훈련 모드와 예측 모드에서 다르게 동작합니다.</li>
<li>배치 정규화는 정규화 및 최적화 수렴 개선에 유용합니다. 대조적으로 내부 공변량 이동을 줄인다는 원래 동기는 타당한 설명이 아닌 것으로 보입니다.</li>
<li>입력 섭동에 덜 민감한 더 강력한 모델을 위해 배치 정규화를 제거하는 것을 고려하십시오 :cite:<code>wang2022removing</code>.</li>
</ul>
<h2 id="연습-문제-exercises-38"><a class="header" href="#연습-문제-exercises-38">연습 문제 (Exercises)</a></h2>
<ol>
<li>배치 정규화 전에 완전 연결 레이어 또는 합성곱 레이어에서 편향 파라미터를 제거해야 합니까? 그 이유는 무엇입니까?</li>
<li>배치 정규화가 있는 경우와 없는 경우 LeNet의 학습률을 비교하십시오.
<ol>
<li>검증 정확도 증가를 플롯하십시오.</li>
<li>두 경우 모두 최적화가 실패하기 전까지 학습률을 얼마나 크게 만들 수 있습니까?</li>
</ol>
</li>
<li>모든 레이어에 배치 정규화가 필요합니까? 실험해 보십시오.</li>
<li>평균만 제거하거나 대안으로 분산만 제거하는 배치 정규화의 "라이트" 버전을 구현하십시오. 어떻게 동작합니까?</li>
<li>파라미터 <code>beta</code>와 <code>gamma</code>를 고정하십시오. 결과를 관찰하고 분석하십시오.</li>
<li>드롭아웃을 배치 정규화로 대체할 수 있습니까? 동작이 어떻게 변경됩니까?</li>
<li>연구 아이디어: 적용할 수 있는 다른 정규화 변환을 생각해 보십시오:
<ol>
<li>확률 적분 변환(probability integral transform)을 적용할 수 있습니까?</li>
<li>전체 순위 공분산 추정치를 사용할 수 있습니까? 아마도 그렇게 하지 않아야 하는 이유는 무엇입니까?</li>
<li>다른 압축 행렬 변형(블록 대각선, 저변위 순위, Monarch 등)을 사용할 수 있습니까?</li>
<li>희소화 압축이 정규화기 역할을 합니까?</li>
<li>사용할 수 있는 다른 투영(예: 볼록 원뿔, 대칭 그룹별 변환)이 있습니까?</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/83">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/84">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/330">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18005">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="잔차-네트워크-resnet와-resnext-residual-networks-resnet-and-resnext"><a class="header" href="#잔차-네트워크-resnet와-resnext-residual-networks-resnet-and-resnext">잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></h1>
<p>:label:<code>sec_resnet</code></p>
<p>점점 더 깊은 네트워크를 설계함에 따라 레이어를 추가하는 것이 네트워크의 복잡성과 표현력을 어떻게 증가시킬 수 있는지 이해하는 것이 필수적이 되었습니다.
더 중요한 것은 레이어를 추가하는 것이 네트워크를 단순히 다르게 만드는 것이 아니라 엄격하게 더 표현력 있게 만드는 네트워크를 설계하는 능력입니다.
약간의 진전을 이루려면 약간의 수학이 필요합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx, init
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
from d2l import tensorflow as d2l
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
import jax
</code></pre>
<h2 id="함수-클래스-function-classes"><a class="header" href="#함수-클래스-function-classes">함수 클래스 (Function Classes)</a></h2>
<p>특정 네트워크 아키텍처(학습률 및 기타 하이퍼파라미터 설정과 함께)가 도달할 수 있는 함수 클래스 $\mathcal{F}$를 고려하십시오.
즉, 모든 $f \in \mathcal{F}$에 대해 적절한 데이터셋에 대한 훈련을 통해 얻을 수 있는 파라미터 세트(예: 가중치 및 편향)가 존재합니다.
$f^<em>$가 우리가 정말로 찾고 싶은 "진실" 함수라고 가정해 봅시다.
그것이 $\mathcal{F}$에 있다면 우리는 좋은 상태이지만 일반적으로 그렇게 운이 좋지는 않을 것입니다.
대신 우리는 $\mathcal{F}$ 내에서 최선의 선택인 $f^</em>_\mathcal{F}$를 찾으려고 노력할 것입니다.
예를 들어,
특성 $\mathbf{X}$와 레이블 $\mathbf{y}$가 있는 데이터셋이 주어지면,
우리는 다음 최적화 문제를 해결하여 그것을 찾으려고 시도할 수 있습니다:</p>
<p>$$f^*_\mathcal{F} \stackrel{\textrm{def}}{=} \mathop{\mathrm{argmin}}_f L(\mathbf{X}, \mathbf{y}, f) \textrm{ subject to } f \in \mathcal{F}.$$</p>
<p>우리는 정규화 :cite:<code>tikhonov1977solutions,morozov2012methods</code>가 $\mathcal{F}$의 복잡도를 제어하고 일관성을 달성할 수 있음을 알고 있으므로, 더 큰 크기의 훈련 데이터는 일반적으로 더 나은 $f^<em>_\mathcal{F}$로 이어집니다.
우리가 다른 더 강력한 아키텍처 $\mathcal{F}'$를 설계하면 더 나은 결과를 얻어야 한다고 가정하는 것이 합리적입니다. 즉, 우리는 $f^</em><em>{\mathcal{F}'}$가 $f^*</em>{\mathcal{F}}$보다 "더 낫기"를 기대합니다. 그러나 $\mathcal{F} \not\subseteq \mathcal{F}'$이면 이런 일이 일어날 것이라는 보장이 없습니다. 사실 $f^<em>_{\mathcal{F}'}$는 더 나쁠 수도 있습니다.
:numref:<code>fig_functionclasses</code>에서 설명한 것처럼, 중첩되지 않은 함수 클래스의 경우 더 큰 함수 클래스가 항상 "진실" 함수 $f^</em>$에 더 가까이 이동하는 것은 아닙니다. 예를 들어,
:numref:<code>fig_functionclasses</code>의 왼쪽에서 $\mathcal{F}_3$은 $\mathcal{F}_1$보다 $f^<em>$에 가깝지만, $\mathcal{F}_6$은 멀어지고 복잡도를 더 높이면 $f^</em>$와의 거리를 줄일 수 있다는 보장이 없습니다.
:numref:<code>fig_functionclasses</code>의 오른쪽과 같이 $\mathcal{F}_1 \subseteq \cdots \subseteq \mathcal{F}_6$인 중첩 함수 클래스를 사용하면 중첩되지 않은 함수 클래스의 앞서 언급한 문제를 피할 수 있습니다.</p>
<p><img src="chapter_convolutional-modern/../img/functionclasses.svg" alt="중첩되지 않은 함수 클래스의 경우, 더 큰(영역으로 표시됨) 함수 클래스가 &quot;진실&quot; 함수($\mathit{f}^*$)에 더 가까워진다는 보장이 없습니다. 중첩 함수 클래스에서는 이런 일이 발생하지 않습니다." />
:label:<code>fig_functionclasses</code></p>
<p>따라서 더 큰 함수 클래스가 더 작은 함수 클래스를 포함하는 경우에만 이를 늘리면 네트워크의 표현력이 엄격하게 증가한다는 것이 보장됩니다.
심층 신경망의 경우,
새로 추가된 레이어를 항등 함수 $f(\mathbf{x}) = \mathbf{x}$로 훈련할 수 있다면 새 모델은 원래 모델만큼 효과적일 것입니다. 새 모델이 훈련 데이터셋에 맞는 더 나은 솔루션을 얻을 수 있으므로, 추가된 레이어는 훈련 오류를 줄이는 것을 더 쉽게 만들 수 있습니다.</p>
<p>이것은 :citet:<code>He.Zhang.Ren.ea.2016</code>가 매우 깊은 컴퓨터 비전 모델을 작업할 때 고려한 질문이었습니다.
제안된 <em>잔차 네트워크</em> (<em>ResNet</em>)의 핵심에는 모든 추가 레이어가 그 요소 중 하나로 항등 함수를 더 쉽게 포함해야 한다는 아이디어가 있습니다.
이러한 고려 사항은 다소 심오하지만 *잔차 블록(residual block)*이라는 놀랍도록 간단한 해결책으로 이어졌습니다.
이를 통해 ResNet은 2015년 ImageNet 대규모 시각 인식 챌린지에서 우승했습니다. 이 디자인은 심층 신경망을 구축하는 방법에 지대한 영향을 미쳤습니다. 예를 들어 잔차 블록은 순환 네트워크에 추가되었습니다 :cite:<code>prakash2016neural,kim2017residual</code>. 마찬가지로 Transformer :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code>는 이를 사용하여 많은 레이어의 네트워크를 효율적으로 쌓습니다. 그래프 신경망 :cite:<code>Kipf.Welling.2016</code>에도 사용되며 기본 개념으로서 컴퓨터 비전에서 광범위하게 사용되었습니다 :cite:<code>Redmon.Farhadi.2018,Ren.He.Girshick.ea.2015</code>.
잔차 네트워크 이전에 고속도로 네트워크(highway networks) :cite:<code>srivastava2015highway</code>가 있었는데, 항등 함수 주변의 우아한 파라미터화는 없지만 동기의 일부를 공유합니다.</p>
<h2 id="잔차-블록"><a class="header" href="#잔차-블록">(<strong>잔차 블록</strong>)</a></h2>
<p>:label:<code>subsec_residual-blks</code></p>
<p>:numref:<code>fig_residual_block</code>에 묘사된 대로 신경망의 국소 부분에 집중해 봅시다. 입력을 $\mathbf{x}$로 표시합니다.
우리는 학습을 통해 얻고자 하는 기저 매핑 $f(\mathbf{x})$가 상단의 활성화 함수에 입력으로 사용된다고 가정합니다.
왼쪽에서,
점선 상자 안의 부분은 $f(\mathbf{x})$를 직접 학습해야 합니다.
오른쪽에서,
점선 상자 안의 부분은 <em>잔차 매핑(residual mapping)</em> $g(\mathbf{x}) = f(\mathbf{x}) - \mathbf{x}$를 학습해야 하며, 여기서 잔차 블록이라는 이름이 유래했습니다.
항등 매핑 $f(\mathbf{x}) = \mathbf{x}$가 원하는 기저 매핑인 경우, 잔차 매핑은 $g(\mathbf{x}) = 0$이 되므로 학습하기가 더 쉽습니다:
점선 상자 안의 상부 가중치 레이어(예: 완전 연결 레이어 및 합성곱 레이어)의 가중치와 편향을 0으로 밀어 넣기만 하면 됩니다.
오른쪽 그림은 ResNet의 <em>잔차 블록</em>을 보여주며, 레이어 입력 $\mathbf{x}$를 덧셈 연산자로 전달하는 실선을 <em>잔차 연결(residual connection)</em> (또는 <em>숏컷 연결(shortcut connection)</em>)이라고 합니다.
잔차 블록을 사용하면 입력이 레이어를 가로질러 잔차 연결을 통해 더 빠르게 순전파될 수 있습니다.
사실,
잔차 블록은 다중 분기 Inception 블록의 특수한 경우로 생각할 수 있습니다:
그중 하나가 항등 매핑인 두 개의 분기가 있습니다.</p>
<p><img src="chapter_convolutional-modern/../img/residual-block.svg" alt="일반 블록(왼쪽)에서 점선 상자 안의 부분은 매핑 $\mathit{f}(\mathbf{x})$를 직접 학습해야 합니다. 잔차 블록(오른쪽)에서 점선 상자 안의 부분은 잔차 매핑 $\mathit{g}(\mathbf{x}) = \mathit{f}(\mathbf{x}) - \mathbf{x}$를 학습해야 하므로 항등 매핑 $\mathit{f}(\mathbf{x}) = \mathbf{x}$를 학습하기가 더 쉽습니다." />
:label:<code>fig_residual_block</code></p>
<p>ResNet은 VGG의 전체 $3\times 3$ 합성곱 레이어 설계를 가지고 있습니다. 잔차 블록에는 동일한 수의 출력 채널을 가진 두 개의 $3\times 3$ 합성곱 레이어가 있습니다. 각 합성곱 레이어 뒤에는 배치 정규화 레이어와 ReLU 활성화 함수가 옵니다. 그런 다음 이 두 합성곱 연산을 건너뛰고 최종 ReLU 활성화 함수 바로 앞에 입력을 직접 더합니다.
이러한 종류의 설계는 두 합성곱 레이어의 출력이 입력과 동일한 모양이어야 함께 더할 수 있음을 요구합니다. 채널 수를 변경하려면 덧셈 연산을 위해 입력을 원하는 모양으로 변환하는 추가 $1\times 1$ 합성곱 레이어를 도입해야 합니다. 아래 코드를 살펴보겠습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class Residual(nn.Block):  #@save
    """ResNet 모델의 잔차 블록."""
    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
        super().__init__(**kwargs)
        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,
                               strides=strides)
        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,
                                   strides=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm()
        self.bn2 = nn.BatchNorm()

    def forward(self, X):
        Y = npx.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        return npx.relu(Y + X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class Residual(nn.Module):  #@save
    """ResNet 모델의 잔차 블록."""
    def __init__(self, num_channels, use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,
                                   stride=strides)
        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,
                                       stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.LazyBatchNorm2d()
        self.bn2 = nn.LazyBatchNorm2d()

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class Residual(tf.keras.Model):  #@save
    """ResNet 모델의 잔차 블록."""
    def __init__(self, num_channels, use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = tf.keras.layers.Conv2D(num_channels, padding='same',
                                            kernel_size=3, strides=strides)
        self.conv2 = tf.keras.layers.Conv2D(num_channels, kernel_size=3,
                                            padding='same')
        self.conv3 = None
        if use_1x1conv:
            self.conv3 = tf.keras.layers.Conv2D(num_channels, kernel_size=1,
                                                strides=strides)
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.bn2 = tf.keras.layers.BatchNormalization()

    def call(self, X):
        Y = tf.keras.activations.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3 is not None:
            X = self.conv3(X)
        Y += X
        return tf.keras.activations.relu(Y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Residual(nn.Module):  #@save
    """ResNet 모델의 잔차 블록."""
    num_channels: int
    use_1x1conv: bool = False
    strides: tuple = (1, 1)
    training: bool = True

    def setup(self):
        self.conv1 = nn.Conv(self.num_channels, kernel_size=(3, 3),
                             padding='same', strides=self.strides)
        self.conv2 = nn.Conv(self.num_channels, kernel_size=(3, 3),
                             padding='same')
        if self.use_1x1conv:
            self.conv3 = nn.Conv(self.num_channels, kernel_size=(1, 1),
                                 strides=self.strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm(not self.training)
        self.bn2 = nn.BatchNorm(not self.training)

    def __call__(self, X):
        Y = nn.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return nn.relu(Y)
</code></pre>
<p>이 코드는 두 가지 유형의 네트워크를 생성합니다: <code>use_1x1conv=False</code>일 때마다 ReLU 비선형성을 적용하기 전에 입력을 출력에 더하는 것과, 더하기 전에 $1 \times 1$ 합성곱을 통해 채널과 해상도를 조정하는 것입니다. :numref:<code>fig_resnet_block</code>은 이를 보여줍니다.</p>
<p><img src="chapter_convolutional-modern/../img/resnet-block.svg" alt="덧셈 연산을 위해 입력을 원하는 모양으로 변환하는 $1 \times 1$ 합성곱이 있거나 없는 ResNet 블록." />
:label:<code>fig_resnet_block</code></p>
<p>이제 $1 \times 1$ 합성곱이 필요하지 않은 [<strong>입력과 출력의 모양이 같은 상황</strong>]을 살펴보겠습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
if tab.selected('mxnet'):
    blk = Residual(3)
    blk.initialize()
if tab.selected('pytorch'):
    blk = Residual(3)
X = d2l.randn(4, 3, 6, 6)
blk(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
blk = Residual(3)
X = d2l.normal((4, 6, 6, 3))
Y = blk(X)
Y.shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
blk = Residual(3)
X = jax.random.normal(d2l.get_key(), (4, 6, 6, 3))
blk.init_with_output(d2l.get_key(), X)[0].shape
</code></pre>
<p>[<strong>출력 채널 수를 늘리면서 출력 높이와 너비를 절반으로 줄이는</strong>] 옵션도 있습니다.
이 경우 <code>use_1x1conv=True</code>를 통해 $1 \times 1$ 합성곱을 사용합니다. 이것은 <code>strides=2</code>를 통해 공간 차원을 줄이기 위해 각 ResNet 블록의 시작 부분에서 유용합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
blk = Residual(6, use_1x1conv=True, strides=2)
if tab.selected('mxnet'):
    blk.initialize()
blk(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
blk = Residual(6, use_1x1conv=True, strides=(2, 2))
blk.init_with_output(d2l.get_key(), X)[0].shape
</code></pre>
<h2 id="resnet-모델"><a class="header" href="#resnet-모델">[<strong>ResNet 모델</strong>]</a></h2>
<p>ResNet의 처음 두 레이어는 앞서 설명한 GoogLeNet의 것과 동일합니다: 64개의 출력 채널과 스트라이드 2가 있는 $7\times 7$ 합성곱 레이어 다음에는 스트라이드 2가 있는 $3\times 3$ 최대 풀링 레이어가 이어집니다. 차이점은 ResNet의 각 합성곱 레이어 뒤에 추가된 배치 정규화 레이어입니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class ResNet(d2l.Classifier):
    def b1(self):
        if tab.selected('mxnet'):
            net = nn.Sequential()
            net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
                    nn.BatchNorm(), nn.Activation('relu'),
                    nn.MaxPool2D(pool_size=3, strides=2, padding=1))
            return net
        if tab.selected('pytorch'):
            return nn.Sequential(
                nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
                nn.LazyBatchNorm2d(), nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        if tab.selected('tensorflow'):
            return tf.keras.models.Sequential([
                tf.keras.layers.Conv2D(64, kernel_size=7, strides=2,
                                       padding='same'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Activation('relu'),
                tf.keras.layers.MaxPool2D(pool_size=3, strides=2,
                                          padding='same')])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class ResNet(d2l.Classifier):
    arch: tuple
    lr: float = 0.1
    num_classes: int = 10
    training: bool = True

    def setup(self):
        self.net = self.create_net()

    def b1(self):
        return nn.Sequential([
            nn.Conv(64, kernel_size=(7, 7), strides=(2, 2), padding='same'),
            nn.BatchNorm(not self.training), nn.relu,
            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2),
                                  padding='same')])
</code></pre>
<p>GoogLeNet은 Inception 블록으로 구성된 4개의 모듈을 사용합니다.
그러나 ResNet은 잔차 블록으로 구성된 4개의 모듈을 사용하며, 각 모듈은 동일한 수의 출력 채널을 가진 여러 잔차 블록을 사용합니다.
첫 번째 모듈의 채널 수는 입력 채널 수와 동일합니다. 스트라이드 2의 최대 풀링 레이어가 이미 사용되었으므로 높이와 너비를 줄일 필요가 없습니다. 후속 각 모듈의 첫 번째 잔차 블록에서 채널 수는 이전 모듈의 채널 수에 비해 두 배가 되고 높이와 너비는 반으로 줄어듭니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
@d2l.add_to_class(ResNet)
def block(self, num_residuals, num_channels, first_block=False):
    blk = nn.Sequential()
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
        else:
            blk.add(Residual(num_channels))
    return blk
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
@d2l.add_to_class(ResNet)
def block(self, num_residuals, num_channels, first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels))
    return nn.Sequential(*blk)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
@d2l.add_to_class(ResNet)
def block(self, num_residuals, num_channels, first_block=False):
    blk = tf.keras.models.Sequential()
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
        else:
            blk.add(Residual(num_channels))
    return blk
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(ResNet)
def block(self, num_residuals, num_channels, first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(num_channels, use_1x1conv=True,
                                strides=(2, 2), training=self.training))
        else:
            blk.append(Residual(num_channels, training=self.training))
    return nn.Sequential(blk)
</code></pre>
<p>그런 다음 모든 모듈을 ResNet에 추가합니다. 여기서 각 모듈에는 두 개의 잔차 블록이 사용됩니다. 마지막으로 GoogLeNet과 마찬가지로 전역 평균 풀링 레이어를 추가하고 완전 연결 레이어 출력을 추가합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(ResNet)
def __init__(self, arch, lr=0.1, num_classes=10):
    super(ResNet, self).__init__()
    self.save_hyperparameters()
    if tab.selected('mxnet'):
        self.net = nn.Sequential()
        self.net.add(self.b1())
        for i, b in enumerate(arch):
            self.net.add(self.block(*b, first_block=(i==0)))
        self.net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))
        self.net.initialize(init.Xavier())
    if tab.selected('pytorch'):
        self.net = nn.Sequential(self.b1())
        for i, b in enumerate(arch):
            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))
        self.net.add_module('last', nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.LazyLinear(num_classes)))
        self.net.apply(d2l.init_cnn)
    if tab.selected('tensorflow'):
        self.net = tf.keras.models.Sequential(self.b1())
        for i, b in enumerate(arch):
            self.net.add(self.block(*b, first_block=(i==0)))
        self.net.add(tf.keras.models.Sequential([
            tf.keras.layers.GlobalAvgPool2D(),
            tf.keras.layers.Dense(units=num_classes)]))
</code></pre>
<pre><code class="language-{.python .input}"># %%tab jax
@d2l.add_to_class(ResNet)
def create_net(self):
    net = nn.Sequential([self.b1()])
    for i, b in enumerate(self.arch):
        net.layers.extend([self.block(*b, first_block=(i==0))])
    net.layers.extend([nn.Sequential([
        # Flax는 GlobalAvg2D 레이어를 제공하지 않습니다
        lambda x: nn.avg_pool(x, window_shape=x.shape[1:3],
                              strides=x.shape[1:3], padding='valid'),
        lambda x: x.reshape((x.shape[0], -1)),
        nn.Dense(self.num_classes)])])
    return net
</code></pre>
<p>각 모듈에는 4개의 합성곱 레이어가 있습니다($1\times 1$ 합성곱 레이어 제외). 첫 번째 $7\times 7$ 합성곱 레이어와 마지막 완전 연결 레이어와 함께 총 18개의 레이어가 있습니다. 따라서 이 모델은 일반적으로 ResNet-18로 알려져 있습니다.
모듈에서 채널 수와 잔차 블록 수를 다르게 구성하여 더 깊은 152-레이어 ResNet-152와 같은 다양한 ResNet 모델을 만들 수 있습니다. ResNet의 주요 아키텍처는 GoogLeNet과 유사하지만 ResNet의 구조는 더 간단하고 수정하기 쉽습니다. 이러한 모든 요인으로 인해 ResNet은 빠르고 광범위하게 사용되었습니다. :numref:<code>fig_resnet18</code>은 전체 ResNet-18을 보여줍니다.</p>
<p><img src="chapter_convolutional-modern/../img/resnet18-90.svg" alt="ResNet-18 아키텍처." />
:label:<code>fig_resnet18</code></p>
<p>ResNet을 훈련하기 전에 [<strong>ResNet의 다양한 모듈에서 입력 모양이 어떻게 변하는지 관찰</strong>]해 보겠습니다. 모든 이전 아키텍처와 마찬가지로 전역 평균 풀링 레이어가 모든 특성을 집계하는 지점까지 해상도는 감소하는 반면 채널 수는 증가합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class ResNet18(ResNet):
    def __init__(self, lr=0.1, num_classes=10):
        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),
                       lr, num_classes)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class ResNet18(ResNet):
    arch: tuple = ((2, 64), (2, 128), (2, 256), (2, 512))
    lr: float = 0.1
    num_classes: int = 10
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet
ResNet18().layer_summary((1, 1, 96, 96))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
ResNet18().layer_summary((1, 96, 96, 1))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
ResNet18(training=False).layer_summary((1, 96, 96, 1))
</code></pre>
<h2 id="훈련-training-13"><a class="header" href="#훈련-training-13">[<strong>훈련 (Training)</strong>]</a></h2>
<p>이전과 마찬가지로 Fashion-MNIST 데이터셋에서 ResNet을 훈련합니다. ResNet은 꽤 강력하고 유연한 아키텍처입니다. 훈련 및 검증 손실을 포착한 플롯은 두 그래프 사이에 상당한 격차를 보여주며 훈련 손실이 훨씬 낮습니다. 이 정도의 유연성을 가진 네트워크의 경우, 더 많은 훈련 데이터가 격차를 줄이고 정확도를 높이는 데 뚜렷한 이점을 제공할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
model = ResNet18(lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
if tab.selected('pytorch'):
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
with d2l.try_gpu():
    model = ResNet18(lr=0.01)
    trainer.fit(model, data)
</code></pre>
<h2 id="resnext"><a class="header" href="#resnext">ResNeXt</a></h2>
<p>:label:<code>subsec_resnext</code></p>
<p>ResNet 설계에서 직면하는 과제 중 하나는 주어진 블록 내에서 비선형성과 차원성 사이의 트레이드오프입니다. 즉, 레이어 수를 늘리거나 합성곱의 너비를 늘려 비선형성을 더할 수 있습니다. 대안 전략은 블록 간에 정보를 전달할 수 있는 채널 수를 늘리는 것입니다. 불행히도 후자는 $c_\textrm{i}$ 채널을 섭취하고 $c_\textrm{o}$ 채널을 방출하는 계산 비용이 $\mathcal{O}(c_\textrm{i} \cdot c_\textrm{o})$에 비례하므로 이차적 페널티가 따릅니다(:numref:<code>sec_channels</code>의 논의 참조).</p>
<p>:numref:<code>fig_inception</code>의 Inception 블록에서 영감을 얻을 수 있습니다. 정보가 별도의 그룹으로 블록을 통해 흐릅니다. 다중 독립 그룹 아이디어를 :numref:<code>fig_resnet_block</code>의 ResNet 블록에 적용하여 ResNeXt의 설계가 이루어졌습니다 :cite:<code>Xie.Girshick.Dollar.ea.2017</code>.
다른 점은 Inception의 뒤죽박죽 변환과 달리,
ResNeXt는 모든 분기에서 <em>동일한</em> 변환을 채택하여 각 분기의 수동 튜닝 필요성을 최소화합니다.</p>
<p><img src="chapter_convolutional-modern/../img/resnext-block.svg" alt="ResNeXt 블록. $\mathit{g}$ 그룹이 있는 그룹화된 합성곱을 사용하는 것은 조밀한 합성곱보다 $\mathit{g}$배 빠릅니다. 중간 채널 수 $\mathit{b}$가 $\mathit{c}$보다 작으면 병목 잔차 블록입니다." />
:label:<code>fig_resnext_block</code></p>
<p>$c_\textrm{i}$에서 $c_\textrm{o}$ 채널로의 합성곱을 $c_\textrm{i}/g$ 크기의 $g$ 그룹 중 하나로 나누어 $c_\textrm{o}/g$ 크기의 $g$ 출력을 생성하는 것을 적절하게도 *그룹화된 합성곱(grouped convolution)*이라고 합니다. 계산 비용(비례적으로)은 $\mathcal{O}(c_\textrm{i} \cdot c_\textrm{o})$에서 $\mathcal{O}(g \cdot (c_\textrm{i}/g) \cdot (c_\textrm{o}/g)) = \mathcal{O}(c_\textrm{i} \cdot c_\textrm{o} / g)$로 줄어듭니다. 즉, $g$배 더 빠릅니다. 더 좋은 점은 출력을 생성하는 데 필요한 파라미터 수도 $c_\textrm{i} \times c_\textrm{o}$ 행렬에서 $(c_\textrm{i}/g) \times (c_\textrm{o}/g)$ 크기의 $g$개의 더 작은 행렬로 줄어들어 역시 $g$배 감소한다는 것입니다. 다음에서는 $c_\textrm{i}$와 $c_\textrm{o}$가 모두 $g$로 나누어떨어진다고 가정합니다.</p>
<p>이 설계의 유일한 과제는 $g$ 그룹 간에 정보가 교환되지 않는다는 것입니다. :numref:<code>fig_resnext_block</code>의 ResNeXt 블록은 두 가지 방식으로 이를 수정합니다: $3 \times 3$ 커널을 사용한 그룹화된 합성곱이 두 개의 $1 \times 1$ 합성곱 사이에 끼어 있습니다. 두 번째 것은 채널 수를 다시 변경하는 이중 역할을 합니다. 이점은 $1 \times 1$ 커널에 대해 $\mathcal{O}(c \cdot b)$ 비용만 지불하고 $3 \times 3$ 커널에 대해 $\mathcal{O}(b^2 / g)$ 비용으로 해결할 수 있다는 것입니다. :numref:<code>subsec_residual-blks</code>의 잔차 블록 구현과 유사하게 잔차 연결은 $1 \times 1$ 합성곱으로 대체(따라서 일반화)됩니다.</p>
<p>:numref:<code>fig_resnext_block</code>의 오른쪽 그림은 결과 네트워크 블록에 대한 훨씬 더 간결한 요약을 제공합니다. 이것은 또한 :numref:<code>sec_cnn-design</code>의 일반적인 현대 CNN 설계에서 중요한 역할을 할 것입니다. 그룹화된 합성곱 아이디어는 AlexNet 구현 :cite:<code>Krizhevsky.Sutskever.Hinton.2012</code>으로 거슬러 올라갑니다. 제한된 메모리를 가진 두 개의 GPU에 네트워크를 분산할 때 구현은 각 GPU를 부작용 없이 자체 채널로 처리했습니다.</p>
<p><code>ResNeXtBlock</code> 클래스의 다음 구현은 <code>groups</code> ($g$)를 인수로 취하며, <code>bot_channels</code> ($b$) 중간(병목) 채널을 갖습니다. 마지막으로 표현의 높이와 너비를 줄여야 할 때 <code>use_1x1conv=True, strides=2</code>로 설정하여 스트라이드 $2$를 추가합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class ResNeXtBlock(nn.Block):  #@save
    """ResNeXt 블록."""
    def __init__(self, num_channels, groups, bot_mul,
                 use_1x1conv=False, strides=1, **kwargs):
        super().__init__(**kwargs)
        bot_channels = int(round(num_channels * bot_mul))
        self.conv1 = nn.Conv2D(bot_channels, kernel_size=1, padding=0,
                               strides=1)
        self.conv2 = nn.Conv2D(bot_channels, kernel_size=3, padding=1, 
                               strides=strides, groups=bot_channels//groups)
        self.conv3 = nn.Conv2D(num_channels, kernel_size=1, padding=0,
                               strides=1)
        self.bn1 = nn.BatchNorm()
        self.bn2 = nn.BatchNorm()
        self.bn3 = nn.BatchNorm()
        if use_1x1conv:
            self.conv4 = nn.Conv2D(num_channels, kernel_size=1,
                                   strides=strides)
            self.bn4 = nn.BatchNorm()
        else:
            self.conv4 = None

    def forward(self, X):
        Y = npx.relu(self.bn1(self.conv1(X)))
        Y = npx.relu(self.bn2(self.conv2(Y)))
        Y = self.bn3(self.conv3(Y))
        if self.conv4:
            X = self.bn4(self.conv4(X))
        return npx.relu(Y + X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class ResNeXtBlock(nn.Module):  #@save
    """ResNeXt 블록."""
    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,
                 strides=1):
        super().__init__()
        bot_channels = int(round(num_channels * bot_mul))
        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)
        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,
                                   stride=strides, padding=1,
                                   groups=bot_channels//groups)
        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)
        self.bn1 = nn.LazyBatchNorm2d()
        self.bn2 = nn.LazyBatchNorm2d()
        self.bn3 = nn.LazyBatchNorm2d()
        if use_1x1conv:
            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1, 
                                       stride=strides)
            self.bn4 = nn.LazyBatchNorm2d()
        else:
            self.conv4 = None

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = F.relu(self.bn2(self.conv2(Y)))
        Y = self.bn3(self.conv3(Y))
        if self.conv4:
            X = self.bn4(self.conv4(X))
        return F.relu(Y + X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class ResNeXtBlock(tf.keras.Model):  #@save
    """ResNeXt 블록."""
    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,
                 strides=1):
        super().__init__()
        bot_channels = int(round(num_channels * bot_mul))
        self.conv1 = tf.keras.layers.Conv2D(bot_channels, 1, strides=1)
        self.conv2 = tf.keras.layers.Conv2D(bot_channels, 3, strides=strides,
                                            padding="same",
                                            groups=bot_channels//groups)
        self.conv3 = tf.keras.layers.Conv2D(num_channels, 1, strides=1)
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.bn2 = tf.keras.layers.BatchNormalization()
        self.bn3 = tf.keras.layers.BatchNormalization()
        if use_1x1conv:
            self.conv4 = tf.keras.layers.Conv2D(num_channels, 1,
                                                strides=strides)
            self.bn4 = tf.keras.layers.BatchNormalization()
        else:
            self.conv4 = None

    def call(self, X):
        Y = tf.keras.activations.relu(self.bn1(self.conv1(X)))
        Y = tf.keras.activations.relu(self.bn2(self.conv2(Y)))
        Y = self.bn3(self.conv3(Y))
        if self.conv4:
            X = self.bn4(self.conv4(X))
        return tf.keras.activations.relu(Y + X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class ResNeXtBlock(nn.Module):  #@save
    """ResNeXt 블록."""
    num_channels: int
    groups: int
    bot_mul: int
    use_1x1conv: bool = False
    strides: tuple = (1, 1)
    training: bool = True

    def setup(self):
        bot_channels = int(round(self.num_channels * self.bot_mul))
        self.conv1 = nn.Conv(bot_channels, kernel_size=(1, 1),
                               strides=(1, 1))
        self.conv2 = nn.Conv(bot_channels, kernel_size=(3, 3),
                               strides=self.strides, padding='same',
                               feature_group_count=bot_channels//self.groups)
        self.conv3 = nn.Conv(self.num_channels, kernel_size=(1, 1),
                               strides=(1, 1))
        self.bn1 = nn.BatchNorm(not self.training)
        self.bn2 = nn.BatchNorm(not self.training)
        self.bn3 = nn.BatchNorm(not self.training)
        if self.use_1x1conv:
            self.conv4 = nn.Conv(self.num_channels, kernel_size=(1, 1),
                                       strides=self.strides)
            self.bn4 = nn.BatchNorm(not self.training)
        else:
            self.conv4 = None

    def __call__(self, X):
        Y = nn.relu(self.bn1(self.conv1(X)))
        Y = nn.relu(self.bn2(self.conv2(Y)))
        Y = self.bn3(self.conv3(Y))
        if self.conv4:
            X = self.bn4(self.conv4(X))
        return nn.relu(Y + X)
</code></pre>
<p>사용법은 앞에서 논의한 <code>ResNetBlock</code>의 사용법과 전적으로 유사합니다. 예를 들어 (<code>use_1x1conv=False, strides=1</code>)을 사용하면 입력과 출력의 모양이 같습니다. 대안으로 <code>use_1x1conv=True, strides=2</code>를 설정하면 출력 높이와 너비가 절반으로 줄어듭니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
blk = ResNeXtBlock(32, 16, 1)
if tab.selected('mxnet'):
    blk.initialize()
X = d2l.randn(4, 32, 96, 96)
blk(X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
blk = ResNeXtBlock(32, 16, 1)
X = d2l.normal((4, 96, 96, 32))
Y = blk(X)
Y.shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
blk = ResNeXtBlock(32, 16, 1)
X = jnp.zeros((4, 96, 96, 32))
blk.init_with_output(d2l.get_key(), X)[0].shape
</code></pre>
<h2 id="요약-및-토론-summary-and-discussion-5"><a class="header" href="#요약-및-토론-summary-and-discussion-5">요약 및 토론 (Summary and Discussion)</a></h2>
<p>중첩된 함수 클래스는 용량을 추가할 때 미묘하게 <em>다른</em> 함수 클래스 대신 엄격하게 <em>더 강력한</em> 함수 클래스를 얻을 수 있도록 하므로 바람직합니다. 이를 달성하는 한 가지 방법은 추가 레이어가 단순히 입력을 출력으로 통과시키도록 하는 것입니다. 잔차 연결은 이를 가능하게 합니다. 결과적으로 이는 단순 함수의 귀납적 편향을 $f(\mathbf{x}) = 0$ 형태에서 $f(\mathbf{x}) = \mathbf{x}$와 같은 형태로 변경합니다.</p>
<p>잔차 매핑은 가중치 레이어의 파라미터를 0으로 미는 것과 같이 항등 함수를 더 쉽게 학습할 수 있습니다. 우리는 잔차 블록을 사용하여 효과적인 <em>심층</em> 신경망을 훈련할 수 있습니다. 입력은 레이어 전체의 잔차 연결을 통해 더 빠르게 순전파될 수 있습니다. 결과적으로 훨씬 더 깊은 네트워크를 훈련할 수 있습니다. 예를 들어 원래 ResNet 논문 :cite:<code>He.Zhang.Ren.ea.2016</code>은 최대 152개 레이어를 허용했습니다. 잔차 네트워크의 또 다른 이점은 훈련 과정 <em>중</em>에 항등 함수로 초기화된 레이어를 추가할 수 있다는 것입니다. 결국 레이어의 기본 동작은 데이터를 변경하지 않고 통과시키는 것입니다. 이는 경우에 따라 매우 큰 네트워크의 훈련을 가속화할 수 있습니다.</p>
<p>잔차 연결 이전에,
게이팅 유닛이 있는 우회 경로가 도입되어
100개 이상의 레이어를 가진 고속도로 네트워크를 효과적으로 훈련했습니다
:cite:<code>srivastava2015highway</code>.
우회 경로로 항등 함수를 사용하여
ResNet은 여러 컴퓨터 비전 작업에서 놀라울 정도로 잘 수행되었습니다.
잔차 연결은 합성곱 또는 순차적 성격의 후속 심층 신경망 설계에 큰 영향을 미쳤습니다.
나중에 소개하겠지만,
Transformer 아키텍처 :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code>는
잔차 연결을 채택(다른 설계 선택과 함께)하며
언어, 비전, 음성, 강화 학습과 같이 다양한 분야에
널리 퍼져 있습니다.</p>
<p>ResNeXt는 합성곱 신경망 설계가 시간이 지남에 따라 어떻게 진화했는지 보여주는 예입니다: 계산을 더 절약하고 활성화 크기(채널 수)와 절충함으로써 저렴한 비용으로 더 빠르고 정확한 네트워크를 가능하게 합니다. 그룹화된 합성곱을 보는 다른 방법은 합성곱 가중치에 대한 블록 대각 행렬을 생각하는 것입니다. 더 효율적인 네트워크로 이어지는 꽤 많은 "트릭"이 있다는 점에 유의하십시오. 예를 들어 ShiftNet :cite:<code>wu2018shift</code>은 단순히 채널에 이동된 활성화를 추가하여 $3 \times 3$ 합성곱의 효과를 모방하여 이번에는 계산 비용 없이 향상된 함수 복잡성을 제공합니다.</p>
<p>지금까지 논의한 설계의 공통적인 특징은 네트워크 설계가 상당히 수동적이며 주로 "올바른" 네트워크 하이퍼파라미터를 찾기 위해 설계자의 독창성에 의존한다는 것입니다. 분명히 실현 가능하지만 인간 시간 측면에서 비용이 많이 들고 결과가 어떤 의미에서 최적이라는 보장이 없습니다. :numref:<code>sec_cnn-design</code>에서는 더 자동화된 방식으로 고품질 네트워크를 얻기 위한 여러 전략을 논의할 것입니다. 특히 RegNetX/Y 모델 :cite:<code>Radosavovic.Kosaraju.Girshick.ea.2020</code>로 이어진 <em>네트워크 설계 공간</em>의 개념을 검토할 것입니다.</p>
<h2 id="연습-문제-exercises-39"><a class="header" href="#연습-문제-exercises-39">연습 문제 (Exercises)</a></h2>
<ol>
<li>:numref:<code>fig_inception</code>의 Inception 블록과 잔차 블록의 주요 차이점은 무엇입니까? 계산, 정확도, 설명할 수 있는 함수 클래스 측면에서 어떻게 비교됩니까?</li>
<li>네트워크의 다양한 변형을 구현하려면 ResNet 논문 :cite:<code>He.Zhang.Ren.ea.2016</code>의 표 1을 참조하십시오.</li>
<li>더 깊은 네트워크를 위해 ResNet은 모델 복잡성을 줄이기 위해 "병목" 아키텍처를 도입합니다. 구현해 보십시오.</li>
<li>ResNet의 후속 버전에서 저자는 "합성곱, 배치 정규화, 활성화" 구조를 "배치 정규화, 활성화, 합성곱" 구조로 변경했습니다. 이 개선을 직접 수행하십시오. 자세한 내용은 :citet:<code>He.Zhang.Ren.ea.2016*1</code>의 그림 1을 참조하십시오.</li>
<li>함수 클래스가 중첩되더라도 왜 함수의 복잡성을 무한정 늘릴 수 없습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/85">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/86">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/8737">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18006">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="조밀하게-연결된-네트워크-densenet-densely-connected-networks-densenet"><a class="header" href="#조밀하게-연결된-네트워크-densenet-densely-connected-networks-densenet">조밀하게 연결된 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></h1>
<p>:label:<code>sec_densenet</code></p>
<p>ResNet은 심층 네트워크의 함수를 파라미터화하는 방법에 대한 관점을 크게 바꾸었습니다. <em>DenseNet</em> (조밀 합성곱 네트워크)은 어느 정도 이것의 논리적 확장입니다 :cite:<code>Huang.Liu.Van-Der-Maaten.ea.2017</code>.
DenseNet은 각 레이어가 이전의 모든 레이어에 연결되는 연결 패턴과
이전 레이어의 특성을 보존하고 재사용하기 위한 (ResNet의 덧셈 연산자가 아닌) 연결(concatenation) 연산 모두를 특징으로 합니다.
어떻게 도달하는지 이해하기 위해 수학으로 잠시 우회해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
import jax
</code></pre>
<h2 id="resnet에서-densenet으로-from-resnet-to-densenet"><a class="header" href="#resnet에서-densenet으로-from-resnet-to-densenet">ResNet에서 DenseNet으로 (From ResNet to DenseNet)</a></h2>
<p>함수에 대한 테일러 확장을 상기하십시오. 점 $x = 0$에서 다음과 같이 쓸 수 있습니다.</p>
<p>$$f(x) = f(0) + x \cdot \left[f'(0) + x \cdot \left[\frac{f''(0)}{2!}  + x \cdot \left[\frac{f'''(0)}{3!}  + \cdots \right]\right]\right].$$</p>
<p>요점은 함수를 점점 더 높은 차수의 항으로 분해한다는 것입니다. 비슷한 맥락에서 ResNet은 함수를 다음과 같이 분해합니다.</p>
<p>$$f(\mathbf{x}) = \mathbf{x} + g(\mathbf{x}).$$</p>
<p>즉, ResNet은 $f$를 간단한 선형 항과 더 복잡한 비선형 항으로 분해합니다.
만약 우리가 두 항 너머의 정보를 포착(반드시 더하는 것은 아님)하고 싶다면 어떨까요?
그러한 솔루션 중 하나가 DenseNet입니다 :cite:<code>Huang.Liu.Van-Der-Maaten.ea.2017</code>.</p>
<p><img src="chapter_convolutional-modern/../img/densenet-block.svg" alt="ResNet(왼쪽)과 DenseNet(오른쪽)의 레이어 간 연결의 주요 차이점: 덧셈 사용과 연결 사용." />
:label:<code>fig_densenet_block</code></p>
<p>:numref:<code>fig_densenet_block</code>에 표시된 것처럼 ResNet과 DenseNet의 주요 차이점은 후자의 경우 출력이 더해지는 대신 <em>연결</em>된다는 것입니다($[,]$로 표시됨).
결과적으로, 우리는 점점 더 복잡해지는 함수 시퀀스를 적용한 후 $\mathbf{x}$에서 해당 값으로의 매핑을 수행합니다:</p>
<p>$$\mathbf{x} \to \left[
\mathbf{x},
f_1(\mathbf{x}),
f_2\left(\left[\mathbf{x}, f_1\left(\mathbf{x}\right)\right]\right), f_3\left(\left[\mathbf{x}, f_1\left(\mathbf{x}\right), f_2\left(\left[\mathbf{x}, f_1\left(\mathbf{x}\right)\right]\right)\right]\right), \ldots\right].$$</p>
<p>결국, 이 모든 함수는 MLP에서 결합되어 특성 수를 다시 줄입니다. 구현 측면에서 이것은 매우 간단합니다:
항을 더하는 대신 연결합니다. DenseNet이라는 이름은 변수 간의 종속성 그래프가 꽤 조밀해진다는 사실에서 유래했습니다. 이러한 체인의 마지막 레이어는 이전의 모든 레이어와 조밀하게 연결됩니다. 조밀한 연결은 :numref:<code>fig_densenet</code>에 나와 있습니다.</p>
<p><img src="chapter_convolutional-modern/../img/densenet.svg" alt="DenseNet의 조밀한 연결. 깊이에 따라 차원성이 어떻게 증가하는지 주목하십시오." />
:label:<code>fig_densenet</code></p>
<p>DenseNet을 구성하는 주요 구성 요소는 *조밀 블록(dense blocks)*과 *전환 레이어(transition layers)*입니다. 전자는 입력과 출력을 연결하는 방법을 정의하고, 후자는 채널 수가 너무 커지지 않도록 제어합니다.
확장 $\mathbf{x} \to \left[\mathbf{x}, f_1(\mathbf{x}), f_2\left(\left[\mathbf{x}, f_1(\mathbf{x})\right]\right), \ldots \right]$은 꽤 고차원적일 수 있기 때문입니다.</p>
<h2 id="조밀-블록-dense-blocks"><a class="header" href="#조밀-블록-dense-blocks">[<strong>조밀 블록 (Dense Blocks)</strong>]</a></h2>
<p>DenseNet은 ResNet의 수정된 "배치 정규화, 활성화, 합성곱" 구조를 사용합니다(:numref:<code>sec_resnet</code>의 연습 문제 참조).
먼저 이 합성곱 블록 구조를 구현합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def conv_block(num_channels):
    blk = nn.Sequential()
    blk.add(nn.BatchNorm(),
            nn.Activation('relu'),
            nn.Conv2D(num_channels, kernel_size=3, padding=1))
    return blk
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def conv_block(num_channels):
    return nn.Sequential(
        nn.LazyBatchNorm2d(), nn.ReLU(),
        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class ConvBlock(tf.keras.layers.Layer):
    def __init__(self, num_channels):
        super(ConvBlock, self).__init__()
        self.bn = tf.keras.layers.BatchNormalization()
        self.relu = tf.keras.layers.ReLU()
        self.conv = tf.keras.layers.Conv2D(
            filters=num_channels, kernel_size=(3, 3), padding='same')

        self.listLayers = [self.bn, self.relu, self.conv]

    def call(self, x):
        y = x
        for layer in self.listLayers.layers:
            y = layer(y)
        y = tf.keras.layers.concatenate([x,y], axis=-1)
        return y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class ConvBlock(nn.Module):
    num_channels: int
    training: bool = True

    @nn.compact
    def __call__(self, X):
        Y = nn.relu(nn.BatchNorm(not self.training)(X))
        Y = nn.Conv(self.num_channels, kernel_size=(3, 3), padding=(1, 1))(Y)
        Y = jnp.concatenate((X, Y), axis=-1)
        return Y
</code></pre>
<p><em>조밀 블록</em>은 여러 합성곱 블록으로 구성되며 각 블록은 동일한 수의 출력 채널을 사용합니다. 그러나 순전파에서는 각 합성곱 블록의 입력과 출력을 채널 차원에서 연결합니다. 지연 평가를 통해 차원성을 자동으로 조정할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class DenseBlock(nn.Block):
    def __init__(self, num_convs, num_channels):
        super().__init__()
        self.net = nn.Sequential()
        for _ in range(num_convs):
            self.net.add(conv_block(num_channels))

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # 채널을 따라 각 블록의 입력과 출력 연결
            X = np.concatenate((X, Y), axis=1)
        return X
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class DenseBlock(nn.Module):
    def __init__(self, num_convs, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(num_channels))
        self.net = nn.Sequential(*layer)

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # 채널을 따라 각 블록의 입력과 출력 연결
            X = torch.cat((X, Y), dim=1)
        return X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class DenseBlock(tf.keras.layers.Layer):
    def __init__(self, num_convs, num_channels):
        super(DenseBlock, self).__init__()
        self.listLayers = []
        for _ in range(num_convs):
            self.listLayers.append(ConvBlock(num_channels))

    def call(self, x):
        for layer in self.listLayers.layers:
            x = layer(x)
        return x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class DenseBlock(nn.Module):
    num_convs: int
    num_channels: int
    training: bool = True

    def setup(self):
        layer = []
        for i in range(self.num_convs):
            layer.append(ConvBlock(self.num_channels, self.training))
        self.net = nn.Sequential(layer)

    def __call__(self, X):
        return self.net(X)
</code></pre>
<p>다음 예제에서는
10개의 출력 채널이 있는 두 개의 합성곱 블록으로 [<strong><code>DenseBlock</code> 인스턴스를 정의</strong>]합니다.
3개의 채널이 있는 입력을 사용할 때 $3 + 10 + 10=23$ 채널의 출력을 얻게 됩니다. 합성곱 블록 채널 수는 입력 채널 수에 대한 출력 채널 수의 증가를 제어합니다. 이를 *성장률(growth rate)*이라고도 합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
blk = DenseBlock(2, 10)
if tab.selected('mxnet'):
    X = np.random.uniform(size=(4, 3, 8, 8))
    blk.initialize()
if tab.selected('pytorch'):
    X = torch.randn(4, 3, 8, 8)
if tab.selected('tensorflow'):
    X = tf.random.uniform((4, 8, 8, 3))
Y = blk(X)
Y.shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
blk = DenseBlock(2, 10)
X = jnp.zeros((4, 8, 8, 3))
Y = blk.init_with_output(d2l.get_key(), X)[0]
Y.shape
</code></pre>
<h2 id="전환-레이어-transition-layers"><a class="header" href="#전환-레이어-transition-layers">[<strong>전환 레이어 (Transition Layers)</strong>]</a></h2>
<p>각 조밀 블록은 채널 수를 늘리므로 너무 많이 추가하면 지나치게 복잡한 모델이 됩니다. <em>전환 레이어</em>는 모델의 복잡성을 제어하는 데 사용됩니다. $1\times 1$ 합성곱을 사용하여 채널 수를 줄입니다. 또한 스트라이드가 2인 평균 풀링을 통해 높이와 너비를 반으로 줄입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def transition_block(num_channels):
    blk = nn.Sequential()
    blk.add(nn.BatchNorm(), nn.Activation('relu'),
            nn.Conv2D(num_channels, kernel_size=1),
            nn.AvgPool2D(pool_size=2, strides=2))
    return blk
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def transition_block(num_channels):
    return nn.Sequential(
        nn.LazyBatchNorm2d(), nn.ReLU(),
        nn.LazyConv2d(num_channels, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class TransitionBlock(tf.keras.layers.Layer):
    def __init__(self, num_channels, **kwargs):
        super(TransitionBlock, self).__init__(**kwargs)
        self.batch_norm = tf.keras.layers.BatchNormalization()
        self.relu = tf.keras.layers.ReLU()
        self.conv = tf.keras.layers.Conv2D(num_channels, kernel_size=1)
        self.avg_pool = tf.keras.layers.AvgPool2D(pool_size=2, strides=2)

    def call(self, x):
        x = self.batch_norm(x)
        x = self.relu(x)
        x = self.conv(x)
        return self.avg_pool(x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class TransitionBlock(nn.Module):
    num_channels: int
    training: bool = True

    @nn.compact
    def __call__(self, X):
        X = nn.BatchNorm(not self.training)(X)
        X = nn.relu(X)
        X = nn.Conv(self.num_channels, kernel_size=(1, 1))(X)
        X = nn.avg_pool(X, window_shape=(2, 2), strides=(2, 2))
        return X
</code></pre>
<p>이전 예제의 조밀 블록 출력에 10개의 채널이 있는 [<strong>전환 레이어를 적용</strong>]합니다. 이렇게 하면 출력 채널 수가 10개로 줄어들고 높이와 너비가 반으로 줄어듭니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
blk = transition_block(10)
blk.initialize()
blk(Y).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
blk = transition_block(10)
blk(Y).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
blk = TransitionBlock(10)
blk(Y).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
blk = TransitionBlock(10)
blk.init_with_output(d2l.get_key(), Y)[0].shape
</code></pre>
<h2 id="densenet-모델"><a class="header" href="#densenet-모델">[<strong>DenseNet 모델</strong>]</a></h2>
<p>다음으로 DenseNet 모델을 구성합니다. DenseNet은 먼저 ResNet과 동일한 단일 합성곱 레이어와 최대 풀링 레이어를 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class DenseNet(d2l.Classifier):
    def b1(self):
        if tab.selected('mxnet'):
            net = nn.Sequential()
            net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
                nn.BatchNorm(), nn.Activation('relu'),
                nn.MaxPool2D(pool_size=3, strides=2, padding=1))
            return net
        if tab.selected('pytorch'):
            return nn.Sequential(
                nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
                nn.LazyBatchNorm2d(), nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        if tab.selected('tensorflow'):
            return tf.keras.models.Sequential([
                tf.keras.layers.Conv2D(
                    64, kernel_size=7, strides=2, padding='same'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.ReLU(),
                tf.keras.layers.MaxPool2D(
                    pool_size=3, strides=2, padding='same')])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class DenseNet(d2l.Classifier):
    num_channels: int = 64
    growth_rate: int = 32
    arch: tuple = (4, 4, 4, 4)
    lr: float = 0.1
    num_classes: int = 10
    training: bool = True

    def setup(self):
        self.net = self.create_net()

    def b1(self):
        return nn.Sequential([
            nn.Conv(64, kernel_size=(7, 7), strides=(2, 2), padding='same'),
            nn.BatchNorm(not self.training),
            nn.relu,
            lambda x: nn.max_pool(x, window_shape=(3, 3),
                                  strides=(2, 2), padding='same')
        ])
</code></pre>
<p>그런 다음 ResNet이 사용하는 잔차 블록으로 구성된 4개의 모듈과 유사하게,
DenseNet은 4개의 조밀 블록을 사용합니다.
ResNet과 마찬가지로 각 조밀 블록에서 사용되는 합성곱 레이어 수를 설정할 수 있습니다. 여기서는 :numref:<code>sec_resnet</code>의 ResNet-18 모델과 일치하도록 4로 설정합니다. 또한 조밀 블록의 합성곱 레이어에 대한 채널 수(즉, 성장률)를 32로 설정하여 각 조밀 블록에 128개의 채널이 추가되도록 합니다.</p>
<p>ResNet에서는 스트라이드가 2인 잔차 블록에 의해 각 모듈 간의 높이와 너비가 줄어듭니다. 여기서는 전환 레이어를 사용하여 높이와 너비를 반으로 줄이고 채널 수를 반으로 줄입니다. ResNet과 유사하게 마지막에 전역 풀링 레이어와 완전 연결 레이어가 연결되어 출력을 생성합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(DenseNet)
def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),
             lr=0.1, num_classes=10):
    super(DenseNet, self).__init__()
    self.save_hyperparameters()
    if tab.selected('mxnet'):
        self.net = nn.Sequential()
        self.net.add(self.b1())
        for i, num_convs in enumerate(arch):
            self.net.add(DenseBlock(num_convs, growth_rate))
            # 이전 조밀 블록의 출력 채널 수
            num_channels += num_convs * growth_rate
            # 조밀 블록 사이에 채널 수를 반으로 줄이는 전환 레이어가 추가됩니다
            if i != len(arch) - 1:
                num_channels //= 2
                self.net.add(transition_block(num_channels))
        self.net.add(nn.BatchNorm(), nn.Activation('relu'),
                     nn.GlobalAvgPool2D(), nn.Dense(num_classes))
        self.net.initialize(init.Xavier())
    if tab.selected('pytorch'):
        self.net = nn.Sequential(self.b1())
        for i, num_convs in enumerate(arch):
            self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,
                                                              growth_rate))
            # 이전 조밀 블록의 출력 채널 수
            num_channels += num_convs * growth_rate
            # 조밀 블록 사이에 채널 수를 반으로 줄이는 전환 레이어가 추가됩니다
            if i != len(arch) - 1:
                num_channels //= 2
                self.net.add_module(f'tran_blk{i+1}', transition_block(
                    num_channels))
        self.net.add_module('last', nn.Sequential(
            nn.LazyBatchNorm2d(), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.LazyLinear(num_classes)))
        self.net.apply(d2l.init_cnn)
    if tab.selected('tensorflow'):
        self.net = tf.keras.models.Sequential(self.b1())
        for i, num_convs in enumerate(arch):
            self.net.add(DenseBlock(num_convs, growth_rate))
            # 이전 조밀 블록의 출력 채널 수
            num_channels += num_convs * growth_rate
            # 조밀 블록 사이에 채널 수를 반으로 줄이는 전환 레이어가 추가됩니다
            if i != len(arch) - 1:
                num_channels //= 2
                self.net.add(TransitionBlock(num_channels))
        self.net.add(tf.keras.models.Sequential([
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.ReLU(),
            tf.keras.layers.GlobalAvgPool2D(),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(num_classes)]))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(DenseNet)
def create_net(self):
    net = self.b1()
    for i, num_convs in enumerate(self.arch):
        net.layers.extend([DenseBlock(num_convs, self.growth_rate,
                                      training=self.training)])
        # 이전 조밀 블록의 출력 채널 수
        num_channels = self.num_channels + (num_convs * self.growth_rate)
        # 조밀 블록 사이에 채널 수를 반으로 줄이는 전환 레이어가 추가됩니다
        if i != len(self.arch) - 1:
            num_channels //= 2
            net.layers.extend([TransitionBlock(num_channels,
                                               training=self.training)])
    net.layers.extend([
        nn.BatchNorm(not self.training),
        nn.relu,
        lambda x: nn.avg_pool(x, window_shape=x.shape[1:3],
                              strides=x.shape[1:3], padding='valid'),
        lambda x: x.reshape((x.shape[0], -1)),
        nn.Dense(self.num_classes)
    ])
    return net
</code></pre>
<h2 id="훈련-training-14"><a class="header" href="#훈련-training-14">[<strong>훈련 (Training)</strong>]</a></h2>
<p>여기서는 더 깊은 네트워크를 사용하고 있으므로, 이 섹션에서는 계산을 단순화하기 위해 입력 높이와 너비를 224에서 96으로 줄일 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
model = DenseNet(lr=0.01)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
with d2l.try_gpu():
    model = DenseNet(lr=0.01)
    trainer.fit(model, data)
</code></pre>
<h2 id="요약-및-토론-summary-and-discussion-6"><a class="header" href="#요약-및-토론-summary-and-discussion-6">요약 및 토론 (Summary and Discussion)</a></h2>
<p>DenseNet을 구성하는 주요 구성 요소는 조밀 블록과 전환 레이어입니다. 후자의 경우 채널 수를 다시 줄이는 전환 레이어를 추가하여 네트워크를 구성할 때 차원성을 통제해야 합니다.
레이어 간 연결 측면에서, 입력과 출력이 더해지는 ResNet과 달리 DenseNet은 채널 차원에서 입력과 출력을 연결합니다.
이러한 연결 연산은 특성을 재사용하여 계산 효율성을 달성하지만, 불행히도 높은 GPU 메모리 소비로 이어집니다.
결과적으로 DenseNet을 적용하려면 훈련 시간을 증가시킬 수 있는 더 메모리 효율적인 구현이 필요할 수 있습니다 :cite:<code>pleiss2017memory</code>.</p>
<h2 id="연습-문제-exercises-40"><a class="header" href="#연습-문제-exercises-40">연습 문제 (Exercises)</a></h2>
<ol>
<li>전환 레이어에서 최대 풀링 대신 평균 풀링을 사용하는 이유는 무엇입니까?</li>
<li>DenseNet 논문에서 언급된 장점 중 하나는 모델 파라미터가 ResNet보다 작다는 것입니다. 왜 그렇습니까?</li>
<li>DenseNet이 비판받은 한 가지 문제는 높은 메모리 소비입니다.
<ol>
<li>실제로 그렇습니까? 입력 모양을 $224\times 224$로 변경하여 실제 GPU 메모리 소비를 경험적으로 비교해 보십시오.</li>
<li>메모리 소비를 줄이는 대안적인 방법을 생각할 수 있습니까? 프레임워크를 어떻게 변경해야 합니까?</li>
</ol>
</li>
<li>DenseNet 논문 :cite:<code>Huang.Liu.Van-Der-Maaten.ea.2017</code>의 표 1에 제시된 다양한 DenseNet 버전을 구현하십시오.</li>
<li>DenseNet 아이디어를 적용하여 MLP 기반 모델을 설계하십시오. :numref:<code>sec_kaggle_house</code>의 주택 가격 예측 작업에 적용하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/87">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/88">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/331">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18008">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="합성곱-네트워크-아키텍처-설계-designing-convolution-network-architectures"><a class="header" href="#합성곱-네트워크-아키텍처-설계-designing-convolution-network-architectures">합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></h1>
<p>:label:<code>sec_cnn-design</code></p>
<p>이전 섹션들에서는 컴퓨터 비전을 위한 현대 네트워크 설계에 대해 살펴보았습니다.
많은 아키텍처가 과학자들의 직관에 크게 의존했으며, 인간의 창의성에 크게 의존하고 심층 네트워크가 제공하는 설계 공간에 대한 체계적인 탐구에는 훨씬 덜 의존했습니다.
그럼에도 불구하고 이 <em>네트워크 엔지니어링</em> 접근 방식은 엄청난 성공을 거두었습니다.</p>
<p>AlexNet(:numref:<code>sec_alexnet</code>)이 ImageNet에서 기존 컴퓨터 비전 모델을 능가한 이래로,
동일한 패턴에 따라 설계된 합성곱 블록을 쌓아 매우 깊은 네트워크를 구축하는 것이 인기를 얻었습니다.
특히 $3 \times 3$ 합성곱은 VGG 네트워크(:numref:<code>sec_vgg</code>)에 의해 대중화되었습니다.
NiN(:numref:<code>sec_nin</code>)은 $1 \times 1$ 합성곱조차도 국소 비선형성을 추가함으로써 유익할 수 있음을 보여주었습니다.
또한 NiN은 모든 위치에 걸쳐 집계함으로써 네트워크 헤드에서 정보를 집계하는 문제를 해결했습니다.
GoogLeNet(:numref:<code>sec_googlenet</code>)은 Inception 블록에서 VGG와 NiN의 장점을 결합하여 다양한 합성곱 너비의 다중 분기를 추가했습니다.
ResNet(:numref:<code>sec_resnet</code>)은 항등 매핑($f(x) = 0$에서)으로 귀납적 편향을 변경했습니다.
이로써 매우 깊은 네트워크가 가능해졌습니다.
거의 10년이 지난 지금도 ResNet 설계는 여전히 인기가 있으며, 이는 그 설계의 증거입니다.
마지막으로 ResNeXt(:numref:<code>subsec_resnext</code>)는 그룹화된 합성곱을 추가하여 파라미터와 계산 간의 더 나은 트레이드오프를 제공했습니다.
비전을 위한 Transformer의 전신인 Squeeze-and-Excitation Networks (SENets)는 위치 간의 효율적인 정보 전송을 가능하게 합니다 :cite:<code>Hu.Shen.Sun.2018</code>.
이는 채널별 전역 주의 함수를 계산하여 달성되었습니다.</p>
<p>지금까지 <em>신경 아키텍처 검색(neural architecture search, NAS)</em> :cite:<code>zoph2016neural,liu2018darts</code>을 통해 얻은 네트워크는 생략했습니다.
우리는 그 비용이 일반적으로 엄청나며 무차별 대입 검색, 유전 알고리즘, 강화 학습 또는 다른 형태의 하이퍼파라미터 최적화에 의존하기 때문에 그렇게 하기로 결정했습니다.
고정된 검색 공간이 주어지면 NAS는 반환된 성능 추정을 기반으로 아키텍처를 자동으로 선택하는 검색 전략을 사용합니다.
NAS의 결과는 단일 네트워크 인스턴스입니다. EfficientNet은 이 검색의 주목할 만한 결과입니다 :cite:<code>tan2019efficientnet</code>.</p>
<p>다음에서는 <em>단일 최고의 네트워크</em>를 찾는 탐구와는 상당히 다른 아이디어를 논의합니다.
계산적으로 상대적으로 저렴하고, 도중에 과학적 통찰력으로 이어지며, 결과의 품질 측면에서 매우 효과적입니다.
<em>네트워크 설계 공간을 설계</em>하기 위한 :citet:<code>Radosavovic.Kosaraju.Girshick.ea.2020</code>의 전략을 검토해 봅시다.
이 전략은 수동 설계와 NAS의 강점을 결합합니다.
<em>네트워크 분포</em>에 대해 작업하고 전체 네트워크 패밀리에 대해 좋은 성능을 얻는 방식으로 분포를 최적화함으로써 이를 달성합니다.
그 결과는 <em>RegNets</em>, 구체적으로 RegNetX 및 RegNetY, 그리고 고성능 CNN 설계를 위한 다양한 지침 원칙입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx, init
from mxnet.gluon import nn

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
from d2l import tensorflow as d2l
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
</code></pre>
<h2 id="anynet-설계-공간-the-anynet-design-space"><a class="header" href="#anynet-설계-공간-the-anynet-design-space">AnyNet 설계 공간 (The AnyNet Design Space)</a></h2>
<p>:label:<code>subsec_the-anynet-design-space</code></p>
<p>아래 설명은 :citet:<code>Radosavovic.Kosaraju.Girshick.ea.2020</code>의 추론을 밀접하게 따르며 책의 범위에 맞게 일부 약어를 사용합니다.
시작하려면 탐색할 네트워크 패밀리를 위한 템플릿이 필요합니다.
이 장의 설계 중 공통점 중 하나는 네트워크가 <em>스템(stem)</em>, <em>바디(body)</em>, *헤드(head)*로 구성된다는 것입니다.
스템은 종종 더 큰 윈도우 크기를 가진 합성곱을 통해 초기 이미지 처리를 수행합니다.
바디는 원시 이미지에서 객체 표현으로 가는 데 필요한 변환의 대부분을 수행하는 다중 블록으로 구성됩니다.
마지막으로 헤드는 이를 다중 클래스 분류를 위한 소프트맥스 회귀기와 같은 원하는 출력으로 변환합니다.
바디는 차례로 감소하는 해상도에서 이미지에 대해 작업하는 다중 단계로 구성됩니다.
사실 스템과 각 후속 단계는 공간 해상도를 4분의 1로 줄입니다.
마지막으로 각 단계는 하나 이상의 블록으로 구성됩니다.
이 패턴은 VGG에서 ResNeXt에 이르기까지 모든 네트워크에 공통적입니다.
실제로 일반적인 AnyNet 네트워크 설계를 위해 :citet:<code>Radosavovic.Kosaraju.Girshick.ea.2020</code>는 :numref:<code>fig_resnext_block</code>의 ResNeXt 블록을 사용했습니다.</p>
<p><img src="chapter_convolutional-modern/../img/anynet.svg" alt="AnyNet 설계 공간. 각 화살표를 따라 있는 숫자 $(\mathit{c}, \mathit{r})$은 해당 지점에서의 채널 수 $c$와 이미지의 해상도 $\mathit{r} \times \mathit{r}$를 나타냅니다. 왼쪽에서 오른쪽으로: 스템, 바디, 헤드로 구성된 일반적인 네트워크 구조; 4단계로 구성된 바디; 단계의 상세 구조; 다운샘플링이 없는 블록과 각 차원에서 해상도를 절반으로 줄이는 블록의 두 가지 대안 구조. 설계 선택에는 깊이 $\mathit{d_i}$, 출력 채널 수 $\mathit{c_i}$, 그룹 수 $\mathit{g_i}$, 그리고 모든 단계 $\mathit{i}$에 대한 병목 비율 $\mathit{k_i}$가 포함됩니다." />
:label:<code>fig_anynet_full</code></p>
<p>:numref:<code>fig_anynet_full</code>에 설명된 구조를 자세히 검토해 봅시다. 언급했듯이 AnyNet은 스템, 바디, 헤드로 구성됩니다. 스템은 RGB 이미지(3채널)를 입력으로 받아 스트라이드가 $2$인 $3 \times 3$ 합성곱을 사용하고 배치 정규화가 뒤따라 해상도를 $r \times r$에서 $r/2 \times r/2$로 절반으로 줄입니다. 또한 바디에 입력으로 사용될 $c_0$ 채널을 생성합니다.</p>
<p>네트워크는 $224 \times 224 \times 3$ 모양의 ImageNet 이미지와 잘 작동하도록 설계되었으므로, 바디는 4단계(상기하자면 $224 / 2^{1+4} = 7$)를 통해 이를 $7 \times 7 \times c_4$로 줄이는 역할을 하며, 각 단계는 결국 스트라이드가 $2$입니다.
마지막으로 헤드는 NiN(:numref:<code>sec_nin</code>)과 유사한 전역 평균 풀링을 통해 완전히 표준적인 설계를 채택하고, $n$-클래스 분류를 위한 $n$차원 벡터를 방출하기 위해 완전 연결 레이어가 뒤따릅니다.</p>
<p>대부분의 관련 설계 결정은 네트워크 바디에 내재되어 있습니다.
바디는 단계적으로 진행되며, 각 단계는 :numref:<code>subsec_resnext</code>에서 논의한 것과 동일한 유형의 ResNeXt 블록으로 구성됩니다.
거기서의 설계는 다시 완전히 일반적입니다: 스트라이드 $2$를 사용하여 해상도를 절반으로 줄이는 블록으로 시작합니다(:numref:<code>fig_anynet_full</code>의 맨 오른쪽).
이에 맞추기 위해 ResNeXt 블록의 잔차 분기는 $1 \times 1$ 합성곱을 통과해야 합니다.
이 블록 뒤에는 해상도와 채널 수를 변경하지 않는 가변 수의 추가 ResNeXt 블록이 잇따릅니다.
일반적인 설계 관행은 합성곱 블록 설계에 약간의 병목 현상을 추가하는 것입니다.
따라서 병목 비율 $k_i \geq 1$로 단계 $i$의 각 블록 내에 $c_i/k_i$ 채널을 제공합니다(실험에서 알 수 있듯이 이는 실제로 효과적이지 않으므로 건너뛰어야 합니다).
마지막으로 ResNeXt 블록을 다루고 있으므로 단계 $i$에서 그룹화된 합성곱에 대한 그룹 수 $g_i$도 선택해야 합니다.</p>
<p>이 겉보기에 일반적인 설계 공간은 그럼에도 불구하고 우리에게 많은 파라미터를 제공합니다:
블록 너비(채널 수) $c_0, \ldots c_4$, 단계별 깊이(블록 수) $d_1, \ldots d_4$, 병목 비율 $k_1, \ldots k_4$, 그룹 너비(그룹 수) $g_1, \ldots g_4$를 설정할 수 있습니다.
총 17개의 파라미터가 추가되어 탐색을 정당화할 수 없을 만큼 많은 수의 구성이 생성됩니다.
이 거대한 설계 공간을 효과적으로 줄이기 위한 도구가 필요합니다.
이곳이 설계 공간의 개념적 아름다움이 들어오는 곳입니다. 그렇게 하기 전에 일반적인 설계를 먼저 구현해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class AnyNet(d2l.Classifier):
    def stem(self, num_channels):
        net = nn.Sequential()
        net.add(nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=2),
                nn.BatchNorm(), nn.Activation('relu'))
        return net
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class AnyNet(d2l.Classifier):
    def stem(self, num_channels):
        return nn.Sequential(
            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),
            nn.LazyBatchNorm2d(), nn.ReLU())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class AnyNet(d2l.Classifier):
    def stem(self, num_channels):
        return tf.keras.models.Sequential([
            tf.keras.layers.Conv2D(num_channels, kernel_size=3, strides=2,
                                   padding='same'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation('relu')])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class AnyNet(d2l.Classifier):
    arch: tuple
    stem_channels: int
    lr: float = 0.1
    num_classes: int = 10
    training: bool = True

    def setup(self):
        self.net = self.create_net()

    def stem(self, num_channels):
        return nn.Sequential([
            nn.Conv(num_channels, kernel_size=(3, 3), strides=(2, 2),
                    padding=(1, 1)),
            nn.BatchNorm(not self.training),
            nn.relu
        ])
</code></pre>
<p>각 단계는 <code>depth</code>개의 ResNeXt 블록으로 구성되며,
<code>num_channels</code>는 블록 너비를 지정합니다.
첫 번째 블록은 입력 이미지의 높이와 너비를 절반으로 줄인다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
@d2l.add_to_class(AnyNet)
def stage(self, depth, num_channels, groups, bot_mul):
    net = nn.Sequential()
    for i in range(depth):
        if i == 0:
            net.add(d2l.ResNeXtBlock(
                num_channels, groups, bot_mul, use_1x1conv=True, strides=2))
        else:
            net.add(d2l.ResNeXtBlock(
                num_channels, num_channels, groups, bot_mul))
    return net
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
@d2l.add_to_class(AnyNet)
def stage(self, depth, num_channels, groups, bot_mul):
    blk = []
    for i in range(depth):
        if i == 0:
            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,
                use_1x1conv=True, strides=2))
        else:
            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))
    return nn.Sequential(*blk)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
@d2l.add_to_class(AnyNet)
def stage(self, depth, num_channels, groups, bot_mul):
    net = tf.keras.models.Sequential()
    for i in range(depth):
        if i == 0:
            net.add(d2l.ResNeXtBlock(num_channels, groups, bot_mul,
                use_1x1conv=True, strides=2))
        else:
            net.add(d2l.ResNeXtBlock(num_channels, groups, bot_mul))
    return net
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(AnyNet)
def stage(self, depth, num_channels, groups, bot_mul):
    blk = []
    for i in range(depth):
        if i == 0:
            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,
                use_1x1conv=True, strides=(2, 2), training=self.training))
        else:
            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,
                                        training=self.training))
    return nn.Sequential(blk)
</code></pre>
<p>네트워크 스템, 바디, 헤드를 합쳐 AnyNet 구현을 완료합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(AnyNet)
def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):
    super(AnyNet, self).__init__()
    self.save_hyperparameters()
    if tab.selected('mxnet'):
        self.net = nn.Sequential()
        self.net.add(self.stem(stem_channels))
        for i, s in enumerate(arch):
            self.net.add(self.stage(*s))
        self.net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))
        self.net.initialize(init.Xavier())
    if tab.selected('pytorch'):
        self.net = nn.Sequential(self.stem(stem_channels))
        for i, s in enumerate(arch):
            self.net.add_module(f'stage{i+1}', self.stage(*s))
        self.net.add_module('head', nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.LazyLinear(num_classes)))
        self.net.apply(d2l.init_cnn)
    if tab.selected('tensorflow'):
        self.net = tf.keras.models.Sequential(self.stem(stem_channels))
        for i, s in enumerate(arch):
            self.net.add(self.stage(*s))
        self.net.add(tf.keras.models.Sequential([
            tf.keras.layers.GlobalAvgPool2D(),
            tf.keras.layers.Dense(units=num_classes)]))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(AnyNet)
def create_net(self):
    net = nn.Sequential([self.stem(self.stem_channels)])
    for i, s in enumerate(self.arch):
        net.layers.extend([self.stage(*s)])
    net.layers.extend([nn.Sequential([
        lambda x: nn.avg_pool(x, window_shape=x.shape[1:3],
                            strides=x.shape[1:3], padding='valid'),
        lambda x: x.reshape((x.shape[0], -1)),
        nn.Dense(self.num_classes)])])
    return net
</code></pre>
<h2 id="설계-공간의-분포-및-파라미터-distributions-and-parameters-of-design-spaces"><a class="header" href="#설계-공간의-분포-및-파라미터-distributions-and-parameters-of-design-spaces">설계 공간의 분포 및 파라미터 (Distributions and Parameters of Design Spaces)</a></h2>
<p>:numref:<code>subsec_the-anynet-design-space</code>에서 논의했듯이 설계 공간의 파라미터는 해당 설계 공간에 있는 네트워크의 하이퍼파라미터입니다.
AnyNet 설계 공간에서 좋은 파라미터를 식별하는 문제를 고려해 보십시오.
우리는 주어진 계산량(예: FLOPs 및 컴퓨팅 시간)에 대해 <em>단일 최고의</em> 파라미터 선택을 찾으려고 시도할 수 있습니다.
각 파라미터에 대해 <em>두 가지</em> 가능한 선택만 허용하더라도, 최고의 솔루션을 찾기 위해 $2^{17} = 131072$ 조합을 탐색해야 합니다.
이는 엄청난 비용 때문에 명백히 실행 불가능합니다.
설상가상으로 우리는 네트워크를 어떻게 설계해야 하는지에 대해 이 연습에서 실제로 아무것도 배우지 못합니다.
다음에 예를 들어 X-스테이지, 시프트 연산 또는 이와 유사한 것을 추가하면 처음부터 다시 시작해야 합니다.
더 나쁜 것은 훈련의 확률성(반올림, 셔플링, 비트 오류) 때문에 두 번의 실행이 정확히 동일한 결과를 생성할 가능성이 낮다는 것입니다.
더 나은 전략은 파라미터 선택이 어떻게 관련되어야 하는지에 대한 일반적인 지침을 결정하려고 노력하는 것입니다.
예를 들어 병목 비율, 채널 수, 블록, 그룹 또는 레이어 간의 변경은 이상적으로는 일련의 간단한 규칙에 의해 관리되어야 합니다.
:citet:<code>radosavovic2019network</code>의 접근 방식은 다음 네 가지 가정에 의존합니다:</p>
<ol>
<li>우리는 일반적인 설계 원칙이 실제로 존재한다고 가정하므로 이러한 요구 사항을 충족하는 많은 네트워크가 좋은 성능을 제공해야 합니다. 결과적으로 네트워크에 대한 <em>분포</em>를 식별하는 것은 합리적인 전략이 될 수 있습니다. 즉, 건초더미에 좋은 바늘이 많이 있다고 가정합니다.</li>
<li>네트워크가 좋은지 평가하기 위해 수렴될 때까지 네트워크를 훈련할 필요는 없습니다. 대신 중간 결과를 최종 정확도에 대한 신뢰할 수 있는 지침으로 사용하는 것으로 충분합니다. 목적 함수를 최적화하기 위해 (근사) 프록시를 사용하는 것을 다중 충실도 최적화(multi-fidelity optimization)라고 합니다 :cite:<code>forrester2007multi</code>. 결과적으로 데이터셋을 몇 번 통과한 후 달성한 정확도를 기반으로 설계 최적화가 수행되어 비용을 크게 줄입니다.</li>
<li>더 작은 규모(더 작은 네트워크)에서 얻은 결과는 더 큰 규모로 일반화됩니다. 결과적으로 최적화는 구조적으로 유사하지만 블록 수가 적고 채널이 적은 네트워크에 대해 수행됩니다. 결국에만 이렇게 찾은 네트워크가 대규모에서도 좋은 성능을 제공하는지 확인해야 합니다.</li>
<li>설계의 측면은 대략적으로 인수 분해될 수 있으므로 결과의 품질에 미치는 영향을 다소 독립적으로 추론할 수 있습니다. 즉, 최적화 문제는 적당히 쉽습니다.</li>
</ol>
<p>이러한 가정을 통해 우리는 많은 네트워크를 저렴하게 테스트할 수 있습니다. 특히 구성 공간에서 균일하게 <em>샘플링</em>하고 성능을 평가할 수 있습니다.
그 후, 해당 네트워크로 달성할 수 있는 오류/정확도의 <em>분포</em>를 검토하여 파라미터 선택의 품질을 평가할 수 있습니다.
$F(e)$를 확률 분포 $p$를 사용하여 추출된 주어진 설계 공간의 네트워크가 범한 오류에 대한 누적 분포 함수(CDF)로 표시합니다. 즉,</p>
<p>$$F(e, p) \stackrel{\textrm{def}}{=} P_{\textrm{net} \sim p} {e(\textrm{net}) \leq e}.$$</p>
<p>우리의 목표는 이제 대부분의 네트워크가 매우 낮은 오류율을 갖고 $p$의 지원(support)이 간결한 <em>네트워크</em>에 대한 분포 $p$를 찾는 것입니다.
물론 이것을 정확하게 수행하는 것은 계산적으로 실행 불가능합니다.
우리는 $p$에서 네트워크 샘플 $\mathcal{Z} \stackrel{\textrm{def}}{=} {\textrm{net}_1, \ldots \textrm{net}_n}$ (각각 오류 $e_1, \ldots, e_n$ 포함)에 의존하고 대신 경험적 CDF $\hat{F}(e, \mathcal{Z})$를 사용합니다:</p>
<p>$$\hat{F}(e, \mathcal{Z}) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(e_i \leq e).$$</p>
<p>한 선택 세트에 대한 CDF가 다른 CDF를 지배(majorizes)(또는 일치)할 때마다 파라미터 선택이 우월(또는 무관)하다는 결론이 나옵니다.
이에 따라 :citet:<code>Radosavovic.Kosaraju.Girshick.ea.2020</code>는 네트워크의 모든 단계 $i$에 대해 공유 네트워크 병목 비율 $k_i = k$를 실험했습니다.
이것은 병목 비율을 지배하는 4개의 파라미터 중 3개를 제거합니다.
이것이 성능에 (부정적인) 영향을 미치는지 평가하기 위해 제한된 분포와 제한되지 않은 분포에서 네트워크를 추출하고 해당 CDF를 비교할 수 있습니다.
:numref:<code>fig_regnet-fig</code>의 첫 번째 패널에서 볼 수 있듯이 이 제약 조건은 네트워크 분포의 정확도에 전혀 영향을 미치지 않는 것으로 나타났습니다.
마찬가지로 네트워크의 다양한 단계에서 발생하는 동일한 그룹 너비 $g_i = g$를 선택할 수 있습니다.
다시 말하지만 :numref:<code>fig_regnet-fig</code>의 두 번째 패널에서 볼 수 있듯이 성능에는 영향을 미치지 않습니다.
두 단계를 합치면 자유 파라미터 수가 6개 줄어듭니다.</p>
<p><img src="chapter_convolutional-modern/../img/regnet-fig.png" alt="설계 공간의 오류 경험적 분포 함수 비교. $\textrm{AnyNet}\mathit{A}$는 원래 설계 공간입니다. $\textrm{AnyNet}\mathit{B}$는 병목 비율을 묶고, $\textrm{AnyNet}\mathit{C}$는 그룹 너비도 묶으며, $\textrm{AnyNet}\mathit{D}$는 단계 전반에 걸쳐 네트워크 깊이를 늘립니다. 왼쪽에서 오른쪽으로: (i) 병목 비율을 묶는 것은 성능에 영향을 미치지 않습니다; (ii) 그룹 너비를 묶는 것은 성능에 영향을 미치지 않습니다; (iii) 단계 전반에 걸쳐 네트워크 너비(채널)를 늘리면 성능이 향상됩니다; (iv) 단계 전반에 걸쳐 네트워크 깊이를 늘리면 성능이 향상됩니다. 그림 제공: :citet:Radosavovic.Kosaraju.Girshick.ea.2020." />
:label:<code>fig_regnet-fig</code></p>
<p>다음으로 우리는 단계의 너비와 깊이에 대한 수많은 잠재적 선택을 줄이는 방법을 찾습니다.
더 깊이 들어갈수록 채널 수가 증가해야 한다는 것은 합리적인 가정입니다. 즉, $c_i \geq c_{i-1}$ (:numref:<code>fig_regnet-fig</code>의 표기법에 따르면 $w_{i+1} \geq w_i$), 이는 $\textrm{AnyNetX}<em>D$를 산출합니다.
마찬가지로 단계가 진행됨에 따라 더 깊어져야 한다고 가정하는 것도 똑같이 합리적입니다. 즉, $d_i \geq d</em>{i-1}$, 이는 $\textrm{AnyNetX}_E$를 산출합니다.
이것은 각각 :numref:<code>fig_regnet-fig</code>의 세 번째와 네 번째 패널에서 실험적으로 검증할 수 있습니다.</p>
<h2 id="regnet"><a class="header" href="#regnet">RegNet</a></h2>
<p>결과 $\textrm{AnyNetX}_E$ 설계 공간은 해석하기 쉬운 설계 원칙을 따르는 단순한 네트워크로 구성됩니다:</p>
<ul>
<li>모든 단계 $i$에 대해 병목 비율 $k_i = k$를 공유합니다.</li>
<li>모든 단계 $i$에 대해 그룹 너비 $g_i = g$를 공유합니다.</li>
<li>단계 전반에 걸쳐 네트워크 너비를 늘립니다: $c_{i} \leq c_{i+1}$.</li>
<li>단계 전반에 걸쳐 네트워크 깊이를 늘립니다: $d_{i} \leq d_{i+1}$.</li>
</ul>
<p>이것은 우리에게 마지막 선택 세트를 남깁니다: 최종 $\textrm{AnyNetX}_E$ 설계 공간의 위 파라미터에 대한 특정 값을 선택하는 방법입니다.
$	extrm{AnyNetX}_E$의 분포에서 가장 성능이 좋은 네트워크를 연구함으로써 다음을 관찰할 수 있습니다: 네트워크의 너비는 이상적으로 네트워크 전반에 걸쳐 블록 인덱스와 함께 선형적으로 증가합니다. 즉, $c_j \approx c_0 + c_a j$, 여기서 $j$는 블록 인덱스이고 기울기 $c_a &gt; 0$입니다.
단계별로만 다른 블록 너비를 선택할 수 있으므로, 이 의존성과 일치하도록 설계된 조각별 상수 함수에 도달합니다.
또한 실험은 병목 비율 $k = 1$이 가장 잘 수행됨을 보여줍니다. 즉, 병목 현상을 전혀 사용하지 않는 것이 좋습니다.</p>
<p>관심 있는 독자는 :citet:<code>Radosavovic.Kosaraju.Girshick.ea.2020</code>를 정독하여 다양한 계산량에 대한 특정 네트워크 설계의 추가 세부 사항을 검토할 것을 권장합니다.
예를 들어 효과적인 32레이어 RegNetX 변형은 $k = 1$(병목 없음), $g = 16$(그룹 너비 16), 첫 번째 및 두 번째 단계에 대해 각각 $c_1 = 32$ 및 $c_2 = 80$ 채널, 깊이는 $d_1=4$ 및 $d_2=6$ 블록으로 선택됩니다.
설계에서 얻은 놀라운 통찰력은 더 큰 규모의 네트워크를 조사할 때에도 여전히 적용된다는 것입니다.
더 좋은 점은 전역 채널 활성화를 가진 Squeeze-and-Excitation(SE) 네트워크 설계(RegNetY)에도 적용된다는 것입니다 :cite:<code>Hu.Shen.Sun.2018</code>.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class RegNetX32(AnyNet):
    def __init__(self, lr=0.1, num_classes=10):
        stem_channels, groups, bot_mul = 32, 16, 1
        depths, channels = (4, 6), (32, 80)
        super().__init__(
            ((depths[0], channels[0], groups, bot_mul),
             (depths[1], channels[1], groups, bot_mul)),
            stem_channels, lr, num_classes)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class RegNetX32(AnyNet):
    lr: float = 0.1
    num_classes: int = 10
    stem_channels: int = 32
    arch: tuple = ((4, 32, 16, 1), (6, 80, 16, 1))
</code></pre>
<p>각 RegNetX 단계가 점진적으로 해상도를 줄이고 출력 채널을 늘리는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
RegNetX32().layer_summary((1, 1, 96, 96))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
RegNetX32().layer_summary((1, 96, 96, 1))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
RegNetX32(training=False).layer_summary((1, 96, 96, 1))
</code></pre>
<h2 id="훈련-training-15"><a class="header" href="#훈련-training-15">훈련 (Training)</a></h2>
<p>Fashion-MNIST 데이터셋에서 32레이어 RegNetX를 훈련하는 것은 이전과 같습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
model = RegNetX32(lr=0.05)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
trainer = d2l.Trainer(max_epochs=10)
data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
with d2l.try_gpu():
    model = RegNetX32(lr=0.01)
    trainer.fit(model, data)
</code></pre>
<h2 id="토론-discussion-4"><a class="header" href="#토론-discussion-4">토론 (Discussion)</a></h2>
<p>비전에 대한 지역성 및 평행 이동 불변성(:numref:<code>sec_why-conv</code>)과 같은 바람직한 귀납적 편향(가정 또는 선호도)으로 인해 CNN은 이 분야에서 지배적인 아키텍처였습니다.
이것은 LeNet부터 Transformer(:numref:<code>sec_transformer</code>) :cite:<code>Dosovitskiy.Beyer.Kolesnikov.ea.2021,touvron2021training</code>가 정확도 측면에서 CNN을 능가하기 시작할 때까지 유지되었습니다.
비전 Transformer 측면의 최근 진전 중 많은 부분이 CNN으로 백포트(backported) <em>될 수</em> 있지만 :cite:<code>liu2022convnet</code>, 더 높은 계산 비용으로만 가능합니다.
마찬가지로 중요한 것은 최근의 하드웨어 최적화(NVIDIA Ampere 및 Hopper)가 Transformer에 유리한 격차를 넓혔다는 것입니다.</p>
<p>Transformer는 CNN보다 지역성 및 평행 이동 불변성에 대한 귀납적 편향 정도가 상당히 낮다는 점에 주목할 가치가 있습니다.
학습된 구조가 우세했던 것은 무엇보다도 최대 50억 개의 이미지가 있는 LAION-400m 및 LAION-5B :cite:<code>schuhmann2022laion</code>와 같은 대규모 이미지 컬렉션의 가용성 때문입니다.
놀랍게도 이 맥락에서 더 관련성 있는 작업 중 일부는 MLP를 포함하기도 합니다 :cite:<code>tolstikhin2021mlp</code>.</p>
<p>요약하자면, 비전 Transformer(:numref:<code>sec_vision-transformer</code>)는 이제 대규모 이미지 분류에서 최첨단 성능을 주도하며 <em>확장성이 귀납적 편향을 이긴다</em>는 것을 보여줍니다 :cite:<code>Dosovitskiy.Beyer.Kolesnikov.ea.2021</code>.
여기에는 다중 헤드 자체 주의(:numref:<code>sec_multihead-attention</code>)가 있는 대규모 Transformer 사전 훈련(:numref:<code>sec_large-pretraining-transformers</code>)이 포함됩니다.
우리는 독자들이 훨씬 더 자세한 토론을 위해 이 장들에 뛰어들기를 권합니다.</p>
<h2 id="연습-문제-exercises-41"><a class="header" href="#연습-문제-exercises-41">연습 문제 (Exercises)</a></h2>
<ol>
<li>단계 수를 4개로 늘리십시오. 더 잘 수행되는 더 깊은 RegNetX를 설계할 수 있습니까?</li>
<li>ResNeXt 블록을 ResNet 블록으로 교체하여 RegNet을 De-ResNeXt-ify하십시오. 새 모델은 어떻게 수행됩니까?</li>
<li>RegNetX의 설계 원칙을 <em>위반</em>하여 "VioNet" 패밀리의 여러 인스턴스를 구현하십시오. 그들은 어떻게 수행됩니까? ($d_i$, $c_i$, $g_i$, $b_i$) 중 가장 중요한 요소는 무엇입니까?</li>
<li>당신의 목표는 "완벽한" MLP를 설계하는 것입니다. 위에서 소개한 설계 원칙을 사용하여 좋은 아키텍처를 찾을 수 있습니까? 작은 네트워크에서 큰 네트워크로 외삽하는 것이 가능합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/7462">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/7463">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/8738">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18009">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="순환-신경망-recurrent-neural-networks"><a class="header" href="#순환-신경망-recurrent-neural-networks">순환 신경망 (Recurrent Neural Networks)</a></h1>
<p>:label:<code>chap_rnn</code></p>
<p>지금까지 우리는 주로 고정 길이 데이터에 초점을 맞추었습니다.
:numref:<code>chap_regression</code>과 :numref:<code>chap_classification</code>에서 선형 및 로지스틱 회귀를 소개하고 :numref:<code>chap_perceptrons</code>에서 다층 퍼셉트론을 소개할 때,
우리는 각 특성 벡터 $\mathbf{x}_i$가 고정된 수의 구성 요소 $x_1, \dots, x_d$로 구성되어 있다고 가정했습니다.
여기서 각 수치 특성 $x_j$는 특정 속성에 해당합니다.
이러한 데이터셋은 때때로 *표 형식(tabular)*이라고 불립니다. 테이블에 배열될 수 있고, 각 예제 $i$는 고유한 행을 갖고 각 속성은 고유한 열을 갖기 때문입니다.
결정적으로 표 형식 데이터의 경우 열에 대한 특정 구조를 거의 가정하지 않습니다.</p>
<p>그 후 :numref:<code>chap_cnn</code>에서 우리는 이미지 데이터로 넘어갔는데, 여기서 입력은 이미지의 각 좌표에 있는 원시 픽셀 값으로 구성됩니다.
이미지 데이터는 전형적인 표 형식 데이터셋의 요건에 거의 맞지 않습니다.
거기서 우리는 계층적 구조와 불변성을 처리하기 위해 합성곱 신경망(CNN)을 호출해야 했습니다.
그러나 우리 데이터는 여전히 고정된 길이였습니다.
모든 Fashion-MNIST 이미지는 $28 \times 28$ 픽셀 값 그리드로 표현됩니다.
게다가 우리의 목표는 단 하나의 이미지를 보고 단일 예측을 출력하는 모델을 개발하는 것이었습니다.
하지만 비디오처럼 이미지의 시퀀스에 직면하거나,
이미지 캡션의 경우처럼 순차적으로 구조화된 예측을 생성해야 하는 임무를 맡으면 어떻게 해야 할까요?</p>
<p>수많은 학습 작업은 순차 데이터를 처리해야 합니다.
이미지 캡션, 음성 합성, 음악 생성 모두 모델이 시퀀스로 구성된 출력을 생성해야 합니다.
시계열 예측, 비디오 분석, 음악 정보 검색과 같은 다른 도메인에서는 모델이 시퀀스인 입력으로부터 학습해야 합니다.
이러한 요구 사항은 종종 동시에 발생합니다. 한 자연어에서 다른 자연어로 텍스트 구절을 번역하거나, 대화에 참여하거나, 로봇을 제어하는 것과 같은 작업은 모델이 순차적으로 구조화된 데이터를 섭취하고 출력할 것을 요구합니다.</p>
<p>순환 신경망(RNN)은 노드 네트워크의 사이클로 생각할 수 있는 <em>순환(recurrent)</em> 연결을 통해 시퀀스의 역학을 포착하는 딥러닝 모델입니다.
이것은 처음에 직관적이지 않게 보일 수 있습니다.
결국 계산 순서를 모호하지 않게 만드는 것은 신경망의 피드포워드 특성입니다.
그러나 순환 엣지는 그러한 모호함이 발생하지 않도록 보장하는 정확한 방식으로 정의됩니다.
순환 신경망은 타임 스텝(또는 시퀀스 스텝)에 걸쳐 <em>펼쳐지며(unrolled)</em>, 각 스텝에서 <em>동일한</em> 기본 파라미터가 적용됩니다.
표준 연결이 각 레이어의 활성화를 <em>동일한 타임 스텝에서</em> 후속 레이어로 전파하기 위해 <em>동기적으로</em> 적용되는 반면,
순환 연결은 <em>동적으로</em> 인접한 타임 스텝에 걸쳐 정보를 전달합니다.
:numref:<code>fig_unfolded-rnn</code>의 펼쳐진 뷰가 보여주듯이,
RNN은 각 레이어의 파라미터(기존 및 순환 모두)가 타임 스텝에 걸쳐 공유되는 피드포워드 신경망으로 생각할 수 있습니다.</p>
<p><img src="chapter_recurrent-neural-networks/../img/unfolded-rnn.svg" alt="왼쪽에는 순환 연결이 순환 엣지를 통해 묘사되어 있습니다. 오른쪽에는 타임 스텝에 걸쳐 RNN을 펼칩니다. 여기서 순환 엣지는 인접한 타임 스텝에 걸쳐 있는 반면, 기존 연결은 동기적으로 계산됩니다." />
:label:<code>fig_unfolded-rnn</code></p>
<p>더 넓게 신경망과 마찬가지로, RNN은 인지 과학자들에 의해 대중화된 뇌 모델로 시작하여 나중에 머신러닝 커뮤니티에서 실용적인 모델링 도구로 채택된 긴 학제 간 역사를 가지고 있습니다.
우리가 딥러닝에 대해 더 넓게 하는 것처럼, 이 책에서는 머신러닝 관점을 채택하여 필기 인식 :cite:<code>graves2008novel</code>, 기계 번역 :cite:<code>Sutskever.Vinyals.Le.2014</code>, 의학적 진단 인식 :cite:<code>Lipton.Kale.2016</code>과 같은 다양한 작업에서 획기적인 결과를 얻어 2010년대에 인기를 얻은 실용적인 도구로서의 RNN에 중점을 둡니다.
더 많은 배경 자료에 관심이 있는 독자는 공개적으로 사용 가능한 포괄적인 리뷰 :cite:<code>Lipton.Berkowitz.Elkan.2015</code>를 참조하십시오.
또한 순차성은 RNN에만 고유한 것이 아닙니다.
예를 들어 우리가 이미 소개한 CNN은 다양한 길이의 데이터(예: 다양한 해상도의 이미지)를 처리하도록 조정될 수 있습니다.
더욱이 RNN은 최근 :numref:<code>chap_attention-and-transformers</code>에서 다룰 Transformer 모델에 상당한 시장 점유율을 내주었습니다.
그러나 RNN은 딥러닝에서 복잡한 순차 구조를 처리하기 위한 기본 모델로 부상했으며 오늘날까지 순차 모델링을 위한 주류 모델로 남아 있습니다.
RNN과 시퀀스 모델링의 이야기는 불가분의 관계에 있으며, 이것은 RNN에 관한 장인 만큼 시퀀스 모델링 문제의 기초에 관한 장이기도 합니다.</p>
<p>한 가지 핵심 통찰력이 시퀀스 모델링의 혁명을 위한 길을 열었습니다.
머신러닝의 많은 기본 작업에 대한 입력과 목표는 고정 길이 벡터로 쉽게 표현될 수 없지만,
그럼에도 불구하고 종종 고정 길이 벡터의 가변 길이 시퀀스로 표현될 수 있습니다.
예를 들어 문서는 단어의 시퀀스로 표현될 수 있고;
의료 기록은 종종 사건(진료, 투약, 절차, 실험실 검사, 진단)의 시퀀스로 표현될 수 있으며;
비디오는 정지 이미지의 가변 길이 시퀀스로 표현될 수 있습니다.</p>
<p>시퀀스 모델은 수많은 응용 분야에서 등장했지만, 이 분야의 기초 연구는 주로 자연어 처리의 핵심 작업에 대한 발전에 의해 주도되었습니다.
따라서 이 장 전체에서 우리는 텍스트 데이터에 대한 설명과 예제에 초점을 맞출 것입니다.
이 예제들의 요령을 터득하면 모델을 다른 데이터 양식에 적용하는 것은 비교적 간단할 것입니다.
다음 몇 섹션에서는 시퀀스에 대한 기본 표기법과 순차적으로 구조화된 모델 출력의 품질을 평가하기 위한 몇 가지 평가 척도를 소개합니다.
그 후 언어 모델의 기본 개념을 논의하고 이 논의를 사용하여 첫 번째 RNN 모델에 동기를 부여합니다.
마지막으로 RNN을 통한 역전파 시 기울기를 계산하는 방법을 설명하고 그러한 네트워크를 훈련할 때 종종 직면하는 몇 가지 문제를 탐구하여, :numref:<code>chap_modern_rnn</code>에서 뒤따를 현대 RNN 아키텍처에 동기를 부여합니다.</p>
<pre><code class="language-toc">:maxdepth: 2

sequence
text-sequence
language-model
rnn
rnn-scratch
rnn-concise
bptt
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="시퀀스로-작업하기-working-with-sequences"><a class="header" href="#시퀀스로-작업하기-working-with-sequences">시퀀스로 작업하기 (Working with Sequences)</a></h1>
<p>:label:<code>sec_sequence</code></p>
<p>지금까지 우리는 입력이 단일 특성 벡터 $\mathbf{x} \in \mathbb{R}^d$로 구성된 모델에 초점을 맞추었습니다.
시퀀스를 처리할 수 있는 모델을 개발할 때 관점의 주요 변화는 이제 특성 벡터의 순서가 있는 리스트 $\mathbf{x}_1, \dots, \mathbf{x}_T$로 구성된 입력에 초점을 맞춘다는 것입니다.
여기서 각 특성 벡터 $\mathbf{x}_t$는 $\mathbb{R}^d$에 있는 타임 스텝 $t \in \mathbb{Z}^+$로 인덱싱됩니다.</p>
<p>일부 데이터셋은 단일 거대한 시퀀스로 구성됩니다.
예를 들어 기후 과학자들이 사용할 수 있는 센서 판독값의 매우 긴 스트림을 고려하십시오.
이러한 경우, 미리 결정된 길이의 하위 시퀀스를 무작위로 샘플링하여 훈련 데이터셋을 만들 수 있습니다.
더 자주 데이터는 시퀀스 모음으로 도착합니다.
다음 예를 고려하십시오:
(i) 문서 모음, 각 문서는 고유한 단어 시퀀스로 표현되고 고유한 길이 $T_i$를 가짐;
(ii) 병원 환자 입원의 시퀀스 표현, 각 입원은 여러 사건으로 구성되고 시퀀스 길이는 대략 입원 기간에 따라 다름.</p>
<p>이전에는 개별 입력을 다룰 때 동일한 기저 분포 $P(X)$에서 독립적으로 샘플링되었다고 가정했습니다.
우리는 여전히 전체 시퀀스(예: 전체 문서 또는 환자 궤적)가 독립적으로 샘플링된다고 가정하지만,
각 타임 스텝에 도착하는 데이터가 서로 독립적이라고 가정할 수는 없습니다.
예를 들어 문서 뒷부분에 나타날 가능성이 높은 단어는 문서 앞부분에 나타나는 단어에 크게 의존합니다.
병원 방문 10일째에 환자가 받을 가능성이 있는 약은 이전 9일 동안 일어난 일에 크게 의존합니다.</p>
<p>이것은 놀라운 일이 아닙니다.
시퀀스의 요소가 관련이 없다고 믿었다면 애초에 시퀀스로 모델링하지 않았을 것입니다.
검색 도구와 최신 이메일 클라이언트에서 널리 사용되는 자동 완성 기능의 유용성을 고려하십시오.
초기 접두사가 주어졌을 때 시퀀스의 가능한 연속이 무엇일지 예측하는 것이(불완전하지만 무작위 추측보다는 낫게) 종종 가능하기 때문에 유용합니다.
대부분의 시퀀스 모델의 경우, 시퀀스의 독립성이나 심지어 정상성(stationarity)도 요구하지 않습니다.
대신 시퀀스 자체가 전체 시퀀스에 대한 고정된 기저 분포에서 샘플링되어야 한다는 것만 요구합니다.</p>
<p>이 유연한 접근 방식은 (i) 문서가 처음과 끝에서 상당히 다르게 보이거나;
(ii) 병원 입원 기간 동안 환자 상태가 회복 또는 사망 쪽으로 진화하거나;
(iii) 추천 시스템과의 지속적인 상호 작용 과정에서 고객 취향이 예측 가능한 방식으로 진화하는 것과 같은 현상을 허용합니다.</p>
<p>때로는 순차적으로 구조화된 입력이 주어졌을 때 고정된 타겟 $y$를 예측하고 싶을 때가 있습니다(예: 영화 리뷰 기반 감성 분석).
다른 때는 고정된 입력이 주어졌을 때 순차적으로 구조화된 타겟($y_1, \ldots, y_T$)을 예측하고 싶을 때가 있습니다(예: 이미지 캡션).
또 다른 경우에는 순차적으로 구조화된 입력을 기반으로 순차적으로 구조화된 타겟을 예측하는 것이 목표입니다(예: 기계 번역 또는 비디오 캡션).
이러한 시퀀스-투-시퀀스 작업은 두 가지 형태를 취합니다:
(i) <em>정렬된(aligned)</em>: 각 타임 스텝의 입력이 해당 타겟과 정렬되는 경우(예: 품사 태깅);
(ii) <em>정렬되지 않은(unaligned)</em>: 입력과 타겟이 반드시 단계별 대응을 보이지 않는 경우(예: 기계 번역).</p>
<p>어떤 종류의 타겟을 처리하는 것에 대해 걱정하기 전에 가장 간단한 문제인 비지도 밀도 모델링(또는 <em>시퀀스 모델링</em>)을 다룰 수 있습니다.
여기서 시퀀스 모음이 주어지면 우리의 목표는 주어진 시퀀스를 볼 가능성이 얼마나 되는지, 즉 $p(\mathbf{x}_1, \ldots, \mathbf{x}_T)$를 알려주는 확률 질량 함수를 추정하는 것입니다.</p>
<pre><code class="language-{.python .input  n=6}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, gluon, init
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=8}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
import numpy as np
</code></pre>
<h2 id="자기회귀-모델-autoregressive-models"><a class="header" href="#자기회귀-모델-autoregressive-models">자기회귀 모델 (Autoregressive Models)</a></h2>
<p>순차적으로 구조화된 데이터를 처리하도록 설계된 특수 신경망을 소개하기 전에, 실제 시퀀스 데이터를 살펴보고 몇 가지 기본적인 직관과 통계 도구를 구축해 보겠습니다.
특히 FTSE 100 지수의 주가 데이터에 초점을 맞출 것입니다 (:numref:<code>fig_ftse100</code>).
각 <em>타임 스텝</em> $t \in \mathbb{Z}^+$에서 우리는 그 시점의 지수 가격 $x_t$를 관찰합니다.</p>
<p><img src="chapter_recurrent-neural-networks/../img/ftse100.png" alt="약 30년 동안의 FTSE 100 지수." />
:width:<code>400px</code>
:label:<code>fig_ftse100</code></p>
<p>이제 트레이더가 다음 타임 스텝에서 지수가 상승할지 하락할지 믿는 것에 따라 전략적으로 지수에 진입하거나 빠져나오면서 단기 거래를 하고 싶다고 가정해 봅시다.
다른 특성(뉴스, 재무 보고 데이터 등)이 없는 경우, 후속 값을 예측하는 데 사용할 수 있는 유일한 신호는 현재까지의 가격 역사입니다.
따라서 트레이더는 다음 타임 스텝에서 지수가 취할 수 있는 가격에 대한 확률 분포</p>
<p>$$P(x_t \mid x_{t-1}, \ldots, x_1)$$</p>
<p>를 아는 데 관심이 있습니다.
연속적인 값을 갖는 확률 변수에 대한 전체 분포를 추정하는 것은 어려울 수 있지만, 트레이더는 분포의 몇 가지 주요 통계, 특히 기댓값과 분산에 집중하는 것으로 만족할 것입니다.
조건부 기댓값</p>
<p>$$\mathbb{E}[(x_t \mid x_{t-1}, \ldots, x_1)],$$</p>
<p>을 추정하기 위한 한 가지 간단한 전략은 선형 회귀 모델을 적용하는 것일 수 있습니다(:numref:<code>sec_linear_regression</code> 상기).
신호 값을 동일한 신호의 이전 값으로 회귀하는 이러한 모델을 자연스럽게 *자기회귀 모델(autoregressive models)*이라고 합니다.
한 가지 큰 문제가 있습니다: 입력 수 $x_{t-1}, \ldots, x_1$이 $t$에 따라 달라집니다.
즉, 우리가 마주치는 데이터의 양에 따라 입력 수가 증가합니다.
따라서 과거 데이터를 훈련 세트로 취급하려면 각 예제마다 특성 수가 다르다는 문제에 봉착하게 됩니다.
이 장의 나머지 내용 대부분은 관심 대상이 $P(x_t \mid x_{t-1}, \ldots, x_1)$ 또는 이 분포의 일부 통계인 <em>자기회귀</em> 모델링 문제에 참여할 때 이러한 문제를 극복하기 위한 기술을 중심으로 전개될 것입니다.</p>
<p>몇 가지 전략이 자주 반복됩니다.
우선, 긴 시퀀스 $x_{t-1}, \ldots, x_1$을 사용할 수 있지만 가까운 미래를 예측할 때 역사적으로 그렇게 멀리 되돌아볼 필요는 없을 수 있다고 믿을 수 있습니다.
이 경우 우리는 길이 $\tau$의 어떤 윈도우에 조건을 걸고 $x_{t-1}, \ldots, x_{t-\tau}$ 관찰만 사용하는 것으로 만족할 수 있습니다.
즉각적인 이점은 이제 적어도 $t &gt; \tau$에 대해 인수 수가 항상 동일하다는 것입니다.
이를 통해 고정 길이 벡터를 입력으로 필요로 하는 모든 선형 모델 또는 심층 네트워크를 훈련할 수 있습니다.
둘째, 과거 관찰의 요약 $h_t$를 유지하는 모델을 개발할 수 있습니다(:numref:<code>fig_sequence-model</code> 참조). 동시에 예측 $\hat{x}<em>t$ 외에 $h_t$를 업데이트합니다.
이것은 $\hat{x}<em>t = P(x_t \mid h</em>{t})$로 $x_t$를 추정할 뿐만 아니라 $h_t = g(h</em>{t-1}, x_{t-1})$ 형태의 업데이트도 수행하는 모델로 이어집니다.
$h_t$는 결코 관찰되지 않으므로 이러한 모델을 *잠재 자기회귀 모델(latent autoregressive models)*이라고도 합니다.</p>
<p><img src="chapter_recurrent-neural-networks/../img/sequence-model.svg" alt="잠재 자기회귀 모델." />
:label:<code>fig_sequence-model</code></p>
<p>과거 데이터로부터 훈련 데이터를 구성하기 위해 일반적으로 윈도우를 무작위로 샘플링하여 예제를 생성합니다.
일반적으로 우리는 시간이 멈출 것이라고 기대하지 않습니다.
그러나 $x_t$의 특정 값은 바뀔 수 있지만, 이전 관찰이 주어졌을 때 각 후속 관찰이 생성되는 역학은 변하지 않는다고 종종 가정합니다.
통계학자들은 변하지 않는 역학을 *정상적(stationary)*이라고 부릅니다.</p>
<h2 id="시퀀스-모델-sequence-models"><a class="header" href="#시퀀스-모델-sequence-models">시퀀스 모델 (Sequence Models)</a></h2>
<p>때로는 특히 언어 작업을 할 때 전체 시퀀스의 결합 확률을 추정하고 싶을 때가 있습니다.
이것은 단어와 같은 이산 <em>토큰</em>으로 구성된 시퀀스로 작업할 때 일반적인 작업입니다.
일반적으로 이렇게 추정된 함수를 <em>시퀀스 모델</em>이라고 하며 자연어 데이터의 경우 <em>언어 모델</em>이라고 합니다.
시퀀스 모델링 분야는 자연어 처리에 의해 너무 많이 주도되어 비언어 데이터를 다룰 때조차 시퀀스 모델을 종종 "언어 모델"이라고 설명합니다.
언어 모델은 온갖 이유로 유용함이 입증되었습니다.
때로는 문장의 우도(likelihood)를 평가하고 싶을 때가 있습니다.
예를 들어 기계 번역 시스템이나 음성 인식 시스템이 생성한 두 후보 출력의 자연스러움을 비교하고 싶을 수 있습니다.
하지만 언어 모델링은 우도를 <em>평가</em>할 수 있는 능력뿐만 아니라 시퀀스를 <em>샘플링</em>하고 가장 가능성 있는 시퀀스에 대해 최적화할 수 있는 능력도 제공합니다.</p>
<p>언어 모델링이 언뜻 보기에는 자기회귀 문제처럼 보이지 않을 수 있지만,
확률의 연쇄 법칙을 적용하여 시퀀스의 결합 밀도 $p(x_1, \ldots, x_T)$를 왼쪽에서 오른쪽 방식으로 조건부 밀도의 곱으로 분해함으로써 언어 모델링을 자기회귀 예측으로 줄일 수 있습니다:</p>
<p>$$P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1).$$</p>
<p>단어와 같은 이산 신호로 작업하는 경우,
자기회귀 모델은 확률적 분류기여야 하며,
왼쪽 문맥이 주어졌을 때 다음에 올 단어에 대해 어휘 전체에 걸친 전체 확률 분포를 출력해야 한다는 점에 유의하십시오.</p>
<h3 id="마르코프-모델-markov-models"><a class="header" href="#마르코프-모델-markov-models">마르코프 모델 (Markov Models)</a></h3>
<p>:label:<code>subsec_markov-models</code></p>
<p>이제 전체 시퀀스 역사 $x_{t-1}, \ldots, x_1$보다는 $\tau$개의 이전 타임 스텝, 즉 $x_{t-1}, \ldots, x_{t-\tau}$에만 조건을 거는 위에서 언급한 전략을 채택하고 싶다고 가정해 봅시다.
예측력의 손실 없이 이전 $\tau$ 단계 너머의 역사를 버릴 수 있을 때마다,
우리는 시퀀스가 <em>마르코프 조건</em>을 만족한다고 말합니다. 즉, <em>미래는 최근 역사가 주어졌을 때 과거와 조건부 독립</em>입니다.
$\tau = 1$일 때 데이터는 <em>1차 마르코프 모델</em>로 특징지어진다고 말하고, $\tau = k$일 때 데이터는 <em>$k$차 마르코프 모델</em>로 특징지어진다고 말합니다.
1차 마르코프 조건이 성립할 때($\tau = 1$), 결합 확률의 인수분해는 이전 <em>단어</em>가 주어졌을 때 각 단어의 확률의 곱이 됩니다:</p>
<p>$$P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}).$$</p>
<p>우리는 마르코프 조건이 <em>대략적으로</em>만 사실이라는 것을 알고 있을 때조차도 마치 마르코프 조건이 충족된 것처럼 진행하는 모델로 작업하는 것이 유용하다는 것을 종종 발견합니다.
실제 텍스트 문서의 경우 왼쪽 문맥을 더 많이 포함할수록 정보를 계속 얻습니다.
하지만 이러한 이득은 빠르게 줄어듭니다.
따라서 때로는 타협하여 유효성이 <em>$k$차</em> 마르코프 조건에 의존하는 모델을 훈련함으로써 계산적 및 통계적 어려움을 제거합니다.
오늘날의 거대한 RNN 및 Transformer 기반 언어 모델조차도 수천 단어 이상의 문맥을 통합하는 경우는 거의 없습니다.</p>
<p>이산 데이터의 경우, 실제 마르코프 모델은 단순히 각 문맥에서 각 단어가 발생한 횟수를 세어 $P(x_t \mid x_{t-1})$의 상대 빈도 추정치를 생성합니다.
데이터가 이산 값만 가정할 때마다(언어와 같이),
가장 가능성 있는 단어 시퀀스는 동적 프로그래밍을 사용하여 효율적으로 계산할 수 있습니다.</p>
<h3 id="디코딩-순서-the-order-of-decoding"><a class="header" href="#디코딩-순서-the-order-of-decoding">디코딩 순서 (The Order of Decoding)</a></h3>
<p>텍스트 시퀀스 $P(x_1, \ldots, x_T)$의 인수분해를 왜 왼쪽에서 오른쪽으로 가는 조건부 확률 체인으로 표현했는지 궁금할 수 있습니다.
왜 오른쪽에서 왼쪽이나 겉보기에 무작위인 순서가 아닐까요?
원칙적으로 $P(x_1, \ldots, x_T)$를 역순으로 펼치는 데는 아무런 문제가 없습니다.
결과는 유효한 인수분해입니다:</p>
<p>$$P(x_1, \ldots, x_T) = P(x_T) \prod_{t=T-1}^1 P(x_t \mid x_{t+1}, \ldots, x_T).$$</p>
<p>그러나 우리가 읽는 것과 동일한 방향(대부분의 언어에서는 왼쪽에서 오른쪽, 아랍어와 히브리어에서는 오른쪽에서 왼쪽)으로 텍스트를 인수분해하는 것이 언어 모델링 작업에 선호되는 데는 여러 가지 이유가 있습니다.
첫째, 이것은 우리가 생각하기에 더 자연스러운 방향입니다.
결국 우리는 모두 매일 텍스트를 읽으며, 이 과정은 어떤 단어와 구문이 다음에 올 가능성이 높은지 예상하는 우리의 능력에 의해 안내됩니다.
다른 사람의 문장을 얼마나 자주 완성했는지 생각해 보십시오.
따라서 이러한 순서대로의 디코딩을 선호할 다른 이유가 없더라도, 이 순서로 예측할 때 무엇이 가능성 있어야 하는지에 대한 더 나은 직관을 가지고 있기 때문에 유용할 것입니다.</p>
<p>둘째, 순서대로 인수분해함으로써 동일한 언어 모델을 사용하여 임의로 긴 시퀀스에 확률을 할당할 수 있습니다.
1단계에서 $t$까지의 확률을 단어 $t+1$까지 확장되는 확률로 변환하려면 단순히 이전 토큰이 주어졌을 때 추가 토큰의 조건부 확률을 곱하면 됩니다:
$P(x_{t+1}, \ldots, x_1) = P(x_{t}, \ldots, x_1) \cdot P(x_{t+1} \mid x_{t}, \ldots, x_1)$.</p>
<p>셋째, 우리는 임의의 다른 위치에 있는 단어보다 인접한 단어를 예측하는 데 더 강력한 예측 모델을 가지고 있습니다.
모든 순서의 인수분해가 유효하지만, 모두 똑같이 쉬운 예측 모델링 문제를 나타내는 것은 아닙니다.
이것은 언어뿐만 아니라 다른 종류의 데이터에도 해당됩니다. 예를 들어 데이터가 인과적으로 구조화된 경우입니다.
예를 들어 우리는 미래의 사건이 과거에 영향을 줄 수 없다고 믿습니다.
따라서 $x_t$를 변경하면 앞으로 $x_{t+1}$에 대해 일어나는 일에 영향을 줄 수 있지만 그 반대는 아닙니다.
즉, $x_t$를 변경해도 과거 사건에 대한 분포는 변경되지 않습니다.
일부 맥락에서 이것은 $P(x_t \mid x_{t+1})$를 예측하는 것보다 $P(x_{t+1} \mid x_t)$를 예측하는 것을 더 쉽게 만듭니다.
예를 들어 어떤 경우에는 어떤 가산적 노이즈 $\epsilon$에 대해 $x_{t+1} = f(x_t) + \epsilon$을 찾을 수 있지만, 그 역은 참이 아닙니다 :cite:<code>Hoyer.Janzing.Mooij.ea.2009</code>.
이것은 좋은 소식입니다. 우리가 추정하는 데 관심이 있는 것은 일반적으로 순방향이기 때문입니다.
:citet:<code>Peters.Janzing.Scholkopf.2017</code>의 책에는 이 주제에 대한 더 많은 내용이 포함되어 있습니다.
우리는 겉핥기만 하고 있습니다.</p>
<h2 id="훈련-training-16"><a class="header" href="#훈련-training-16">훈련 (Training)</a></h2>
<p>텍스트 데이터에 관심을 집중하기 전에 먼저 연속 값 합성 데이터로 이것을 시도해 봅시다.</p>
<p>(<strong>여기서 1000개의 합성 데이터는 타임 스텝의 0.01배에 적용된 삼각 함수 <code>sin</code>을 따릅니다.
문제를 좀 더 흥미롭게 만들기 위해 각 샘플에 가산적 노이즈를 섞습니다.</strong>)
이 시퀀스에서 각각 특성과 레이블로 구성된 훈련 예제를 추출합니다.</p>
<pre><code class="language-{.python .input  n=10}">%%tab all
class Data(d2l.DataModule):
    def __init__(self, batch_size=16, T=1000, num_train=600, tau=4):
        self.save_hyperparameters()
        self.time = d2l.arange(1, T + 1, dtype=d2l.float32)
        if tab.selected('mxnet', 'pytorch'):
            self.x = d2l.sin(0.01 * self.time) + d2l.randn(T) * 0.2
        if tab.selected('tensorflow'):
            self.x = d2l.sin(0.01 * self.time) + d2l.normal([T]) * 0.2
        if tab.selected('jax'):
            key = d2l.get_key()
            self.x = d2l.sin(0.01 * self.time) + jax.random.normal(key,
                                                                   [T]) * 0.2
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
data = Data()
d2l.plot(data.time, data.x, 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
</code></pre>
<p>시작하기 위해, 우리는 데이터가 $\tau^{\textrm{th}}$차 마르코프 조건을 만족하는 것처럼 행동하는 모델을 시도합니다.
따라서 과거 $\tau$개의 관찰만 사용하여 $x_t$를 예측합니다.
[<strong>따라서 각 타임 스텝마다 레이블 $y  = x_t$와 특성 $\mathbf{x}<em>t = [x</em>{t-\tau}, \ldots, x_{t-1}]$을 가진 예제가 있습니다.</strong>]
예리한 독자라면 $y_1, \ldots, y_\tau$에 대한 충분한 역사가 부족하기 때문에 이것이 $1000-\tau$개의 예제를 낳는다는 것을 눈치챘을 것입니다.
처음 $\tau$ 시퀀스를 0으로 채울 수도 있지만, 일을 단순하게 유지하기 위해 지금은 삭제합니다.
결과 데이터셋에는 $T - \tau$개의 예제가 포함되어 있으며, 모델에 대한 각 입력은 시퀀스 길이가 $\tau$입니다.
우리는 sin 함수의 주기를 포함하는 (<strong>처음 600개 예제에 대한 데이터 반복자를 생성</strong>)합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(Data)
def get_dataloader(self, train):
    features = [self.x[i : self.T-self.tau+i] for i in range(self.tau)]
    self.features = d2l.stack(features, 1)
    self.labels = d2l.reshape(self.x[self.tau:], (-1, 1))
    i = slice(0, self.num_train) if train else slice(self.num_train, None)
    return self.get_tensorloader([self.features, self.labels], train, i)
</code></pre>
<p>이 예제에서 우리 모델은 표준 선형 회귀가 될 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
model = d2l.LinearRegression(lr=0.01)
trainer = d2l.Trainer(max_epochs=5)
trainer.fit(model, data)
</code></pre>
<h2 id="예측-prediction-1"><a class="header" href="#예측-prediction-1">예측 (Prediction)</a></h2>
<p>[<strong>모델을 평가하기 위해 먼저 1단계 앞선 예측에서 얼마나 잘 수행되는지 확인합니다</strong>].</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
onestep_preds = d2l.numpy(model(data.features))
d2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',
         legend=['labels', '1-step preds'], figsize=(6, 3))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
onestep_preds = model.apply({'params': trainer.state.params}, data.features)
d2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',
         legend=['labels', '1-step preds'], figsize=(6, 3))
</code></pre>
<p>이 예측들은 $t=1000$인 끝부분 근처에서도 좋아 보입니다.</p>
<p>하지만 타임 스텝 604(<code>n_train + tau</code>)까지만 시퀀스 데이터를 관찰했고 미래로 몇 단계 예측하고 싶다면 어떨까요?
불행히도 타임 스텝 609에 대한 1단계 앞선 예측을 직접 계산할 수 없습니다. $x_{604}$까지만 보았기 때문에 해당 입력을 모르기 때문입니다.
우리는 이전 예측을 후속 예측을 위한 모델의 입력으로 꽂아 넣고, 원하는 타임 스텝에 도달할 때까지 한 번에 한 단계씩 앞으로 투영함으로써 이 문제를 해결할 수 있습니다:</p>
<p>$$\begin{aligned}
\hat{x}<em>{605} &amp;= f(x</em>{601}, x_{602}, x_{603}, x_{604}), \n
\hat{x}<em>{606} &amp;= f(x</em>{602}, x_{603}, x_{604}, \hat{x}<em>{605}), \n
\hat{x}</em>{607} &amp;= f(x_{603}, x_{604}, \hat{x}<em>{605}, \hat{x}</em>{606}),\n
\hat{x}<em>{608} &amp;= f(x</em>{604}, \hat{x}<em>{605}, \hat{x}</em>{606}, \hat{x}<em>{607}),\n
\hat{x}</em>{609} &amp;= f(\hat{x}<em>{605}, \hat{x}</em>{606}, \hat{x}<em>{607}, \hat{x}</em>{608}),\n
&amp;\vdots
\end{aligned}$$</p>
<p>일반적으로 관찰된 시퀀스 $x_1, \ldots, x_t$에 대해, 타임 스텝 $t+k$에서의 예측 출력 $\hat{x}<em>{t+k}$를 $k$<em>-단계 앞선 예측</em>이라고 합니다.
우리는 $x</em>{604}$까지 관찰했으므로 $k$-단계 앞선 예측은 $\hat{x}_{604+k}$입니다.
다시 말해, 다단계 앞선 예측을 하려면 우리 자신의 예측을 계속 사용해야 합니다.
이것이 어떻게 진행되는지 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
multistep_preds = d2l.zeros(data.T)
multistep_preds[:] = data.x
for i in range(data.num_train + data.tau, data.T):
    multistep_preds[i] = model(
        d2l.reshape(multistep_preds[i-data.tau : i], (1, -1)))
multistep_preds = d2l.numpy(multistep_preds)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
multistep_preds = tf.Variable(d2l.zeros(data.T))
multistep_preds[:].assign(data.x)
for i in range(data.num_train + data.tau, data.T):
    multistep_preds[i].assign(d2l.reshape(model(
        d2l.reshape(multistep_preds[i-data.tau : i], (1, -1))), ()))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
multistep_preds = d2l.zeros(data.T)
multistep_preds = multistep_preds.at[:].set(data.x)
for i in range(data.num_train + data.tau, data.T):
    pred = model.apply({'params': trainer.state.params},
                       d2l.reshape(multistep_preds[i-data.tau : i], (1, -1)))
    multistep_preds = multistep_preds.at[i].set(pred.item())
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
d2l.plot([data.time[data.tau:], data.time[data.num_train+data.tau:]],
         [onestep_preds, multistep_preds[data.num_train+data.tau:]], 'time',
         'x', legend=['1-step preds', 'multistep preds'], figsize=(6, 3))
</code></pre>
<p>불행히도 이 경우 우리는 장엄하게 실패합니다.
예측은 몇 단계 후에 꽤 빨리 상수로 감쇠합니다.
미래로 더 멀리 예측할 때 알고리즘이 왜 그렇게 훨씬 나쁘게 수행되었을까요?
궁극적으로 이것은 오류가 쌓인다는 사실 때문입니다.
1단계 후에 오류 $\epsilon_1 = \bar\epsilon$이 있다고 가정해 봅시다.
이제 2단계에 대한 <em>입력</em>이 $\epsilon_1$에 의해 섭동되므로, 어떤 상수 $c$에 대해 $\epsilon_2 = \bar\epsilon + c \epsilon_1$ 정도의 오류를 겪게 되고, 이런 식으로 계속됩니다.
예측은 실제 관찰에서 빠르게 발산할 수 있습니다.
여러분은 이미 이 일반적인 현상에 익숙할 수 있습니다.
예를 들어 향후 24시간 동안의 일기 예보는 꽤 정확한 경향이 있지만 그 이상은 정확도가 급격히 떨어집니다.
우리는 이 장과 그 이후에 걸쳐 이를 개선하기 위한 방법을 논의할 것입니다.</p>
<p>$k = 1, 4, 16, 64$에 대해 전체 시퀀스에서 예측을 계산하여 [$k$-단계 앞선 예측의 어려움을 자세히 살펴봅시다].</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
def k_step_pred(k):
    features = []
    for i in range(data.tau):
        features.append(data.x[i : i+data.T-data.tau-k+1])
    # (i+tau)번째 요소는 (i+1)단계 앞선 예측을 저장합니다
    for i in range(k):
        preds = model(d2l.stack(features[i : i+data.tau], 1))
        features.append(d2l.reshape(preds, -1))
    return features[data.tau:]
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def k_step_pred(k):
    features = []
    for i in range(data.tau):
        features.append(data.x[i : i+data.T-data.tau-k+1])
    # (i+tau)번째 요소는 (i+1)단계 앞선 예측을 저장합니다
    for i in range(k):
        preds = model.apply({'params': trainer.state.params},
                            d2l.stack(features[i : i+data.tau], 1))
        features.append(d2l.reshape(preds, -1))
    return features[data.tau:]
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
steps = (1, 4, 16, 64)
preds = k_step_pred(steps[-1])
d2l.plot(data.time[data.tau+steps[-1]-1:],
         [d2l.numpy(preds[k-1]) for k in steps], 'time', 'x',
         legend=[f'{k}-step preds' for k in steps], figsize=(6, 3))
</code></pre>
<p>이것은 미래로 더 멀리 예측하려고 할 때 예측의 품질이 어떻게 변하는지 명확하게 보여줍니다.
4단계 앞선 예측은 여전히 좋아 보이지만 그 이상은 거의 쓸모가 없습니다.</p>
<h2 id="요약-summary-30"><a class="header" href="#요약-summary-30">요약 (Summary)</a></h2>
<p>보간과 외삽 사이에는 꽤 큰 난이도 차이가 있습니다.
결과적으로 시퀀스가 있는 경우 훈련할 때 항상 데이터의 시간 순서를 존중하십시오. 즉, 미래 데이터에 대해 훈련하지 마십시오.
이런 종류의 데이터가 주어지면 시퀀스 모델은 추정을 위한 전문 통계 도구가 필요합니다.
두 가지 인기 있는 선택은 자기회귀 모델과 잠재 변수 자기회귀 모델입니다.
인과 모델(예: 시간이 앞으로 진행됨)의 경우 순방향을 추정하는 것이 일반적으로 역방향보다 훨씬 쉽습니다.
타임 스텝 $t$까지의 관찰된 시퀀스에 대해, 타임 스텝 $t+k$에서의 예측 출력은 $k$<em>-단계 앞선 예측</em>입니다.
$k$를 늘려 시간적으로 더 멀리 예측할수록 오류가 누적되고 예측 품질이 종종 극적으로 저하됩니다.</p>
<h2 id="연습-문제-exercises-42"><a class="header" href="#연습-문제-exercises-42">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 섹션의 실험에서 모델을 개선하십시오.
<ol>
<li>과거 4개 이상의 관찰을 통합합니까? 실제로 몇 개가 필요합니까?</li>
<li>노이즈가 없다면 과거 관찰이 몇 개나 필요합니까? 힌트: $\sin$과 $\cos$를 미분 방정식으로 쓸 수 있습니다.</li>
<li>총 특성 수를 일정하게 유지하면서 더 오래된 관찰을 통합할 수 있습니까? 이것이 정확도를 향상시킵니까? 그 이유는 무엇입니까?</li>
<li>신경망 아키텍처를 변경하고 성능을 평가하십시오. 새 모델을 더 많은 에폭으로 훈련할 수 있습니다. 무엇을 관찰합니까?</li>
</ol>
</li>
<li>투자자가 매수할 좋은 증권을 찾고 싶어 합니다.
그들은 과거 수익률을 보고 어떤 것이 잘 될지 결정합니다.
이 전략에서 무엇이 잘못될 수 있습니까?</li>
<li>인과 관계가 텍스트에도 적용됩니까? 어느 정도까지입니까?</li>
<li>데이터의 역학을 포착하기 위해 잠재 자기회귀 모델이 필요할 수 있는 예를 드십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/113">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/114">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1048">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18010">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="원시-텍스트를-시퀀스-데이터로-변환하기-converting-raw-text-into-sequence-data"><a class="header" href="#원시-텍스트를-시퀀스-데이터로-변환하기-converting-raw-text-into-sequence-data">원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></h1>
<p>:label:<code>sec_text-sequence</code></p>
<p>이 책 전체에서 우리는 종종 단어, 문자 또는 단어 조각의 시퀀스로 표현되는 텍스트 데이터로 작업할 것입니다.
시작하려면 원시 텍스트를 적절한 형식의 시퀀스로 변환하기 위한 몇 가지 기본 도구가 필요합니다.
일반적인 전처리 파이프라인은 다음 단계를 실행합니다:</p>
<ol>
<li>텍스트를 문자열로 메모리에 로드합니다.</li>
<li>문자열을 토큰(예: 단어 또는 문자)으로 분할합니다.</li>
<li>각 어휘 요소를 수치 인덱스와 연관시키는 어휘 딕셔너리를 구축합니다.</li>
<li>텍스트를 수치 인덱스 시퀀스로 변환합니다.</li>
</ol>
<pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<pre><code class="language-{.python .input  n=2}">%%tab mxnet
import collections
import re
from d2l import mxnet as d2l
from mxnet import np, npx
import random
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=3}">%%tab pytorch
import collections
import re
from d2l import torch as d2l
import torch
import random
</code></pre>
<pre><code class="language-{.python .input  n=4}">%%tab tensorflow
import collections
import re
from d2l import tensorflow as d2l
import tensorflow as tf
import random
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
import collections
from d2l import jax as d2l
import jax
from jax import numpy as jnp
import random
import re
</code></pre>
<h2 id="데이터셋-읽기-reading-the-dataset-2"><a class="header" href="#데이터셋-읽기-reading-the-dataset-2">데이터셋 읽기 (Reading the Dataset)</a></h2>
<p>여기서는 30,000개 이상의 단어가 포함된 H. G. Wells의 <a href="http://www.gutenberg.org/ebooks/35">타임 머신(The Time Machine)</a> 책으로 작업할 것입니다.
실제 응용 프로그램은 일반적으로 훨씬 더 큰 데이터셋을 포함하지만, 이것은 전처리 파이프라인을 보여주기에 충분합니다.
다음 <code>_download</code> 메서드는 (<strong>원시 텍스트를 문자열로 읽습니다</strong>).</p>
<pre><code class="language-{.python .input  n=5}">%%tab all
class TimeMachine(d2l.DataModule): #@save
    """타임 머신 데이터셋."""
    def _download(self):
        fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,
                             '090b5e7e70c295757f55df93cb0a180b9691891a')
        with open(fname) as f:
            return f.read()

data = TimeMachine()
raw_text = data._download()
raw_text[:60]
</code></pre>
<p>단순함을 위해 원시 텍스트를 전처리할 때 구두점과 대문자를 무시합니다.</p>
<pre><code class="language-{.python .input  n=6}">%%tab all
@d2l.add_to_class(TimeMachine)  #@save
def _preprocess(self, text):
    return re.sub('[^A-Za-z]+', ' ', text).lower()

text = data._preprocess(raw_text)
text[:60]
</code></pre>
<h2 id="토큰화-tokenization"><a class="header" href="#토큰화-tokenization">토큰화 (Tokenization)</a></h2>
<p><em>토큰</em>은 텍스트의 원자적(나눌 수 없는) 단위입니다.
각 타임 스텝은 1개의 토큰에 해당하지만, 정확히 무엇이 토큰을 구성하는지는 설계 선택입니다.
예를 들어 "Baby needs a new pair of shoes"라는 문장을 7개의 단어 시퀀스로 표현할 수 있으며, 여기서 모든 단어의 집합은 큰 어휘(일반적으로 수만 또는 수십만 단어)를 구성합니다.
또는 훨씬 더 작은 어휘(고유한 ASCII 문자는 256개뿐임)를 사용하여 동일한 문장을 훨씬 더 긴 30개의 문자 시퀀스로 표현할 수 있습니다.
아래에서는 전처리된 텍스트를 문자 시퀀스로 토큰화합니다.</p>
<pre><code class="language-{.python .input  n=7}">%%tab all
@d2l.add_to_class(TimeMachine)  #@save
def _tokenize(self, text):
    return list(text)

tokens = data._tokenize(text)
','.join(tokens[:30])
</code></pre>
<h2 id="어휘-vocabulary"><a class="header" href="#어휘-vocabulary">어휘 (Vocabulary)</a></h2>
<p>이 토큰들은 여전히 문자열입니다.
그러나 우리 모델에 대한 입력은 궁극적으로 수치 입력으로 구성되어야 합니다.
[<strong>다음으로 <em>어휘(vocabularies)</em>, 즉 각 고유한 토큰 값을 고유한 인덱스와 연관시키는 객체를 구축하기 위한 클래스를 소개합니다.</strong>]
먼저 훈련 *말뭉치(corpus)*에서 고유한 토큰 집합을 결정합니다.
그런 다음 각 고유 토큰에 수치 인덱스를 할당합니다.
드문 어휘 요소는 편의를 위해 종종 삭제됩니다.
훈련 또는 테스트 시 이전에 보지 못했거나 어휘에서 삭제된 토큰을 만날 때마다,
이것이 <em>알 수 없는</em> 값임을 나타내는 특수 "<unk>" 토큰으로 표현합니다.</p>
<pre><code class="language-{.python .input  n=8}">%%tab all
class Vocab:  #@save
    """텍스트용 어휘."""
    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):
        # 필요한 경우 2D 리스트를 평탄화
        if tokens and isinstance(tokens[0], list):
            tokens = [token for line in tokens for token in line]
        # 토큰 빈도 계산
        counter = collections.Counter(tokens)
        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                  reverse=True)
        # 고유 토큰 리스트
        self.idx_to_token = list(sorted(set(['&lt;unk&gt;'] + reserved_tokens + [
            token for token, freq in self.token_freqs if freq &gt;= min_freq])))
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if hasattr(indices, '__len__') and len(indices) &gt; 1:
            return [self.idx_to_token[int(index)] for index in indices]
        return self.idx_to_token[indices]

    @property
    def unk(self):  # 알 수 없는 토큰의 인덱스
        return self.token_to_idx['&lt;unk&gt;']
</code></pre>
<p>이제 데이터셋에 대한 [<strong>어휘를 구축</strong>]하여 문자열 시퀀스를 수치 인덱스 리스트로 변환합니다.
정보를 잃지 않았으며 데이터셋을 원래(문자열) 표현으로 쉽게 다시 변환할 수 있다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input  n=9}">%%tab all
vocab = Vocab(tokens)
indices = vocab[tokens[:10]]
print('indices:', indices)
print('words:', vocab.to_tokens(indices))
</code></pre>
<h2 id="모두-합치기-putting-it-all-together"><a class="header" href="#모두-합치기-putting-it-all-together">모두 합치기 (Putting It All Together)</a></h2>
<p>위의 클래스와 메서드를 사용하여,
우리는 [<strong>모든 것을 <code>TimeMachine</code> 클래스의 다음 <code>build</code> 메서드에 패키징</strong>]합니다. 이 메서드는 토큰 인덱스 리스트인 <code>corpus</code>와 <em>타임 머신</em> 말뭉치의 어휘인 <code>vocab</code>을 반환합니다.
여기서 수행한 수정 사항은 다음과 같습니다:
(i) 이후 섹션의 훈련을 단순화하기 위해 텍스트를 단어가 아닌 문자로 토큰화합니다;
(ii) <em>타임 머신</em> 데이터셋의 각 텍스트 라인이 반드시 문장이나 단락인 것은 아니므로 <code>corpus</code>는 토큰 리스트의 리스트가 아니라 단일 리스트입니다.</p>
<pre><code class="language-{.python .input  n=10}">%%tab all
@d2l.add_to_class(TimeMachine)  #@save
def build(self, raw_text, vocab=None):
    tokens = self._tokenize(self._preprocess(raw_text))
    if vocab is None: vocab = Vocab(tokens)
    corpus = [vocab[token] for token in tokens]
    return corpus, vocab

corpus, vocab = data.build(raw_text)
len(corpus), len(vocab)
</code></pre>
<h2 id="탐색적-언어-통계-exploratory-language-statistics"><a class="header" href="#탐색적-언어-통계-exploratory-language-statistics">탐색적 언어 통계 (Exploratory Language Statistics)</a></h2>
<p>:label:<code>subsec_natural-lang-stat</code></p>
<p>실제 말뭉치와 단어에 대해 정의된 <code>Vocab</code> 클래스를 사용하여 말뭉치에서의 단어 사용에 관한 기본 통계를 검사할 수 있습니다.
아래에서는 <em>타임 머신</em>에서 사용된 단어로 어휘를 구축하고 가장 자주 발생하는 10개 단어를 인쇄합니다.</p>
<pre><code class="language-{.python .input  n=11}">%%tab all
words = text.split()
vocab = Vocab(words)
vocab.token_freqs[:10]
</code></pre>
<p>(<strong>가장 자주 사용되는 10개 단어</strong>)가 그리 설명적이지 않다는 점에 유의하십시오.
우리가 무작위로 아무 책이나 선택했더라도 매우 유사한 목록을 보게 될 것이라고 상상할 수도 있습니다.
"the"와 "a" 같은 관사, "i"와 "my" 같은 대명사, "of", "to", "in" 같은 전치사는 일반적인 구문론적 역할을 수행하기 때문에 자주 발생합니다.
흔하지만 특별히 설명적이지 않은 이러한 단어들을 종종 (<em><strong>불용어(stop words)</strong></em>)라고 하며,
소위 BoW(bag-of-words) 표현에 기반한 이전 세대의 텍스트 분류기에서는 가장 자주 필터링되었습니다.
그러나 현대의 RNN 및 Transformer 기반 신경망 모델로 작업할 때는 의미를 전달하므로 필터링할 필요가 없습니다.
목록을 더 아래로 내려다보면 단어 빈도가 빠르게 감소한다는 것을 알 수 있습니다.
$10^{\textrm{th}}$로 가장 빈번한 단어는 가장 인기 있는 단어보다 $1/5$ 미만으로 흔합니다.
단어 빈도는 순위가 내려갈수록 거듭제곱 법칙 분포(구체적으로는 Zipfian)를 따르는 경향이 있습니다.
더 나은 아이디어를 얻기 위해 [<strong>단어 빈도 그림을 그립니다</strong>].</p>
<pre><code class="language-{.python .input  n=12}">%%tab all
freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
</code></pre>
<p>처음 몇 단어를 예외로 처리하면, 나머지 모든 단어는 로그-로그 플롯에서 대략 직선을 따릅니다.
이 현상은 <em>Zipf의 법칙</em>으로 포착됩니다.
$i^\textrm{th}$로 가장 빈번한 단어의 빈도 $n_i$는 다음과 같습니다:</p>
<p>$$n_i \propto \frac{1}{i^\alpha},$$
:eqlabel:<code>eq_zipf_law</code></p>
<p>이는 다음과 같습니다:</p>
<p>$$\log n_i = - \alpha \log i + c,$$</p>
<p>여기서 $\alpha$는 분포를 특징짓는 지수이고 $c$는 상수입니다.
통계를 세어 단어를 모델링하고 싶다면 이것은 이미 우리에게 생각할 거리를 줍니다.
결국 우리는 드문 단어라고도 알려진 꼬리 부분의 빈도를 상당히 과대평가하게 될 것입니다. 하지만 [<strong>두 개의 연속 단어(bigrams), 세 개의 연속 단어(trigrams)와 같은 다른 단어 조합은 어떻습니까?</strong>], 그리고 그 이상은요?
bigram 빈도가 단일 단어(unigram) 빈도와 같은 방식으로 동작하는지 봅시다.</p>
<pre><code class="language-{.python .input  n=13}">%%tab all
bigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]
bigram_vocab = Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]
</code></pre>
<p>한 가지 주목할 점이 있습니다. 가장 빈번한 10개의 단어 쌍 중 9개는 불용어로 구성되어 있고 실제 책과 관련된 것은 단 하나, "the time"뿐입니다. 게다가 trigram 빈도가 같은 방식으로 동작하는지 봅시다.</p>
<pre><code class="language-{.python .input  n=14}">%%tab all
trigram_tokens = ['--'.join(triple) for triple in zip(
    words[:-2], words[1:-1], words[2:]))]
trigram_vocab = Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]
</code></pre>
<p>이제 unigrams, bigrams, trigrams 이 세 모델 간의 [<strong>토큰 빈도를 시각화</strong>]해 봅시다.</p>
<pre><code class="language-{.python .input  n=15}">%%tab all
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
</code></pre>
<p>이 그림은 꽤 흥미진진합니다.
첫째, unigram 단어를 넘어 단어 시퀀스도 시퀀스 길이에 따라 :eqref:<code>eq_zipf_law</code>에서 더 작은 지수 $\alpha$를 갖지만 Zipf의 법칙을 따르는 것으로 보입니다.
둘째, 고유한 $n$-gram의 수가 그리 크지 않습니다.
이것은 언어에 꽤 많은 구조가 있다는 희망을 줍니다.
셋째, 많은 $n$-gram이 매우 드물게 발생합니다.
이것은 특정 방법을 언어 모델링에 부적합하게 만들고 딥러닝 모델의 사용에 동기를 부여합니다.
우리는 다음 섹션에서 이에 대해 논의할 것입니다.</p>
<h2 id="요약-summary-31"><a class="header" href="#요약-summary-31">요약 (Summary)</a></h2>
<p>텍스트는 딥러닝에서 접하는 가장 일반적인 형태의 시퀀스 데이터 중 하나입니다.
토큰을 구성하는 일반적인 선택은 문자, 단어, 단어 조각입니다.
텍스트를 전처리하기 위해 우리는 보통 (i) 텍스트를 토큰으로 분할하고; (ii) 토큰 문자열을 수치 인덱스로 매핑하기 위한 어휘를 구축하고; (iii) 모델이 조작할 수 있도록 텍스트 데이터를 토큰 인덱스로 변환합니다.
실제로 단어의 빈도는 Zipf의 법칙을 따르는 경향이 있습니다. 이것은 개별 단어(unigrams)뿐만 아니라 $n$-grams에도 해당됩니다.</p>
<h2 id="연습-문제-exercises-43"><a class="header" href="#연습-문제-exercises-43">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 섹션의 실험에서 텍스트를 단어로 토큰화하고 <code>Vocab</code> 인스턴스의 <code>min_freq</code> 인수 값을 변경하십시오. <code>min_freq</code>의 변경이 결과 어휘의 크기에 미치는 영향을 정성적으로 설명하십시오.</li>
<li>이 말뭉치에서 unigrams, bigrams, trigrams에 대한 Zipfian 분포의 지수를 추정하십시오.</li>
<li>다른 데이터 소스를 찾으십시오(표준 머신러닝 데이터셋 다운로드, 다른 퍼블릭 도메인 책 선택, 웹사이트 스크래핑 등). 각각에 대해 단어 및 문자 수준 모두에서 데이터를 토큰화하십시오. <code>min_freq</code>의 동등한 값에서 어휘 크기가 <em>타임 머신</em> 말뭉치와 어떻게 비교됩니까. 이 말뭉치에 대한 unigram 및 bigram 분포에 해당하는 Zipfian 분포의 지수를 추정하십시오. <em>타임 머신</em> 말뭉치에서 관찰한 값과 어떻게 비교됩니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/117">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/118">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1049">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18011">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="언어-모델-language-models"><a class="header" href="#언어-모델-language-models">언어 모델 (Language Models)</a></h1>
<p>:label:<code>sec_language-model</code></p>
<p>:numref:<code>sec_text-sequence</code>에서 우리는 텍스트 시퀀스를 토큰으로 매핑하는 방법을 보았습니다. 여기서 토큰은 단어나 문자와 같은 이산적인 관찰의 시퀀스로 간주될 수 있습니다.
길이가 $T$인 텍스트 시퀀스의 토큰이 차례로 $x_1, x_2, \ldots, x_T$라고 가정합시다.
*언어 모델(Language models)*의 목표는 전체 시퀀스의 결합 확률을 추정하는 것입니다:</p>
<p>$$P(x_1, x_2, \ldots, x_T),$$</p>
<p>여기서 :numref:<code>sec_sequence</code>의 통계 도구를 적용할 수 있습니다.</p>
<p>언어 모델은 매우 유용합니다. 예를 들어, 이상적인 언어 모델은 단순히 한 번에 하나의 토큰 $x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$을 뽑는 것만으로 자연스러운 텍스트를 스스로 생성해야 합니다.
타자기를 사용하는 원숭이와는 달리, 그러한 모델에서 나오는 모든 텍스트는 영문 텍스트와 같은 자연어처럼 보일 것입니다.
더 나아가 이전 대화 조각에 텍스트를 조건부로 하여 의미 있는 대화를 생성하는 데에도 충분할 것입니다.
분명히 우리는 문법적으로 합리적인 콘텐츠를 생성하는 것이 아니라 텍스트를 <em>이해</em>해야 하므로 그러한 시스템을 설계하는 것과는 아직 거리가 멉니다.</p>
<p>그럼에도 불구하고 언어 모델은 제한된 형태에서도 큰 도움이 됩니다.
예를 들어 "to recognize speech"와 "to wreck a nice beach"라는 구절은 매우 비슷하게 들립니다.
이는 음성 인식에서 모호성을 유발할 수 있는데, 언어 모델을 통해 두 번째 번역을 기이한 것으로 거부함으로써 쉽게 해결할 수 있습니다.
마찬가지로 문서 요약 알고리즘에서는 "dog bites man"이 "man bites dog"보다 훨씬 더 빈번하다거나, "I want to eat grandma"는 다소 충격적인 진술인 반면 "I want to eat, grandma"는 훨씬 더 온화하다는 것을 아는 것이 가치가 있습니다.</p>
<pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<pre><code class="language-{.python .input  n=2}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=3}">%%tab pytorch
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input  n=4}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from jax import numpy as jnp
</code></pre>
<h2 id="언어-모델-학습-learning-language-models"><a class="header" href="#언어-모델-학습-learning-language-models">언어 모델 학습 (Learning Language Models)</a></h2>
<p>분명한 질문은 문서나 토큰 시퀀스를 어떻게 모델링해야 하는가입니다.
텍스트 데이터를 단어 수준에서 토큰화한다고 가정해 봅시다.
기본 확률 규칙을 적용하여 시작해 봅시다:</p>
<p>$$P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1}).$$</p>
<p>예를 들어,
네 단어를 포함하는 텍스트 시퀀스의 확률은 다음과 같이 주어집니다:</p>
<p>$$\begin{aligned}&amp;P(\textrm{deep}, \textrm{learning}, \textrm{is}, \textrm{fun}) &amp;=P(\textrm{deep}) P(\textrm{learning}  \mid  \textrm{deep}) P(\textrm{is}  \mid  \textrm{deep}, \textrm{learning}) P(\textrm{fun}  \mid  \textrm{deep}, \textrm{learning}, \textrm{is}).\end{aligned}$$</p>
<h3 id="마르코프-모델과-n-gram-markov-models-and-n-grams"><a class="header" href="#마르코프-모델과-n-gram-markov-models-and-n-grams">마르코프 모델과 $n$-gram (Markov Models and $n$-grams)</a></h3>
<p>:label:<code>subsec_markov-models-and-n-grams</code></p>
<p>:numref:<code>sec_sequence</code>의 시퀀스 모델 분석 중에서,
언어 모델링에 마르코프 모델을 적용해 봅시다.
시퀀스에 대한 분포가 $P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$를 만족하면 1차 마르코프 속성을 만족합니다. 더 높은 차수는 더 긴 의존성에 해당합니다. 이는 시퀀스를 모델링하기 위해 적용할 수 있는 여러 가지 근사로 이어집니다:</p>
<p>$$
\begin{aligned}
P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2) P(x_3) P(x_4),\P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).
\end{aligned}
$$</p>
<p>하나, 둘, 세 개의 변수를 포함하는 확률 공식은 일반적으로 각각 <em>유니그램(unigram)</em>, <em>바이그램(bigram)</em>, <em>트라이그램(trigram)</em> 모델이라고 합니다.
언어 모델을 계산하려면 단어의 확률과 이전 몇 단어가 주어졌을 때 단어의 조건부 확률을 계산해야 합니다.
그러한 확률은 언어 모델 파라미터라는 점에 유의하십시오.</p>
<h3 id="단어-빈도-word-frequency"><a class="header" href="#단어-빈도-word-frequency">단어 빈도 (Word Frequency)</a></h3>
<p>여기서 우리는
훈련 데이터셋이 모든 위키백과 항목, <a href="https://en.wikipedia.org/wiki/Project_Gutenberg">구텐베르크 프로젝트</a>,
웹에 게시된 모든 텍스트와 같은 대규모 텍스트 말뭉치라고 가정합니다.
단어의 확률은 훈련 데이터셋에서 주어진 단어의 상대적 단어 빈도(relative word frequency)로부터 계산할 수 있습니다.
예를 들어 추정치 $\hat{P}(\textrm{deep})$은 "deep"이라는 단어로 시작하는 문장의 확률로 계산할 수 있습니다.
약간 덜 정확한 접근 방식은 "deep"이라는 단어의 모든 발생 횟수를 세고 이를 말뭉치의 총 단어 수로 나누는 것입니다.
이것은 특히 빈번한 단어에 대해 꽤 잘 작동합니다. 계속해서 우리는 다음을 추정하려고 시도할 수 있습니다.</p>
<p>$$\hat{P}(\textrm{learning} \mid \textrm{deep}) = \frac{n(\textrm{deep, learning})}{n(\textrm{deep})},$$</p>
<p>여기서 $n(x)$와 $n(x, x')$는 각각 단일 단어와 연속된 단어 쌍의 발생 횟수입니다.
불행히도 단어 쌍의 확률을 추정하는 것은 다소 더 어렵습니다. "deep learning"의 발생 빈도가 훨씬 적기 때문입니다.
특히 일부 특이한 단어 조합의 경우 정확한 추정치를 얻을 만큼 충분한 발생 횟수를 찾기가 까다로울 수 있습니다.
:numref:<code>subsec_natural-lang-stat</code>의 경험적 결과가 시사하듯이,
세 단어 조합 및 그 이상에서는 상황이 악화됩니다.
우리의 데이터셋에서 보지 못할 가능성이 높은 그럴듯한 세 단어 조합이 많이 있을 것입니다.
그러한 단어 조합에 0이 아닌 카운트를 할당하는 솔루션을 제공하지 않는 한, 언어 모델에서 이를 사용할 수 없습니다. 데이터셋이 작거나 단어가 매우 드문 경우, 그중 하나도 찾지 못할 수 있습니다.</p>
<h3 id="라플라스-평활화-laplace-smoothing"><a class="header" href="#라플라스-평활화-laplace-smoothing">라플라스 평활화 (Laplace Smoothing)</a></h3>
<p>일반적인 전략은 어떤 형태의 *라플라스 평활화(Laplace smoothing)*를 수행하는 것입니다.
해결책은 모든 카운트에 작은 상수를 더하는 것입니다.
훈련 세트의 총 단어 수를 $n$, 고유 단어 수를 $m$이라고 합시다.
이 해결책은 예를 들어 다음과 같이 단일 단어에 도움이 됩니다.</p>
<p>$$\begin{aligned}
\hat{P}(x) &amp; = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \
\hat{P}(x' \mid x) &amp; = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \
\hat{P}(x'' \mid x,x') &amp; = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
\end{aligned}$$</p>
<p>여기서 $\epsilon_1,\epsilon_2, \epsilon_3$는 하이퍼파라미터입니다.
$\epsilon_1$을 예로 들어봅시다:
$\epsilon_1 = 0$일 때 평활화가 적용되지 않습니다;
$\epsilon_1$이 양의 무한대에 접근하면 $\hat{P}(x)$는 균등 확률 $1/m$에 접근합니다.
위의 내용은 다른 기술이 달성할 수 있는 것의 다소 원시적인 변형입니다 :cite:<code>Wood.Gasthaus.Archambeau.ea.2011</code>.</p>
<p>불행히도 이와 같은 모델은 다음과 같은 이유로 금방 다루기 힘들어집니다.
첫째, :numref:<code>subsec_natural-lang-stat</code>에서 논의한 바와 같이
많은 $n$-gram이 매우 드물게 발생하여 라플라스 평활화가 언어 모델링에 부적합합니다.
둘째, 모든 카운트를 저장해야 합니다.
셋째, 이것은 단어의 의미를 완전히 무시합니다. 예를 들어 "cat"과 "feline"은 관련된 문맥에서 발생해야 합니다.
이러한 모델을 추가적인 문맥에 맞게 조정하는 것은 꽤 어렵습니다.
반면 딥러닝 기반 언어 모델은 이를 고려하는 데 적합합니다.
마지막으로 긴 단어 시퀀스는 거의 확실하게 새로운 것이므로, 이전에 본 단어 시퀀스의 빈도를 단순히 세는 모델은 거기서 성능이 떨어질 수밖에 없습니다.
따라서 이 장의 나머지 부분에서는 언어 모델링을 위해 신경망을 사용하는 데 초점을 맞춥니다.</p>
<h2 id="퍼플렉서티-perplexity"><a class="header" href="#퍼플렉서티-perplexity">퍼플렉서티 (Perplexity)</a></h2>
<p>:label:<code>subsec_perplexity</code></p>
<p>다음으로 언어 모델의 품질을 측정하는 방법에 대해 논의해 보겠습니다. 이는 후속 섹션에서 모델을 평가하는 데 사용할 것입니다.
한 가지 방법은 텍스트가 얼마나 놀라운지 확인하는 것입니다.
좋은 언어 모델은 다음에 올 토큰을 높은 정확도로 예측할 수 있습니다.
다음 언어 모델들이 제안한 "It is raining"이라는 구절의 이어짐을 고려해 보십시오:</p>
<ol>
<li>"It is raining outside"</li>
<li>"It is raining banana tree"</li>
<li>"It is raining piouw;kcj pwepoiut"</li>
</ol>
<p>품질 면에서 예제 1이 분명히 최고입니다. 단어들이 합리적이고 논리적으로 일관성이 있습니다.
의미적으로 어떤 단어가 뒤따를지 정확하게 반영하지 않을 수도 있지만("in San Francisco"와 "in winter"는 완벽하게 합리적인 확장이었을 것입니다), 모델은 어떤 종류의 단어가 뒤따르는지 포착할 수 있습니다.
예제 2는 말도 안 되는 확장을 생성하여 상당히 나쁩니다. 그럼에도 불구하고 적어도 모델은 단어 철자법과 단어 간의 어느 정도의 상관관계를 학습했습니다. 마지막으로 예제 3은 데이터에 제대로 적합하지 않은 훈련이 잘못된 모델을 나타냅니다.</p>
<p>시퀀스의 우도(likelihood)를 계산하여 모델의 품질을 측정할 수 있습니다.
불행히도 이것은 이해하기 어렵고 비교하기 어려운 숫자입니다.
결국 짧은 시퀀스가 긴 시퀀스보다 발생할 가능성이 훨씬 높으므로, 톨스토이의 대작 <em>전쟁과 평화</em>에서 모델을 평가하면 생텍쥐페리의 소설 <em>어린 왕자</em>보다 훨씬 작은 우도를 생성할 것입니다. 빠진 것은 평균에 해당하는 것입니다.</p>
<p>여기서 정보 이론이 유용합니다.
우리는 소프트맥스 회귀(:numref:<code>subsec_info_theory_basics</code>)를 소개할 때 엔트로피, 놀람(surprisal), 크로스 엔트로피를 정의했습니다.
텍스트를 압축하고 싶다면 현재 토큰 세트가 주어졌을 때 다음 토큰을 예측하는 것에 대해 물을 수 있습니다.
더 좋은 언어 모델은 다음 토큰을 더 정확하게 예측할 수 있게 해 줄 것입니다.
따라서 시퀀스를 압축하는 데 더 적은 비트를 소비할 수 있게 해 줄 것입니다.
따라서 우리는 시퀀스의 모든 $n$ 토큰에 대해 평균을 낸 크로스 엔트로피 손실로 이를 측정할 수 있습니다:</p>
<p>$$\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),$$
:eqlabel:<code>eq_avg_ce_for_lm</code></p>
<p>여기서 $P$는 언어 모델에 의해 주어지고 $x_t$는 시퀀스의 타임 스텝 $t$에서 관찰된 실제 토큰입니다.
이것은 길이가 다른 문서의 성능을 비교할 수 있게 만듭니다. 역사적인 이유로 자연어 처리 과학자들은 *퍼플렉서티(perplexity)*라는 수량을 사용하는 것을 선호합니다. 요컨대, 이것은 :eqref:<code>eq_avg_ce_for_lm</code>의 지수입니다:</p>
<p>$$\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).$$</p>
<p>퍼플렉서티는 다음 토큰을 선택할 때 우리가 가진 실제 선택지 수의 기하 평균의 역수로 이해하는 것이 가장 좋습니다. 몇 가지 경우를 살펴보겠습니다:</p>
<ul>
<li>최선의 시나리오에서 모델은 항상 대상 토큰의 확률을 1로 완벽하게 추정합니다. 이 경우 모델의 퍼플렉서티는 1입니다.</li>
<li>최악의 시나리오에서 모델은 항상 대상 토큰의 확률을 0으로 예측합니다. 이 상황에서 퍼플렉서티는 양의 무한대입니다.</li>
<li>기준선(baseline)에서 모델은 어휘의 모든 사용 가능한 토큰에 대해 균등 분포를 예측합니다. 이 경우 퍼플렉서티는 어휘의 고유 토큰 수와 같습니다. 사실 압축 없이 시퀀스를 저장해야 한다면 이것이 인코딩을 위해 할 수 있는 최선일 것입니다. 따라서 이는 유용한 모델이라면 반드시 깨야 하는 중요한 상한선을 제공합니다.</li>
</ul>
<h2 id="시퀀스-분할-partitioning-sequences"><a class="header" href="#시퀀스-분할-partitioning-sequences">시퀀스 분할 (Partitioning Sequences)</a></h2>
<p>:label:<code>subsec_partitioning-seqs</code></p>
<p>우리는 신경망을 사용하여 언어 모델을 설계하고
텍스트 시퀀스에서 현재 토큰 세트가 주어졌을 때 다음 토큰을 예측하는 모델의 성능을 평가하기 위해 퍼플렉서티를 사용할 것입니다.
모델을 소개하기 전에, 모델이 한 번에 미리 정의된 길이의 시퀀스 미니배치를 처리한다고 가정해 봅시다.
이제 문제는 [<strong>입력 시퀀스와 타겟 시퀀스의 미니배치를 무작위로 읽는 방법</strong>]입니다.</p>
<p>데이터셋이 <code>corpus</code>에 있는 $T$개의 토큰 인덱스 시퀀스 형태를 취한다고 가정해 봅시다.
우리는 이것을 부분 시퀀스로 분할할 것이며, 각 부분 시퀀스는 $n$개의 토큰(타임 스텝)을 가집니다.
각 에폭마다 전체 데이터셋의 (거의) 모든 토큰을 반복하고 가능한 모든 길이 $n$ 부분 시퀀스를 얻기 위해 무작위성을 도입할 수 있습니다.
더 구체적으로, 각 에폭이 시작될 때 무작위로 균일하게 샘플링된 $d
in [0,n)$개의 첫 번째 토큰을 버립니다.
나머지 시퀀스는 $m=\lfloor (T-d)/n \rfloor$개의 부분 시퀀스로 분할됩니다.
타임 스텝 $t$에서 토큰 $x_t$로 시작하는 길이 $n$ 부분 시퀀스를 $\mathbf x_t = [x_t, \ldots, x_{t+n-1}]$로 표시합니다.
결과적으로 $m$개의 분할된 부분 시퀀스는
$\mathbf x_d, \mathbf x_{d+n}, \ldots, \mathbf x_{d+n(m-1)}.$
각 부분 시퀀스는 언어 모델의 입력 시퀀스로 사용됩니다.</p>
<p>언어 모델링의 경우,
목표는 지금까지 본 토큰을 기반으로 다음 토큰을 예측하는 것이므로 타겟(레이블)은 한 토큰만큼 이동된 원래 시퀀스입니다.
임의의 입력 시퀀스 $\mathbf x_t$에 대한 타겟 시퀀스는 길이가 $n$인 $\mathbf x_{t+1}$입니다.</p>
<p><img src="chapter_recurrent-neural-networks/../img/lang-model-data.svg" alt="분할된 길이 5 부분 시퀀스에서 5쌍의 입력 시퀀스와 타겟 시퀀스 얻기." />
:label:<code>fig_lang_model_data</code></p>
<p>:numref:<code>fig_lang_model_data</code>는 $n=5$ 및 $d=2$일 때 5쌍의 입력 시퀀스와 타겟 시퀀스를 얻는 예를 보여줍니다.</p>
<pre><code class="language-{.python .input  n=5}">%%tab all
@d2l.add_to_class(d2l.TimeMachine)  #@save
def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):
    super(d2l.TimeMachine, self).__init__()
    self.save_hyperparameters()
    corpus, self.vocab = self.build(self._download())
    array = d2l.tensor([corpus[i:i+num_steps+1] 
                        for i in range(len(corpus)-num_steps)])
    self.X, self.Y = array[:,:-1], array[:,1:]
</code></pre>
<p>언어 모델을 훈련하기 위해,
우리는 입력 시퀀스와 타겟 시퀀스 쌍을 미니배치로 무작위로 샘플링할 것입니다.
다음 데이터 로더는 매번 데이터셋에서 미니배치를 무작위로 생성합니다.
인수 <code>batch_size</code>는 각 미니배치의 부분 시퀀스 예제 수를 지정하고 <code>num_steps</code>는 토큰 단위의 부분 시퀀스 길이입니다.</p>
<pre><code class="language-{.python .input  n=6}">%%tab all
@d2l.add_to_class(d2l.TimeMachine)  #@save
def get_dataloader(self, train):
    idx = slice(0, self.num_train) if train else slice(
        self.num_train, self.num_train + self.num_val)
    return self.get_tensorloader([self.X, self.Y], train, idx)
</code></pre>
<p>다음에서 볼 수 있듯이,
타겟 시퀀스의 미니배치는
입력 시퀀스를 한 토큰만큼 이동시켜 얻을 수 있습니다.</p>
<pre><code class="language-{.python .input  n=7}">%%tab all
data = d2l.TimeMachine(batch_size=2, num_steps=10)
for X, Y in data.train_dataloader():
    print('X:', X, '\nY:', Y)
    break
</code></pre>
<h2 id="요약-및-토론-summary-and-discussion-7"><a class="header" href="#요약-및-토론-summary-and-discussion-7">요약 및 토론 (Summary and Discussion)</a></h2>
<p>언어 모델은 텍스트 시퀀스의 결합 확률을 추정합니다. 긴 시퀀스의 경우 $n$-gram은 의존성을 잘라냄으로써 편리한 모델을 제공합니다. 그러나 라플라스 평활화를 통해 빈번하지 않은 단어 조합을 효율적으로 처리하기에는 구조는 많지만 빈도가 충분하지 않습니다. 따라서 후속 섹션에서는 신경망 언어 모델링에 초점을 맞출 것입니다.
언어 모델을 훈련하기 위해 입력 시퀀스와 타겟 시퀀스 쌍을 미니배치로 무작위로 샘플링할 수 있습니다. 훈련 후에는 언어 모델 품질을 측정하기 위해 퍼플렉서티를 사용할 것입니다.</p>
<p>언어 모델은 데이터 크기, 모델 크기, 훈련 컴퓨팅 양을 늘리면 확장될 수 있습니다. 대규모 언어 모델은 입력 텍스트 지침이 주어졌을 때 출력 텍스트를 예측함으로써 원하는 작업을 수행할 수 있습니다. 나중에 논의하겠지만(예: :numref:<code>sec_large-pretraining-transformers</code>),
현재 대규모 언어 모델은 다양한 작업에서 최첨단 시스템의 기반을 형성합니다.</p>
<h2 id="연습-문제-exercises-44"><a class="header" href="#연습-문제-exercises-44">연습 문제 (Exercises)</a></h2>
<ol>
<li>훈련 데이터셋에 100,000개의 단어가 있다고 가정합니다. 4-gram은 얼마나 많은 단어 빈도와 다중 단어 인접 빈도를 저장해야 합니까?</li>
<li>대화를 어떻게 모델링하시겠습니까?</li>
<li>긴 시퀀스 데이터를 읽기 위해 생각할 수 있는 다른 방법은 무엇입니까?</li>
<li>각 에폭 시작 시 처음 몇 개의 토큰을 무작위로 버리는 우리의 방법을 고려해 보십시오.
<ol>
<li>그것이 정말로 문서의 시퀀스에 대해 완벽하게 균일한 분포로 이어집니까?</li>
<li>상황을 더 균일하게 만들기 위해 무엇을 해야 합니까?</li>
</ol>
</li>
<li>시퀀스 예제가 완전한 문장이 되기를 원한다면 미니배치 샘플링에 어떤 문제가 발생합니까? 어떻게 해결할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/117">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/118">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1049">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18012">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="순환-신경망-recurrent-neural-networks-1"><a class="header" href="#순환-신경망-recurrent-neural-networks-1">순환 신경망 (Recurrent Neural Networks)</a></h1>
<p>:label:<code>sec_rnn</code></p>
<p>:numref:<code>sec_language-model</code>에서 우리는 언어 모델링을 위한 마르코프 모델과 $n$-gram을 설명했습니다. 여기서 타임 스텝 $t$에서의 토큰 $x_t$의 조건부 확률은 이전 $n-1$ 토큰에만 의존합니다.
타임 스텝 $t-(n-1)$ 이전의 토큰이 $x_t$에 미칠 수 있는 영향을 통합하려면 $n$을 늘려야 합니다.
그러나 모델 파라미터의 수는 그에 따라 기하급수적으로 증가할 것입니다. 어휘 집합 $\mathcal{V}$에 대해 $|V|^n$개의 숫자를 저장해야 하기 때문입니다.
따라서 $P(x_t \mid x_{t-1}, \ldots, x_{t-n+1})$을 모델링하는 대신 잠재 변수 모델을 사용하는 것이 바람직합니다.</p>
<p>$$P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1}),$$</p>
<p>여기서 $h_{t-1}$은 타임 스텝 $t-1$까지의 시퀀스 정보를 저장하는 *은닉 상태(hidden state)*입니다.
일반적으로
어떤 타임 스텝 $t$에서의 은닉 상태는 현재 입력 $x_{t}$와 이전 은닉 상태 $h_{t-1}$ 모두를 기반으로 계산될 수 있습니다:</p>
<p>$$h_t = f(x_{t}, h_{t-1}).$$
:eqlabel:<code>eq_ht_xt</code></p>
<p>:eqref:<code>eq_ht_xt</code>의 함수 $f$가 충분히 강력하다면 잠재 변수 모델은 근사가 아닙니다. 결국 $h_t$는 지금까지 관찰한 모든 데이터를 단순히 저장할 수 있습니다.
그러나 이는 잠재적으로 계산과 저장 모두를 비싸게 만들 수 있습니다.</p>
<p>:numref:<code>chap_perceptrons</code>에서 은닉 유닛이 있는 은닉층에 대해 논의했던 것을 상기해 보십시오.
은닉층과 은닉 상태는 매우 다른 개념을 나타낸다는 점에 주목할 가치가 있습니다.
설명했듯이 은닉층은 입력에서 출력으로 가는 경로에서 보이지 않게 숨겨진 레이어입니다.
은닉 상태는 기술적으로 말해서 주어진 단계에서 우리가 하는 모든 일에 대한 <em>입력</em>이며, 이전 타임 스텝의 데이터를 봐야만 계산할 수 있습니다.</p>
<p><em>순환 신경망(Recurrent neural networks)</em> (RNN)은 은닉 상태가 있는 신경망입니다. RNN 모델을 소개하기 전에 먼저 :numref:<code>sec_mlp</code>에서 소개한 MLP 모델을 다시 살펴보겠습니다.</p>
<pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="은닉-상태가-없는-신경망-neural-networks-without-hidden-states"><a class="header" href="#은닉-상태가-없는-신경망-neural-networks-without-hidden-states">은닉 상태가 없는 신경망 (Neural Networks without Hidden States)</a></h2>
<p>단일 은닉층이 있는 MLP를 살펴봅시다.
은닉층의 활성화 함수를 $\phi$라고 합시다.
배치 크기 $n$과 $d$개의 입력을 가진 예제 미니배치 $\mathbf{X} \in \mathbb{R}^{n \times d}$가 주어지면, 은닉층 출력 $\mathbf{H} \in \mathbb{R}^{n \times h}$는 다음과 같이 계산됩니다.</p>
<p>$$\mathbf{H} = \phi(\mathbf{X} \mathbf{W}<em>{\textrm{xh}} + \mathbf{b}</em>\textrm{h}).$$
:eqlabel:<code>rnn_h_without_state</code></p>
<p>:eqref:<code>rnn_h_without_state</code>에서 우리는 가중치 파라미터 $\mathbf{W}<em>{\textrm{xh}} \in \mathbb{R}^{d \times h}$, 편향 파라미터 $\mathbf{b}</em>\textrm{h} \in \mathbb{R}^{1 \times h}$, 그리고 은닉층에 대한 은닉 유닛 수 $h$를 갖습니다.
이렇게 준비된 상태에서 합산 중에 브로드캐스팅(:numref:<code>subsec_broadcasting</code> 참조)을 적용합니다.
다음으로 은닉층 출력 $\mathbf{H}$는 출력 레이어의 입력으로 사용되며, 이는 다음과 같이 주어집니다.</p>
<p>$$\mathbf{O} = \mathbf{H} \mathbf{W}<em>{\textrm{hq}} + \mathbf{b}</em>\textrm{q},$$</p>
<p>여기서 $\mathbf{O} \in \mathbb{R}^{n \times q}$는 출력 변수, $\mathbf{W}<em>{\textrm{hq}} \in \mathbb{R}^{h \times q}$는 가중치 파라미터, $\mathbf{b}</em>\textrm{q} \in \mathbb{R}^{1 \times q}$는 출력 레이어의 편향 파라미터입니다. 분류 문제인 경우 $\mathrm{softmax}(\mathbf{O})$를 사용하여 출력 범주의 확률 분포를 계산할 수 있습니다.</p>
<p>이것은 이전에 :numref:<code>sec_sequence</code>에서 해결한 회귀 문제와 전적으로 유사하므로 세부 사항은 생략합니다.
특성-레이블 쌍을 무작위로 선택하고 자동 미분 및 확률적 경사 하강법을 통해 네트워크의 파라미터를 학습할 수 있다고 말하는 것으로 충분합니다.</p>
<h2 id="은닉-상태가-있는-순환-신경망-recurrent-neural-networks-with-hidden-states"><a class="header" href="#은닉-상태가-있는-순환-신경망-recurrent-neural-networks-with-hidden-states">은닉 상태가 있는 순환 신경망 (Recurrent Neural Networks with Hidden States)</a></h2>
<p>:label:<code>subsec_rnn_w_hidden_states</code></p>
<p>은닉 상태가 있으면 상황이 완전히 다릅니다. 구조를 좀 더 자세히 살펴봅시다.</p>
<p>타임 스텝 $t$에 입력 미니배치 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$가 있다고 가정합니다.
즉, $n$개의 시퀀스 예제로 구성된 미니배치에 대해 $\mathbf{X}<em>t$의 각 행은 시퀀스의 타임 스텝 $t$에 있는 한 예제에 해당합니다.
다음으로, 타임 스텝 $t$의 은닉층 출력을 $\mathbf{H}<em>t  \in \mathbb{R}^{n \times h}$로 표시합니다.
MLP와 달리 여기서는 이전 타임 스텝의 은닉층 출력 $\mathbf{H}</em>{t-1}$을 저장하고 현재 타임 스텝에서 이전 타임 스텝의 은닉층 출력을 사용하는 방법을 설명하기 위해 새로운 가중치 파라미터 $\mathbf{W}</em>{\textrm{hh}} \in \mathbb{R}^{h \times h}$를 도입합니다. 구체적으로 현재 타임 스텝의 은닉층 출력 계산은 현재 타임 스텝의 입력과 이전 타임 스텝의 은닉층 출력에 의해 결정됩니다:</p>
<p>$$\mathbf{H}<em>t = \phi(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xh}} + \mathbf{H}</em>{t-1} \mathbf{W}<em>{\textrm{hh}}  + \mathbf{b}</em>\textrm{h}).$$
:eqlabel:<code>rnn_h_with_state</code></p>
<p>:eqref:<code>rnn_h_without_state</code>와 비교할 때, :eqref:<code>rnn_h_with_state</code>는 항 $\mathbf{H}<em>{t-1} \mathbf{W}</em>{\textrm{hh}}$를 하나 더 추가하여 :eqref:<code>eq_ht_xt</code>를 인스턴스화합니다.
인접한 타임 스텝의 은닉층 출력 $\mathbf{H}<em>t$와 $\mathbf{H}</em>{t-1}$ 사이의 관계로부터,
우리는 이러한 변수가 신경망의 현재 타임 스텝의 상태나 메모리처럼 현재 타임 스텝까지의 시퀀스 역사 정보를 캡처하고 유지했다는 것을 알 수 있습니다. 따라서 이러한 은닉층 출력을 *은닉 상태(hidden state)*라고 합니다.
은닉 상태는 현재 타임 스텝에서 이전 타임 스텝의 동일한 정의를 사용하므로 :eqref:<code>rnn_h_with_state</code>의 계산은 *순환적(recurrent)*입니다. 따라서 말했듯이 순환 계산을 기반으로 한 은닉 상태를 가진 신경망을 *순환 신경망(recurrent neural networks)*이라고 합니다.
RNN에서 :eqref:<code>rnn_h_with_state</code>의 계산을 수행하는 레이어를 *순환 레이어(recurrent layers)*라고 합니다.</p>
<p>RNN을 구성하는 방법에는 여러 가지가 있습니다.
:eqref:<code>rnn_h_with_state</code>에 의해 정의된 은닉 상태를 가진 것들이 매우 일반적입니다.
타임 스텝 $t$에 대해 출력 레이어의 출력은 MLP의 계산과 유사합니다:</p>
<p>$$\mathbf{O}<em>t = \mathbf{H}<em>t \mathbf{W}</em>{\textrm{hq}} + \mathbf{b}</em>\textrm{q}.$$</p>
<p>RNN의 파라미터에는 은닉층의 가중치 $\mathbf{W}<em>{\textrm{xh}} \in \mathbb{R}^{d \times h}, \mathbf{W}</em>{\textrm{hh}} \in \mathbb{R}^{h \times h}$ 및 편향 $\mathbf{b}<em>\textrm{h} \in \mathbb{R}^{1 \times h}$와 출력 레이어의 가중치 $\mathbf{W}</em>{\textrm{hq}} \in \mathbb{R}^{h \times q}$ 및 편향 $\mathbf{b}_\textrm{q} \in \mathbb{R}^{1 \times q}$가 포함됩니다.
다른 타임 스텝에서도 RNN은 항상 이러한 모델 파라미터를 사용한다는 점을 언급할 가치가 있습니다.
따라서 RNN의 파라미터화 비용은 타임 스텝 수가 증가해도 증가하지 않습니다.</p>
<p>:numref:<code>fig_rnn</code>은 인접한 세 타임 스텝에서의 RNN 계산 로직을 보여줍니다.
임의의 타임 스텝 $t$에서 은닉 상태의 계산은 다음과 같이 취급될 수 있습니다:
(i) 현재 타임 스텝 $t$의 입력 $\mathbf{X}<em>t$와 이전 타임 스텝 $t-1$의 은닉 상태 $\mathbf{H}</em>{t-1}$을 연결(concatenating)합니다;
(ii) 연결 결과를 활성화 함수 $\phi$가 있는 완전 연결 레이어에 공급합니다.
그러한 완전 연결 레이어의 출력은 현재 타임 스텝 $t$의 은닉 상태 $\mathbf{H}<em>t$입니다.
이 경우 모델 파라미터는 :eqref:<code>rnn_h_with_state</code>의 $\mathbf{W}</em>{\textrm{xh}}$와 $\mathbf{W}<em>{\textrm{hh}}$의 연결, 그리고 편향 $\mathbf{b}</em>\textrm{h}$입니다.
현재 타임 스텝 $t$의 은닉 상태 $\mathbf{H}<em>t$는 다음 타임 스텝 $t+1$의 은닉 상태 $\mathbf{H}</em>{t+1}$을 계산하는 데 참여합니다.
더욱이 $\mathbf{H}_t$는 현재 타임 스텝 $t$의 출력 $\mathbf{O}_t$를 계산하기 위해 완전 연결 출력 레이어에도 공급됩니다.</p>
<p><img src="chapter_recurrent-neural-networks/../img/rnn.svg" alt="은닉 상태가 있는 RNN." />
:label:<code>fig_rnn</code></p>
<p>방금 우리는 은닉 상태에 대한 $\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xh}} + \mathbf{H}<em>{t-1} \mathbf{W}</em>{\textrm{hh}}$ 계산이
$\mathbf{X}<em>t$와 $\mathbf{H}</em>{t-1}$의 연결과
$\mathbf{W}<em>{\textrm{xh}}$와 $\mathbf{W}</em>{\textrm{hh}}$의 연결의
행렬 곱셈과 동일하다고 언급했습니다.
이것은 수학적으로 증명될 수 있지만,
다음에서는 간단한 코드 스니펫을 데모로 사용합니다.
우선 모양이 각각 (3, 1), (1, 4), (3, 4), (4, 4)인 행렬 <code>X</code>, <code>W_xh</code>, <code>H</code>, <code>W_hh</code>를 정의합니다.
<code>X</code>에 <code>W_xh</code>를 곱하고, <code>H</code>에 <code>W_hh</code>를 곱한 다음 이 두 곱을 더하면 (3, 4) 모양의 행렬을 얻습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
X, W_xh = d2l.randn(3, 1), d2l.randn(1, 4)
H, W_hh = d2l.randn(3, 4), d2l.randn(4, 4)
d2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X, W_xh = d2l.normal((3, 1)), d2l.normal((1, 4))
H, W_hh = d2l.normal((3, 4)), d2l.normal((4, 4))
d2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X, W_xh = jax.random.normal(d2l.get_key(), (3, 1)), jax.random.normal(
                                                        d2l.get_key(), (1, 4))
H, W_hh = jax.random.normal(d2l.get_key(), (3, 4)), jax.random.normal(
                                                        d2l.get_key(), (4, 4))
d2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)
</code></pre>
<p>이제 행렬 <code>X</code>와 <code>H</code>를 열을 따라(축 1) 연결하고,
행렬 <code>W_xh</code>와 <code>W_hh</code>를 행을 따라(축 0) 연결합니다.
이 두 연결은 각각 (3, 5) 모양과 (5, 4) 모양의 행렬을 생성합니다.
이 두 연결된 행렬을 곱하면 위와 동일한 (3, 4) 모양의 출력 행렬을 얻습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
d2l.matmul(d2l.concat((X, H), 1), d2l.concat((W_xh, W_hh), 0))
</code></pre>
<h2 id="rnn-기반-문자-수준-언어-모델-rnn-based-character-level-language-models"><a class="header" href="#rnn-기반-문자-수준-언어-모델-rnn-based-character-level-language-models">RNN 기반 문자 수준 언어 모델 (RNN-Based Character-Level Language Models)</a></h2>
<p>:numref:<code>sec_language-model</code>의 언어 모델링에서 우리의 목표는 현재 및 과거 토큰을 기반으로 다음 토큰을 예측하는 것임을 상기하십시오.
따라서 원래 시퀀스를 한 토큰만큼 이동시켜 타겟(레이블)으로 사용합니다.
:citet:<code>Bengio.Ducharme.Vincent.ea.2003</code>는 언어 모델링에 신경망을 사용할 것을 처음 제안했습니다.
다음에서는 RNN을 사용하여 언어 모델을 구축하는 방법을 설명합니다.
미니배치 크기를 1로, 텍스트 시퀀스를 "machine"이라고 합시다.
후속 섹션에서의 훈련을 단순화하기 위해 텍스트를 단어가 아닌 문자로 토큰화하고 *문자 수준 언어 모델(character-level language model)*을 고려합니다.
:numref:<code>fig_rnn_train</code>은 문자 수준 언어 모델링을 위해 RNN을 통해 현재 및 이전 문자를 기반으로 다음 문자를 예측하는 방법을 보여줍니다.</p>
<p><img src="chapter_recurrent-neural-networks/../img/rnn-train.svg" alt="RNN 기반 문자 수준 언어 모델. 입력 및 타겟 시퀀스는 각각 &quot;machin&quot;과 &quot;achine&quot;입니다." />
:label:<code>fig_rnn_train</code></p>
<p>훈련 과정 동안,
각 타임 스텝에 대해 출력 레이어의 출력에 대해 소프트맥스 연산을 실행한 다음 교차 엔트로피 손실을 사용하여 모델 출력과 타겟 간의 오차를 계산합니다.
은닉층에서 은닉 상태의 순환 계산으로 인해, :numref:<code>fig_rnn_train</code>의 타임 스텝 3의 출력 $\mathbf{O}_3$은 텍스트 시퀀스 "m", "a", "c"에 의해 결정됩니다. 훈련 데이터의 시퀀스 다음 문자가 "h"이므로 타임 스텝 3의 손실은 특성 시퀀스 "m", "a", "c"와 이 타임 스텝의 타겟 "h"를 기반으로 생성된 다음 문자의 확률 분포에 따라 달라집니다.</p>
<p>실제로 각 토큰은 $d$차원 벡터로 표현되며 배치 크기 $n&gt;1$을 사용합니다. 따라서 타임 스텝 $t$에서의 입력 $\mathbf X_t$는 $n\times d$ 행렬이 되며, 이는 :numref:<code>subsec_rnn_w_hidden_states</code>에서 논의한 것과 동일합니다.</p>
<p>다음 섹션에서는 문자 수준 언어 모델을 위한 RNN을 구현할 것입니다.</p>
<h2 id="요약-summary-32"><a class="header" href="#요약-summary-32">요약 (Summary)</a></h2>
<p>은닉 상태에 대해 순환 계산을 사용하는 신경망을 순환 신경망(RNN)이라고 합니다.
RNN의 은닉 상태는 현재 타임 스텝까지 시퀀스의 역사적 정보를 캡처할 수 있습니다. 순환 계산을 사용하면 RNN 모델 파라미터의 수는 타임 스텝 수가 증가해도 증가하지 않습니다. 응용 분야와 관련하여 RNN을 사용하여 문자 수준 언어 모델을 만들 수 있습니다.</p>
<h2 id="연습-문제-exercises-45"><a class="header" href="#연습-문제-exercises-45">연습 문제 (Exercises)</a></h2>
<ol>
<li>RNN을 사용하여 텍스트 시퀀스의 다음 문자를 예측하는 경우, 임의의 출력에 필요한 차원은 무엇입니까?</li>
<li>RNN이 텍스트 시퀀스의 모든 이전 토큰을 기반으로 어떤 타임 스텝에서의 토큰의 조건부 확률을 표현할 수 있는 이유는 무엇입니까?</li>
<li>긴 시퀀스를 통해 역전파하면 기울기에 어떤 일이 발생합니까?</li>
<li>이 섹션에서 설명한 언어 모델과 관련된 몇 가지 문제는 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/337">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1050">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1051">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/180013">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="순환-신경망-밑바닥부터-구현하기-recurrent-neural-network-implementation-from-scratch"><a class="header" href="#순환-신경망-밑바닥부터-구현하기-recurrent-neural-network-implementation-from-scratch">순환 신경망 밑바닥부터 구현하기 (Recurrent Neural Network Implementation from Scratch)</a></h1>
<p>:label:<code>sec_rnn-scratch</code></p>
<p>우리는 이제 밑바닥부터 RNN을 구현할 준비가 되었습니다.
특히, 이 RNN이 문자 수준 언어 모델로 기능하도록 훈련하고 (:numref:<code>sec_rnn</code> 참조) :numref:<code>sec_text-sequence</code>에 설명된 데이터 처리 단계에 따라 H. G. Wells의 <em>타임 머신</em> 전체 텍스트로 구성된 말뭉치에서 훈련할 것입니다.
데이터셋을 로드하는 것부터 시작합니다.</p>
<pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<pre><code class="language-{.python .input  n=2}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import autograd, gluon, np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input  n=5}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
import math
</code></pre>
<h2 id="rnn-모델-rnn-model"><a class="header" href="#rnn-모델-rnn-model">RNN 모델 (RNN Model)</a></h2>
<p>RNN 모델을 구현하기 위한 클래스를 정의하는 것으로 시작합니다(:numref:<code>subsec_rnn_w_hidden_states</code>).
은닉 유닛의 수 <code>num_hiddens</code>는 조정 가능한 하이퍼파라미터라는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class RNNScratch(d2l.Module):  #@save
    """밑바닥부터 구현된 RNN 모델."""
    def __init__(self, num_inputs, num_hiddens, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.W_xh = d2l.randn(num_inputs, num_hiddens) * sigma
            self.W_hh = d2l.randn(
                num_hiddens, num_hiddens) * sigma
            self.b_h = d2l.zeros(num_hiddens)
        if tab.selected('pytorch'):
            self.W_xh = nn.Parameter(
                d2l.randn(num_inputs, num_hiddens) * sigma)
            self.W_hh = nn.Parameter(
                d2l.randn(num_hiddens, num_hiddens) * sigma)
            self.b_h = nn.Parameter(d2l.zeros(num_hiddens))
        if tab.selected('tensorflow'):
            self.W_xh = tf.Variable(d2l.normal(
                (num_inputs, num_hiddens)) * sigma)
            self.W_hh = tf.Variable(d2l.normal(
                (num_hiddens, num_hiddens)) * sigma)
            self.b_h = tf.Variable(d2l.zeros(num_hiddens))
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab jax
class RNNScratch(nn.Module):  #@save
    """밑바닥부터 구현된 RNN 모델."""
    num_inputs: int
    num_hiddens: int
    sigma: float = 0.01

    def setup(self):
        self.W_xh = self.param('W_xh', nn.initializers.normal(self.sigma),
                               (self.num_inputs, self.num_hiddens))
        self.W_hh = self.param('W_hh', nn.initializers.normal(self.sigma),
                               (self.num_hiddens, self.num_hiddens))
        self.b_h = self.param('b_h', nn.initializers.zeros, (self.num_hiddens))
</code></pre>
<p>[<strong>아래의 <code>forward</code> 메서드는 현재 입력과 이전 타임 스텝의 모델 상태가 주어졌을 때 임의의 타임 스텝에서 출력과 은닉 상태를 계산하는 방법을 정의합니다.</strong>]
RNN 모델은 <code>inputs</code>의 가장 바깥쪽 차원을 반복하여 한 번에 한 타임 스텝씩 은닉 상태를 업데이트한다는 점에 유의하십시오.
여기서 모델은 $	anh$ 활성화 함수를 사용합니다(:numref:<code>subsec_tanh</code>).</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(RNNScratch)  #@save
def forward(self, inputs, state=None):
    if state is None:
        # 모양이 (batch_size, num_hiddens)인 초기 상태
        if tab.selected('mxnet'):
            state = d2l.zeros((inputs.shape[1], self.num_hiddens),
                              ctx=inputs.ctx)
        if tab.selected('pytorch'):
            state = d2l.zeros((inputs.shape[1], self.num_hiddens),
                              device=inputs.device)
        if tab.selected('tensorflow'):
            state = d2l.zeros((inputs.shape[1], self.num_hiddens))
    else:
        state, = state
        if tab.selected('tensorflow'):
            state = d2l.reshape(state, (-1, self.num_hiddens))
    outputs = []
    for X in inputs:  # inputs의 모양: (num_steps, batch_size, num_inputs) 
        state = d2l.tanh(d2l.matmul(X, self.W_xh) +
                         d2l.matmul(state, self.W_hh) + self.b_h)
        outputs.append(state)
    return outputs, state
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab jax
@d2l.add_to_class(RNNScratch)  #@save
def __call__(self, inputs, state=None):
    if state is not None:
        state, = state
    outputs = []
    for X in inputs:  # inputs의 모양: (num_steps, batch_size, num_inputs) 
        state = d2l.tanh(d2l.matmul(X, self.W_xh) + (
            d2l.matmul(state, self.W_hh) if state is not None else 0)
                         + self.b_h)
        outputs.append(state)
    return outputs, state
</code></pre>
<p>다음과 같이 입력 시퀀스의 미니배치를 RNN 모델에 공급할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100
rnn = RNNScratch(num_inputs, num_hiddens)
X = d2l.ones((num_steps, batch_size, num_inputs))
outputs, state = rnn(X)
</code></pre>
<pre><code class="language-{.python .input  n=11}">%%tab jax
batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100
rnn = RNNScratch(num_inputs, num_hiddens)
X = d2l.ones((num_steps, batch_size, num_inputs))
(outputs, state), _ = rnn.init_with_output(d2l.get_key(), X)
</code></pre>
<p>RNN 모델이 올바른 모양의 결과를 생성하는지 확인하여 은닉 상태의 차원성이 유지되는지 확인해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
def check_len(a, n):  #@save
    """리스트의 길이를 확인합니다."""
    assert len(a) == n, f'list\'s length {len(a)} != expected length {n}'
    
def check_shape(a, shape):  #@save
    """텐서의 모양을 확인합니다."""
    assert a.shape == shape, \
            f'tensor\'s shape {a.shape} != expected shape {shape}'

check_len(outputs, num_steps)
check_shape(outputs[0], (batch_size, num_hiddens))
check_shape(state, (batch_size, num_hiddens))
</code></pre>
<h2 id="rnn-기반-언어-모델-rnn-based-language-model"><a class="header" href="#rnn-기반-언어-모델-rnn-based-language-model">RNN 기반 언어 모델 (RNN-Based Language Model)</a></h2>
<p>다음 <code>RNNLMScratch</code> 클래스는 RNN 기반 언어 모델을 정의합니다.
여기서 우리는 <code>__init__</code> 메서드의 <code>rnn</code> 인수를 통해 RNN을 전달합니다.
언어 모델을 훈련할 때 입력과 출력은 동일한 어휘에서 나옵니다.
따라서 이들은 어휘 크기와 동일한 동일한 차원을 갖습니다.
모델을 평가하기 위해 퍼플렉서티를 사용한다는 점에 유의하십시오.
:numref:<code>subsec_perplexity</code>에서 논의했듯이, 이를 통해 길이가 다른 시퀀스를 비교할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
class RNNLMScratch(d2l.Classifier):  #@save
    """밑바닥부터 구현된 RNN 기반 언어 모델."""
    def __init__(self, rnn, vocab_size, lr=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.init_params()
        
    def init_params(self):
        self.W_hq = nn.Parameter(
            d2l.randn(
                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)
        self.b_q = nn.Parameter(d2l.zeros(self.vocab_size)) 

    def training_step(self, batch):
        l = self.loss(self(*batch[:-1]), batch[-1])
        self.plot('ppl', d2l.exp(l), train=True)
        return l
        
    def validation_step(self, batch):
        l = self.loss(self(*batch[:-1]), batch[-1])
        self.plot('ppl', d2l.exp(l), train=False)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow
class RNNLMScratch(d2l.Classifier):  #@save
    """밑바닥부터 구현된 RNN 기반 언어 모델."""
    def __init__(self, rnn, vocab_size, lr=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.init_params()
        
    def init_params(self):
        if tab.selected('mxnet'):
            self.W_hq = d2l.randn(
                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma
            self.b_q = d2l.zeros(self.vocab_size)        
            for param in self.get_scratch_params():
                param.attach_grad()
        if tab.selected('tensorflow'):
            self.W_hq = tf.Variable(d2l.normal(
                (self.rnn.num_hiddens, self.vocab_size)) * self.rnn.sigma)
            self.b_q = tf.Variable(d2l.zeros(self.vocab_size))
        
    def training_step(self, batch):
        l = self.loss(self(*batch[:-1]), batch[-1])
        self.plot('ppl', d2l.exp(l), train=True)
        return l
        
    def validation_step(self, batch):
        l = self.loss(self(*batch[:-1]), batch[-1])
        self.plot('ppl', d2l.exp(l), train=False)
</code></pre>
<pre><code class="language-{.python .input  n=14}">%%tab jax
class RNNLMScratch(d2l.Classifier):  #@save
    """밑바닥부터 구현된 RNN 기반 언어 모델."""
    rnn: nn.Module
    vocab_size: int
    lr: float = 0.01

    def setup(self):
        self.W_hq = self.param('W_hq', nn.initializers.normal(self.rnn.sigma),
                               (self.rnn.num_hiddens, self.vocab_size))
        self.b_q = self.param('b_q', nn.initializers.zeros, (self.vocab_size))

    def training_step(self, params, batch, state):
        value, grads = jax.value_and_grad(
            self.loss, has_aux=True)(params, batch[:-1], batch[-1], state)
        l, _ = value
        self.plot('ppl', d2l.exp(l), train=True)
        return value, grads

    def validation_step(self, params, batch, state):
        l, _ = self.loss(params, batch[:-1], batch[-1], state)
        self.plot('ppl', d2l.exp(l), train=False)
</code></pre>
<h3 id="원-핫-인코딩-one-hot-encoding"><a class="header" href="#원-핫-인코딩-one-hot-encoding">[<strong>원-핫 인코딩 (One-Hot Encoding)</strong>]</a></h3>
<p>각 토큰은 해당 단어/문자/단어 조각의 어휘 내 위치를 나타내는 수치 인덱스로 표현된다는 것을 상기하십시오.
(각 타임 스텝마다) 단일 입력 노드가 있는 신경망을 구축하고 싶은 유혹을 받을 수 있습니다.
이것은 충분히 가까운 두 값이 비슷하게 취급되어야 하는 가격이나 온도와 같은 수치 입력을 다룰 때 작동합니다.
하지만 이것은 말이 안 됩니다.
우리 어휘의 $45^{	extrm{th}}$번째와 $46^{	extrm{th}}$번째 단어는 "their"와 "said"인데, 그 의미는 전혀 비슷하지 않습니다.</p>
<p>이러한 범주형 데이터를 다룰 때 가장 일반적인 전략은 <em>원-핫 인코딩</em>으로 각 항목을 나타내는 것입니다(:numref:<code>subsec_classification-problem</code> 참조).
원-핫 인코딩은 길이가 어휘 크기 $N$으로 주어지는 벡터이며, 모든 항목은 $0$으로 설정되지만 우리 토큰에 해당하는 항목은 $1$로 설정됩니다.
예를 들어 어휘에 5개의 요소가 있는 경우, 인덱스 0과 2에 해당하는 원-핫 벡터는 다음과 같습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
npx.one_hot(np.array([0, 2]), 5)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
F.one_hot(torch.tensor([0, 2]), 5)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.one_hot(tf.constant([0, 2]), 5)
</code></pre>
<pre><code class="language-{.python .input  n=18}">%%tab jax
jax.nn.one_hot(jnp.array([0, 2]), 5)
</code></pre>
<p>(<strong>우리가 각 반복에서 샘플링하는 미니배치는 (배치 크기, 타임 스텝 수) 모양을 갖습니다.
각 입력을 원-핫 벡터로 나타내면, 각 미니배치를 3차원 텐서로 생각할 수 있으며, 여기서 세 번째 축을 따른 길이는 어휘 크기(<code>len(vocab)</code>)로 주어집니다.</strong>)
우리는 종종 입력을 전치하여 (타임 스텝 수, 배치 크기, 어휘 크기) 모양의 출력을 얻습니다.
이렇게 하면 미니배치의 은닉 상태를 타임 스텝별로 업데이트하기 위해 가장 바깥쪽 차원을 더 편리하게 반복할 수 있습니다(예: 위의 <code>forward</code> 메서드에서).</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(RNNLMScratch)  #@save
def one_hot(self, X):
    # 출력 모양: (num_steps, batch_size, vocab_size)    
    if tab.selected('mxnet'):
        return npx.one_hot(X.T, self.vocab_size)
    if tab.selected('pytorch'):
        return F.one_hot(X.T, self.vocab_size).type(torch.float32)
    if tab.selected('tensorflow'):
        return tf.one_hot(tf.transpose(X), self.vocab_size)
    if tab.selected('jax'):
        return jax.nn.one_hot(X.T, self.vocab_size)
</code></pre>
<h3 id="rnn-출력-변환-transforming-rnn-outputs"><a class="header" href="#rnn-출력-변환-transforming-rnn-outputs">RNN 출력 변환 (Transforming RNN Outputs)</a></h3>
<p>언어 모델은 완전 연결 출력 레이어를 사용하여 각 타임 스텝에서 RNN 출력을 토큰 예측으로 변환합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(RNNLMScratch)  #@save
def output_layer(self, rnn_outputs):
    outputs = [d2l.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]
    return d2l.stack(outputs, 1)

@d2l.add_to_class(RNNLMScratch)  #@save
def forward(self, X, state=None):
    embs = self.one_hot(X)
    rnn_outputs, _ = self.rnn(embs, state)
    return self.output_layer(rnn_outputs)
</code></pre>
<p>[<strong>순전파 계산이 올바른 모양의 출력을 생성하는지 확인</strong>]해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
model = RNNLMScratch(rnn, num_inputs)
outputs = model(d2l.ones((batch_size, num_steps), dtype=d2l.int64))
check_shape(outputs, (batch_size, num_steps, num_inputs))
</code></pre>
<pre><code class="language-{.python .input  n=23}">%%tab jax
model = RNNLMScratch(rnn, num_inputs)
outputs, _ = model.init_with_output(d2l.get_key(),
                                    d2l.ones((batch_size, num_steps),
                                             dtype=d2l.int32))
check_shape(outputs, (batch_size, num_steps, num_inputs))
</code></pre>
<h2 id="기울기-클리핑-gradient-clipping"><a class="header" href="#기울기-클리핑-gradient-clipping">[<strong>기울기 클리핑 (Gradient Clipping)</strong>]</a></h2>
<p>신경망을 단일 타임 스텝 내에서도 많은 레이어가 입력과 출력을 분리한다는 의미에서 "깊다"고 생각하는 데 이미 익숙하지만, 시퀀스 길이는 새로운 깊이 개념을 도입합니다.
입력에서 출력 방향으로 네트워크를 통과하는 것 외에도,
첫 번째 타임 스텝의 입력은
최종 타임 스텝에서 모델의 출력에 영향을 미치기 위해
타임 스텝을 따라 $T$개 레이어의 체인을 통과해야 합니다.
역방향으로 보면, 각 반복에서 우리는 시간을 거슬러 기울기를 역전파하여
길이 $\mathcal{O}(T)$의 행렬 곱 체인을 생성합니다.
:numref:<code>sec_numerical_stability</code>에서 언급했듯이,
이는 가중치 행렬의 속성에 따라 기울기가 폭발하거나 사라지는 수치적 불안정성을 초래할 수 있습니다.</p>
<p>사라지는 기울기와 폭발하는 기울기를 다루는 것은 RNN을 설계할 때 기본적인 문제이며 현대 신경망 아키텍처에서 가장 큰 발전 중 일부에 영감을 주었습니다.
다음 장에서는 기울기 소실 문제를 완화하기 위해 설계된 특수 아키텍처에 대해 이야기할 것입니다.
그러나 현대 RNN조차도 종종 기울기 폭발로 고통받습니다.
투박하지만 어디에나 있는 솔루션 중 하나는 단순히 기울기를 잘라내어(clip) 결과적으로 "클리핑된" 기울기가 더 작은 값을 갖도록 강제하는 것입니다.</p>
<p>일반적으로 경사 하강법으로 어떤 목적을 최적화할 때, 우리는 관심 있는 파라미터(예: 벡터 $\mathbf{x}$)를 반복적으로 업데이트하지만 음의 기울기 $\mathbf{g}$ 방향으로 밀어 넣습니다(확률적 경사 하강법에서는 무작위로 샘플링된 미니배치에서 이 기울기를 계산합니다).
예를 들어 학습률 $\eta &gt; 0$을 사용하면 각 업데이트는 $\mathbf{x} \gets \mathbf{x} - \eta \mathbf{g}$ 형태를 취합니다.
목적 함수 $f$가 충분히 매끄럽다고 가정해 봅시다.
공식적으로 우리는 목적 함수가 상수 $L$로 *립시츠 연속(Lipschitz continuous)*이라고 말합니다. 즉, 임의의 $\mathbf{x}$와 $\mathbf{y}$에 대해 다음을 갖습니다.</p>
<p>$$|f(\mathbf{x}) - f(\mathbf{y})| \leq L |\mathbf{x} - \mathbf{y}|.$$</p>
<p>보시다시피 $\eta \mathbf{g}$를 빼서 파라미터 벡터를 업데이트할 때 목적 함수의 값 변화는 다음과 같이 학습률, 기울기의 노름, $L$에 따라 달라집니다:</p>
<p>$$|f(\mathbf{x}) - f(\mathbf{x} - \eta\mathbf{g})| \leq L \eta|g\u0002|.$$</p>
<p>즉, 목적 함수는 $L \eta |g\u0002|$ 이상 변경될 수 없습니다.
이 상한선에 대해 작은 값을 갖는 것은 좋거나 나쁜 것으로 볼 수 있습니다.
단점으로는 목적 함수의 값을 줄일 수 있는 속도를 제한하고 있다는 것입니다.
장점으로는 이것이 어떤 한 경사 단계에서 우리가 얼마나 잘못될 수 있는지를 제한합니다.</p>
<p>기울기가 폭발한다고 말할 때, 우리는 $|g\u0002|$가 지나치게 커진다는 것을 의미합니다.
이 최악의 경우, 단일 경사 단계에서 수천 번의 훈련 반복 과정에서 이루어진 모든 진전을 취소할 수 있을 정도로 많은 손상을 입힐 수 있습니다.
기울기가 그렇게 클 수 있으면 신경망 훈련은 종종 발산하여 목적 함수의 값을 줄이지 못합니다.
다른 경우에는 훈련이 결국 수렴하지만 손실의 거대한 스파이크로 인해 불안정합니다.</p>
<p>$L \eta |g\u0002|$의 크기를 제한하는 한 가지 방법은 학습률 $\eta$를 아주 작은 값으로 줄이는 것입니다.
이것은 업데이트를 편향시키지 않는다는 장점이 있습니다.
하지만 큰 기울기를 <em>드물게</em> 얻는다면 어떨까요?
이 과감한 조치는 드문 기울기 폭발 이벤트를 처리하기 위해 모든 단계에서 우리의 진전을 늦춥니다.
인기 있는 대안은 다음과 같이 주어진 반지름 $\theta$의 공에 기울기 $\mathbf{g}$를 투영하는 <em>기울기 클리핑(gradient clipping)</em> 휴리스틱을 채택하는 것입니다:</p>
<p>(<strong>$$\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{|\mathbf{g}|}\right) \mathbf{g}.$$</strong>)</p>
<p>이것은 기울기 노름이 $\theta$를 초과하지 않도록 보장하고 업데이트된 기울기가 $\mathbf{g}$의 원래 방향과 완전히 정렬되도록 합니다.
또한 주어진 미니배치(및 그 안의 주어진 샘플)가 파라미터 벡터에 미칠 수 있는 영향을 제한하는 바람직한 부작용도 있습니다.
이것은 모델에 어느 정도의 견고성을 부여합니다.
분명히 하자면, 이것은 핵(hack)입니다.
기울기 클리핑은 우리가 항상 실제 기울기를 따르는 것은 아니며 가능한 부작용에 대해 분석적으로 추론하기 어렵다는 것을 의미합니다.
그러나 이것은 매우 유용한 핵이며 대부분의 딥러닝 프레임워크의 RNN 구현에서 널리 채택되고 있습니다.</p>
<p>아래에서는 <code>d2l.Trainer</code> 클래스의 <code>fit_epoch</code> 메서드에 의해 호출되는 기울기 클리핑 메서드를 정의합니다(:numref:<code>sec_linear_scratch</code> 참조).
기울기 노름을 계산할 때 모든 모델 파라미터를 연결하여 단일 거대한 파라미터 벡터로 취급한다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
@d2l.add_to_class(d2l.Trainer)  #@save
def clip_gradients(self, grad_clip_val, model):
    params = model.parameters()
    if not isinstance(params, list):
        params = [p.data() for p in params.values()]    
    norm = math.sqrt(sum((p.grad ** 2).sum() for p in params))
    if norm &gt; grad_clip_val:
        for param in params:
            param.grad[:] *= grad_clip_val / norm
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
@d2l.add_to_class(d2l.Trainer)  #@save
def clip_gradients(self, grad_clip_val, model):
    params = [p for p in model.parameters() if p.requires_grad]
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm &gt; grad_clip_val:
        for param in params:
            param.grad[:] *= grad_clip_val / norm
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
@d2l.add_to_class(d2l.Trainer)  #@save
def clip_gradients(self, grad_clip_val, grads):
    grad_clip_val = tf.constant(grad_clip_val, dtype=tf.float32)
    new_grads = [tf.convert_to_tensor(grad) if isinstance(
        grad, tf.IndexedSlices) else grad for grad in grads]    
    norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)) for grad in new_grads))
    if tf.greater(norm, grad_clip_val):
        for i, grad in enumerate(new_grads):
            new_grads[i] = grad * grad_clip_val / norm
        return new_grads
    return grads
</code></pre>
<pre><code class="language-{.python .input  n=27}">%%tab jax
@d2l.add_to_class(d2l.Trainer)  #@save
def clip_gradients(self, grad_clip_val, grads):
    grad_leaves, _ = jax.tree_util.tree_flatten(grads)
    norm = jnp.sqrt(sum(jnp.vdot(x, x) for x in grad_leaves))
    clip = lambda grad: jnp.where(norm &lt; grad_clip_val,
                                  grad, grad * (grad_clip_val / norm))
    return jax.tree_util.tree_map(clip, grads)
</code></pre>
<h2 id="훈련-training-17"><a class="header" href="#훈련-training-17">훈련 (Training)</a></h2>
<p><em>타임 머신</em> 데이터셋(<code>data</code>)을 사용하여 밑바닥부터 구현된 RNN(<code>rnn</code>)을 기반으로 문자 수준 언어 모델(<code>model</code>)을 훈련합니다.
먼저 기울기를 계산한 다음 클리핑하고 마지막으로 클리핑된 기울기를 사용하여 모델 파라미터를 업데이트한다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.TimeMachine(batch_size=1024, num_steps=32)
if tab.selected('mxnet', 'pytorch', 'jax'):
    rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)
    model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)
    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)
        model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)
    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)
trainer.fit(model, data)
</code></pre>
<h2 id="디코딩-decoding"><a class="header" href="#디코딩-decoding">디코딩 (Decoding)</a></h2>
<p>일단 언어 모델이 학습되면, 다음 토큰을 예측하는 데 사용할 뿐만 아니라 이전에 예측된 토큰을 입력의 다음 토큰인 것처럼 취급하여 각 후속 토큰을 계속 예측할 수 있습니다.
때로는 문서의 시작 부분에서 시작하는 것처럼 텍스트를 생성하고 싶을 때가 있습니다.
그러나 사용자 제공 접두사에 언어 모델을 조건부로 설정하는 것이 종종 유용합니다.
예를 들어 검색 엔진을 위한 자동 완성 기능이나 이메일 작성 시 사용자를 돕기 위한 기능을 개발하는 경우,
지금까지 작성한 내용(접두사)을 입력하고 그럴듯한 연속을 생성하기를 원할 것입니다.</p>
<p>[<strong>다음 <code>predict</code> 메서드는 사용자 제공 <code>prefix</code>를 섭취한 후 한 번에 한 문자씩 연속을 생성합니다</strong>].
<code>prefix</code>의 문자를 반복할 때, 은닉 상태를 다음 타임 스텝으로 계속 전달하지만 출력은 생성하지 않습니다.
이것을 <em>워밍업(warm-up)</em> 기간이라고 합니다.
접두사를 섭취한 후, 이제 후속 문자를 방출할 준비가 되었습니다. 각 문자는 다음 타임 스텝의 입력으로 모델에 피드백됩니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(RNNLMScratch)  #@save
def predict(self, prefix, num_preds, vocab, device=None):
    state, outputs = None, [vocab[prefix[0]]]
    for i in range(len(prefix) + num_preds - 1):
        if tab.selected('mxnet'):
            X = d2l.tensor([[outputs[-1]]], ctx=device)
        if tab.selected('pytorch'):
            X = d2l.tensor([[outputs[-1]]], device=device)
        if tab.selected('tensorflow'):
            X = d2l.tensor([[outputs[-1]]])
        embs = self.one_hot(X)
        rnn_outputs, state = self.rnn(embs, state)
        if i &lt; len(prefix) - 1:  # 워밍업 기간
            outputs.append(vocab[prefix[i + 1]])
        else:  # num_preds 단계 예측
            Y = self.output_layer(rnn_outputs)
            outputs.append(int(d2l.reshape(d2l.argmax(Y, axis=2), 1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(RNNLMScratch)  #@save
def predict(self, prefix, num_preds, vocab, params):
    state, outputs = None, [vocab[prefix[0]]]
    for i in range(len(prefix) + num_preds - 1):
        X = d2l.tensor([[outputs[-1]]])
        embs = self.one_hot(X)
        rnn_outputs, state = self.rnn.apply({'params': params['rnn']},
                                            embs, state)
        if i &lt; len(prefix) - 1:  # 워밍업 기간
            outputs.append(vocab[prefix[i + 1]])
        else:  # num_preds 단계 예측
            Y = self.apply({'params': params}, rnn_outputs,
                           method=self.output_layer)
            outputs.append(int(d2l.reshape(d2l.argmax(Y, axis=2), 1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
</code></pre>
<p>다음에서는 접두사를 지정하고 20개의 추가 문자를 생성하도록 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
model.predict('it has', 20, data.vocab, d2l.try_gpu())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
model.predict('it has', 20, data.vocab)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
model.predict('it has', 20, data.vocab, trainer.state.params)
</code></pre>
<p>위의 RNN 모델을 밑바닥부터 구현하는 것은 유익하지만 편리하지는 않습니다.
다음 섹션에서는 딥러닝 프레임워크를 활용하여 표준 아키텍처를 사용하여 RNN을 빠르게 만들고, 고도로 최적화된 라이브러리 함수에 의존하여 성능 향상을 얻는 방법을 볼 것입니다.</p>
<h2 id="요약-summary-33"><a class="header" href="#요약-summary-33">요약 (Summary)</a></h2>
<p>우리는 사용자 제공 텍스트 접두사를 따르는 텍스트를 생성하도록 RNN 기반 언어 모델을 훈련할 수 있습니다.
간단한 RNN 언어 모델은 입력 인코딩, RNN 모델링, 출력 생성으로 구성됩니다.
훈련 중에 기울기 클리핑은 폭발하는 기울기 문제를 완화할 수 있지만 사라지는 기울기 문제를 해결하지는 않습니다. 실험에서 우리는 간단한 RNN 언어 모델을 구현하고 문자 수준에서 토큰화된 텍스트 시퀀스에 대해 기울기 클리핑을 사용하여 훈련했습니다. 접두사에 조건을 걸어 언어 모델을 사용하여 그럴듯한 연속을 생성할 수 있으며, 이는 자동 완성 기능과 같은 많은 응용 프로그램에서 유용함이 입증되었습니다.</p>
<h2 id="연습-문제-exercises-46"><a class="header" href="#연습-문제-exercises-46">연습 문제 (Exercises)</a></h2>
<ol>
<li>구현된 언어 모델은 <em>타임 머신</em>의 맨 처음 토큰까지의 모든 과거 토큰을 기반으로 다음 토큰을 예측합니까?</li>
<li>어떤 하이퍼파라미터가 예측에 사용되는 역사의 길이를 제어합니까?</li>
<li>원-핫 인코딩이 각 객체에 대해 다른 임베딩을 선택하는 것과 동일함을 보이십시오.</li>
<li>퍼플렉서티를 개선하기 위해 하이퍼파라미터(예: 에폭 수, 은닉 유닛 수, 미니배치의 타임 스텝 수, 학습률)를 조정하십시오. 이 간단한 아키텍처를 고수하면서 얼마나 낮출 수 있습니까?</li>
<li>원-핫 인코딩을 학습 가능한 임베딩으로 대체하십시오. 이것이 더 나은 성능으로 이어집니까?</li>
<li><em>타임 머신</em>에서 훈련된 이 언어 모델이 H. G. Wells의 다른 책, 예: *우주 전쟁(The War of the Worlds)*에서 얼마나 잘 작동하는지 결정하기 위한 실험을 수행하십시오.</li>
<li>다른 저자가 쓴 책에서 이 모델의 퍼플렉서티를 평가하기 위한 또 다른 실험을 수행하십시오.</li>
<li>가장 가능성 있는 다음 문자를 선택하는 대신 샘플링을 사용하도록 예측 메서드를 수정하십시오.
<ul>
<li>무슨 일이 일어납니까?</li>
<li>모델을 더 가능성 있는 출력 쪽으로 편향시키십시오. 예를 들어 $\alpha &gt; 1$에 대해 $q(x_t \mid x_{t-1}, \ldots, x_1) \propto P(x_t \mid x_{t-1}, \ldots, x_1)^\alpha$에서 샘플링합니다.</li>
</ul>
</li>
<li>기울기를 클리핑하지 않고 이 섹션의 코드를 실행하십시오. 무슨 일이 일어납니까?</li>
<li>이 섹션에서 사용된 활성화 함수를 ReLU로 대체하고 이 섹션의 실험을 반복하십시오. 여전히 기울기 클리핑이 필요합니까? 그 이유는 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/336">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/486">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1052">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18014">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="순환-신경망-간결한-구현-concise-implementation-of-recurrent-neural-networks"><a class="header" href="#순환-신경망-간결한-구현-concise-implementation-of-recurrent-neural-networks">순환 신경망 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></h1>
<p>:label:<code>sec_rnn-concise</code></p>
<p>대부분의 밑바닥부터 구현과 마찬가지로, :numref:<code>sec_rnn-scratch</code>는 각 구성 요소가 어떻게 작동하는지에 대한 통찰력을 제공하도록 설계되었습니다.
하지만 매일 RNN을 사용하거나 프로덕션 코드를 작성할 때는 구현 시간을 줄이고(공통 모델 및 함수에 대한 라이브러리 코드 제공) 계산 시간을 줄이는(이러한 라이브러리 구현을 최적화하여) 라이브러리에 더 의존하고 싶을 것입니다.
이 섹션에서는 딥러닝 프레임워크에서 제공하는 고수준 API를 사용하여 동일한 언어 모델을 보다 효율적으로 구현하는 방법을 보여줍니다.
이전과 마찬가지로 <em>타임 머신</em> 데이터셋을 로드하는 것으로 시작합니다.</p>
<pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn, rnn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
</code></pre>
<h2 id="모델-정의하기-defining-the-model-3"><a class="header" href="#모델-정의하기-defining-the-model-3">[<strong>모델 정의하기 (Defining the Model)</strong>]</a></h2>
<p>고수준 API로 구현된 RNN을 사용하여 다음 클래스를 정의합니다.</p>
<p>:begin_tab:<code>mxnet</code>
구체적으로 은닉 상태를 초기화하기 위해 멤버 메서드 <code>begin_state</code>를 호출합니다.
이것은 미니배치의 각 예제에 대한 초기 은닉 상태를 포함하는 리스트를 반환하며, 그 모양은 (은닉층 수, 배치 크기, 은닉 유닛 수)입니다.
나중에 소개될 일부 모델(예: LSTM)의 경우 이 리스트에는 다른 정보도 포함됩니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
Flax는 현재 바닐라 RNN의 간결한 구현을 위한 RNNCell을 제공하지 않습니다. Flax <code>linen</code> API에는 LSTM 및 GRU와 같은 더 진보된 RNN 변형이 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class RNN(d2l.Module):  #@save
    """고수준 API로 구현된 RNN 모델."""
    def __init__(self, num_hiddens):
        super().__init__()
        self.save_hyperparameters()        
        self.rnn = rnn.RNN(num_hiddens)
        
    def forward(self, inputs, H=None):
        if H is None:
            H, = self.rnn.begin_state(inputs.shape[1], ctx=inputs.ctx)
        outputs, (H, ) = self.rnn(inputs, (H, ))
        return outputs, H
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class RNN(d2l.Module):  #@save
    """고수준 API로 구현된 RNN 모델."""
    def __init__(self, num_inputs, num_hiddens):
        super().__init__()
        self.save_hyperparameters()
        self.rnn = nn.RNN(num_inputs, num_hiddens)
        
    def forward(self, inputs, H=None):
        return self.rnn(inputs, H)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class RNN(d2l.Module):  #@save
    """고수준 API로 구현된 RNN 모델."""
    def __init__(self, num_hiddens):
        super().__init__()
        self.save_hyperparameters()            
        self.rnn = tf.keras.layers.SimpleRNN(
            num_hiddens, return_sequences=True, return_state=True,
            time_major=True)
        
    def forward(self, inputs, H=None):
        outputs, H = self.rnn(inputs, H)
        return outputs, H
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class RNN(nn.Module):  #@save
    """고수준 API로 구현된 RNN 모델."""
    num_hiddens: int

    @nn.compact
    def __call__(self, inputs, H=None):
        raise NotImplementedError
</code></pre>
<p>:numref:<code>sec_rnn-scratch</code>의 <code>RNNLMScratch</code> 클래스를 상속하여,
다음 <code>RNNLM</code> 클래스는 완전한 RNN 기반 언어 모델을 정의합니다.
별도의 완전 연결 출력 레이어를 생성해야 한다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
class RNNLM(d2l.RNNLMScratch):  #@save
    """고수준 API로 구현된 RNN 기반 언어 모델."""
    def init_params(self):
        self.linear = nn.LazyLinear(self.vocab_size)
        
    def output_layer(self, hiddens):
        return d2l.swapaxes(self.linear(hiddens), 0, 1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow
class RNNLM(d2l.RNNLMScratch):  #@save
    """고수준 API로 구현된 RNN 기반 언어 모델."""
    def init_params(self):
        if tab.selected('mxnet'):
            self.linear = nn.Dense(self.vocab_size, flatten=False)
            self.initialize()
        if tab.selected('tensorflow'):
            self.linear = tf.keras.layers.Dense(self.vocab_size)
        
    def output_layer(self, hiddens):
        if tab.selected('mxnet'):
            return d2l.swapaxes(self.linear(hiddens), 0, 1)        
        if tab.selected('tensorflow'):
            return d2l.transpose(self.linear(hiddens), (1, 0, 2))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class RNNLM(d2l.RNNLMScratch):  #@save
    """고수준 API로 구현된 RNN 기반 언어 모델."""
    training: bool = True

    def setup(self):
        self.linear = nn.Dense(self.vocab_size)

    def output_layer(self, hiddens):
        return d2l.swapaxes(self.linear(hiddens), 0, 1)

    def forward(self, X, state=None):
        embs = self.one_hot(X)
        rnn_outputs, _ = self.rnn(embs, state, self.training)
        return self.output_layer(rnn_outputs)
</code></pre>
<h2 id="훈련-및-예측-training-and-predicting"><a class="header" href="#훈련-및-예측-training-and-predicting">훈련 및 예측 (Training and Predicting)</a></h2>
<p>모델을 훈련하기 전에, [<strong>무작위 가중치로 초기화된 모델로 예측</strong>]해 봅시다.
네트워크를 훈련하지 않았으므로 터무니없는 예측을 생성할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
data = d2l.TimeMachine(batch_size=1024, num_steps=32)
if tab.selected('mxnet', 'tensorflow'):
    rnn = RNN(num_hiddens=32)
if tab.selected('pytorch'):
    rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)
model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)
model.predict('it has', 20, data.vocab)
</code></pre>
<p>다음으로 [<strong>고수준 API를 활용하여 모델을 훈련</strong>]합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
if tab.selected('mxnet', 'pytorch'):
    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)
trainer.fit(model, data)
</code></pre>
<p>:numref:<code>sec_rnn-scratch</code>와 비교할 때,
이 모델은 비슷한 퍼플렉서티를 달성하지만 최적화된 구현으로 인해 더 빠르게 실행됩니다.
이전과 마찬가지로 지정된 접두사 문자열에 이어지는 예측 토큰을 생성할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
model.predict('it has', 20, data.vocab, d2l.try_gpu())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
model.predict('it has', 20, data.vocab)
</code></pre>
<h2 id="요약-summary-34"><a class="header" href="#요약-summary-34">요약 (Summary)</a></h2>
<p>딥러닝 프레임워크의 고수준 API는 표준 RNN 구현을 제공합니다.
이 라이브러리는 표준 모델을 다시 구현하는 데 시간을 낭비하지 않도록 도와줍니다.
더욱이
프레임워크 구현은 종종 고도로 최적화되어 있어
밑바닥부터 구현한 것과 비교할 때 상당한(계산적) 성능 향상을 가져옵니다.</p>
<h2 id="연습-문제-exercises-47"><a class="header" href="#연습-문제-exercises-47">연습 문제 (Exercises)</a></h2>
<ol>
<li>고수준 API를 사용하여 RNN 모델을 과대적합하게 만들 수 있습니까?</li>
<li>RNN을 사용하여 :numref:<code>sec_sequence</code>의 자기회귀 모델을 구현하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/335">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1053">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/2211">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18015">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="시간-경과에-따른-역전파-backpropagation-through-time"><a class="header" href="#시간-경과에-따른-역전파-backpropagation-through-time">시간 경과에 따른 역전파 (Backpropagation Through Time)</a></h1>
<p>:label:<code>sec_bptt</code></p>
<p>:numref:<code>sec_rnn-scratch</code>의 연습 문제를 완료했다면, 가끔 발생하는 엄청난 기울기로 인해 훈련이 불안정해지는 것을 방지하기 위해 기울기 클리핑이 필수적이라는 것을 알았을 것입니다.
우리는 폭발하는 기울기가 긴 시퀀스를 가로질러 역전파하는 데서 비롯된다고 암시했습니다.
수많은 현대 RNN 아키텍처를 소개하기 전에, 시퀀스 모델에서 <em>역전파</em>가 어떻게 작동하는지 수학적으로 자세히 살펴보겠습니다.
바라건대 이 논의가 <em>사라지는(vanishing)</em> 기울기와 <em>폭발하는(exploding)</em> 기울기의 개념에 어느 정도 정확성을 가져다줄 것입니다.
:numref:<code>sec_backprop</code>에서 MLP를 소개할 때 계산 그래프를 통한 순전파 및 역전파에 대한 논의를 기억한다면,
RNN의 순전파는 비교적 간단할 것입니다.
RNN에 역전파를 적용하는 것을 *시간 경과에 따른 역전파(backpropagation through time)*라고 합니다 :cite:<code>Werbos.1990</code>.
이 절차는 RNN의 계산 그래프를 한 번에 한 타임 스텝씩 확장(또는 펼치기)해야 합니다.
펼쳐진 RNN은 본질적으로
펼쳐진 네트워크 전체에서 동일한 파라미터가 반복되어
각 타임 스텝에 나타나는 특수한 속성을 가진 피드포워드 신경망입니다.
그런 다음 다른 피드포워드 신경망과 마찬가지로 연쇄 법칙을 적용하여 펼쳐진 네트워크를 통해 기울기를 역전파할 수 있습니다.
각 파라미터에 대한 기울기는 펼쳐진 네트워크에서 파라미터가 발생하는 모든 위치에 걸쳐 합산되어야 합니다.
이러한 가중치 묶음을 처리하는 것은 합성곱 신경망에 대한 장에서 익숙할 것입니다.</p>
<p>시퀀스가 꽤 길 수 있기 때문에 복잡한 문제가 발생합니다.
천 개 이상의 토큰으로 구성된 텍스트 시퀀스로 작업하는 것은 드문 일이 아닙니다.
이것은 계산(너무 많은 메모리) 및 최적화(수치적 불안정성) 관점 모두에서 문제를 제기합니다.
첫 번째 단계의 입력은 출력에 도달하기 전에 1000개 이상의 행렬 곱을 통과하며,
기울기를 계산하기 위해 또 다른 1000개의 행렬 곱이 필요합니다.
이제 무엇이 잘못될 수 있고 실제로 어떻게 해결해야 하는지 분석해 보겠습니다.</p>
<h2 id="rnn의-기울기-분석-analysis-of-gradients-in-rnns"><a class="header" href="#rnn의-기울기-분석-analysis-of-gradients-in-rnns">RNN의 기울기 분석 (Analysis of Gradients in RNNs)</a></h2>
<p>:label:<code>subsec_bptt_analysis</code></p>
<p>RNN이 작동하는 방식에 대한 단순화된 모델로 시작합니다.
이 모델은 은닉 상태의 세부 사항과 업데이트 방법에 대한 세부 사항을 무시합니다.
여기서 수학적 표기법은 스칼라, 벡터, 행렬을 명시적으로 구분하지 않습니다.
우리는 단지 약간의 직관을 개발하려고 노력하고 있습니다.
이 단순화된 모델에서 타임 스텝 $t$에서의 은닉 상태를 $h_t$, 입력을 $x_t$, 출력을 $o_t$로 표시합니다.
:numref:<code>subsec_rnn_w_hidden_states</code>에서의 논의를 상기하면,
입력과 은닉 상태는 은닉층의 하나의 가중치 변수와 곱해지기 전에 연결될 수 있습니다.
따라서 우리는 $w_\textrm{h}$와 $w_\textrm{o}$를 사용하여 각각 은닉층과 출력 레이어의 가중치를 나타냅니다.
결과적으로 각 타임 스텝에서의 은닉 상태와 출력은 다음과 같습니다.</p>
<p>$$\begin{aligned}h_t &amp;= f(x_t, h_{t-1}, w_\textrm{h}),\o_t &amp;= g(h_t, w_\textrm{o}),\end{aligned}$$:eqlabel:<code>eq_bptt_ht_ot</code></p>
<p>여기서 $f$와 $g$는 각각 은닉층과 출력 레이어의 변환입니다.
따라서 우리는 순환 계산을 통해 서로 의존하는 값의 체인 {\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \ldots}를 갖습니다.
순전파는 꽤 간단합니다.
우리가 필요한 것은 $(x_t, h_t, o_t)$ 삼중쌍을 한 번에 한 타임 스텝씩 반복하는 것입니다.
출력 $o_t$와 원하는 타겟 $y_t$ 사이의 불일치는 다음과 같이 모든 $T$ 타임 스텝에 걸쳐 목적 함수에 의해 평가됩니다.</p>
<p>$$L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_\textrm{h}, w_\textrm{o}) = \frac{1}{T}\sum_{t=1}^T l(y_t, o_t).$$</p>
<p>역전파의 경우, 특히 목적 함수 $L$의 파라미터 $w_\textrm{h}$에 대한 기울기를 계산할 때 문제는 좀 더 까다롭습니다.
구체적으로 연쇄 법칙에 의해,</p>
<p>$$\begin{aligned}\frac{\partial L}{\partial w_\textrm{h}}  &amp; = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_\textrm{h}}  \&amp; = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_\textrm{o})}{\partial h_t}  \frac{\partial h_t}{\partial w_\textrm{h}}.\end{aligned}$$:eqlabel:<code>eq_bptt_partial_L_wh</code></p>
<p>:eqref:<code>eq_bptt_partial_L_wh</code>에 있는 곱의 첫 번째와 두 번째 인수는 계산하기 쉽습니다.
세 번째 인수 $\partial h_t/\partial w_\textrm{h}$는 상황이 까다로워지는 부분인데, $h_t$에 대한 파라미터 $w_\textrm{h}$의 효과를 순환적으로 계산해야 하기 때문입니다.
:eqref:<code>eq_bptt_ht_ot</code>의 순환 계산에 따르면,
$h_t$는 $h_{t-1}$과 $w_\textrm{h}$ 모두에 의존하며,
여기서 $h_{t-1}$의 계산도 $w_\textrm{h}$에 의존합니다.
따라서 연쇄 법칙을 사용하여 $w_\textrm{h}$에 대한 $h_t$의 전도함수(total derivate)를 평가하면 다음과 같습니다.</p>
<p>$$\frac{\partial h_t}{\partial w_\textrm{h}}= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}} +\frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_\textrm{h}}.$$:eqlabel:<code>eq_bptt_partial_ht_wh_recur</code></p>
<p>위의 기울기를 유도하기 위해, $t=1, 2,\ldots$에 대해 $a_{0}=0$ 및 $a_{t}=b_{t}+c_{t}a_{t-1}$을 만족하는 세 시퀀스 {a_{t}},{b_{t}},{c_{t}}가 있다고 가정합니다.
그러면 $t\geq 1$에 대해 다음을 보이는 것은 쉽습니다.</p>
<p>$$a_{t}=b_{t}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t}c_{j}\right)b_{i}.$$:eqlabel:<code>eq_bptt_at</code></p>
<p>다음에 따라 $a_t$, $b_t$, $c_t$를 대입하면</p>
<p>$$\begin{aligned}a_t &amp;= \frac{\partial h_t}{\partial w_\textrm{h}},\b_t &amp;= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}}, \c_t &amp;= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial h_{t-1}},
\end{aligned}$$</p>
<p>:eqref:<code>eq_bptt_partial_ht_wh_recur</code>의 기울기 계산은 $a_{t}=b_{t}+c_{t}a_{t-1}$을 만족합니다.
따라서 :eqref:<code>eq_bptt_at</code>에 따라 다음을 사용하여 :eqref:<code>eq_bptt_partial_ht_wh_recur</code>의 순환 계산을 제거할 수 있습니다.</p>
<p>$$\frac{\partial h_t}{\partial w_\textrm{h}}=\frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_\textrm{h})}{\partial h_{j-1}} \right) \frac{\partial f(x_{i},h_{i-1},w_\textrm{h})}{\partial w_\textrm{h}}.$$:eqlabel:<code>eq_bptt_partial_ht_wh_gen</code></p>
<p>연쇄 법칙을 사용하여 $\partial h_t/\partial w_\textrm{h}$를 재귀적으로 계산할 수 있지만, $t$가 클 때마다 이 체인이 매우 길어질 수 있습니다.
이 문제를 다루기 위한 몇 가지 전략을 논의해 봅시다.</p>
<h3 id="전체-계산-full-computation"><a class="header" href="#전체-계산-full-computation">전체 계산 (Full Computation)</a></h3>
<p>한 가지 아이디어는 :eqref:<code>eq_bptt_partial_ht_wh_gen</code>에서 전체 합을 계산하는 것일 수 있습니다.
하지만 이것은 매우 느리고 기울기가 폭발할 수 있습니다.
초기 조건의 미묘한 변화가 결과에 큰 영향을 미칠 수 있기 때문입니다.
즉, 초기 조건의 미미한 변화가 결과의 불균형한 변화로 이어지는 나비 효과와 유사한 것을 볼 수 있습니다.
이것은 일반적으로 바람직하지 않습니다.
결국 우리는 잘 일반화되는 강력한 추정기를 찾고 있습니다.
따라서 이 전략은 실제로 거의 사용되지 않습니다.</p>
<h3 id="타임-스텝-자르기-truncating-time-steps"><a class="header" href="#타임-스텝-자르기-truncating-time-steps">타임 스텝 자르기 (Truncating Time Steps)###</a></h3>
<p>대안으로,
우리는 $\tau$ 단계 후 :eqref:<code>eq_bptt_partial_ht_wh_gen</code>의 합을 자를 수 있습니다.
이것이 지금까지 우리가 논의해 온 것입니다.
이것은 $\partial h_{t-\tau}/\partial w_\textrm{h}$에서 합을 종료함으로써 실제 기울기의 <em>근사치</em>로 이어집니다.
실제로 이것은 꽤 잘 작동합니다.
이것은 일반적으로 절단된 시간 경과에 따른 역전파(truncated backpropagation through time)라고 불리는 것입니다 :cite:<code>Jaeger.2002</code>.
이것의 결과 중 하나는 모델이 장기적인 결과보다는 주로 단기적인 영향에 초점을 맞춘다는 것입니다.
이것은 실제로 <em>바람직</em>한데, 추정치를 더 단순하고 안정적인 모델 쪽으로 편향시키기 때문입니다.</p>
<h3 id="무작위-자르기-randomized-truncation"><a class="header" href="#무작위-자르기-randomized-truncation">무작위 자르기 (Randomized Truncation)</a></h3>
<p>마지막으로, 우리는 $\partial h_t/\partial w_\textrm{h}$를 기댓값에서는 정확하지만 시퀀스를 자르는 확률 변수로 대체할 수 있습니다.
이것은 미리 정의된 $0 \leq \pi_t \leq 1$을 갖는 $\xi_t$ 시퀀스를 사용하여 달성됩니다.
여기서 $P(\xi_t = 0) = 1-\pi_t$이고 $P(\xi_t = \pi_t^{-1}) = \pi_t$이므로 $E[\xi_t] = 1$입니다.
우리는 이것을 사용하여 :eqref:<code>eq_bptt_partial_ht_wh_recur</code>의 기울기 $\partial h_t/\partial w_\textrm{h}$를 다음과 같이 대체합니다.</p>
<p>$$z_t= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}} +\xi_t \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_\textrm{h}}.$$</p>
<p>$\xi_t$의 정의에서 $E[z_t] = \partial h_t/\partial w_\textrm{h}$가 따릅니다.
$\xi_t = 0$일 때마다 순환 계산은 해당 타임 스텝 $t$에서 종료됩니다.
이것은 다양한 길이의 시퀀스의 가중 합으로 이어지며, 긴 시퀀스는 드물지만 적절하게 가중치가 부여됩니다.
이 아이디어는 :citet:<code>Tallec.Ollivier.2017</code>에 의해 제안되었습니다.</p>
<h3 id="전략-비교-comparing-strategies"><a class="header" href="#전략-비교-comparing-strategies">전략 비교 (Comparing Strategies)</a></h3>
<p><img src="chapter_recurrent-neural-networks/../img/truncated-bptt.svg" alt="RNN에서 기울기를 계산하기 위한 전략 비교. 위에서 아래로: 무작위 자르기, 일반 자르기, 전체 계산." />
:label:<code>fig_truncated_bptt</code></p>
<p>:numref:<code>fig_truncated_bptt</code>는 RNN에 대한 시간 경과에 따른 역전파를 사용하여 <em>타임 머신</em>의 처음 몇 글자를 분석할 때 세 가지 전략을 보여줍니다:</p>
<ul>
<li>첫 번째 행은 텍스트를 다양한 길이의 세그먼트로 분할하는 무작위 자르기입니다.</li>
<li>두 번째 행은 텍스트를 동일한 길이의 하위 시퀀스로 나누는 일반 자르기입니다. 이것이 우리가 RNN 실험에서 해왔던 것입니다.</li>
<li>세 번째 행은 계산적으로 실행 불가능한 표현으로 이어지는 전체 시간 경과에 따른 역전파입니다.</li>
</ul>
<p>불행히도 이론적으로는 매력적이지만 무작위 자르기는 일반 자르기보다 훨씬 더 잘 작동하지 않는데, 이는 여러 요인 때문일 가능성이 큽니다.
첫째, 과거로의 여러 역전파 단계 후 관찰의 효과는 실제로 의존성을 포착하기에 충분합니다.
둘째, 증가된 분산은 더 많은 단계에서 기울기가 더 정확하다는 사실을 상쇄합니다.
셋째, 우리는 실제로 짧은 범위의 상호 작용만 있는 모델을 <em>원합니다</em>.
따라서 정기적으로 자른 시간 경과에 따른 역전파는 바람직할 수 있는 약간의 정규화 효과를 갖습니다.</p>
<h2 id="시간-경과에-따른-역전파-상세-backpropagation-through-time-in-detail"><a class="header" href="#시간-경과에-따른-역전파-상세-backpropagation-through-time-in-detail">시간 경과에 따른 역전파 상세 (Backpropagation Through Time in Detail)</a></h2>
<p>일반적인 원칙을 논의한 후, 시간 경과에 따른 역전파에 대해 자세히 논의해 봅시다.
:numref:<code>subsec_bptt_analysis</code>의 분석과 대조적으로, 다음에서는 분해된 모든 모델 파라미터에 대한 목적 함수의 기울기를 계산하는 방법을 보여줄 것입니다.
일을 단순하게 유지하기 위해 편향 파라미터가 없고 은닉층의 활성화 함수가 항등 매핑($\phi(x)=x$)을 사용하는 RNN을 고려합니다.
타임 스텝 $t$에 대해 단일 예제 입력과 타겟을 각각 $\mathbf{x}_t \in \mathbb{R}^d$와 $y_t$라고 합시다.
은닉 상태 $\mathbf{h}_t \in \mathbb{R}^h$와 출력 $\mathbf{o}_t \in \mathbb{R}^q$는 다음과 같이 계산됩니다.</p>
<p>$$\begin{aligned}\mathbf{h}<em>t &amp;= \mathbf{W}</em>\textrm{hx} \mathbf{x}<em>t + \mathbf{W}</em>\textrm{hh} \mathbf{h}<em>{t-1},\\mathbf{o}<em>t &amp;= \mathbf{W}</em>\textrm{qh} \mathbf{h}</em>{t},\end{aligned}$$</p>
<p>여기서 $\mathbf{W}<em>\textrm{hx} \in \mathbb{R}^{h \times d}$, $\mathbf{W}</em>\textrm{hh} \in \mathbb{R}^{h \times h}$, $\mathbf{W}_\textrm{qh} \in \mathbb{R}^{q \times h}$는 가중치 파라미터입니다.
$l(\mathbf{o}_t, y_t)$를 타임 스텝 $t$에서의 손실이라고 합시다.
우리의 목적 함수, 시퀀스 시작부터 $T$ 타임 스텝에 걸친 손실은 다음과 같습니다.</p>
<p>$$L = \frac{1}{T} \sum_{t=1}^T l(\mathbf{o}_t, y_t).$$</p>
<p>RNN 계산 중 모델 변수와 파라미터 간의 종속성을 시각화하기 위해, :numref:<code>fig_rnn_bptt</code>와 같이 모델에 대한 계산 그래프를 그릴 수 있습니다.
예를 들어 타임 스텝 3의 은닉 상태 $\mathbf{h}<em>3$의 계산은 모델 파라미터 $\mathbf{W}</em>\textrm{hx}$와 $\mathbf{W}_\textrm{hh}$, 이전 타임 스텝의 은닉 상태 $\mathbf{h}_2$, 현재 타임 스텝의 입력 $\mathbf{x}_3$에 의존합니다.</p>
<p><img src="chapter_recurrent-neural-networks/../img/rnn-bptt.svg" alt="3개의 타임 스텝을 가진 RNN 모델에 대한 종속성을 보여주는 계산 그래프. 상자는 변수(음영 없음) 또는 파라미터(음영 있음)를 나타내고 원은 연산자를 나타냅니다." />
:label:<code>fig_rnn_bptt</code></p>
<p>방금 언급했듯이 :numref:<code>fig_rnn_bptt</code>의 모델 파라미터는 $\mathbf{W}<em>\textrm{hx}$, $\mathbf{W}</em>\textrm{hh}$, $\mathbf{W}<em>\textrm{qh}$입니다.
일반적으로 이 모델을 훈련하려면 이러한 파라미터에 대한 기울기 계산 $\partial L/\partial \mathbf{W}</em>\textrm{hx}$, $\partial L/\partial \mathbf{W}<em>\textrm{hh}$, $\partial L/\partial \mathbf{W}</em>\textrm{qh}$가 필요합니다.
:numref:<code>fig_rnn_bptt</code>의 종속성에 따라 화살표의 반대 방향으로 순회하여 기울기를 차례로 계산하고 저장할 수 있습니다.
연쇄 법칙에서 모양이 다른 행렬, 벡터, 스칼라의 곱셈을 유연하게 표현하기 위해 :numref:<code>sec_backprop</code>에서 설명한 대로 $\textrm{prod}$ 연산자를 계속 사용합니다.</p>
<p>우선, 임의의 타임 스텝 $t$에서 모델 출력에 대한 목적 함수를 미분하는 것은 꽤 간단합니다:</p>
<p>$$\frac{\partial L}{\partial \mathbf{o}_t} =  \frac{\partial l (\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t} \in \mathbb{R}^q.$$:eqlabel:<code>eq_bptt_partial_L_ot</code></p>
<p>이제 출력 레이어의 파라미터 $\mathbf{W}<em>\textrm{qh}$에 대한 목적 함수의 기울기를 계산할 수 있습니다: $\partial L/\partial \mathbf{W}</em>\textrm{qh} \in \mathbb{R}^{q \times h}$.
:numref:<code>fig_rnn_bptt</code>를 기반으로 목적 함수 $L$은 $\mathbf{o}_1, \ldots, \mathbf{o}<em>T$를 통해 $\mathbf{W}</em>\textrm{qh}$에 의존합니다.
연쇄 법칙을 사용하면 다음을 얻습니다.</p>
<p>$$
\frac{\partial L}{\partial \mathbf{W}<em>\textrm{qh}}
= \sum</em>{t=1}^T \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{o}<em>t}, \frac{\partial \mathbf{o}<em>t}{\partial \mathbf{W}</em>\textrm{qh}}\right)
= \sum</em>{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top,
$$</p>
<p>여기서 $\partial L/\partial \mathbf{o}_t$는 :eqref:<code>eq_bptt_partial_L_ot</code>에 의해 주어집니다.</p>
<p>다음으로, :numref:<code>fig_rnn_bptt</code>에 표시된 것처럼,
마지막 타임 스텝 $T$에서 목적 함수 $L$은 $\mathbf{o}_T$를 통해서만 은닉 상태 $\mathbf{h}_T$에 의존합니다.
따라서 연쇄 법칙을 사용하여 기울기 $\partial L/\partial \mathbf{h}_T \in \mathbb{R}^h$를 쉽게 찾을 수 있습니다.</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}_T} = \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}<em>T} \right) = \mathbf{W}</em>\textrm{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T}.$$:eqlabel:<code>eq_bptt_partial_L_hT_final_step</code></p>
<p>목적 함수 $L$이 $\mathbf{h}_{t+1}$과 $\mathbf{o}_t$를 통해 $\mathbf{h}_t$에 의존하는 $t &lt; T$인 타임 스텝의 경우 더 까다로워집니다.
연쇄 법칙에 따라,
임의의 타임 스텝 $t &lt; T$에서 은닉 상태의 기울기 $\partial L/\partial \mathbf{h}_t \in \mathbb{R}^h$는 다음과 같이 순환적으로 계산될 수 있습니다:</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}<em>t} = \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{h}</em>{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} \right) + \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{o}<em>t}, \frac{\partial \mathbf{o}<em>t}{\partial \mathbf{h}<em>t} \right) = \mathbf{W}</em>\textrm{hh}^\top \frac{\partial L}{\partial \mathbf{h}</em>{t+1}} + \mathbf{W}</em>\textrm{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t}.$$:eqlabel:<code>eq_bptt_partial_L_ht_recur</code></p>
<p>분석을 위해, 임의의 타임 스텝 $1 \leq t \leq T$에 대해 순환 계산을 확장하면 다음을 얻습니다.</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}<em>t}= \sum</em>{i=t}^T {\left(\mathbf{W}<em>\textrm{hh}^\top\right)}^{T-i} \mathbf{W}</em>\textrm{qh}^\top \frac{\partial L}{\partial \mathbf{o}_{T+t-i}}.$$:eqlabel:<code>eq_bptt_partial_L_ht</code></p>
<p>:eqref:<code>eq_bptt_partial_L_ht</code>에서 이 간단한 선형 예제가 이미 긴 시퀀스 모델의 몇 가지 주요 문제를 보여준다는 것을 알 수 있습니다:
잠재적으로 매우 큰 거듭제곱의 $\mathbf{W}_\textrm{hh}^\top$를 포함합니다.
그 안에서 1보다 작은 고유값은 사라지고 1보다 큰 고유값은 발산합니다.
이것은 수치적으로 불안정하며, 이는 사라지는 기울기와 폭발하는 기울기의 형태로 나타납니다.
이를 해결하는 한 가지 방법은 :numref:<code>subsec_bptt_analysis</code>에서 논의한 대로 계산적으로 편리한 크기에서 타임 스텝을 자르는 것입니다.
실제로 이 자르기는 주어진 타임 스텝 수 후에 기울기를 분리(detaching)함으로써 영향을 받을 수도 있습니다.
나중에 장단기 메모리(long short-term memory)와 같은 더 정교한 시퀀스 모델이 이를 어떻게 더 완화할 수 있는지 보게 될 것입니다.</p>
<p>마지막으로 :numref:<code>fig_rnn_bptt</code>는 목적 함수 $L$이 은닉 상태 $\mathbf{h}<em>1, \ldots, \mathbf{h}<em>T$를 통해 은닉층의 모델 파라미터 $\mathbf{W}</em>\textrm{hx}$와 $\mathbf{W}</em>\textrm{hh}$에 의존함을 보여줍니다.
이러한 파라미터에 대한 기울기 $\partial L / \partial \mathbf{W}<em>\textrm{hx} \in \mathbb{R}^{h \times d}$와 $\partial L / \partial \mathbf{W}</em>\textrm{hh} \in \mathbb{R}^{h \times h}$를 계산하기 위해 연쇄 법칙을 적용하면 다음을 얻습니다.</p>
<p>$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf{W}<em>\textrm{hx}}
&amp;= \sum</em>{t=1}^T \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{h}<em>t}, \frac{\partial \mathbf{h}<em>t}{\partial \mathbf{W}</em>\textrm{hx}}\right)
= \sum</em>{t=1}^T \frac{\partial L}{\partial \mathbf{h}<em>t} \mathbf{x}<em>t^\top,\
\frac{\partial L}{\partial \mathbf{W}</em>\textrm{hh}}
&amp;= \sum</em>{t=1}^T \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{h}<em>t}, \frac{\partial \mathbf{h}<em>t}{\partial \mathbf{W}</em>\textrm{hh}}\right)
= \sum</em>{t=1}^T \frac{\partial L}{\partial \mathbf{h}<em>t} \mathbf{h}</em>{t-1}^\top,
\end{aligned}
$$</p>
<p>여기서 :eqref:<code>eq_bptt_partial_L_hT_final_step</code>과 :eqref:<code>eq_bptt_partial_L_ht_recur</code>에 의해 순환적으로 계산되는 $\partial L/\partial \mathbf{h}_t$는 수치적 안정성에 영향을 미치는 핵심 양입니다.</p>
<p>시간 경과에 따른 역전파는 RNN에 역전파를 적용하는 것이므로, :numref:<code>sec_backprop</code>에서 설명했듯이 RNN 훈련은 순전파와 시간 경과에 따른 역전파를 번갈아 가며 수행합니다.
더욱이 시간 경과에 따른 역전파는 위의 기울기를 차례로 계산하고 저장합니다.
구체적으로 $\partial L / \partial \mathbf{W}<em>\textrm{hx}$와 $\partial L / \partial \mathbf{W}</em>\textrm{hh}$의 계산 모두에 사용하기 위해 $\partial L/\partial \mathbf{h}_t$를 저장하는 것과 같이, 저장된 중간 값은 중복 계산을 피하기 위해 재사용됩니다.</p>
<h2 id="요약-summary-35"><a class="header" href="#요약-summary-35">요약 (Summary)</a></h2>
<p>시간 경과에 따른 역전파는 은닉 상태가 있는 시퀀스 모델에 역전파를 적용한 것일 뿐입니다.
계산 편의성과 수치적 안정성을 위해 정기적 또는 무작위와 같은 자르기가 필요합니다.
행렬의 높은 거듭제곱은 발산하거나 사라지는 고유값으로 이어질 수 있습니다. 이는 폭발하거나 사라지는 기울기의 형태로 나타납니다.
효율적인 계산을 위해 시간 경과에 따른 역전파 중에 중간 값이 캐시됩니다.</p>
<h2 id="연습-문제-exercises-48"><a class="header" href="#연습-문제-exercises-48">연습 문제 (Exercises)</a></h2>
<ol>
<li>고유값 $\lambda_i$와 그에 대응하는 고유 벡터 $\mathbf{v}<em>i$ ($i = 1, \ldots, n$)를 갖는 대칭 행렬 $\mathbf{M} \in \mathbb{R}^{n \times n}$이 있다고 가정합니다. 일반성을 잃지 않고 $|
\lambda_i| \geq |\lambda</em>{i+1}|$ 순서로 정렬되어 있다고 가정합니다.
<ol>
<li>$\mathbf{M}^k$가 고유값 $\lambda_i^k$를 가짐을 보이십시오.</li>
<li>무작위 벡터 $\mathbf{x} \in \mathbb{R}^n$에 대해 $\mathbf{M}^k \mathbf{x}$가 높은 확률로 $\mathbf{M}$의 고유 벡터 $\mathbf{v}_1$과 매우 잘 정렬될 것임을 증명하십시오. 이 진술을 공식화하십시오.</li>
<li>위의 결과는 RNN의 기울기에 대해 무엇을 의미합니까?</li>
</ol>
</li>
<li>기울기 클리핑 외에 순환 신경망에서 기울기 폭발에 대처할 수 있는 다른 방법을 생각할 수 있습니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/334">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="현대-순환-신경망-modern-recurrent-neural-networks"><a class="header" href="#현대-순환-신경망-modern-recurrent-neural-networks">현대 순환 신경망 (Modern Recurrent Neural Networks)</a></h1>
<p>:label:<code>chap_modern_rnn</code></p>
<p>이전 장에서는 순환 신경망(RNN) 뒤에 숨겨진 핵심 아이디어를 소개했습니다.
그러나 합성곱 신경망과 마찬가지로 RNN 아키텍처에도 엄청난 양의 혁신이 있었고, 실제 상황에서 성공적임이 입증된 여러 복잡한 설계로 정점에 달했습니다.
특히 가장 인기 있는 설계들은 기울기 소실 및 폭발로 대표되는 RNN이 직면한 악명 높은 수치적 불안정성을 완화하기 위한 메커니즘을 특징으로 합니다.
:numref:<code>chap_rnn</code>에서 우리는 투박한 기울기 클리핑(gradient clipping) 휴리스틱을 적용하여 기울기 폭발을 다루었음을 상기하십시오.
이 핵(hack)의 효능에도 불구하고, 기울기 소실 문제는 여전히 남아 있습니다.</p>
<p>이 장에서는 시퀀스를 위한 가장 성공적인 RNN 아키텍처 뒤에 숨겨진 핵심 아이디어를 소개하며, 이는 두 논문에서 비롯됩니다.
첫 번째 논문인 <em>Long Short-Term Memory</em> :cite:<code>Hochreiter.Schmidhuber.1997</code>는 네트워크의 은닉층에 있는 전통적인 노드를 대체하는 계산 단위인 *메모리 셀(memory cell)*을 도입합니다.
이러한 메모리 셀을 통해 네트워크는 이전 순환 네트워크가 겪었던 훈련의 어려움을 극복할 수 있습니다.
직관적으로 메모리 셀은 각 메모리 셀의 내부 상태 값을 가중치 1의 순환 엣지를 따라 여러 연속적인 타임 스텝에 걸쳐 계단식으로 유지함으로써 기울기 소실 문제를 피합니다.
일련의 곱셈 게이트는 네트워크가 메모리 상태로 허용할 입력뿐만 아니라 메모리 상태의 내용이 모델의 출력에 언제 영향을 주어야 하는지를 결정하는 데 도움을 줍니다.</p>
<p>두 번째 논문인 <em>Bidirectional Recurrent Neural Networks</em> :cite:<code>Schuster.Paliwal.1997</code>는 시퀀스의 어느 지점에서든 출력을 결정하기 위해 미래(후속 타임 스텝)와 과거(이전 타임 스텝)의 정보를 모두 사용하는 아키텍처를 도입합니다.
이는 과거 입력만 출력에 영향을 줄 수 있었던 이전 네트워크와 대조됩니다.
양방향(Bidirectional) RNN은 수많은 다른 작업들 중에서도 자연어 처리의 시퀀스 레이블링 작업을 위한 주류가 되었습니다.
다행히도 이 두 가지 혁신은 상호 배타적이지 않으며 음소 분류 :cite:<code>Graves.Schmidhuber.2005</code> 및 필기 인식 :cite:<code>graves2008novel</code>을 위해 성공적으로 결합되었습니다.</p>
<p>이 장의 첫 섹션들에서는 LSTM 아키텍처, 게이트 순환 유닛(GRU)이라고 불리는 더 가벼운 버전, 양방향 RNN 뒤에 숨겨진 핵심 아이디어 및 RNN 레이어가 어떻게 함께 쌓여 심층(deep) RNN을 형성하는지에 대한 간략한 설명을 제공할 것입니다.
그 후, 기계 번역과 함께 <em>인코더-디코더(encoder--decoder)</em> 아키텍처 및 *빔 검색(beam search)*과 같은 핵심 아이디어를 소개하며 시퀀스-투-시퀀스 작업에서의 RNN 응용을 살펴볼 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

lstm
gru
deep-rnn
bi-rnn
machine-translation-and-dataset
encoder-decoder
seq2seq
beam-search
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="장단기-메모리-lstm-long-short-term-memory-lstm"><a class="header" href="#장단기-메모리-lstm-long-short-term-memory-lstm">장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></h1>
<p>:label:<code>sec_lstm</code></p>
<p>역전파를 사용하여 최초의 Elman 스타일 RNN이 훈련된 직후 :cite:<code>elman1990finding</code>, 장기 의존성 학습 문제(사라지는 기울기 및 폭발하는 기울기로 인한)가 두드러졌으며, 벤지오(Bengio)와 호크라이터(Hochreiter)가 이 문제를 논의했습니다 :cite:<code>bengio1994learning,Hochreiter.Bengio.Frasconi.ea.2001</code>.
호크라이터는 이미 1991년 석사 학위 논문에서 이 문제를 명확히 기술했지만, 논문이 독일어로 작성되었기 때문에 결과가 널리 알려지지 않았습니다.
기울기 클리핑이 폭발하는 기울기에는 도움이 되지만, 사라지는 기울기를 처리하는 데는 더 정교한 솔루션이 필요한 것으로 보입니다.
사라지는 기울기를 해결하기 위한 최초이자 가장 성공적인 기술 중 하나가 :citet:<code>Hochreiter.Schmidhuber.1997</code>에 의한 장단기 메모리(long short-term memory, LSTM) 모델의 형태로 등장했습니다.
LSTM은 표준 순환 신경망과 유사하지만 여기서는 각 일반 순환 노드가 *메모리 셀(memory cell)*로 대체됩니다.
각 메모리 셀은 <em>내부 상태</em>, 즉 고정 가중치 1의 자체 연결된 순환 엣지를 가진 노드를 포함하여 기울기가 사라지거나 폭발하지 않고 많은 타임 스텝을 가로질러 전달될 수 있도록 보장합니다.</p>
<p>"장단기 메모리"라는 용어는 다음과 같은 직관에서 유래했습니다.
단순 순환 신경망은 가중치 형태의 <em>장기 메모리</em>를 가집니다.
가중치는 훈련 중에 천천히 변하며 데이터에 대한 일반적인 지식을 인코딩합니다.
또한 각 노드에서 후속 노드로 전달되는 덧없는 활성화 형태의 <em>단기 메모리</em>를 가집니다.
LSTM 모델은 메모리 셀을 통해 중간 유형의 저장을 도입합니다.
메모리 셀은 특정 연결 패턴의 더 단순한 노드들로 구성된 합성 단위로, 곱셈 노드가 새로 포함되었습니다.</p>
<pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import rnn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="게이트-메모리-셀-gated-memory-cell"><a class="header" href="#게이트-메모리-셀-gated-memory-cell">게이트 메모리 셀 (Gated Memory Cell)</a></h2>
<p>각 메모리 셀은 <em>내부 상태</em>와 (i) 주어진 입력이 내부 상태에 영향을 주어야 하는지(<em>입력 게이트</em>), (ii) 내부 상태를 0으로 씻어내야 하는지(<em>삭제 게이트</em>), (iii) 주어진 뉴런의 내부 상태가 셀의 출력에 영향을 주도록 허용되어야 하는지(<em>출력 게이트</em>)를 결정하는 여러 곱셈 게이트를 갖추고 있습니다.</p>
<h3 id="게이트-은닉-상태-gated-hidden-state"><a class="header" href="#게이트-은닉-상태-gated-hidden-state">게이트 은닉 상태 (Gated Hidden State)</a></h3>
<p>바닐라 RNN과 LSTM의 주요 차이점은 후자가 은닉 상태의 게이팅을 지원한다는 것입니다.
이는 은닉 상태가 언제 <em>업데이트</em>되어야 하는지, 그리고 언제 <em>리셋</em>되어야 하는지에 대한 전용 메커니즘을 가지고 있음을 의미합니다.
이러한 메커니즘은 학습되며 위에서 나열한 우려 사항을 해결합니다.
예를 들어 첫 번째 토큰이 매우 중요하다면 첫 번째 관찰 후에 은닉 상태를 업데이트하지 않도록 학습할 것입니다.
마찬가지로 관련 없는 일시적인 관찰을 건너뛰도록 학습할 것입니다.
마지막으로 필요할 때마다 잠재 상태를 리셋하도록 학습할 것입니다.
아래에서 이에 대해 자세히 논의합니다.</p>
<h3 id="입력-게이트-삭제-게이트-출력-게이트-input-gate-forget-gate-and-output-gate"><a class="header" href="#입력-게이트-삭제-게이트-출력-게이트-input-gate-forget-gate-and-output-gate">입력 게이트, 삭제 게이트, 출력 게이트 (Input Gate, Forget Gate, and Output Gate)</a></h3>
<p>:numref:<code>fig_lstm_0</code>에 설명된 것처럼 LSTM 게이트로 들어가는 데이터는 현재 타임 스텝의 입력과 이전 타임 스텝의 은닉 상태입니다.
시그모이드 활성화 함수가 있는 세 개의 완전 연결 레이어가 입력, 삭제, 출력 게이트의 값을 계산합니다.
시그모이드 활성화의 결과로 세 게이트의 모든 값은 $(0, 1)$ 범위에 있습니다.
또한 일반적으로 <em>tanh</em> 활성화 함수로 계산되는 <em>입력 노드</em>가 필요합니다.
직관적으로 <em>입력 게이트</em>는 입력 노드의 값 중 얼마만큼을 현재 메모리 셀 내부 상태에 추가해야 하는지를 결정합니다.
<em>삭제 게이트</em>는 메모리의 현재 값을 유지할지 아니면 씻어낼지 결정합니다.
그리고 <em>출력 게이트</em>는 메모리 셀이 현재 타임 스텝의 출력에 영향을 주어야 하는지 결정합니다.</p>
<p><img src="chapter_recurrent-modern/../img/lstm-0.svg" alt="LSTM 모델에서 입력 게이트, 삭제 게이트, 출력 게이트 계산." />
:label:<code>fig_lstm_0</code></p>
<p>수학적으로 은닉 유닛이 $h$개, 배치 크기가 $n$, 입력 수가 $d$라고 가정합니다.
따라서 입력은 $\mathbf{X}<em>t \in \mathbb{R}^{n \times d}$이고 이전 타임 스텝의 은닉 상태는 $\mathbf{H}</em>{t-1} \in \mathbb{R}^{n \times h}$입니다.
이에 따라 타임 스텝 $t$에서의 게이트는 다음과 같이 정의됩니다: 입력 게이트는 $\mathbf{I}_t \in \mathbb{R}^{n \times h}$, 삭제 게이트는 $\mathbf{F}_t \in \mathbb{R}^{n \times h}$, 출력 게이트는 $\mathbf{O}_t \in \mathbb{R}^{n \times h}$입니다.
이들은 다음과 같이 계산됩니다:</p>
<p>$$
\begin{aligned}
\mathbf{I}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xi}} + \mathbf{H}</em>{t-1} \mathbf{W}<em>{\textrm{hi}} + \mathbf{b}</em>\textrm{i}),\
\mathbf{F}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xf}} + \mathbf{H}</em>{t-1} \mathbf{W}<em>{\textrm{hf}} + \mathbf{b}</em>\textrm{f}),\
\mathbf{O}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xo}} + \mathbf{H}</em>{t-1} \mathbf{W}<em>{\textrm{ho}} + \mathbf{b}</em>\textrm{o}),
\end{aligned}
$$</p>
<p>여기서 $\mathbf{W}<em>{\textrm{xi}}, \mathbf{W}</em>{\textrm{xf}}, \mathbf{W}<em>{\textrm{xo}} \in \mathbb{R}^{d \times h}$와 $\mathbf{W}</em>{\textrm{hi}}, \mathbf{W}<em>{\textrm{hf}}, \mathbf{W}</em>{\textrm{ho}} \in \mathbb{R}^{h \times h}$는 가중치 파라미터이고 $\mathbf{b}<em>\textrm{i}, \mathbf{b}</em>\textrm{f}, \mathbf{b}_\textrm{o} \in \mathbb{R}^{1 \times h}$는 편향 파라미터입니다.
합산 중에 브로드캐스팅(:numref:<code>subsec_broadcasting</code> 참조)이 트리거됨에 유의하십시오.
우리는 입력 값을 간격 $(0, 1)$에 매핑하기 위해 시그모이드 함수(:numref:<code>sec_mlp</code>에서 소개됨)를 사용합니다.</p>
<h3 id="입력-노드-input-node"><a class="header" href="#입력-노드-input-node">입력 노드 (Input Node)</a></h3>
<p>다음으로 메모리 셀을 설계합니다.
아직 다양한 게이트의 동작을 지정하지 않았으므로, 먼저 <em>입력 노드</em> $\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$를 도입합니다.
그 계산은 위에서 설명한 세 게이트의 계산과 유사하지만 활성화 함수로 $(-1, 1)$ 범위의 값을 갖는 $\tanh$ 함수를 사용합니다.
이는 타임 스텝 $t$에서 다음과 같은 방정식으로 이어집니다:</p>
<p>$$\tilde{\mathbf{C}}<em>t = \textrm{tanh}(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xc}} + \mathbf{H}</em>{t-1} \mathbf{W}<em>{\textrm{hc}} + \mathbf{b}</em>\textrm{c}),$$</p>
<p>여기서 $\mathbf{W}<em>{\textrm{xc}} \in \mathbb{R}^{d \times h}$와 $\mathbf{W}</em>{\textrm{hc}} \in \mathbb{R}^{h \times h}$는 가중치 파라미터이고 $\mathbf{b}_\textrm{c} \in \mathbb{R}^{1 \times h}$는 편향 파라미터입니다.</p>
<p>입력 노드에 대한 간단한 그림은 :numref:<code>fig_lstm_1</code>에 나와 있습니다.</p>
<p><img src="chapter_recurrent-modern/../img/lstm-1.svg" alt="LSTM 모델에서 입력 노드 계산." />
:label:<code>fig_lstm_1</code></p>
<h3 id="메모리-셀-내부-상태-memory-cell-internal-state"><a class="header" href="#메모리-셀-내부-상태-memory-cell-internal-state">메모리 셀 내부 상태 (Memory Cell Internal State)</a></h3>
<p>LSTM에서 입력 게이트 $\mathbf{I}_t$는 $\tilde{\mathbf{C}}_t$를 통해 얼마나 많은 새 데이터를 고려할지를 지배하고, 삭제 게이트 $\mathbf{F}<em>t$는 이전 셀 내부 상태 $\mathbf{C}</em>{t-1} \in \mathbb{R}^{n \times h}$ 중 얼마만큼을 유지할지를 결정합니다.
아다마르 곱 연산자 $\odot$를 사용하여 다음과 같은 업데이트 방정식에 도달합니다:</p>
<p>$$\mathbf{C}_t = \mathbf{F}<em>t \odot \mathbf{C}</em>{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t.$$</p>
<p>삭제 게이트가 항상 1이고 입력 게이트가 항상 0이면, 메모리 셀 내부 상태 $\mathbf{C}_{t-1}$은 영원히 일정하게 유지되어 각 후속 타임 스텝으로 변경되지 않고 전달됩니다.
그러나 입력 게이트와 삭제 게이트는 모델에 이 값을 언제 변경하지 않고 유지할지, 그리고 언제 후속 입력에 대응하여 이를 섭동시킬지 학습할 수 있는 유연성을 제공합니다.
실제로 이 설계는 기울기 소실 문제를 완화하여 특히 긴 시퀀스 길이를 가진 데이터셋에 직면할 때 훈련하기 훨씬 쉬운 모델로 이어집니다.</p>
<p>따라서 :numref:<code>fig_lstm_2</code>의 흐름도에 도달합니다.</p>
<p><img src="chapter_recurrent-modern/../img/lstm-2.svg" alt="LSTM 모델에서 메모리 셀 내부 상태 계산." /></p>
<p>:label:<code>fig_lstm_2</code></p>
<h3 id="은닉-상태-hidden-state"><a class="header" href="#은닉-상태-hidden-state">은닉 상태 (Hidden State)</a></h3>
<p>마지막으로 메모리 셀의 출력, 즉 다른 레이어에서 보는 은닉 상태 $\mathbf{H}_t \in \mathbb{R}^{n \times h}$를 계산하는 방법을 정의해야 합니다.
여기서 출력 게이트가 작동합니다.
LSTM에서는 먼저 메모리 셀 내부 상태에 $\tanh$를 적용한 다음, 이번에는 출력 게이트와 함께 또 다른 요소별 곱셈을 적용합니다.
이렇게 하면 $\mathbf{H}_t$의 값이 항상 $(-1, 1)$ 간격에 있게 됩니다:</p>
<p>$$\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t).$$</p>
<p>출력 게이트가 1에 가까울 때마다 메모리 셀 내부 상태가 후속 레이어에 억제 없이 영향을 주도록 허용하는 반면, 출력 게이트 값이 0에 가까울 때는 현재 메모리가 현재 타임 스텝에서 네트워크의 다른 레이어에 영향을 주지 않도록 방지합니다.
메모리 셀은 출력 게이트가 0에 가까운 값을 취하는 동안 네트워크의 나머지 부분에 영향을 주지 않고 많은 타임 스텝에 걸쳐 정보를 축적할 수 있으며,
출력 게이트가 0에 가까운 값에서 1에 가까운 값으로 뒤집히자마자 후속 타임 스텝에서 갑자기 네트워크에 영향을 줄 수 있음에 유의하십시오. :numref:<code>fig_lstm_3</code>에 데이터 흐름의 그래픽 설명이 있습니다.</p>
<p><img src="chapter_recurrent-modern/../img/lstm-3.svg" alt="LSTM 모델에서 은닉 상태 계산." />
:label:<code>fig_lstm_3</code></p>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-4"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-4">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>이제 LSTM을 밑바닥부터 구현해 봅시다.
:numref:<code>sec_rnn-scratch</code>의 실험과 마찬가지로 먼저 <em>타임 머신</em> 데이터셋을 로드합니다.</p>
<h3 id="모델-파라미터-초기화"><a class="header" href="#모델-파라미터-초기화">[<strong>모델 파라미터 초기화</strong>]</a></h3>
<p>다음으로 모델 파라미터를 정의하고 초기화해야 합니다.
이전과 마찬가지로 하이퍼파라미터 <code>num_hiddens</code>가 은닉 유닛의 수를 결정합니다.
가중치는 0.01 표준 편차를 갖는 가우스 분포에 따라 초기화하고 편향은 0으로 설정합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class LSTMScratch(d2l.Module):
    def __init__(self, num_inputs, num_hiddens, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()

        if tab.selected('mxnet'):
            init_weight = lambda *shape: d2l.randn(*shape) * sigma
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              d2l.zeros(num_hiddens))
        if tab.selected('pytorch'):
            init_weight = lambda *shape: nn.Parameter(d2l.randn(*shape) * sigma)
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              nn.Parameter(d2l.zeros(num_hiddens)))
        if tab.selected('tensorflow'):
            init_weight = lambda *shape: tf.Variable(d2l.normal(shape) * sigma)
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              tf.Variable(d2l.zeros(num_hiddens)))

        self.W_xi, self.W_hi, self.b_i = triple()  # 입력 게이트
        self.W_xf, self.W_hf, self.b_f = triple()  # 삭제 게이트
        self.W_xo, self.W_ho, self.b_o = triple()  # 출력 게이트
        self.W_xc, self.W_hc, self.b_c = triple()  # 입력 노드
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class LSTMScratch(d2l.Module):
    num_inputs: int
    num_hiddens: int
    sigma: float = 0.01

    def setup(self):
        init_weight = lambda name, shape: self.param(name,
                                                     nn.initializers.normal(self.sigma),
                                                     shape)
        triple = lambda name : (
            init_weight(f'W_x{name}', (self.num_inputs, self.num_hiddens)),
            init_weight(f'W_h{name}', (self.num_hiddens, self.num_hiddens)),
            self.param(f'b_{name}', nn.initializers.zeros, (self.num_hiddens)))

        self.W_xi, self.W_hi, self.b_i = triple('i')  # 입력 게이트
        self.W_xf, self.W_hf, self.b_f = triple('f')  # 삭제 게이트
        self.W_xo, self.W_ho, self.b_o = triple('o')  # 출력 게이트
        self.W_xc, self.W_hc, self.b_c = triple('c')  # 입력 노드
</code></pre>
<p>:begin_tab:<code>pytorch, mxnet, tensorflow</code>
[<strong>실제 모델</strong>]은 위에서 설명한 대로 정의되며 세 개의 게이트와 입력 노드로 구성됩니다.
은닉 상태만 출력 레이어로 전달된다는 점에 유의하십시오.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
[<strong>실제 모델</strong>]은 위에서 설명한 대로 정의되며 세 개의 게이트와 입력 노드로 구성됩니다.
은닉 상태만 출력 레이어로 전달된다는 점에 유의하십시오.
<code>forward</code> 메서드에서 긴 for-루프를 사용하면 첫 번째 실행 시 JIT 컴파일 시간이 매우 길어집니다. 이에 대한 해결책으로 매 타임 스텝마다 상태를 업데이트하기 위해 for-루프를 사용하는 대신, JAX에는 동일한 동작을 달성하기 위한 <code>jax.lax.scan</code> 유틸리티 변환이 있습니다.
이는 <code>carry</code>라고 불리는 초기 상태와 선행 축에서 스캔되는 <code>inputs</code> 배열을 인수로 취합니다. <code>scan</code> 변환은 최종적으로 기대하는 대로 최종 상태와 쌓인 출력들을 반환합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(LSTMScratch)
def forward(self, inputs, H_C=None):
    if H_C is None:
        # 모양이 (batch_size, num_hiddens)인 초기 상태
        if tab.selected('mxnet'):
            H = d2l.zeros((inputs.shape[1], self.num_hiddens),
                          ctx=inputs.ctx)
            C = d2l.zeros((inputs.shape[1], self.num_hiddens),
                          ctx=inputs.ctx)
        if tab.selected('pytorch'):
            H = d2l.zeros((inputs.shape[1], self.num_hiddens),
                          device=inputs.device)
            C = d2l.zeros((inputs.shape[1], self.num_hiddens),
                          device=inputs.device)
        if tab.selected('tensorflow'):
            H = d2l.zeros((inputs.shape[1], self.num_hiddens))
            C = d2l.zeros((inputs.shape[1], self.num_hiddens))
    else:
        H, C = H_C
    outputs = []
    for X in inputs:
        I = d2l.sigmoid(d2l.matmul(X, self.W_xi) +
                        d2l.matmul(H, self.W_hi) + self.b_i)
        F = d2l.sigmoid(d2l.matmul(X, self.W_xf) +
                        d2l.matmul(H, self.W_hf) + self.b_f)
        O = d2l.sigmoid(d2l.matmul(X, self.W_xo) +
                        d2l.matmul(H, self.W_ho) + self.b_o)
        C_tilde = d2l.tanh(d2l.matmul(X, self.W_xc) +
                           d2l.matmul(H, self.W_hc) + self.b_c)
        C = F * C + I * C_tilde
        H = O * d2l.tanh(C)
        outputs.append(H)
    return outputs, (H, C)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(LSTMScratch)
def forward(self, inputs, H_C=None):
    # 입력을 반복하는 대신 lax.scan 프리미티브를 사용하여 jit 컴파일 시간을 절약합니다.
    def scan_fn(carry, X):
        H, C = carry
        I = d2l.sigmoid(d2l.matmul(X, self.W_xi) + (
            d2l.matmul(H, self.W_hi)) + self.b_i)
        F = d2l.sigmoid(d2l.matmul(X, self.W_xf) +
                        d2l.matmul(H, self.W_hf) + self.b_f)
        O = d2l.sigmoid(d2l.matmul(X, self.W_xo) +
                        d2l.matmul(H, self.W_ho) + self.b_o)
        C_tilde = d2l.tanh(d2l.matmul(X, self.W_xc) +
                           d2l.matmul(H, self.W_hc) + self.b_c)
        C = F * C + I * C_tilde
        H = O * d2l.tanh(C)
        return (H, C), H  # carry, y 반환

    if H_C is None:
        batch_size = inputs.shape[1]
        carry = jnp.zeros((batch_size, self.num_hiddens)), \
                jnp.zeros((batch_size, self.num_hiddens))
    else:
        carry = H_C

    # scan은 scan_fn, 초기 carry 상태, 스캔할 선행 축이 있는 xs를 인수로 취합니다
    carry, outputs = jax.lax.scan(scan_fn, carry, inputs)
    return outputs, carry
</code></pre>
<h3 id="훈련-training-및-예측"><a class="header" href="#훈련-training-및-예측">[<strong>훈련 (Training)</strong>] 및 예측</a></h3>
<p>:numref:<code>sec_rnn-scratch</code>의 <code>RNNLMScratch</code> 클래스를 인스턴스화하여 LSTM 모델을 훈련해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.TimeMachine(batch_size=1024, num_steps=32)
if tab.selected('mxnet', 'pytorch', 'jax'):
    lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)
    model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)
        model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1)
trainer.fit(model, data)
</code></pre>
<h2 id="간결한-구현-concise-implementation-1"><a class="header" href="#간결한-구현-concise-implementation-1">[<strong>간결한 구현 (Concise Implementation)</strong>]</a></h2>
<p>고수준 API를 사용하여 LSTM 모델을 직접 인스턴스화할 수 있습니다.
이는 위에서 우리가 명시적으로 만든 모든 구성 세부 사항을 캡슐화합니다.
앞서 우리가 상세히 설명한 많은 부분에 대해 Python 대신 컴파일된 연산자를 사용하므로 코드가 훨씬 빠릅니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class LSTM(d2l.RNN):
    def __init__(self, num_hiddens):
        d2l.Module.__init__(self)
        self.save_hyperparameters()
        self.rnn = rnn.LSTM(num_hiddens)

    def forward(self, inputs, H_C=None):
        if H_C is None: H_C = self.rnn.begin_state(
            inputs.shape[1], ctx=inputs.ctx)
        return self.rnn(inputs, H_C)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class LSTM(d2l.RNN):
    def __init__(self, num_inputs, num_hiddens):
        d2l.Module.__init__(self)
        self.save_hyperparameters()
        self.rnn = nn.LSTM(num_inputs, num_hiddens)

    def forward(self, inputs, H_C=None):
        return self.rnn(inputs, H_C)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class LSTM(d2l.RNN):
    def __init__(self, num_hiddens):
        d2l.Module.__init__(self)
        self.save_hyperparameters()
        self.rnn = tf.keras.layers.LSTM(
                num_hiddens, return_sequences=True,
                return_state=True, time_major=True)

    def forward(self, inputs, H_C=None):
        outputs, *H_C = self.rnn(inputs, H_C)
        return outputs, H_C
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class LSTM(d2l.RNN):
    num_hiddens: int

    @nn.compact
    def __call__(self, inputs, H_C=None, training=False):
        if H_C is None:
            batch_size = inputs.shape[1]
            H_C = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),
                                                        (batch_size,),
                                                        self.num_hiddens)

        LSTM = nn.scan(nn.OptimizedLSTMCell, variable_broadcast="params",
                       in_axes=0, out_axes=0, split_rngs={"params": False})

        H_C, outputs = LSTM()(H_C, inputs)
        return outputs, H_C
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
if tab.selected('pytorch'):
    lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)
if tab.selected('mxnet', 'tensorflow', 'jax'):
    lstm = LSTM(num_hiddens=32)
if tab.selected('mxnet', 'pytorch', 'jax'):
    model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=4)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=4)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
model.predict('it has', 20, data.vocab, d2l.try_gpu())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
model.predict('it has', 20, data.vocab)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
model.predict('it has', 20, data.vocab, trainer.state.params)
</code></pre>
<p>LSTM은 의미 있는 상태 제어를 가진 전형적인 잠재 변수 자기회귀 모델입니다.
다중 레이어, 잔차 연결, 다양한 유형의 정규화 등 수년 동안 많은 변형이 제안되었습니다. 그러나 시퀀스의 장거리 의존성 때문에 LSTM 및 기타 시퀀스 모델(예: GRU)을 훈련하는 것은 비용이 많이 듭니다.
나중에 우리는 일부 사례에서 사용될 수 있는 Transformer와 같은 대안 모델을 만나게 될 것입니다.</p>
<h2 id="요약-summary-36"><a class="header" href="#요약-summary-36">요약 (Summary)</a></h2>
<p>LSTM은 1997년에 발표되었지만, 2000년대 중반 예측 대회에서 여러 차례 승리하면서 큰 주목을 받았고,
2011년부터 2017년 Transformer 모델이 부상하기 전까지 시퀀스 학습을 위한 지배적인 모델이 되었습니다.
심지어 Transformer조차도 LSTM이 도입한 아키텍처 설계 혁신에서 몇 가지 핵심 아이디어를 얻었습니다.</p>
<p>LSTM에는 정보의 흐름을 제어하는 세 가지 유형의 게이트가 있습니다:
입력 게이트, 삭제 게이트, 출력 게이트입니다.
LSTM의 은닉층 출력에는 은닉 상태와 메모리 셀 내부 상태가 포함됩니다.
은닉 상태만 출력 레이어로 전달되는 반면 메모리 셀 내부 상태는 완전히 내부적으로 유지됩니다.
LSTM은 사라지는 기울기와 폭발하는 기울기를 완화할 수 있습니다.</p>
<h2 id="연습-문제-exercises-49"><a class="header" href="#연습-문제-exercises-49">연습 문제 (Exercises)</a></h2>
<ol>
<li>하이퍼파라미터를 조정하고 실행 시간, 퍼플렉서티 및 출력 시퀀스에 미치는 영향을 분석하십시오.</li>
<li>단순히 문자 시퀀스가 아니라 적절한 단어를 생성하도록 모델을 변경하려면 어떻게 해야 합니까?</li>
<li>주어진 은닉 차원에 대해 GRU, LSTM 및 일반 RNN의 계산 비용을 비교하십시오. 훈련 및 추론 비용에 특히 주의를 기울이십시오.</li>
<li>후보 메모리 셀이 $\tanh$ 함수를 사용하여 값 범위를 $-1$과 $1$ 사이로 보장하는데, 왜 은닉 상태가 출력 값 범위를 $-1$과 $1$ 사이로 보장하기 위해 다시 $\tanh$ 함수를 사용해야 합니까?</li>
<li>문자 시퀀스 예측이 아닌 시계열 예측을 위한 LSTM 모델을 구현하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/343">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1057">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3861">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18016">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=5}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="게이트-순환-유닛-gru-gated-recurrent-units-gru"><a class="header" href="#게이트-순환-유닛-gru-gated-recurrent-units-gru">게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></h1>
<p>:label:<code>sec_gru</code></p>
<p>RNN, 특히 LSTM 아키텍처(:numref:<code>sec_lstm</code>)가 2010년대에 빠르게 인기를 얻으면서, 많은 연구자들은 내부 상태와 곱셈 게이팅 메커니즘을 통합한다는 핵심 아이디어를 유지하면서도 계산 속도를 높이는 것을 목표로 단순화된 아키텍처를 실험하기 시작했습니다. 게이트 순환 유닛(GRU) :cite:<code>Cho.Van-Merrienboer.Bahdanau.ea.2014</code>은 LSTM 메모리 셀의 간소화된 버전을 제공하며, 종종 비슷한 성능을 달성하면서도 계산 속도가 더 빠르다는 장점이 있습니다 :cite:<code>Chung.Gulcehre.Cho.ea.2014</code>.</p>
<pre><code class="language-{.python .input  n=6}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import rnn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input  n=8}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="리셋-게이트와-업데이트-게이트-reset-gate-and-update-gate"><a class="header" href="#리셋-게이트와-업데이트-게이트-reset-gate-and-update-gate">리셋 게이트와 업데이트 게이트 (Reset Gate and Update Gate)</a></h2>
<p>여기서 LSTM의 세 가지 게이트는 *리셋 게이트(reset gate)*와 *업데이트 게이트(update gate)*라는 두 가지 게이트로 대체됩니다. LSTM과 마찬가지로 이러한 게이트에는 시그모이드 활성화 함수가 주어져 값이 구간 $(0, 1)$에 있게 됩니다. 직관적으로 리셋 게이트는 이전 상태를 얼마나 기억하고 싶은지를 제어합니다. 마찬가지로 업데이트 게이트는 새 상태가 이전 상태의 복사본인 정도를 제어할 수 있게 해 줍니다. :numref:<code>fig_gru_1</code>은 현재 타임 스텝의 입력과 이전 타임 스텝의 은닉 상태가 주어졌을 때 GRU의 리셋 게이트와 업데이트 게이트에 대한 입력을 보여줍니다. 게이트의 출력은 시그모이드 활성화 함수가 있는 두 개의 완전 연결 레이어에 의해 제공됩니다.</p>
<p><img src="chapter_recurrent-modern/../img/gru-1.svg" alt="GRU 모델에서 리셋 게이트와 업데이트 게이트 계산하기." />
:label:<code>fig_gru_1</code></p>
<p>수학적으로, 주어진 타임 스텝 $t$에 대해 입력이 미니배치 $\mathbf{X}<em>t \in \mathbb{R}^{n \times d}$ (예제 수 $=n$; 입력 수 $=d$)이고 이전 타임 스텝의 은닉 상태가 $\mathbf{H}</em>{t-1} \in \mathbb{R}^{n \times h}$ (은닉 유닛 수 $=h$)라고 가정합시다. 그러면 리셋 게이트 $\mathbf{R}_t \in \mathbb{R}^{n \times h}$와 업데이트 게이트 $\mathbf{Z}_t \in \mathbb{R}^{n \times h}$는 다음과 같이 계산됩니다:</p>
<p>$$
\begin{aligned}
\mathbf{R}<em>t = \sigma(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xr}} + \mathbf{H}</em>{t-1} \mathbf{W}<em>{\textrm{hr}} + \mathbf{b}</em>\textrm{r}),<br />
\mathbf{Z}<em>t = \sigma(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xz}} + \mathbf{H}</em>{t-1} \mathbf{W}<em>{\textrm{hz}} + \mathbf{b}</em>\textrm{z}),
\end{aligned}
$$</p>
<p>여기서 $\mathbf{W}<em>{\textrm{xr}}, \mathbf{W}</em>{\textrm{xz}} \in \mathbb{R}^{d \times h}$와 $\mathbf{W}<em>{\textrm{hr}}, \mathbf{W}</em>{\textrm{hz}} \in \mathbb{R}^{h \times h}$는 가중치 파라미터이고 $\mathbf{b}<em>\textrm{r}, \mathbf{b}</em>\textrm{z} \in \mathbb{R}^{1 \times h}$는 편향 파라미터입니다.</p>
<h2 id="후보-은닉-상태-candidate-hidden-state"><a class="header" href="#후보-은닉-상태-candidate-hidden-state">후보 은닉 상태 (Candidate Hidden State)</a></h2>
<p>다음으로, 리셋 게이트 $\mathbf{R}_t$를 :eqref:<code>rnn_h_with_state</code>의 일반적인 업데이트 메커니즘과 통합하여, 타임 스텝 $t$에서 다음과 같은 <em>후보 은닉 상태(candidate hidden state)</em> $\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$를 얻습니다:</p>
<p>$$\tilde{\mathbf{H}}<em>t = \tanh(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xh}} + \left(\mathbf{R}<em>t \odot \mathbf{H}</em>{t-1}\right) \mathbf{W}</em>{\textrm{hh}} + \mathbf{b}_\textrm{h}),$$
:eqlabel:<code>gru_tilde_H</code></p>
<p>여기서 $\mathbf{W}<em>{\textrm{xh}} \in \mathbb{R}^{d \times h}$와 $\mathbf{W}</em>{\textrm{hh}} \in \mathbb{R}^{h \times h}$는 가중치 파라미터이고, $\mathbf{b}_\textrm{h} \in \mathbb{R}^{1 \times h}$는 편향이며, 기호 $\odot$는 하다마드(요소별) 곱 연산자입니다. 여기서는 tanh 활성화 함수를 사용합니다.</p>
<p>아직 업데이트 게이트의 동작을 통합해야 하므로 이 결과는 <em>후보</em>입니다. :eqref:<code>rnn_h_with_state</code>와 비교할 때, 이전 상태의 영향은 이제 :eqref:<code>gru_tilde_H</code>에서 $\mathbf{R}<em>t$와 $\mathbf{H}</em>{t-1}$의 요소별 곱셈을 통해 줄어들 수 있습니다. 리셋 게이트 $\mathbf{R}_t$의 항목들이 1에 가까울 때마다 :eqref:<code>rnn_h_with_state</code>와 같은 바닐라 RNN을 복구합니다. 리셋 게이트 $\mathbf{R}_t$의 모든 항목이 0에 가까우면 후보 은닉 상태는 $\mathbf{X}_t$를 입력으로 하는 MLP의 결과입니다. 따라서 기존의 은닉 상태는 기본값으로 <em>리셋</em>됩니다.</p>
<p>:numref:<code>fig_gru_2</code>는 리셋 게이트를 적용한 후의 계산 흐름을 보여줍니다.</p>
<p><img src="chapter_recurrent-modern/../img/gru-2.svg" alt="GRU 모델에서 후보 은닉 상태 계산하기." />
:label:<code>fig_gru_2</code></p>
<h2 id="은닉-상태-hidden-state-1"><a class="header" href="#은닉-상태-hidden-state-1">은닉 상태 (Hidden State)</a></h2>
<p>마지막으로, 업데이트 게이트 $\mathbf{Z}_t$의 효과를 통합해야 합니다. 이는 새로운 은닉 상태 $\mathbf{H}<em>t \in \mathbb{R}^{n \times h}$가 이전 상태 $\mathbf{H}</em>{t-1}$과 얼마나 일치하는지, 그리고 새로운 후보 상태 $\tilde{\mathbf{H}}_t$와 얼마나 유사한지를 결정합니다. 업데이트 게이트 $\mathbf{Z}<em>t$는 단순히 $\mathbf{H}</em>{t-1}$과 $\tilde{\mathbf{H}}_t$의 요소별 볼록 조합(convex combination)을 취함으로써 이 목적으로 사용될 수 있습니다. 이는 GRU의 최종 업데이트 방정식으로 이어집니다:</p>
<p>$$\mathbf{H}_t = \mathbf{Z}<em>t \odot \mathbf{H}</em>{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.$$</p>
<p>업데이트 게이트 $\mathbf{Z}_t$가 1에 가까울 때마다 우리는 단순히 이전 상태를 유지합니다. 이 경우 $\mathbf{X}_t$의 정보는 무시되어 종속성 체인에서 타임 스텝 $t$를 효과적으로 건너뜁니다. 반대로 $\mathbf{Z}_t$가 0에 가까울 때마다 새로운 잠재 상태 $\mathbf{H}_t$는 후보 잠재 상태 $\tilde{\mathbf{H}}_t$에 접근합니다. :numref:<code>fig_gru_3</code>은 업데이트 게이트가 작동한 후의 계산 흐름을 보여줍니다.</p>
<p><img src="chapter_recurrent-modern/../img/gru-3.svg" alt="GRU 모델에서 은닉 상태 계산하기." />
:label:<code>fig_gru_3</code></p>
<p>요약하자면, GRU는 다음과 같은 두 가지 독특한 특징을 가지고 있습니다:</p>
<ul>
<li>리셋 게이트는 시퀀스에서 단기 종속성을 캡처하는 데 도움이 됩니다.</li>
<li>업데이트 게이트는 시퀀스에서 장기 종속성을 캡처하는 데 도움이 됩니다.</li>
</ul>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-5"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-5">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>GRU 모델을 더 잘 이해하기 위해 밑바닥부터 구현해 봅시다.</p>
<h3 id="모델-파라미터-초기화-initializing-model-parameters-1"><a class="header" href="#모델-파라미터-초기화-initializing-model-parameters-1">모델 파라미터 초기화 (Initializing Model Parameters)</a></h3>
<p>첫 번째 단계는 모델 파라미터를 초기화하는 것입니다. 가중치는 표준 편차가 <code>sigma</code>인 가우스 분포에서 추출하고 편향은 0으로 설정합니다. 하이퍼파라미터 <code>num_hiddens</code>는 은닉 유닛의 수를 정의합니다. 업데이트 게이트, 리셋 게이트 및 후보 은닉 상태와 관련된 모든 가중치와 편향을 인스턴스화합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class GRUScratch(d2l.Module):
    def __init__(self, num_inputs, num_hiddens, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        
        if tab.selected('mxnet'):
            init_weight = lambda *shape: d2l.randn(*shape) * sigma
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              d2l.zeros(num_hiddens))            
        if tab.selected('pytorch'):
            init_weight = lambda *shape: nn.Parameter(d2l.randn(*shape) * sigma)
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              nn.Parameter(d2l.zeros(num_hiddens)))
        if tab.selected('tensorflow'):
            init_weight = lambda *shape: tf.Variable(d2l.normal(shape) * sigma)
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              tf.Variable(d2l.zeros(num_hiddens)))            
            
        self.W_xz, self.W_hz, self.b_z = triple()  # 업데이트 게이트
        self.W_xr, self.W_hr, self.b_r = triple()  # 리셋 게이트
        self.W_xh, self.W_hh, self.b_h = triple()  # 후보 은닉 상태        
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class GRUScratch(d2l.Module):
    num_inputs: int
    num_hiddens: int
    sigma: float = 0.01

    def setup(self):
        init_weight = lambda name, shape: self.param(name,
                                                     nn.initializers.normal(self.sigma),
                                                     shape)
        triple = lambda name : (
            init_weight(f'W_x{name}', (self.num_inputs, self.num_hiddens)),
            init_weight(f'W_h{name}', (self.num_hiddens, self.num_hiddens)),
            self.param(f'b_{name}', nn.initializers.zeros, (self.num_hiddens)))

        self.W_xz, self.W_hz, self.b_z = triple('z')  # 업데이트 게이트
        self.W_xr, self.W_hr, self.b_r = triple('r')  # 리셋 게이트
        self.W_xh, self.W_hh, self.b_h = triple('h')  # 후보 은닉 상태
</code></pre>
<h3 id="모델-정의하기-defining-the-model-4"><a class="header" href="#모델-정의하기-defining-the-model-4">모델 정의하기 (Defining the Model)</a></h3>
<p>이제 GRU 순방향 계산을 정의할 준비가 되었습니다. 그 구조는 업데이트 방정식이 더 복잡하다는 점을 제외하면 기본 RNN 셀과 동일합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(GRUScratch)
def forward(self, inputs, H=None):
    if H is None:
        # 모양 (batch_size, num_hiddens)인 초기 상태
        if tab.selected('mxnet'):
            H = d2l.zeros((inputs.shape[1], self.num_hiddens),
                          ctx=inputs.ctx)
        if tab.selected('pytorch'):
            H = d2l.zeros((inputs.shape[1], self.num_hiddens),
                          device=inputs.device)
        if tab.selected('tensorflow'):
            H = d2l.zeros((inputs.shape[1], self.num_hiddens))
    outputs = []
    for X in inputs:
        Z = d2l.sigmoid(d2l.matmul(X, self.W_xz) +
                        d2l.matmul(H, self.W_hz) + self.b_z)
        R = d2l.sigmoid(d2l.matmul(X, self.W_xr) +
                        d2l.matmul(H, self.W_hr) + self.b_r)
        H_tilde = d2l.tanh(d2l.matmul(X, self.W_xh) +
                           d2l.matmul(R * H, self.W_hh) + self.b_h)
        H = Z * H + (1 - Z) * H_tilde
        outputs.append(H)
    return outputs, H
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(GRUScratch)
def forward(self, inputs, H=None):
    # 입력에 대해 루프를 도는 대신 lax.scan 프리미티브를 사용합니다. 
    # scan은 jit 컴파일 시간을 절약해 주기 때문입니다
    def scan_fn(H, X):
        Z = d2l.sigmoid(d2l.matmul(X, self.W_xz) + d2l.matmul(H, self.W_hz) +
                        self.b_z)
        R = d2l.sigmoid(d2l.matmul(X, self.W_xr) +
                        d2l.matmul(H, self.W_hr) + self.b_r)
        H_tilde = d2l.tanh(d2l.matmul(X, self.W_xh) +
                           d2l.matmul(R * H, self.W_hh) + self.b_h)
        H = Z * H + (1 - Z) * H_tilde
        return H, H  # carry, y 반환

    if H is None:
        batch_size = inputs.shape[1]
        carry = jnp.zeros((batch_size, self.num_hiddens))
    else:
        carry = H

    # scan은 scan_fn, 초기 carry 상태, 스캔할 선행 축이 있는 xs를 인수로 받습니다
    carry, outputs = jax.lax.scan(scan_fn, carry, inputs)
    return outputs, carry
</code></pre>
<h3 id="훈련-training-18"><a class="header" href="#훈련-training-18">훈련 (Training)</a></h3>
<p><em>타임 머신</em> 데이터셋에서 언어 모델을 훈련하는 것은 :numref:<code>sec_rnn-scratch</code>에서와 정확히 동일한 방식으로 작동합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.TimeMachine(batch_size=1024, num_steps=32)
if tab.selected('mxnet', 'pytorch', 'jax'):
    gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)
    model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)
        model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1)
trainer.fit(model, data)
</code></pre>
<h2 id="간결한-구현-concise-implementation-2"><a class="header" href="#간결한-구현-concise-implementation-2">간결한 구현 (Concise Implementation)</a></h2>
<p>고수준 API에서는 GRU 모델을 직접 인스턴스화할 수 있습니다. 이는 위에서 우리가 명시적으로 만든 모든 구성 세부 사항을 캡슐화합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class GRU(d2l.RNN):
    def __init__(self, num_inputs, num_hiddens):
        d2l.Module.__init__(self)
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.rnn = rnn.GRU(num_hiddens)
        if tab.selected('pytorch'):
            self.rnn = nn.GRU(num_inputs, num_hiddens)
        if tab.selected('tensorflow'):
            self.rnn = tf.keras.layers.GRU(num_hiddens, return_sequences=True, 
                                           return_state=True)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class GRU(d2l.RNN):
    num_hiddens: int

    @nn.compact
    def __call__(self, inputs, H=None, training=False):
        if H is None:
            batch_size = inputs.shape[1]
            H = nn.GRUCell.initialize_carry(jax.random.PRNGKey(0),
                                            (batch_size,), self.num_hiddens)

        GRU = nn.scan(nn.GRUCell, variable_broadcast="params",
                      in_axes=0, out_axes=0, split_rngs={"params": False})

        H, outputs = GRU()(H, inputs)
        return outputs, H
</code></pre>
<p>코드는 Python 대신 컴파일된 연산자를 사용하므로 훈련 속도가 훨씬 빠릅니다.</p>
<pre><code class="language-{.python .input}">%%tab all
if tab.selected('mxnet', 'pytorch', 'tensorflow'):
    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)
if tab.selected('jax'):
    gru = GRU(num_hiddens=32)
if tab.selected('mxnet', 'pytorch', 'jax'):
    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)
trainer.fit(model, data)
</code></pre>
<p>훈련 후, 훈련 세트에서의 퍼플렉서티와 제공된 접두사를 따르는 예측 시퀀스를 인쇄합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
model.predict('it has', 20, data.vocab, d2l.try_gpu())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
model.predict('it has', 20, data.vocab)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
model.predict('it has', 20, data.vocab, trainer.state.params)
</code></pre>
<h2 id="요약-summary-37"><a class="header" href="#요약-summary-37">요약 (Summary)</a></h2>
<p>LSTM과 비교하여 GRU는 비슷한 성능을 달성하지만 계산 부하가 더 적은 경향이 있습니다. 일반적으로 단순한 RNN에 비해 LSTM 및 GRU와 같은 게이트 RNN은 타임 스텝 거리가 먼 시퀀스에 대한 종속성을 더 잘 캡처할 수 있습니다. GRU는 리셋 게이트가 켜져 있을 때마다 기본 RNN을 극단적인 경우로 포함합니다. 또한 업데이트 게이트를 켜서 하위 시퀀스를 건너뛸 수도 있습니다.</p>
<h2 id="연습-문제-exercises-50"><a class="header" href="#연습-문제-exercises-50">연습 문제 (Exercises)</a></h2>
<ol>
<li>타임 스텝 $t'$의 입력만 사용하여 타임 스텝 $t &gt; t'$의 출력을 예측하고 싶다고 가정해 봅시다. 각 타임 스텝에 대한 리셋 게이트와 업데이트 게이트의 최적 값은 무엇입니까?</li>
<li>하이퍼파라미터를 조정하고 실행 시간, 퍼플렉서티 및 출력 시퀀스에 미치는 영향을 분석하십시오.</li>
<li><code>rnn.RNN</code>과 <code>rnn.GRU</code> 구현의 런타임, 퍼플렉서티 및 출력 문자열을 서로 비교하십시오.</li>
<li>리셋 게이트만 있거나 업데이트 게이트만 있는 것과 같이 GRU의 일부만 구현하면 어떻게 됩니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/342">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1056">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3860">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18017">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="심층-순환-신경망-deep-recurrent-neural-networks"><a class="header" href="#심층-순환-신경망-deep-recurrent-neural-networks">심층 순환 신경망 (Deep Recurrent Neural Networks)</a></h1>
<p>:label:<code>sec_deep_rnn</code></p>
<p>지금까지 우리는 시퀀스 입력, 단일 은닉 RNN 레이어 및 출력 레이어로 구성된 네트워크를 정의하는 데 집중했습니다.
임의의 타임 스텝의 입력과 해당 출력 사이에 은닉층이 하나만 있음에도 불구하고, 이러한 네트워크가 깊다는 의미가 있습니다.
첫 번째 타임 스텝의 입력은 최종 타임 스텝 $T$(종종 100단계 또는 1000단계 이후)의 출력에 영향을 줄 수 있습니다.
이러한 입력은 최종 출력에 도달하기 전에 순환 레이어의 $T$번 적용을 통과합니다.
그러나 우리는 종종 주어진 타임 스텝의 입력과 동일한 타임 스텝의 출력 사이의 복잡한 관계를 표현하는 능력도 유지하고 싶어 합니다.
따라서 우리는 종종 시간 방향뿐만 아니라 입력에서 출력 방향으로도 깊은 RNN을 구성합니다.
이것은 우리가 이미 MLP와 심층 CNN 개발에서 접했던 깊이의 개념과 정확히 일치합니다.</p>
<p>이러한 종류의 심층 RNN을 구축하는 표준 방법은 매우 간단합니다: RNN을 서로 위에 쌓는 것입니다.
길이 $T$의 시퀀스가 주어지면 첫 번째 RNN은 마찬가지로 길이 $T$의 출력 시퀀스를 생성합니다.
이것들은 차례로 다음 RNN 레이어의 입력을 구성합니다.
이 짧은 섹션에서는 이 설계 패턴을 설명하고 그러한 쌓인(stacked) RNN을 코딩하는 방법의 간단한 예를 제시합니다.
아래 :numref:<code>fig_deep_rnn</code>에서는 $L$개의 은닉층이 있는 심층 RNN을 보여줍니다.
각 은닉 상태는 순차적 입력에 대해 작동하고 순차적 출력을 생성합니다.
더욱이 각 타임 스텝에서 임의의 RNN 셀(:numref:<code>fig_deep_rnn</code>의 흰색 상자)은 이전 타임 스텝에서의 동일한 레이어 값과 동일한 타임 스텝에서의 이전 레이어 값 모두에 의존합니다.</p>
<p><img src="chapter_recurrent-modern/../img/deep-rnn.svg" alt="심층 RNN의 아키텍처." />
:label:<code>fig_deep_rnn</code></p>
<p>공식적으로, 타임 스텝 $t$에서 미니배치 입력 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$(예제 수 $=n$; 각 예제의 입력 수 $=d$)가 있다고 가정합니다.
동일한 타임 스텝에서,
$l^\textrm{th}$ 은닉층($l=1,\ldots,L$)의 은닉 상태를 $\mathbf{H}_t^{(l)} \in \mathbb{R}^{n \times h}$(은닉 유닛 수 $=h$)라고 하고 출력 레이어 변수를 $\mathbf{O}_t \in \mathbb{R}^{n \times q}$(출력 수: $q$)라고 합시다.
$\mathbf{H}_t^{(0)} = \mathbf{X}_t$로 설정하면,
활성화 함수 $\phi_l$을 사용하는 $l^\textrm{th}$ 은닉층의 은닉 상태는 다음과 같이 계산됩니다:</p>
<p>$$\mathbf{H}<em>t^{(l)} = \phi_l(\mathbf{H}<em>t^{(l-1)} \mathbf{W}</em>{\textrm{xh}}^{(l)} + \mathbf{H}</em>{t-1}^{(l)} \mathbf{W}<em>{\textrm{hh}}^{(l)}  + \mathbf{b}</em>\textrm{h}^{(l)}),$$
:eqlabel:<code>eq_deep_rnn_H</code></p>
<p>여기서 가중치 $\mathbf{W}<em>{\textrm{xh}}^{(l)} \in \mathbb{R}^{h \times h}$ 및 $\mathbf{W}</em>{\textrm{hh}}^{(l)} \in \mathbb{R}^{h \times h}$와
편향 $\mathbf{b}_\textrm{h}^{(l)} \in \mathbb{R}^{1 \times h}$는 $l^\textrm{th}$ 은닉층의 모델 파라미터입니다.</p>
<p>마지막으로 출력 레이어의 계산은 최종 $L^\textrm{th}$ 은닉층의 은닉 상태에만 기반합니다:</p>
<p>$$\mathbf{O}<em>t = \mathbf{H}<em>t^{(L)} \mathbf{W}</em>{\textrm{hq}} + \mathbf{b}</em>\textrm{q},$$</p>
<p>여기서 가중치 $\mathbf{W}<em>{\textrm{hq}} \in \mathbb{R}^{h \times q}$와 편향 $\mathbf{b}</em>\textrm{q} \in \mathbb{R}^{1 \times q}$는 출력 레이어의 모델 파라미터입니다.</p>
<p>MLP와 마찬가지로 은닉층 수 $L$과 은닉 유닛 수 $h$는 우리가 조정할 수 있는 하이퍼파라미터입니다.
일반적인 RNN 레이어 너비($h$)는 $(64, 2056)$ 범위에 있고 일반적인 깊이($L$)는 $(1, 8)$ 범위에 있습니다.
또한 :eqref:<code>eq_deep_rnn_H</code>의 은닉 상태 계산을 LSTM 또는 GRU의 것으로 대체하여 깊은 게이트(deep-gated) RNN을 쉽게 얻을 수 있습니다.</p>
<pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import rnn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-6"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-6">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>다층 RNN을 밑바닥부터 구현하기 위해, 각 레이어를 자체 학습 가능한 파라미터를 가진 <code>RNNScratch</code> 인스턴스로 취급할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow
class StackedRNNScratch(d2l.Module):
    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.rnns = [d2l.RNNScratch(num_inputs if i==0 else num_hiddens,
                                    num_hiddens, sigma)
                     for i in range(num_layers)]
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class StackedRNNScratch(d2l.Module):
    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.rnns = nn.Sequential(*[d2l.RNNScratch(
            num_inputs if i==0 else num_hiddens, num_hiddens, sigma)
                                    for i in range(num_layers)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class StackedRNNScratch(d2l.Module):
    num_inputs: int
    num_hiddens: int
    num_layers: int
    sigma: float = 0.01

    def setup(self):
        self.rnns = [d2l.RNNScratch(self.num_inputs if i==0 else self.num_hiddens,
                                    self.num_hiddens, self.sigma)
                     for i in range(self.num_layers)]
</code></pre>
<p>다층 순전파 계산은 단순히 레이어별로 순전파 계산을 수행합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(StackedRNNScratch)
def forward(self, inputs, Hs=None):
    outputs = inputs
    if Hs is None: Hs = [None] * self.num_layers
    for i in range(self.num_layers):
        outputs, Hs[i] = self.rnns[i](outputs, Hs[i])
        outputs = d2l.stack(outputs, 0)
    return outputs, Hs
</code></pre>
<p>예를 들어,
<em>타임 머신</em> 데이터셋에서 심층 GRU 모델을 훈련합니다(:numref:<code>sec_rnn-scratch</code>에서와 동일).
일을 단순하게 유지하기 위해 레이어 수를 2로 설정합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.TimeMachine(batch_size=1024, num_steps=32)
if tab.selected('mxnet', 'pytorch', 'jax'):
    rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),
                                  num_hiddens=32, num_layers=2)
    model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)
    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),
                                  num_hiddens=32, num_layers=2)
        model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)
    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)
trainer.fit(model, data)
</code></pre>
<h2 id="간결한-구현-concise-implementation-3"><a class="header" href="#간결한-구현-concise-implementation-3">간결한 구현 (Concise Implementation)</a></h2>
<p>:begin_tab:<code>pytorch, mxnet, tensorflow</code>
다행히 RNN의 여러 레이어를 구현하는 데 필요한 많은 물류 세부 사항은 고수준 API에서 쉽게 사용할 수 있습니다.
우리의 간결한 구현은 그러한 내장 기능을 사용할 것입니다.
코드는 이전에 :numref:<code>sec_gru</code>에서 사용한 코드를 일반화하여, 단 한 개의 레이어만 선택하는 기본값 대신 레이어 수를 명시적으로 지정할 수 있게 해 줍니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
Flax는 RNN을 구현할 때 미니멀리즘적인 접근 방식을 취합니다. RNN에서 레이어 수를 정의하거나 드롭아웃과 결합하는 기능은 기본적으로 제공되지 않습니다.
우리의 간결한 구현은 모든 내장 기능을 사용하고 그 위에 <code>num_layers</code> 및 <code>dropout</code> 기능을 추가할 것입니다.
코드는 이전에 :numref:<code>sec_gru</code>에서 사용한 코드를 일반화하여, 단일 레이어의 기본값 대신 레이어 수를 명시적으로 지정할 수 있게 해 줍니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class GRU(d2l.RNN):  #@save
    """다층 GRU 모델."""
    def __init__(self, num_hiddens, num_layers, dropout=0):
        d2l.Module.__init__(self)
        self.save_hyperparameters()
        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class GRU(d2l.RNN):  #@save
    """다층 GRU 모델."""
    def __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):
        d2l.Module.__init__(self)
        self.save_hyperparameters()
        self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,
                          dropout=dropout)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class GRU(d2l.RNN):  #@save
    """다층 GRU 모델."""
    def __init__(self, num_hiddens, num_layers, dropout=0):
        d2l.Module.__init__(self)
        self.save_hyperparameters()
        gru_cells = [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)
                     for _ in range(num_layers)]
        self.rnn = tf.keras.layers.RNN(gru_cells, return_sequences=True,
                                       return_state=True, time_major=True)

    def forward(self, X, state=None):
        outputs, *state = self.rnn(X, state)
        return outputs, state
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class GRU(d2l.RNN):  #@save
    """다층 GRU 모델."""
    num_hiddens: int
    num_layers: int
    dropout: float = 0

    @nn.compact
    def __call__(self, X, state=None, training=False):
        outputs = X
        new_state = []
        if state is None:
            batch_size = X.shape[1]
            state = [nn.GRUCell.initialize_carry(jax.random.PRNGKey(0),
                    (batch_size,), self.num_hiddens)] * self.num_layers

        GRU = nn.scan(nn.GRUCell, variable_broadcast="params",
                      in_axes=0, out_axes=0, split_rngs={"params": False})

        # 마지막을 제외한 모든 GRU 레이어 뒤에 드롭아웃 레이어 도입
        for i in range(self.num_layers - 1):
            layer_i_state, X = GRU()(state[i], outputs)
            new_state.append(layer_i_state)
            X = nn.Dropout(self.dropout, deterministic=not training)(X)

        # 드롭아웃이 없는 최종 GRU 레이어
        out_state, X = GRU()(state[-1], X)
        new_state.append(out_state)
        return X, jnp.array(new_state)
</code></pre>
<p>하이퍼파라미터 선택과 같은 아키텍처 결정은 :numref:<code>sec_gru</code>와 매우 유사합니다.
우리는 고유 토큰 수인 <code>vocab_size</code>와 동일한 수의 입력 및 출력을 선택합니다.
은닉 유닛의 수는 여전히 32입니다.
유일한 차이점은 이제 (<strong><code>num_layers</code>의 값을 지정하여 의미 있는 수의 은닉층을 선택한다는 점입니다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab mxnet
gru = GRU(num_hiddens=32, num_layers=2)
model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)

# 실행에 1시간 이상 소요됨 (MXNet의 수정 대기 중)
# trainer.fit(model, data)
# model.predict('it has', 20, data.vocab, d2l.try_gpu())
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch, tensorflow, jax
if tab.selected('tensorflow', 'jax'):
    gru = GRU(num_hiddens=32, num_layers=2)
if tab.selected('pytorch'):
    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)
if tab.selected('pytorch', 'jax'):
    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)
trainer.fit(model, data)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
model.predict('it has', 20, data.vocab, d2l.try_gpu())
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
model.predict('it has', 20, data.vocab)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
model.predict('it has', 20, data.vocab, trainer.state.params)
</code></pre>
<h2 id="요약-summary-38"><a class="header" href="#요약-summary-38">요약 (Summary)</a></h2>
<p>심층 RNN에서 은닉 상태 정보는 현재 레이어의 다음 타임 스텝과 다음 레이어의 현재 타임 스텝으로 전달됩니다.
LSTM, GRU 또는 바닐라 RNN과 같은 다양한 형태의 심층 RNN이 존재합니다.
편리하게도 이러한 모델들은 모두 딥러닝 프레임워크의 고수준 API의 일부로 사용할 수 있습니다.
모델의 초기화에는 주의가 필요합니다.
전반적으로 심층 RNN은 적절한 수렴을 보장하기 위해 상당한 양의 작업(학습률 및 클리핑 등)이 필요합니다.</p>
<h2 id="연습-문제-exercises-51"><a class="header" href="#연습-문제-exercises-51">연습 문제 (Exercises)</a></h2>
<ol>
<li>GRU를 LSTM으로 교체하고 정확도와 훈련 속도를 비교하십시오.</li>
<li>훈련 데이터를 늘려 여러 책을 포함하십시오. 퍼플렉서티 스케일에서 얼마나 낮게 갈 수 있습니까?</li>
<li>텍스트를 모델링할 때 다른 저자의 소스를 결합하고 싶습니까? 왜 이것이 좋은 아이디어일까요? 무엇이 잘못될 수 있을까요?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/340">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1058">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3862">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18018">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="양방향-순환-신경망-bidirectional-recurrent-neural-networks"><a class="header" href="#양방향-순환-신경망-bidirectional-recurrent-neural-networks">양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></h1>
<p>:label:<code>sec_bi_rnn</code></p>
<p>지금까지 시퀀스 학습 작업의 실행 예제는 언어 모델링이었습니다. 여기서 우리는 시퀀스의 모든 이전 토큰이 주어졌을 때 다음 토큰을 예측하는 것을 목표로 합니다.
이 시나리오에서 우리는 왼쪽 문맥에 대해서만 조건을 걸기를 원하므로 표준 RNN의 단방향 연결이 적절해 보입니다.
그러나 시퀀스의 모든 타임 스텝에서 예측을 왼쪽 및 오른쪽 문맥 모두에 조건부로 설정하는 것이 완전히 괜찮은 다른 많은 시퀀스 학습 작업 문맥이 있습니다.
예를 들어 품사 탐지를 고려해 보십시오.
주어진 단어와 관련된 품사를 평가할 때 왜 양방향의 문맥을 고려하지 말아야 할까요?</p>
<p>관심 있는 실제 작업에서 모델을 미세 조정하기 전의 사전 훈련 연습으로 종종 유용한 또 다른 일반적인 작업은 텍스트 문서에서 무작위 토큰을 마스킹한 다음 누락된 토큰의 값을 예측하도록 시퀀스 모델을 훈련하는 것입니다.
빈칸 뒤에 무엇이 오는지에 따라 누락된 토큰의 가능성 있는 값이 극적으로 변한다는 점에 유의하십시오:</p>
<ul>
<li>나는 <code>___</code>.</li>
<li>나는 <code>___</code> 배고프다.</li>
<li>나는 <code>___</code> 배고프고, 돼지 반 마리를 먹을 수 있다.</li>
</ul>
<p>첫 번째 문장에서 "행복하다"는 가능성 있는 후보인 것 같습니다.
두 번째 문장에서는 "매우"와 같은 단어가 그럴듯해 보이지만, "매우"는 세 번째 문장과는 어울리지 않는 것 같습니다.</p>
<p>다행히 간단한 기술로 단방향 RNN을 양방향 RNN으로 변환할 수 있습니다 :cite:<code>Schuster.Paliwal.1997</code>.
우리는 단순히 동일한 입력에 대해 작동하고 반대 방향으로 연결된 두 개의 단방향 RNN 레이어를 구현합니다 (:numref:<code>fig_birnn</code>).
첫 번째 RNN 레이어의 경우 첫 번째 입력은 $\mathbf{x}_1$이고 마지막 입력은 $\mathbf{x}_T$이지만,
두 번째 RNN 레이어의 경우 첫 번째 입력은 $\mathbf{x}_T$이고 마지막 입력은 $\mathbf{x}_1$입니다.
이 양방향 RNN 레이어의 출력을 생성하기 위해, 우리는 단순히 두 개의 기저 단방향 RNN 레이어의 해당 출력을 함께 연결(concatenate)합니다.</p>
<p><img src="chapter_recurrent-modern/../img/birnn.svg" alt="양방향 RNN의 아키텍처." />
:label:<code>fig_birnn</code></p>
<p>공식적으로 임의의 타임 스텝 $t$에 대해 미니배치 입력 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$(예제 수 $=n$; 각 예제의 입력 수 $=d$)를 고려하고 은닉층 활성화 함수를 $\phi$라고 합시다.
양방향 아키텍처에서 이 타임 스텝에 대한 순방향 및 역방향 은닉 상태는 각각 $\overrightarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$와 $\overleftarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$입니다. 여기서 $h$는 은닉 유닛의 수입니다.
순방향 및 역방향 은닉 상태 업데이트는 다음과 같습니다:</p>
<p>$$
\begin{aligned}
\overrightarrow{\mathbf{H}}<em>t &amp;= \phi(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xh}}^{(f)} + \overrightarrow{\mathbf{H}}</em>{t-1} \mathbf{W}<em>{\textrm{hh}}^{(f)}  + \mathbf{b}</em>\textrm{h}^{(f)}),\
\overleftarrow{\mathbf{H}}<em>t &amp;= \phi(\mathbf{X}<em>t \mathbf{W}</em>{\textrm{xh}}^{(b)} + \overleftarrow{\mathbf{H}}</em>{t+1} \mathbf{W}<em>{\textrm{hh}}^{(b)}  + \mathbf{b}</em>\textrm{h}^{(b)}),
\end{aligned}
$$</p>
<p>여기서 가중치 $\mathbf{W}<em>{\textrm{xh}}^{(f)} \in \mathbb{R}^{d \times h}, \mathbf{W}</em>{\textrm{hh}}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}<em>{\textrm{xh}}^{(b)} \in \mathbb{R}^{d \times h}, \textrm{ 및 } \mathbf{W}</em>{\textrm{hh}}^{(b)} \in \mathbb{R}^{h \times h}$와 편향 $\mathbf{b}<em>\textrm{h}^{(f)} \in \mathbb{R}^{1 \times h}$ 및 $\mathbf{b}</em>\textrm{h}^{(b)} \in \mathbb{R}^{1 \times h}$는 모두 모델 파라미터입니다.</p>
<p>다음으로, 순방향 및 역방향 은닉 상태 $\overrightarrow{\mathbf{H}}_t$와 $\overleftarrow{\mathbf{H}}_t$를 연결하여 출력 레이어에 공급할 은닉 상태 $\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$를 얻습니다.
여러 은닉층이 있는 심층 양방향 RNN에서 이러한 정보는 다음 양방향 레이어의 <em>입력</em>으로 전달됩니다.
마지막으로 출력 레이어는 출력 $\mathbf{O}_t \in \mathbb{R}^{n \times q}$(출력 수 $=q$)를 계산합니다:</p>
<p>$$\mathbf{O}<em>t = \mathbf{H}<em>t \mathbf{W}</em>{\textrm{hq}} + \mathbf{b}</em>\textrm{q}.$$</p>
<p>여기서 가중치 행렬 $\mathbf{W}<em>{\textrm{hq}} \in \mathbb{R}^{2h \times q}$와 편향 $\mathbf{b}</em>\textrm{q} \in \mathbb{R}^{1 \times q}$는 출력 레이어의 모델 파라미터입니다.
기술적으로 두 방향이 서로 다른 수의 은닉 유닛을 가질 수 있지만, 이 설계 선택은 실제로 거의 이루어지지 않습니다.
이제 양방향 RNN의 간단한 구현을 보여줍니다.</p>
<pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import npx, np
from mxnet.gluon import rnn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from jax import numpy as jnp
</code></pre>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-7"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-7">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>양방향 RNN을 밑바닥부터 구현하려면, 별도의 학습 가능한 파라미터를 가진 두 개의 단방향 <code>RNNScratch</code> 인스턴스를 포함할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class BiRNNScratch(d2l.Module):
    def __init__(self, num_inputs, num_hiddens, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)
        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)
        self.num_hiddens *= 2  # 출력 차원이 두 배가 됩니다
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class BiRNNScratch(d2l.Module):
    num_inputs: int
    num_hiddens: int
    sigma: float = 0.01

    def setup(self):
        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)
        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)
        self.num_hiddens *= 2  # 출력 차원이 두 배가 됩니다
</code></pre>
<p>순방향 및 역방향 RNN의 상태는 별도로 업데이트되는 반면, 이 두 RNN의 출력은 연결됩니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(BiRNNScratch)
def forward(self, inputs, Hs=None):
    f_H, b_H = Hs if Hs is not None else (None, None)
    f_outputs, f_H = self.f_rnn(inputs, f_H)
    b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)
    outputs = [d2l.concat((f, b), -1) for f, b in zip(
        f_outputs, reversed(b_outputs))]
    return outputs, (f_H, b_H)
</code></pre>
<h2 id="간결한-구현-concise-implementation-4"><a class="header" href="#간결한-구현-concise-implementation-4">간결한 구현 (Concise Implementation)</a></h2>
<p>:begin_tab:<code>pytorch, mxnet, tensorflow</code>
고수준 API를 사용하여 양방향 RNN을 더 간결하게 구현할 수 있습니다.
여기서는 GRU 모델을 예로 듭니다.
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
Flax API는 RNN 레이어를 제공하지 않으므로 <code>bidirectional</code> 인수의 개념이 없습니다.
양방향 레이어가 필요한 경우 스크래치 구현에서 보여준 것처럼 입력을 수동으로 반전시켜야 합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
class BiGRU(d2l.RNN):
    def __init__(self, num_inputs, num_hiddens):
        d2l.Module.__init__(self)
        self.save_hyperparameters()
        if tab.selected('mxnet'):
            self.rnn = rnn.GRU(num_hiddens, bidirectional=True)
        if tab.selected('pytorch'):
            self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)
        self.num_hiddens *= 2
</code></pre>
<h2 id="요약-summary-39"><a class="header" href="#요약-summary-39">요약 (Summary)</a></h2>
<p>양방향 RNN에서 각 타임 스텝의 은닉 상태는 현재 타임 스텝 이전과 이후의 데이터에 의해 동시에 결정됩니다. 양방향 RNN은 주로 시퀀스 인코딩과 양방향 문맥이 주어졌을 때의 관찰값 추정에 유용합니다. 양방향 RNN은 긴 기울기 체인 때문에 훈련 비용이 매우 많이 듭니다.</p>
<h2 id="연습-문제-exercises-52"><a class="header" href="#연습-문제-exercises-52">연습 문제 (Exercises)</a></h2>
<ol>
<li>서로 다른 방향이 다른 수의 은닉 유닛을 사용하면 $\mathbf{H}_t$의 모양이 어떻게 변합니까?</li>
<li>여러 은닉층이 있는 양방향 RNN을 설계하십시오.</li>
<li>다의성(Polysemy)은 자연어에서 흔합니다. 예를 들어 "bank"라는 단어는 "i went to the bank to deposit cash"와 "i went to the bank to sit down" 문맥에서 서로 다른 의미를 갖습니다. 문맥 시퀀스와 단어가 주어졌을 때 올바른 문맥에서의 단어 벡터 표현이 반환되도록 신경망 모델을 어떻게 설계할 수 있을까요? 다의성을 처리하는 데 선호되는 신경망 아키텍처 유형은 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/339">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1059">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18019">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="기계-번역과-데이터셋-machine-translation-and-the-dataset"><a class="header" href="#기계-번역과-데이터셋-machine-translation-and-the-dataset">기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></h1>
<p>:label:<code>sec_machine_translation</code></p>
<p>현대 RNN에 대한 광범위한 관심을 불러일으킨 주요 돌파구 중 하나는 통계적 <em>기계 번역</em> 응용 분야에서의 큰 발전이었습니다.
여기서 모델은 한 언어의 문장을 제시받고 다른 언어의 해당 문장을 예측해야 합니다.
여기서 문장의 길이는 서로 다를 수 있으며, 두 언어의 문법 구조 차이로 인해 두 문장의 해당 단어가 동일한 순서로 나타나지 않을 수 있음에 유의하십시오.</p>
<p>많은 문제들이 이러한 두 "정렬되지 않은" 시퀀스 간의 매핑 성격을 가지고 있습니다.
예를 들어 대화 프롬프트에서 응답으로, 또는 질문에서 답변으로의 매핑이 그 예입니다.
광범위하게 이러한 문제를 <em>시퀀스-투-시퀀스(sequence-to-sequence, seq2seq)</em> 문제라고 하며, 이 장의 나머지 부분과 :numref:<code>chap_attention-and-transformers</code>의 상당 부분에서 우리의 초점이 됩니다.</p>
<p>이 섹션에서는 기계 번역 문제와 후속 예제에서 사용할 예제 데이터셋을 소개합니다.
수십 년 동안 언어 간 번역의 통계적 정식화가 인기 있었으며 :cite:<code>Brown.Cocke.Della-Pietra.ea.1988,Brown.Cocke.Della-Pietra.ea.1990</code>,
연구자들이 신경망 접근 방식을 작동시키기 전에도 마찬가지였습니다(방법들은 종종 <em>신경 기계 번역</em>이라는 용어 아래 하나로 묶였습니다).</p>
<p>먼저 데이터를 처리하기 위한 새로운 코드가 필요합니다.
:numref:<code>sec_language-model</code>에서 본 언어 모델링과 달리,
여기서 각 예제는 두 개의 별도 텍스트 시퀀스, 즉 소스 언어의 시퀀스와 타겟 언어의 시퀀스(번역)로 구성됩니다.
다음 코드 스니펫은 훈련을 위해 전처리된 데이터를 미니배치로 로드하는 방법을 보여줍니다.</p>
<pre><code class="language-{.python .input  n=2}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
import os
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=3}">%%tab pytorch
from d2l import torch as d2l
import torch
import os
</code></pre>
<pre><code class="language-{.python .input  n=4}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
import os
</code></pre>
<pre><code class="language-{.python .input  n=4}">%%tab jax
from d2l import jax as d2l
from jax import numpy as jnp
import os
</code></pre>
<h2 id="데이터셋-다운로드-및-전처리"><a class="header" href="#데이터셋-다운로드-및-전처리">[<strong>데이터셋 다운로드 및 전처리</strong>]</a></h2>
<p>시작하기 위해, <a href="http://www.manythings.org/anki/">Tatoeba 프로젝트의 이국어 문장 쌍</a>으로 구성된 영어-프랑스어 데이터셋을 다운로드합니다.
데이터셋의 각 줄은 영어 텍스트 시퀀스(<em>소스</em>)와 번역된 프랑스어 텍스트 시퀀스(<em>타겟</em>)로 구성된 탭 구분 쌍입니다.
각 텍스트 시퀀스는 단 한 문장일 수도 있고, 여러 문장으로 된 단락일 수도 있음에 유의하십시오.</p>
<pre><code class="language-{.python .input  n=5}">%%tab all
class MTFraEng(d2l.DataModule):  #@save
    """영어-프랑스어 데이터셋."""
    def _download(self):
        d2l.extract(d2l.download(
            d2l.DATA_URL+'fra-eng.zip', self.root, 
            '94646ad1522d915e7b0f9296181140edcf86a4f5'))
        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:
            return f.read()
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
data = MTFraEng() 
raw_text = data._download()
print(raw_text[:75])
</code></pre>
<p>데이터셋을 다운로드한 후,
원시 텍스트 데이터에 대해 [<strong>여러 전처리 단계를 진행</strong>]합니다.
예를 들어 줄 바꿈 없는 공백(non-breaking space)을 일반 공백으로 바꾸고,
대문자를 소문자로 변환하며,
단어와 구두점 사이에 공백을 삽입합니다.</p>
<pre><code class="language-{.python .input  n=6}">%%tab all
@d2l.add_to_class(MTFraEng)  #@save
def _preprocess(self, text):
    # 줄 바꿈 없는 공백을 일반 공백으로 교체
    text = text.replace('\u202f', ' ').replace('\xa0', ' ')
    # 단어와 구두점 사이에 공백 삽입
    no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '
    out = [' ' + char if i &gt; 0 and no_space(char, text[i - 1]) else char
           for i, char in enumerate(text.lower())]
    return ''.join(out)
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
text = data._preprocess(raw_text)
print(text[:80])
</code></pre>
<h2 id="토큰화-tokenization-1"><a class="header" href="#토큰화-tokenization-1">[<strong>토큰화 (Tokenization)</strong>]</a></h2>
<p>:numref:<code>sec_language-model</code>의 문자 수준 토큰화와 달리,
기계 번역을 위해 여기서는 단어 수준 토큰화를 선호합니다
(오늘날의 최첨단 모델은 더 복잡한 토큰화 기술을 사용합니다).
다음 <code>_tokenize</code> 메서드는 처음 <code>max_examples</code>개의 텍스트 시퀀스 쌍을 토큰화하며,
여기서 각 토큰은 단어이거나 구두점입니다.
시퀀스의 끝을 나타내기 위해 모든 시퀀스 끝에 특수 “&lt;eos&gt;” 토큰을 추가합니다.
모델이 토큰별로 시퀀스를 생성하여 예측할 때, “&lt;eos&gt;” 토큰의 생성은 출력 시퀀스가 완료되었음을 시사할 수 있습니다.
마지막으로 아래 메서드는 <code>src</code>와 <code>tgt</code>라는 두 개의 토큰 리스트의 리스트를 반환합니다.
구체적으로 <code>src[i]</code>는 소스 언어(여기서는 영어)의 $i^\textrm{th}$번째 텍스트 시퀀스의 토큰 리스트이고, <code>tgt[i]</code>는 타겟 언어(여기서는 프랑스어)의 토큰 리스트입니다.</p>
<pre><code class="language-{.python .input  n=7}">%%tab all
@d2l.add_to_class(MTFraEng)  #@save
def _tokenize(self, text, max_examples=None):
    src, tgt = [], []
    for i, line in enumerate(text.split('\n')):
        if max_examples and i &gt; max_examples: break
        parts = line.split('\t')
        if len(parts) == 2:
            # 빈 토큰 건너뛰기
            src.append([t for t in f'{parts[0]} &lt;eos&gt;'.split(' ') if t])
            tgt.append([t for t in f'{parts[1]} &lt;eos&gt;'.split(' ') if t])
    return src, tgt
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
src, tgt = data._tokenize(text)
src[:6], tgt[:6]
</code></pre>
<p>[<strong>텍스트 시퀀스당 토큰 수의 히스토그램을 그려봅시다.</strong>]
이 간단한 영어-프랑스어 데이터셋에서 대부분의 텍스트 시퀀스는 20개 미만의 토큰을 가집니다.</p>
<pre><code class="language-{.python .input  n=8}">%%tab all
#@save
def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
    """리스트 길이 쌍에 대한 히스토그램을 그립니다."""
    d2l.set_figsize()
    _, _, patches = d2l.plt.hist(
        [[len(l) for l in xlist], [len(l) for l in ylist]])
    d2l.plt.xlabel(xlabel)
    d2l.plt.ylabel(ylabel)
    for patch in patches[1].patches:
        patch.set_hatch('/')
    d2l.plt.legend(legend)
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
show_list_len_pair_hist(['source', 'target'], '# tokens per sequence',
                        'count', src, tgt);
</code></pre>
<h2 id="고정-길이-시퀀스-로드하기-loading-sequences-of-fixed-length"><a class="header" href="#고정-길이-시퀀스-로드하기-loading-sequences-of-fixed-length">고정 길이 시퀀스 로드하기 (Loading Sequences of Fixed Length)</a></h2>
<p>:label:<code>subsec_loading-seq-fixed-len</code></p>
<p>언어 모델링에서 [<strong>각 예제 시퀀스</strong>](한 문장의 세그먼트이거나 여러 문장에 걸친 범위)가 (<strong>고정된 길이를 가졌음</strong>)을 상기하십시오.
이는 :numref:<code>sec_language-model</code>의 <code>num_steps</code>(타임 스텝 또는 토큰의 수) 인수에 의해 지정되었습니다.
기계 번역에서 각 예제는 소스 및 타겟 텍스트 시퀀스의 쌍이며, 여기서 두 텍스트 시퀀스는 길이가 다를 수 있습니다.</p>
<p>계산 효율성을 위해,
우리는 여전히 *자르기(truncation)*와 *패딩(padding)*을 통해 한 번에 텍스트 시퀀스의 미니배치를 처리할 수 있습니다.
동일한 미니배치의 모든 시퀀스가 동일한 길이 <code>num_steps</code>를 가져야 한다고 가정합시다.
텍스트 시퀀스에 <code>num_steps</code>보다 적은 토큰이 있는 경우, 길이가 <code>num_steps</code>에 도달할 때까지 끝에 특수 "&lt;pad&gt;" 토큰을 계속 추가합니다.
그렇지 않으면 처음 <code>num_steps</code>개의 토큰만 취하고 나머지는 버림으로써 텍스트 시퀀스를 자릅니다.
이런 식으로 모든 텍스트 시퀀스는 동일한 모양의 미니배치로 로드될 수 있도록 동일한 길이를 갖게 됩니다.
더욱이 패딩 토큰을 제외한 소스 시퀀스의 길이도 기록합니다.
이 정보는 나중에 다룰 일부 모델에서 필요할 것입니다.</p>
<p>기계 번역 데이터셋은 언어 쌍으로 구성되므로,
소스 언어와 타겟 언어 각각에 대해 두 개의 어휘를 별도로 구축할 수 있습니다.
단어 수준 토큰화를 사용하면 어휘 크기가 문자 수준 토큰화를 사용할 때보다 훨씬 더 커집니다.
이를 완화하기 위해 여기서는 두 번 미만으로 나타나는 드문 토큰을 동일한 알 수 없는("&lt;unk&gt;") 토큰으로 취급합니다.
나중에 설명하겠지만(:numref:<code>fig_seq2seq</code>),
타겟 시퀀스로 훈련할 때 디코더 출력(레이블 토큰)은 한 토큰만큼 이동된 동일한 디코더 입력(타겟 토큰)일 수 있으며,
특수 문장 시작("&lt;bos&gt;") 토큰이 타겟 시퀀스를 예측하기 위한 첫 번째 입력 토큰으로 사용될 것입니다(:numref:<code>fig_seq2seq_predict</code>).</p>
<pre><code class="language-{.python .input  n=9}">%%tab all
@d2l.add_to_class(MTFraEng)  #@save
def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):
    super(MTFraEng, self).__init__()
    self.save_hyperparameters()
    self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(
        self._download())
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(MTFraEng)  #@save
def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):
    def _build_array(sentences, vocab, is_tgt=False):
        pad_or_trim = lambda seq, t: (
            seq[:t] if len(seq) &gt; t else seq + ['&lt;pad&gt;'] * (t - len(seq)))
        sentences = [pad_or_trim(s, self.num_steps) for s in sentences]
        if is_tgt:
            sentences = [['&lt;bos&gt;'] + s for s in sentences]
        if vocab is None:
            vocab = d2l.Vocab(sentences, min_freq=2)
        array = d2l.tensor([vocab[s] for s in sentences])
        valid_len = d2l.reduce_sum(
            d2l.astype(array != vocab['&lt;pad&gt;'], d2l.int32), 1)
        return array, vocab, valid_len
    src, tgt = self._tokenize(self._preprocess(raw_text), 
                              self.num_train + self.num_val)
    src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)
    tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)
    return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),
            src_vocab, tgt_vocab)
</code></pre>
<h2 id="데이터셋-읽기"><a class="header" href="#데이터셋-읽기">[<strong>데이터셋 읽기</strong>]</a></h2>
<p>마지막으로 데이터 반복자를 반환하기 위해 <code>get_dataloader</code> 메서드를 정의합니다.</p>
<pre><code class="language-{.python .input  n=10}">%%tab all
@d2l.add_to_class(MTFraEng)  #@save
def get_dataloader(self, train):
    idx = slice(0, self.num_train) if train else slice(self.num_train, None)
    return self.get_tensorloader(self.arrays, train, idx)
</code></pre>
<p>영어-프랑스어 데이터셋에서 [<strong>첫 번째 미니배치를 읽어봅시다.</strong>]</p>
<pre><code class="language-{.python .input  n=11}">%%tab all
data = MTFraEng(batch_size=3)
src, tgt, src_valid_len, label = next(iter(data.train_dataloader()))
print('source:', d2l.astype(src, d2l.int32))
print('decoder input:', d2l.astype(tgt, d2l.int32))
print('source len excluding pad:', d2l.astype(src_valid_len, d2l.int32))
print('label:', d2l.astype(label, d2l.int32))
</code></pre>
<p>위의 <code>_build_arrays</code> 메서드에 의해 처리된 소스 및 타겟 시퀀스 쌍을 보여줍니다(문자열 형식).</p>
<pre><code class="language-{.python .input  n=12}">%%tab all
@d2l.add_to_class(MTFraEng)  #@save
def build(self, src_sentences, tgt_sentences):
    raw_text = '\n'.join([src + '\t' + tgt for src, tgt in zip(
        src_sentences, tgt_sentences)])
    arrays, _, _ = self._build_arrays(
        raw_text, self.src_vocab, self.tgt_vocab)
    return arrays
</code></pre>
<pre><code class="language-{.python .input  n=13}">%%tab all
src, tgt, _,  _ = data.build(['hi .'], ['salut .'])
print('source:', data.src_vocab.to_tokens(d2l.astype(src[0], d2l.int32)))
print('target:', data.tgt_vocab.to_tokens(d2l.astype(tgt[0], d2l.int32)))
</code></pre>
<h2 id="요약-summary-40"><a class="header" href="#요약-summary-40">요약 (Summary)</a></h2>
<p>자연어 처리에서 <em>기계 번역</em>이란 <em>소스</em> 언어의 텍스트 문자열을 나타내는 시퀀스에서 <em>타겟</em> 언어의 그럴듯한 번역을 나타내는 문자열로 자동으로 매핑하는 작업을 말합니다. 단어 수준 토큰화를 사용하면 어휘 크기가 문자 수준 토큰화를 사용할 때보다 훨씬 커지지만 시퀀스 길이는 훨씬 짧아집니다. 큰 어휘 크기를 완화하기 위해 드문 토큰을 어떤 "알 수 없는" 토큰으로 취급할 수 있습니다. 텍스트 시퀀스를 자르고 패딩하여 모든 시퀀스가 미니배치로 로드될 수 있도록 동일한 길이를 갖게 할 수 있습니다. 현대적인 구현에서는 종종 패딩에 대한 과도한 계산 낭비를 피하기 위해 비슷한 길이의 시퀀스를 버킷팅(bucket)합니다.</p>
<h2 id="연습-문제-exercises-53"><a class="header" href="#연습-문제-exercises-53">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>_tokenize</code> 메서드에서 <code>max_examples</code> 인수의 다양한 값을 시도해 보십시오. 이것이 소스 언어와 타겟 언어의 어휘 크기에 어떤 영향을 미칩니까?</li>
<li>중국어와 일본어 같은 일부 언어의 텍스트에는 단어 경계 표시(예: 공백)가 없습니다. 그러한 경우에도 단어 수준 토큰화가 여전히 좋은 아이디어일까요? 왜 그런가요 혹은 왜 아닌가요?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/344">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1060">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3863">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18020">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<h1 id="인코더-디코더-아키텍처-the-encoder--decoder-architecture"><a class="header" href="#인코더-디코더-아키텍처-the-encoder--decoder-architecture">인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></h1>
<p>:label:<code>sec_encoder-decoder</code></p>
<p>기계 번역(:numref:<code>sec_machine_translation</code>)과 같은 일반적인 시퀀스-투-시퀀스 문제에서 입력과 출력은 정렬되지 않은 가변 길이의 시퀀스입니다.
이러한 종류의 데이터를 처리하기 위한 표준 접근 방식은 두 개의 주요 구성 요소로 이루어진 <em>인코더-디코더(encoder--decoder)</em> 아키텍처(:numref:<code>fig_encoder_decoder</code>)를 설계하는 것입니다:
가변 길이 시퀀스를 입력으로 받는 <em>인코더(encoder)</em>,
그리고 인코딩된 입력과 타겟 시퀀스의 왼쪽 문맥을 입력으로 받아 타겟 시퀀스의 후속 토큰을 예측하는 조건부 언어 모델 역할을 하는 *디코더(decoder)*입니다.</p>
<p><img src="chapter_recurrent-modern/../img/encoder-decoder.svg" alt="인코더-디코더 아키텍처." />
:label:<code>fig_encoder_decoder</code></p>
<p>영어를 프랑스어로 기계 번역하는 예를 들어 보겠습니다.
영어 입력 시퀀스 "They", "are", "watching", "."가 주어지면,
이 인코더-디코더 아키텍처는 먼저 가변 길이 입력을 상태로 인코딩한 다음,
상태를 디코딩하여 번역된 시퀀스를 출력으로 토큰별로 생성합니다: "Ils", "regardent", ".".
인코더-디코더 아키텍처는 후속 섹션에서 다룰 다양한 시퀀스-투-시퀀스 모델의 기초를 형성하므로,
이 섹션에서는 이 아키텍처를 나중에 구현될 인터페이스로 변환할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet.gluon import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
</code></pre>
<h2 id="인코더-encoder"><a class="header" href="#인코더-encoder">(<strong>인코더 (Encoder)</strong>)</a></h2>
<p>인코더 인터페이스에서는 인코더가 가변 길이 시퀀스를 입력 <code>X</code>로 받는다는 것만 지정합니다.
구현은 이 기본 <code>Encoder</code> 클래스를 상속하는 모든 모델에서 제공될 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class Encoder(nn.Block):  #@save
    """인코더-디코더 아키텍처를 위한 기본 인코더 인터페이스."""
    def __init__(self):
        super().__init__()

    # 나중에 추가 인수가 있을 수 있습니다(예: 패딩을 제외한 길이)
    def forward(self, X, *args):
        raise NotImplementedError
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class Encoder(nn.Module):  #@save
    """인코더-디코더 아키텍처를 위한 기본 인코더 인터페이스."""
    def __init__(self):
        super().__init__()

    # 나중에 추가 인수가 있을 수 있습니다(예: 패딩을 제외한 길이)
    def forward(self, X, *args):
        raise NotImplementedError
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class Encoder(tf.keras.layers.Layer):  #@save
    """인코더-디코더 아키텍처를 위한 기본 인코더 인터페이스."""
    def __init__(self):
        super().__init__()

    # 나중에 추가 인수가 있을 수 있습니다(예: 패딩을 제외한 길이)
    def call(self, X, *args):
        raise NotImplementedError
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Encoder(nn.Module):  #@save
    """인코더-디코더 아키텍처를 위한 기본 인코더 인터페이스."""
    def setup(self):
        raise NotImplementedError

    # 나중에 추가 인수가 있을 수 있습니다(예: 패딩을 제외한 길이)
    def __call__(self, X, *args):
        raise NotImplementedError
</code></pre>
<h2 id="디코더-decoder"><a class="header" href="#디코더-decoder">[<strong>디코더 (Decoder)</strong>]</a></h2>
<p>다음 디코더 인터페이스에서는 인코더 출력(<code>enc_all_outputs</code>)을 인코딩된 상태로 변환하기 위해 추가적인 <code>init_state</code> 메서드를 추가합니다.
이 단계에는 :numref:<code>sec_machine_translation</code>에서 설명한 입력의 유효 길이와 같은 추가 입력이 필요할 수 있음에 유의하십시오.
가변 길이 시퀀스를 토큰별로 생성하기 위해, 디코더는 매번 입력(예: 이전 타임 스텝에서 생성된 토큰)과 인코딩된 상태를 현재 타임 스텝의 출력 토큰으로 매핑할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class Decoder(nn.Block):  #@save
    """인코더-디코더 아키텍처를 위한 기본 디코더 인터페이스."""
    def __init__(self):
        super().__init__()

    # 나중에 추가 인수가 있을 수 있습니다(예: 패딩을 제외한 길이)
    def init_state(self, enc_all_outputs, *args):
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class Decoder(nn.Module):  #@save
    """인코더-디코더 아키텍처를 위한 기본 디코더 인터페이스."""
    def __init__(self):
        super().__init__()

    # 나중에 추가 인수가 있을 수 있습니다(예: 패딩을 제외한 길이)
    def init_state(self, enc_all_outputs, *args):
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class Decoder(tf.keras.layers.Layer):  #@save
    """인코더-디코더 아키텍처를 위한 기본 디코더 인터페이스."""
    def __init__(self):
        super().__init__()

    # 나중에 추가 인수가 있을 수 있습니다(예: 패딩을 제외한 길이)
    def init_state(self, enc_all_outputs, *args):
        raise NotImplementedError

    def call(self, X, state):
        raise NotImplementedError
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Decoder(nn.Module):  #@save
    """인코더-디코더 아키텍처를 위한 기본 디코더 인터페이스."""
    def setup(self):
        raise NotImplementedError

    # 나중에 추가 인수가 있을 수 있습니다(예: 패딩을 제외한 길이)
    def init_state(self, enc_all_outputs, *args):
        raise NotImplementedError

    def __call__(self, X, state):
        raise NotImplementedError
</code></pre>
<h2 id="인코더와-디코더-결합하기-putting-the-encoder-and-decoder-together"><a class="header" href="#인코더와-디코더-결합하기-putting-the-encoder-and-decoder-together">[<strong>인코더와 디코더 결합하기 (Putting the Encoder and Decoder Together)</strong>]</a></h2>
<p>순전파에서 인코더의 출력은 인코딩된 상태를 생성하는 데 사용되며, 이 상태는 디코더의 입력 중 하나로 추가 사용됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
class EncoderDecoder(d2l.Classifier):  #@save
    """인코더-디코더 아키텍처를 위한 기본 클래스."""
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, enc_X, dec_X, *args):
        enc_all_outputs = self.encoder(enc_X, *args)
        dec_state = self.decoder.init_state(enc_all_outputs, *args)
        # 디코더 출력만 반환
        return self.decoder(dec_X, dec_state)[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class EncoderDecoder(d2l.Classifier):  #@save
    """인코더-디코더 아키텍처를 위한 기본 클래스."""
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def call(self, enc_X, dec_X, *args):
        enc_all_outputs = self.encoder(enc_X, *args, training=True)
        dec_state = self.decoder.init_state(enc_all_outputs, *args)
        # 디코더 출력만 반환
        return self.decoder(dec_X, dec_state, training=True)[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class EncoderDecoder(d2l.Classifier):  #@save
    """인코더-디코더 아키텍처를 위한 기본 클래스."""
    encoder: nn.Module
    decoder: nn.Module
    training: bool

    def __call__(self, enc_X, dec_X, *args):
        enc_all_outputs = self.encoder(enc_X, *args, training=self.training)
        dec_state = self.decoder.init_state(enc_all_outputs, *args)
        # 디코더 출력만 반환
        return self.decoder(dec_X, dec_state, training=self.training)[0]
</code></pre>
<p>다음 섹션에서는 이 인코더-디코더 아키텍처를 기반으로 시퀀스-투-시퀀스 모델을 설계하기 위해 RNN을 적용하는 방법을 볼 것입니다.</p>
<h2 id="요약-summary-41"><a class="header" href="#요약-summary-41">요약 (Summary)</a></h2>
<p>인코더-디코더 아키텍처는 모두 가변 길이 시퀀스로 구성된 입력과 출력을 처리할 수 있으므로 기계 번역과 같은 시퀀스-투-시퀀스 문제에 적합합니다.
인코더는 가변 길이 시퀀스를 입력으로 받아 고정된 모양의 상태로 변환합니다.
디코더는 고정된 모양의 인코딩된 상태를 가변 길이 시퀀스로 매핑합니다.</p>
<h2 id="연습-문제-exercises-54"><a class="header" href="#연습-문제-exercises-54">연습 문제 (Exercises)</a></h2>
<ol>
<li>신경망을 사용하여 인코더-디코더 아키텍처를 구현한다고 가정해 봅시다. 인코더와 디코더가 반드시 동일한 유형의 신경망이어야 합니까?</li>
<li>기계 번역 외에 인코더-디코더 아키텍처를 적용할 수 있는 다른 응용 사례를 생각할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/341">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1061">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3864">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18021">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<h1 id="기계-번역을-위한-시퀀스-투-시퀀스-학습-sequence-to-sequence-learning-for-machine-translation"><a class="header" href="#기계-번역을-위한-시퀀스-투-시퀀스-학습-sequence-to-sequence-learning-for-machine-translation">기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></h1>
<p>:label:<code>sec_seq2seq</code></p>
<p>입력과 출력이 각각 정렬되지 않은 가변 길이 시퀀스로 구성된 기계 번역(:numref:<code>sec_machine_translation</code>에서 논의됨)과 같은 소위 시퀀스-투-시퀀스 문제에서,
우리는 일반적으로 인코더-디코더 아키텍처(:numref:<code>sec_encoder-decoder</code>)에 의존합니다.
이 섹션에서는 인코더와 디코더가 모두 RNN으로 구현된 인코더-디코더 아키텍처를 기계 번역 작업에 적용하는 것을 시연합니다 :cite:<code>Sutskever.Vinyals.Le.2014,Cho.Van-Merrienboer.Gulcehre.ea.2014</code>.</p>
<p>여기서 인코더 RNN은 가변 길이 시퀀스를 입력으로 받아 고정 모양의 은닉 상태로 변환합니다.
나중에 :numref:<code>chap_attention-and-transformers</code>에서는 전체 입력을 하나의 고정 길이 표현으로 압축할 필요 없이 인코딩된 입력에 액세스할 수 있게 해 주는 주의(attention) 메커니즘을 소개할 것입니다.</p>
<p>그런 다음 출력 시퀀스를 한 번에 한 토큰씩 생성하기 위해,
별도의 RNN으로 구성된 디코더 모델은
입력 시퀀스와 출력의 이전 토큰 모두가 주어졌을 때 각 후속 타겟 토큰을 예측합니다.
훈련 중에 디코더는 일반적으로 공식 "ground truth" 레이블의 이전 토큰에 조건을 겁니다.
그러나 테스트 시에는 이미 예측된 토큰에 디코더의 각 출력을 조건부로 설정하고 싶을 것입니다.
인코더를 무시한다면 시퀀스-투-시퀀스 아키텍처의 디코더는 일반적인 언어 모델처럼 동작합니다.
:numref:<code>fig_seq2seq</code>는 기계 번역에서 시퀀스-투-시퀀스 학습을 위해 두 개의 RNN을 사용하는 방법을 보여줍니다.</p>
<p><img src="chapter_recurrent-modern/../img/seq2seq.svg" alt="RNN 인코더와 RNN 디코더를 사용한 시퀀스-투-시퀀스 학습." />
:label:<code>fig_seq2seq</code></p>
<p>:numref:<code>fig_seq2seq</code>에서 특수 "&lt;eos&gt;" 토큰은 시퀀스의 끝을 표시합니다.
이 토큰이 생성되면 모델은 예측을 중단할 수 있습니다.
RNN 디코더의 초기 타임 스텝에는 알아야 할 두 가지 특별한 설계 결정이 있습니다:
첫째, 모든 입력을 특수 문장 시작("&lt;bos&gt;") 토큰으로 시작합니다.
둘째, 매 디코딩 타임 스텝마다 인코더의 최종 은닉 상태를 디코더에 공급할 수 있습니다 :cite:<code>Cho.Van-Merrienboer.Gulcehre.ea.2014</code>.
:citet:<code>Sutskever.Vinyals.Le.2014</code>와 같은 일부 다른 설계에서는 RNN 인코더의 최종 은닉 상태를 첫 번째 디코딩 단계에서만 디코더의 은닉 상태를 시작하는 데 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
import collections
from d2l import mxnet as d2l
import math
from mxnet import np, npx, init, gluon, autograd
from mxnet.gluon import nn, rnn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import collections
from d2l import torch as d2l
import math
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import collections
from d2l import tensorflow as d2l
import math
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
import collections
from d2l import jax as d2l
from flax import linen as nn
from functools import partial
import jax
from jax import numpy as jnp
import math
import optax
</code></pre>
<h2 id="교사-강요-teacher-forcing"><a class="header" href="#교사-강요-teacher-forcing">교사 강요 (Teacher Forcing)</a></h2>
<p>입력 시퀀스에서 인코더를 실행하는 것은 비교적 간단하지만,
디코더의 입력과 출력을 처리하는 데는 더 많은 주의가 필요합니다.
가장 일반적인 접근 방식은 때때로 *교사 강요(teacher forcing)*라고 불립니다.
여기서 원래 타겟 시퀀스(토큰 레이블)가 디코더에 입력으로 제공됩니다.
더 구체적으로,
특수 문장 시작 토큰과 마지막 토큰을 제외한 원래 타겟 시퀀스가 디코더의 입력으로 연결되며,
디코더 출력(훈련용 레이블)은 한 토큰만큼 이동된 원래 타겟 시퀀스입니다:
"&lt;bos&gt;", "Ils", "regardent", "." $\rightarrow$
"Ils", "regardent", ".", "&lt;eos&gt;" (:numref:<code>fig_seq2seq</code>).</p>
<p>:numref:<code>subsec_loading-seq-fixed-len</code>의 구현은 교사 강요를 위한 훈련 데이터를 준비했으며,
여기서 자기 지도 학습을 위해 토큰을 이동시키는 것은 :numref:<code>sec_language-model</code>의 언어 모델 훈련과 유사합니다.
대안적인 접근 방식은 이전 타임 스텝에서 <em>예측된</em> 토큰을 디코더의 현재 입력으로 공급하는 것입니다.</p>
<p>다음에서는 :numref:<code>fig_seq2seq</code>에 묘사된 설계를 더 자세히 설명합니다.
우리는 :numref:<code>sec_machine_translation</code>에서 소개된 영어-프랑스어 데이터셋에서 기계 번역을 위해 이 모델을 훈련할 것입니다.</p>
<h2 id="인코더-encoder-1"><a class="header" href="#인코더-encoder-1">인코더 (Encoder)</a></h2>
<p>인코더는 가변 길이의 입력 시퀀스를 고정 모양의 <em>문맥 변수(context variable)</em> $\mathbf{c}$로 변환함을 상기하십시오 (:numref:<code>fig_seq2seq</code> 참조).</p>
<p>단일 시퀀스 예제(배치 크기 1)를 고려하십시오.
입력 시퀀스가 $x_1, \ldots, x_T$이고 $x_t$가 $t^{\textrm{th}}$번째 토큰이라고 가정합니다.
타임 스텝 $t$에서 RNN은 $x_t$에 대한 입력 특성 벡터 $\mathbf{x}_t$와 이전 타임 스텝의 은닉 상태 $\mathbf{h} _{t-1}$을 현재 은닉 상태 $\mathbf{h}_t$로 변환합니다.
우리는 함수 $f$를 사용하여 RNN의 순환 레이어의 변환을 표현할 수 있습니다:</p>
<p>$$\mathbf{h}_t = f(\mathbf{x}<em>t, \mathbf{h}</em>{t-1}). $$</p>
<p>일반적으로 인코더는 사용자 정의 함수 $q$를 통해 모든 타임 스텝의 은닉 상태를 문맥 변수로 변환합니다:</p>
<p>$$\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T).$$</p>
<p>예를 들어 :numref:<code>fig_seq2seq</code>에서 문맥 변수는 입력 시퀀스의 최종 토큰을 처리한 후의 인코더 RNN의 표현에 해당하는 은닉 상태 $\mathbf{h}_T$일 뿐입니다.</p>
<p>이 예제에서 우리는 단방향 RNN을 사용하여 인코더를 설계했습니다. 여기서 은닉 상태는 해당 타임 스텝 및 그 이전의 입력 하위 시퀀스에만 의존합니다.
양방향 RNN을 사용하여 인코더를 구성할 수도 있습니다.
이 경우 은닉 상태는 타임 스텝 이전과 이후의 하위 시퀀스(현재 타임 스텝의 입력 포함)에 의존하며, 이는 전체 시퀀스의 정보를 인코딩합니다.</p>
<p>이제 [<strong>RNN 인코더를 구현</strong>]해 봅시다.
입력 시퀀스의 각 토큰에 대한 특성 벡터를 얻기 위해 *임베딩 레이어(embedding layer)*를 사용한다는 점에 유의하십시오.
임베딩 레이어의 가중치는 행렬이며, 행의 수는 입력 어휘 크기(<code>vocab_size</code>)에 해당하고 열의 수는 특성 벡터의 차원(<code>embed_size</code>)에 해당합니다.
임의의 입력 토큰 인덱스 $i$에 대해, 임베딩 레이어는 가중치 행렬의 $i^{\textrm{th}}$번째 행(0부터 시작)을 가져와 특성 벡터를 반환합니다.
여기서는 다층 GRU로 인코더를 구현합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class Seq2SeqEncoder(d2l.Encoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 인코더."""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)
        self.initialize(init.Xavier())
            
    def forward(self, X, *args):
        # X 모양: (batch_size, num_steps)
        embs = self.embedding(d2l.transpose(X))
        # embs 모양: (num_steps, batch_size, embed_size)    
        outputs, state = self.rnn(embs)
        # outputs 모양: (num_steps, batch_size, num_hiddens)
        # state 모양: (num_layers, batch_size, num_hiddens)
        return outputs, state
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def init_seq2seq(module):  #@save
    """시퀀스-투-시퀀스 학습을 위한 가중치 초기화."""
    if type(module) == nn.Linear:
         nn.init.xavier_uniform_(module.weight)
    if type(module) == nn.GRU:
        for param in module._flat_weights_names:
            if "weight" in param:
                nn.init.xavier_uniform_(module._parameters[param])
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class Seq2SeqEncoder(d2l.Encoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 인코더."""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)
        self.apply(init_seq2seq)
            
    def forward(self, X, *args):
        # X 모양: (batch_size, num_steps)
        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int64))
        # embs 모양: (num_steps, batch_size, embed_size)
        outputs, state = self.rnn(embs)
        # outputs 모양: (num_steps, batch_size, num_hiddens)
        # state 모양: (num_layers, batch_size, num_hiddens)
        return outputs, state
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class Seq2SeqEncoder(d2l.Encoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 인코더."""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)
        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)
            
    def call(self, X, *args):
        # X 모양: (batch_size, num_steps)
        embs = self.embedding(d2l.transpose(X))
        # embs 모양: (num_steps, batch_size, embed_size)    
        outputs, state = self.rnn(embs)
        # outputs 모양: (num_steps, batch_size, num_hiddens)
        # state 모양: (num_layers, batch_size, num_hiddens)
        return outputs, state
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Seq2SeqEncoder(d2l.Encoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 인코더."""
    vocab_size: int
    embed_size: int
    num_hiddens: int
    num_layers: int
    dropout: float = 0

    def setup(self):
        self.embedding = nn.Embed(self.vocab_size, self.embed_size)
        self.rnn = d2l.GRU(self.num_hiddens, self.num_layers, self.dropout)

    def __call__(self, X, *args, training=False):
        # X 모양: (batch_size, num_steps)
        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int32))
        # embs 모양: (num_steps, batch_size, embed_size)
        outputs, state = self.rnn(embs, training=training)
        # outputs 모양: (num_steps, batch_size, num_hiddens)
        # state 모양: (num_layers, batch_size, num_hiddens)
        return outputs, state
</code></pre>
<p>구체적인 예를 들어 [<strong>위의 인코더 구현을 설명해 봅시다.</strong>]
아래에서는 은닉 유닛 수가 16인 2층 GRU 인코더를 인스턴스화합니다.
시퀀스 입력 미니배치 <code>X</code>(배치 크기 $=4$; 타임 스텝 수 $=9$)가 주어지면, 모든 타임 스텝에서 최종 레이어의 은닉 상태(인코더의 순환 레이어에 의해 반환된 <code>enc_outputs</code>)는 (타임 스텝 수, 배치 크기, 은닉 유닛 수) 모양의 텐서입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2
batch_size, num_steps = 4, 9
encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)
X = d2l.zeros((batch_size, num_steps))
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    enc_outputs, enc_state = encoder(X)
if tab.selected('jax'):
    (enc_outputs, enc_state), _ = encoder.init_with_output(d2l.get_key(), X)

d2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))
</code></pre>
<p>여기서는 GRU를 사용하고 있으므로, 최종 타임 스텝에서 다층 은닉 상태의 모양은 (은닉층 수, 배치 크기, 은닉 유닛 수)입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
if tab.selected('mxnet', 'pytorch', 'jax'):
    d2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))
if tab.selected('tensorflow'):
    d2l.check_len(enc_state, num_layers)
    d2l.check_shape(enc_state[0], (batch_size, num_hiddens))
</code></pre>
<h2 id="디코더-decoder-1"><a class="header" href="#디코더-decoder-1">[<strong>디코더 (Decoder)</strong>]</a></h2>
<p>:label:<code>sec_seq2seq_decoder</code></p>
<p>타겟 출력 시퀀스 $y_1, y_2, \ldots, y_{T'}$가 주어졌을 때, 각 타임 스텝 $t'$에 대해(입력 시퀀스 타임 스텝과 구별하기 위해 $t^\prime$를 사용함),
디코더는 타겟의 이전 토큰들 $y_1, \ldots, y_{t'}$와 문맥 변수 $\mathbf{c}$에 조건부로 설정된, $y_{t'+1}$ 단계에서 발생 가능한 각 토큰에 예측 확률을 할당합니다. 즉, $P(y_{t'+1} \mid y_1, \ldots, y_{t'}, \mathbf{c})$.</p>
<p>타겟 시퀀스의 후속 토큰 $t'+1$을 예측하기 위해, RNN 디코더는 이전 단계의 타겟 토큰 $y_{t^\prime}$, 이전 타임 스텝의 은닉 RNN 상태 $\mathbf{s}<em>{t^\prime-1}$, 그리고 문맥 변수 $\mathbf{c}$를 입력으로 취하여 현재 타임 스텝의 은닉 상태 $\mathbf{s}</em>{t^\prime}$로 변환합니다.
우리는 함수 $g$를 사용하여 디코더 은닉층의 변환을 표현할 수 있습니다:</p>
<p>$$\mathbf{s}<em>{t^\prime} = g(y</em>{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1}).$$
:eqlabel:<code>eq_seq2seq_s_t</code></p>
<p>디코더의 은닉 상태를 얻은 후, 출력 레이어와 소프트맥스 연산을 사용하여 후속 출력 토큰 ${t'+1}$에 대한 예측 분포 $p(y_{t^{\prime}+1} \mid y_1, \ldots, y_{t'}, \mathbf{c})$를 계산할 수 있습니다.</p>
<p>:numref:<code>fig_seq2seq</code>를 따라 아래와 같이 디코더를 구현할 때, 인코더의 최종 타임 스텝 은닉 상태를 직접 사용하여 디코더의 은닉 상태를 초기화합니다.
이는 RNN 인코더와 RNN 디코더가 동일한 수의 레이어와 은닉 유닛을 가질 것을 요구합니다.
인코딩된 입력 시퀀스 정보를 추가로 통합하기 위해, 문맥 변수는 모든 타임 스텝에서 디코더 입력과 연결됩니다.
출력 토큰의 확률 분포를 예측하기 위해, RNN 디코더의 최종 레이어 은닉 상태를 변환하는 완전 연결 레이어를 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class Seq2SeqDecoder(d2l.Decoder):
    """시퀀스-투-시퀀스 학습을 위한 RNN 디코더."""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)
        self.dense = nn.Dense(vocab_size, flatten=False)
        self.initialize(init.Xavier())
            
    def init_state(self, enc_all_outputs, *args):
        return enc_all_outputs 

    def forward(self, X, state):
        # X 모양: (batch_size, num_steps)
        # embs 모양: (num_steps, batch_size, embed_size)
        embs = self.embedding(d2l.transpose(X))
        enc_output, hidden_state = state
        # context 모양: (batch_size, num_hiddens)
        context = enc_output[-1]
        # context를 (num_steps, batch_size, num_hiddens)로 브로드캐스트
        context = np.tile(context, (embs.shape[0], 1, 1))
        # 특성 차원에서 연결
        embs_and_context = d2l.concat((embs, context), -1)
        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)
        outputs = d2l.swapaxes(self.dense(outputs), 0, 1)
        # outputs 모양: (batch_size, num_steps, vocab_size)
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        return outputs, [enc_output, hidden_state]
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class Seq2SeqDecoder(d2l.Decoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 디코더."""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens,
                           num_layers, dropout)
        self.dense = nn.LazyLinear(vocab_size)
        self.apply(init_seq2seq)
            
    def init_state(self, enc_all_outputs, *args):
        return enc_all_outputs

    def forward(self, X, state):
        # X 모양: (batch_size, num_steps)
        # embs 모양: (num_steps, batch_size, embed_size)
        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int32))
        enc_output, hidden_state = state
        # context 모양: (batch_size, num_hiddens)
        context = enc_output[-1]
        # context를 (num_steps, batch_size, num_hiddens)로 브로드캐스트
        context = context.repeat(embs.shape[0], 1, 1)
        # 특성 차원에서 연결
        embs_and_context = d2l.concat((embs, context), -1)
        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)
        outputs = d2l.swapaxes(self.dense(outputs), 0, 1)
        # outputs 모양: (batch_size, num_steps, vocab_size)
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        return outputs, [enc_output, hidden_state]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class Seq2SeqDecoder(d2l.Decoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 디코더."""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)
        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)
        self.dense = tf.keras.layers.Dense(vocab_size)
            
    def init_state(self, enc_all_outputs, *args):
        return enc_all_outputs

    def call(self, X, state):
        # X 모양: (batch_size, num_steps)
        # embs 모양: (num_steps, batch_size, embed_size)
        embs = self.embedding(d2l.transpose(X))
        enc_output, hidden_state = state
        # context 모양: (batch_size, num_hiddens)
        context = enc_output[-1]
        # context를 (num_steps, batch_size, num_hiddens)로 브로드캐스트
        context = tf.tile(tf.expand_dims(context, 0), (embs.shape[0], 1, 1))
        # 특성 차원에서 연결
        embs_and_context = d2l.concat((embs, context), -1)
        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)
        outputs = d2l.transpose(self.dense(outputs), (1, 0, 2))
        # outputs 모양: (batch_size, num_steps, vocab_size)
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        return outputs, [enc_output, hidden_state]
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Seq2SeqDecoder(d2l.Decoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 디코더."""
    vocab_size: int
    embed_size: int
    num_hiddens: int
    num_layers: int
    dropout: float = 0

    def setup(self):
        self.embedding = nn.Embed(self.vocab_size, self.embed_size)
        self.rnn = d2l.GRU(self.num_hiddens, self.num_layers, self.dropout)
        self.dense = nn.Dense(self.vocab_size)

    def init_state(self, enc_all_outputs, *args):
        return enc_all_outputs

    def __call__(self, X, state, training=False):
        # X 모양: (batch_size, num_steps)
        # embs 모양: (num_steps, batch_size, embed_size)
        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int32))
        enc_output, hidden_state = state
        # context 모양: (batch_size, num_hiddens)
        context = enc_output[-1]
        # context를 (num_steps, batch_size, num_hiddens)로 브로드캐스트
        context = jnp.tile(context, (embs.shape[0], 1, 1))
        # 특성 차원에서 연결
        embs_and_context = d2l.concat((embs, context), -1)
        outputs, hidden_state = self.rnn(embs_and_context, hidden_state,
                                         training=training)
        outputs = d2l.swapaxes(self.dense(outputs), 0, 1)
        # outputs 모양: (batch_size, num_steps, vocab_size)
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        return outputs, [enc_output, hidden_state]
</code></pre>
<p>구현된 디코더를 설명하기 위해,
아래에서는 앞서 언급한 인코더와 동일한 하이퍼파라미터로 이를 인스턴스화합니다.
보시다시피 디코더의 출력 모양은 (배치 크기, 타임 스텝 수, 어휘 크기)가 되며, 여기서 텐서의 최종 차원은 예측된 토큰 분포를 저장합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)
if tab.selected('mxnet', 'pytorch', 'tensorflow'):
    state = decoder.init_state(encoder(X))
    dec_outputs, state = decoder(X, state)
if tab.selected('jax'):
    state = decoder.init_state(encoder.init_with_output(d2l.get_key(), X)[0])
    (dec_outputs, state), _ = decoder.init_with_output(d2l.get_key(), X,
                                                       state)


d2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))
if tab.selected('mxnet', 'pytorch', 'jax'):
    d2l.check_shape(state[1], (num_layers, batch_size, num_hiddens))
if tab.selected('tensorflow'):
    d2l.check_len(state[1], num_layers)
    d2l.check_shape(state[1][0], (batch_size, num_hiddens))
</code></pre>
<p>위의 RNN 인코더-디코더 모델의 레이어들은 :numref:<code>fig_seq2seq_details</code>에 요약되어 있습니다.</p>
<p><img src="chapter_recurrent-modern/../img/seq2seq-details.svg" alt="RNN 인코더-디코더 모델의 레이어들." />
:label:<code>fig_seq2seq_details</code></p>
<h2 id="시퀀스-투-시퀀스-학습을-위한-인코더-디코더-encoder--decoder-for-sequence-to-sequence-learning"><a class="header" href="#시퀀스-투-시퀀스-학습을-위한-인코더-디코더-encoder--decoder-for-sequence-to-sequence-learning">시퀀스-투-시퀀스 학습을 위한 인코더-디코더 (Encoder--Decoder for Sequence-to-Sequence Learning)</a></h2>
<p>코드에서 모든 것을 합치면 다음과 같습니다:</p>
<pre><code class="language-{.python .input}">%%tab pytorch, tensorflow, mxnet
class Seq2Seq(d2l.EncoderDecoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 인코더-디코더."""
    def __init__(self, encoder, decoder, tgt_pad, lr):
        super().__init__(encoder, decoder)
        self.save_hyperparameters()
        
    def validation_step(self, batch):
        Y_hat = self(*batch[:-1])
        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)
        
    def configure_optimizers(self):
        # Adam 최적화기가 여기서 사용됩니다
        if tab.selected('mxnet'):
            return gluon.Trainer(self.parameters(), 'adam',
                                 {'learning_rate': self.lr})
        if tab.selected('pytorch'):
            return torch.optim.Adam(self.parameters(), lr=self.lr)
        if tab.selected('tensorflow'):
            return tf.keras.optimizers.Adam(learning_rate=self.lr)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Seq2Seq(d2l.EncoderDecoder):  #@save
    """시퀀스-투-시퀀스 학습을 위한 RNN 인코더-디코더."""
    encoder: nn.Module
    decoder: nn.Module
    tgt_pad: int
    lr: float

    def validation_step(self, params, batch, state):
        l, _ = self.loss(params, batch[:-1], batch[-1], state)
        self.plot('loss', l, train=False)

    def configure_optimizers(self):
        # Adam 최적화기가 여기서 사용됩니다
        return optax.adam(learning_rate=self.lr)
</code></pre>
<h2 id="마스킹이-있는-손실-함수-loss-function-with-masking"><a class="header" href="#마스킹이-있는-손실-함수-loss-function-with-masking">마스킹이 있는 손실 함수 (Loss Function with Masking)</a></h2>
<p>각 타임 스텝에서 디코더는 출력 토큰에 대한 확률 분포를 예측합니다.
언어 모델링과 마찬가지로, 분포를 얻기 위해 소프트맥스를 적용하고 최적화를 위해 교차 엔트로피 손실을 계산할 수 있습니다.
:numref:<code>sec_machine_translation</code>에서 특수 패딩 토큰이 시퀀스 끝에 추가되어 다양한 길이의 시퀀스가 동일한 모양의 미니배치로 효율적으로 로드될 수 있음을 상기하십시오.
그러나 패딩 토큰의 예측은 손실 계산에서 제외되어야 합니다.
이를 위해, 관련 없는 예측과 0의 곱셈이 0이 되도록 [<strong>관련 없는 항목을 0 값으로 마스킹</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(Seq2Seq)
def loss(self, Y_hat, Y):
    l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)
    mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)
    return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(Seq2Seq)
@partial(jax.jit, static_argnums=(0, 5))
def loss(self, params, X, Y, state, averaged=False):
    Y_hat = state.apply_fn({'params': params}, *X,
                           rngs={'dropout': state.dropout_rng})
    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))
    Y = d2l.reshape(Y, (-1,))
    fn = optax.softmax_cross_entropy_with_integer_labels
    l = fn(Y_hat, Y)
    mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)
    return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask), {}
</code></pre>
<h2 id="훈련-training-19"><a class="header" href="#훈련-training-19">[<strong>훈련 (Training)</strong>]</a></h2>
<p>:label:<code>sec_seq2seq_training</code></p>
<p>이제 기계 번역 데이터셋에서 시퀀스-투-시퀀스 학습을 위해 [<strong>RNN 인코더-디코더 모델을 생성하고 훈련</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.MTFraEng(batch_size=128) 
embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2
if tab.selected('mxnet', 'pytorch', 'jax'):
    encoder = Seq2SeqEncoder(
        len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)
    decoder = Seq2SeqDecoder(
        len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
if tab.selected('mxnet', 'pytorch'):
    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                    lr=0.005)
if tab.selected('jax'):
    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                    lr=0.005, training=True)
if tab.selected('mxnet', 'pytorch', 'jax'):
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        encoder = Seq2SeqEncoder(
            len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)
        decoder = Seq2SeqDecoder(
            len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
        model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                        lr=0.005)
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)
trainer.fit(model, data)
</code></pre>
<h2 id="예측-prediction-2"><a class="header" href="#예측-prediction-2">[<strong>예측 (Prediction)</strong>]</a></h2>
<p>각 단계에서 출력 시퀀스를 예측하기 위해,
이전 타임 스텝에서 예측된 토큰이 입력으로 디코더에 공급됩니다.
한 가지 간단한 전략은 각 단계에서 예측할 때 디코더에 의해 가장 높은 확률이 할당된 토큰을 샘플링하는 것입니다.
훈련에서와 마찬가지로 초기 타임 스텝에서는 문장 시작("&lt;bos&gt;") 토큰이 디코더에 공급됩니다.
이 예측 과정은 :numref:<code>fig_seq2seq_predict</code>에 설명되어 있습니다.
문장 끝("&lt;eos&gt;") 토큰이 예측되면 출력 시퀀스의 예측이 완료됩니다.</p>
<p><img src="chapter_recurrent-modern/../img/seq2seq-predict.svg" alt="RNN 인코더-디코더를 사용하여 토큰별로 출력 시퀀스 예측하기." />
:label:<code>fig_seq2seq_predict</code></p>
<p>다음 섹션에서는 빔 검색(:numref:<code>sec_beam-search</code>)에 기반한 더 정교한 전략을 소개할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
@d2l.add_to_class(d2l.EncoderDecoder)  #@save
def predict_step(self, batch, device, num_steps,
                 save_attention_weights=False):
    if tab.selected('mxnet', 'pytorch'):
        batch = [d2l.to(a, device) for a in batch]
    src, tgt, src_valid_len, _ = batch
    if tab.selected('mxnet', 'pytorch'):
        enc_all_outputs = self.encoder(src, src_valid_len)
    if tab.selected('tensorflow'):
        enc_all_outputs = self.encoder(src, src_valid_len, training=False)
    dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)
    outputs, attention_weights = [d2l.expand_dims(tgt[:, 0], 1), ], []
    for _ in range(num_steps):
        if tab.selected('mxnet', 'pytorch'):
            Y, dec_state = self.decoder(outputs[-1], dec_state)
        if tab.selected('tensorflow'):
            Y, dec_state = self.decoder(outputs[-1], dec_state, training=False)
        outputs.append(d2l.argmax(Y, 2))
        # 주의 가중치 저장 (나중에 다룰 예정)
        if save_attention_weights:
            attention_weights.append(self.decoder.attention_weights)
    return d2l.concat(outputs[1:], 1), attention_weights
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(d2l.EncoderDecoder)  #@save
def predict_step(self, params, batch, num_steps,
                 save_attention_weights=False):
    src, tgt, src_valid_len, _ = batch
    enc_all_outputs, inter_enc_vars = self.encoder.apply(
        {'params': params['encoder']}, src, src_valid_len, training=False,
        mutable='intermediates')
    # 인코더 주의 가중치를 포함하는 inter_enc_vars가 비어 있지 않으면 저장합니다. (나중에 다룰 예정)
    enc_attention_weights = []
    if bool(inter_enc_vars) and save_attention_weights:
        # intermediates 컬렉션에 저장된 인코더 주의 가중치
        enc_attention_weights = inter_enc_vars[
            'intermediates']['enc_attention_weights'][0]

    dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)
    outputs, attention_weights = [d2l.expand_dims(tgt[:,0], 1), ], []
    for _ in range(num_steps):
        (Y, dec_state), inter_dec_vars = self.decoder.apply(
            {'params': params['decoder']}, outputs[-1], dec_state,
            training=False, mutable='intermediates')
        outputs.append(d2l.argmax(Y, 2))
        # 주의 가중치 저장 (나중에 다룰 예정)
        if save_attention_weights:
            # intermediates 컬렉션에 저장된 디코더 주의 가중치
            dec_attention_weights = inter_dec_vars[
                'intermediates']['dec_attention_weights'][0]
            attention_weights.append(dec_attention_weights)
    return d2l.concat(outputs[1:], 1), (attention_weights,
                                        enc_attention_weights)
</code></pre>
<h2 id="예측된-시퀀스의-평가-evaluation-of-predicted-sequences"><a class="header" href="#예측된-시퀀스의-평가-evaluation-of-predicted-sequences">예측된 시퀀스의 평가 (Evaluation of Predicted Sequences)</a></h2>
<p>예측된 시퀀스를 타겟 시퀀스(ground truth)와 비교하여 평가할 수 있습니다.
하지만 두 시퀀스 간의 유사성을 비교하기 위한 적절한 척도는 정확히 무엇일까요?</p>
<p>BLEU(Bilingual Evaluation Understudy)는 원래 기계 번역 결과를 평가하기 위해 제안되었지만 :cite:<code>Papineni.Roukos.Ward.ea.2002</code>,
다양한 응용 프로그램에서 출력 시퀀스의 품질을 측정하는 데 널리 사용되어 왔습니다.
원칙적으로 예측된 시퀀스의 임의의 $n$-gram(:numref:<code>subsec_markov-models-and-n-grams</code>)에 대해, BLEU는 이 $n$-gram이 타겟 시퀀스에 나타나는지 여부를 평가합니다.</p>
<p>$p_n$을 $n$-gram의 정밀도(precision)라고 하며,
예측된 시퀀스와 타겟 시퀀스에서 일치하는 $n$-gram의 수를 예측된 시퀀스의 $n$-gram 수로 나눈 비율로 정의됩니다.
설명하자면, 타겟 시퀀스 $A, B, C, D, E, F$와 예측된 시퀀스 $A, B, B, C, D$가 주어지면,
우리는 $p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$을 갖습니다.
이제 $\textrm{len}<em>\textrm{label}$과 $\textrm{len}</em>\textrm{pred}$를
각각 타겟 시퀀스와 예측된 시퀀스의 토큰 수라고 합시다.
그러면 BLEU는 다음과 같이 정의됩니다.</p>
<p>$$ \exp\left(\min\left(0, 1 - \frac{\textrm{len}<em>\textrm{label}}{\textrm{len}</em>\textrm{pred}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},$$
:eqlabel:<code>eq_bleu</code></p>
<p>여기서 $k$는 매칭을 위한 가장 긴 $n$-gram입니다.</p>
<p>:eqref:<code>eq_bleu</code>의 BLEU 정의에 따르면, 예측된 시퀀스가 타겟 시퀀스와 같을 때마다 BLEU는 1입니다.
더욱이,
더 긴 $n$-gram을 일치시키는 것이 더 어렵기 때문에,
BLEU는 더 긴 $n$-gram이 높은 정밀도를 가질 때 더 큰 가중치를 할당합니다.
구체적으로 $p_n$이 고정되어 있을 때, $p_n^{1/2^n}$은 $n$이 커짐에 따라 증가합니다(원본 논문은 $p_n^{1/n}$을 사용합니다).
또한,
더 짧은 시퀀스를 예측하는 것이 더 높은 $p_n$ 값을 산출하는 경향이 있으므로,
:eqref:<code>eq_bleu</code>에서 곱셈 항 앞의 계수는 더 짧은 예측된 시퀀스에 페널티를 줍니다.
예를 들어 $k=2$일 때 타겟 시퀀스 $A, B, C, D, E, F$와 예측된 시퀀스 $A, B$가 주어지면,
$p_1 = p_2 = 1$임에도 불구하고 페널티 계수 $\exp(1-6/2) \approx 0.14$가 BLEU를 낮춥니다.</p>
<p>우리는 [<strong>BLEU 척도를 구현</strong>]합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def bleu(pred_seq, label_seq, k):  #@save
    """BLEU를 계산합니다."""
    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
    len_pred, len_label = len(pred_tokens), len(label_tokens)
    score = math.exp(min(0, 1 - len_label / len_pred))
    for n in range(1, min(k, len_pred) + 1):
        num_matches, label_subs = 0, collections.defaultdict(int)
        for i in range(len_label - n + 1):
            label_subs[' '.join(label_tokens[i: i + n])] += 1
        for i in range(len_pred - n + 1):
            if label_subs[' '.join(pred_tokens[i: i + n])] &gt; 0:
                num_matches += 1
                label_subs[' '.join(pred_tokens[i: i + n])] -= 1
        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
    return score
</code></pre>
<p>마지막으로,
훈련된 RNN 인코더-디코더를 사용하여 [<strong>몇 가지 영어 문장을 프랑스어로 번역</strong>]하고 결과의 BLEU를 계산합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
engs = ['go .', 'i lost .', 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    preds, _ = model.predict_step(
        data.build(engs, fras), d2l.try_gpu(), data.num_steps)
if tab.selected('jax'):
    preds, _ = model.predict_step(trainer.state.params, data.build(engs, fras),
                                  data.num_steps)
for en, fr, p in zip(engs, fras, preds):
    translation = []
    for token in data.tgt_vocab.to_tokens(p):
        if token == '&lt;eos&gt;':
            break
        translation.append(token)        
    print(f'{en} =&gt; {translation}, bleu,'
          f'{bleu(" ".join(translation), fr, k=2):.3f}')
</code></pre>
<h2 id="요약-summary-42"><a class="header" href="#요약-summary-42">요약 (Summary)</a></h2>
<p>인코더-디코더 아키텍처의 설계에 따라 두 개의 RNN을 사용하여 시퀀스-투-시퀀스 학습을 위한 모델을 설계할 수 있습니다.
인코더-디코더 훈련에서 교사 강요 접근 방식은 (예측과 대조적으로) 원래 출력 시퀀스를 디코더에 공급합니다.
인코더와 디코더를 구현할 때 다층 RNN을 사용할 수 있습니다.
마스크를 사용하여 손실을 계산할 때와 같이 관련 없는 계산을 걸러낼 수 있습니다.
출력 시퀀스를 평가하기 위해 BLEU는 예측된 시퀀스와 타겟 시퀀스 사이의 $n$-gram을 일치시키는 인기 있는 척도입니다.</p>
<h2 id="연습-문제-exercises-55"><a class="header" href="#연습-문제-exercises-55">연습 문제 (Exercises)</a></h2>
<ol>
<li>하이퍼파라미터를 조정하여 번역 결과를 개선할 수 있습니까?</li>
<li>손실 계산에서 마스크를 사용하지 않고 실험을 다시 실행해 보십시오. 어떤 결과가 관찰됩니까? 왜 그런가요?</li>
<li>인코더와 디코더의 레이어 수나 은닉 유닛 수가 다른 경우, 디코더의 은닉 상태를 어떻게 초기화할 수 있을까요?</li>
<li>훈련에서 교사 강요를 디코더에 이전 타임 스텝의 예측을 공급하는 것으로 대체하십시오. 이것이 성능에 어떤 영향을 미칩니까?</li>
<li>GRU를 LSTM으로 교체하여 실험을 다시 실행하십시오.</li>
<li>디코더의 출력 레이어를 설계하는 다른 방법이 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/345">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1062">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3865">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18022">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="빔-검색-beam-search"><a class="header" href="#빔-검색-beam-search">빔 검색 (Beam Search)</a></h1>
<p>:label:<code>sec_beam-search</code></p>
<p>:numref:<code>sec_seq2seq</code>에서 우리는 인코더-디코더 아키텍처와 이를 엔드-투-엔드로 훈련하기 위한 표준 기술을 소개했습니다. 그러나 테스트 시간 예측과 관련하여, 우리는 어떤 타임 스텝에서 특수 문장 끝("&lt;eos&gt;") 토큰을 예측할 때까지 다음에 올 예측 확률이 가장 높은 토큰을 각 타임 스텝에서 선택하는 <em>그리디(greedy)</em> 전략만 언급했습니다.
이 섹션에서는 먼저 이 <em>그리디 검색(greedy search)</em> 전략을 공식화하고 실무자들이 흔히 겪는 몇 가지 문제를 식별하는 것으로 시작합니다.
그 후, 이 전략을 두 가지 대안인 <em>전수 검색(exhaustive search)</em> (예시적이지만 실용적이지 않음) 및 <em>빔 검색(beam search)</em> (실무에서의 표준 방법)과 비교합니다.</p>
<p>:numref:<code>sec_seq2seq</code>의 관례를 빌려 수학적 표기법을 설정해 보겠습니다.
임의의 타임 스텝 $t'$에서 디코더는 이전 토큰들 $y_1, \ldots, y_{t'}$와 인코더가 입력 시퀀스를 나타내기 위해 생성한 문맥 변수 $\mathbf{c}$에 조건부로 설정된, 어휘의 각 토큰이 시퀀스에서 다음에 올 확률( $y_{t'+1}$의 가능성 있는 값)을 나타내는 예측을 출력합니다.
계산 비용을 정량화하기 위해, $\mathcal{Y}$를 출력 어휘(특수 문장 끝 토큰 "&lt;eos&gt;" 포함)라고 합시다.
또한 출력 시퀀스의 최대 토큰 수를 $T'$로 지정합시다.
우리의 목표는 가능한 모든 $\mathcal{O}(\left|\mathcal{Y}\right|^{T'})$ 출력 시퀀스 중에서 이상적인 출력을 찾는 것입니다.
이는 "&lt;eos&gt;" 토큰이 발생하면 후속 토큰이 없기 때문에 고유한 출력의 수를 약간 과대평가하는 것입니다.
그러나 우리의 목적을 위해 이 숫자는 검색 공간의 크기를 대략적으로 포착합니다.</p>
<h2 id="그리디-검색-greedy-search"><a class="header" href="#그리디-검색-greedy-search">그리디 검색 (Greedy Search)</a></h2>
<p>:numref:<code>sec_seq2seq</code>의 간단한 <em>그리디 검색</em> 전략을 고려해 보십시오.
여기서 임의의 타임 스텝 $t'$에 대해 우리는 단순히 $\mathcal{Y}$에서 가장 높은 조건부 확률을 가진 토큰을 선택합니다. 즉,</p>
<p>$$y_{t'} = \operatorname*{argmax}<em>{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y</em>{t'-1}, \mathbf{c}).$$</p>
<p>모델이 "&lt;eos&gt;"를 출력하거나 최대 길이 $T'$에 도달하면 출력 시퀀스가 완료됩니다.</p>
<p>이 전략은 합리적으로 보일 수 있으며, 실제로 그리 나쁘지 않습니다!
계산적으로 얼마나 부담이 없는지를 고려하면, 가성비가 매우 좋다고 할 수 있습니다.
그러나 효율성을 잠시 제쳐둔다면, (그리디하게 선택된) <em>가장 가능성 있는 토큰</em>의 시퀀스가 아니라 <em>가장 가능성 있는 시퀀스</em>를 찾는 것이 더 합리적으로 보일 수 있습니다.
이 두 객체는 상당히 다를 수 있음이 밝혀졌습니다.
가장 가능성 있는 시퀀스는 표현식 $\prod_{t'=1}^{T'} P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$를 최대화하는 시퀀스입니다.
기계 번역 예제에서 디코더가 기저의 생성 프로세스 확률을 진정으로 복구했다면, 이것은 우리에게 가장 가능성 있는 번역을 제공할 것입니다.
불행히도 그리디 검색이 이 시퀀스를 제공할 것이라는 보장은 없습니다.</p>
<p>예를 들어 설명해 보겠습니다.
출력 사전에 "A", "B", "C", "&lt;eos&gt;"라는 네 개의 토큰이 있다고 가정해 봅시다.
:numref:<code>fig_s2s-prob1</code>에서 각 타임 스텝 아래의 네 숫자는 해당 타임 스텝에서 각각 "A", "B", "C", "&lt;eos&gt;"를 생성할 조건부 확률을 나타냅니다.</p>
<p><img src="chapter_recurrent-modern/../img/s2s-prob1.svg" alt="각 타임 스텝에서 그리디 검색은 조건부 확률이 가장 높은 토큰을 선택합니다." />
:label:<code>fig_s2s-prob1</code></p>
<p>각 타임 스텝에서 그리디 검색은 가장 높은 조건부 확률을 가진 토큰을 선택합니다.
따라서 출력 시퀀스 "A", "B", "C", "&lt;eos&gt;"가 예측될 것입니다 (:numref:<code>fig_s2s-prob1</code>).
이 출력 시퀀스의 조건부 확률은 $0.5\times0.4\times0.4\times0.6 = 0.048$입니다.</p>
<p>다음으로 :numref:<code>fig_s2s-prob2</code>의 다른 예를 살펴봅시다.
:numref:<code>fig_s2s-prob1</code>과 달리 타임 스텝 2에서 우리는 <em>두 번째</em>로 높은 조건부 확률을 가진 토큰 "C"를 선택합니다.</p>
<p><img src="chapter_recurrent-modern/../img/s2s-prob2.svg" alt="각 타임 스텝 아래의 네 숫자는 해당 타임 스텝에서 &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;&lt;eos&gt;&quot;를 생성할 조건부 확률을 나타냅니다. 타임 스텝 2에서 두 번째로 높은 조건부 확률을 가진 토큰 &quot;C&quot;가 선택됩니다." />
:label:<code>fig_s2s-prob2</code></p>
<p>타임 스텝 3의 기반이 되는 타임 스텝 1과 2의 출력 하위 시퀀스가 :numref:<code>fig_s2s-prob1</code>의 "A"와 "B"에서 :numref:<code>fig_s2s-prob2</code>의 "A"와 "C"로 변경되었으므로,
타임 스텝 3에서 각 토큰의 조건부 확률도 :numref:<code>fig_s2s-prob2</code>에서 변경되었습니다.
타임 스텝 3에서 토큰 "B"를 선택한다고 가정합시다.
이제 타임 스텝 4는 처음 세 타임 스텝의 출력 하위 시퀀스인 "A", "C", "B"에 조건부이며, 이는 :numref:<code>fig_s2s-prob1</code>의 "A", "B", "C"에서 변경되었습니다.
따라서 :numref:<code>fig_s2s-prob2</code>의 타임 스텝 4에서 각 토큰을 생성할 조건부 확률도 :numref:<code>fig_s2s-prob1</code>의 것과 다릅니다.
결과적으로 :numref:<code>fig_s2s-prob2</code>에서 출력 시퀀스 "A", "C", "B", "&lt;eos&gt;"의 조건부 확률은 $0.5\times0.3 \times0.6\times0.6=0.054$이며,
이는 :numref:<code>fig_s2s-prob1</code>의 그리디 검색보다 큽니다.
이 예에서 그리디 검색에 의해 얻은 출력 시퀀스 "A", "B", "C", "&lt;eos&gt;"는 최적이 아닙니다.</p>
<h2 id="전수-검색-exhaustive-search"><a class="header" href="#전수-검색-exhaustive-search">전수 검색 (Exhaustive Search)</a></h2>
<p>목표가 가장 가능성 있는 시퀀스를 얻는 것이라면, 우리는 <em>전수 검색</em>을 사용하는 것을 고려할 수 있습니다:
가능한 모든 출력 시퀀스를 조건부 확률과 함께 열거한 다음, 가장 높은 예측 확률을 기록하는 시퀀스를 출력합니다.</p>
<p>이것이 분명히 우리가 원하는 것을 제공하겠지만,
어휘 크기에 의해 주어지는 거대한 베이스를 가진 시퀀스 길이에 기하급수적인 $\mathcal{O}(\left|\mathcal{Y}\right|^{T'})$의 감당할 수 없는 계산 비용이 수반될 것입니다.
예를 들어 $|y|=10000$이고 $T'=10$일 때(실제 응용 사례와 비교하면 둘 다 작은 숫자임), 우리는 $10000^{10} = 10^{40}$개의 시퀀스를 평가해야 하는데, 이는 이미 예측 가능한 어떤 컴퓨터의 능력도 벗어납니다.
반면 그리디 검색의 계산 비용은 $\mathcal{O}(\left|\mathcal{Y}\right|T')$입니다: 기적적으로 저렴하지만 최적과는 거리가 멉니다.
예를 들어 $|y|=10000$이고 $T'=10$일 때, 우리는 $10000\times10=10^5$개의 시퀀스만 평가하면 됩니다.</p>
<h2 id="빔-검색-beam-search-1"><a class="header" href="#빔-검색-beam-search-1">빔 검색 (Beam Search)</a></h2>
<p>시퀀스 디코딩 전략을 하나의 스펙트럼으로 볼 수 있는데, <em>빔 검색</em>은 그리디 검색의 효율성과 전수 검색의 최적성 사이에서 타협을 취합니다.
빔 검색의 가장 간단한 버전은 단일 하이퍼파라미터인 <em>빔 크기(beam size)</em> $k$로 특징지어집니다.
이 용어를 설명해 보겠습니다.
타임 스텝 1에서 우리는 가장 높은 예측 확률을 가진 $k$개의 토큰을 선택합니다.
각각은 각각 $k$개의 후보 출력 시퀀스의 첫 번째 토큰이 됩니다.
각 후속 타임 스텝에서 이전 타임 스텝의 $k$개 후보 출력 시퀀스를 기반으로,
우리는 $k\left|\mathcal{Y}\right|$개의 가능한 선택지 중에서 가장 높은 예측 확률을 가진 $k$개의 후보 출력 시퀀스를 계속 선택합니다.</p>
<p><img src="chapter_recurrent-modern/../img/beam-search.svg" alt="빔 검색 과정(빔 크기 $=2$, 출력 시퀀스의 최대 길이 $=3$). 후보 출력 시퀀스는 $\mathit{A}$, $\mathit{C}$, $\mathit{AB}$, $\mathit{CE}$, $\mathit{ABD}$, $\mathit{CED}$입니다." />
:label:<code>fig_beam-search</code></p>
<p>:numref:<code>fig_beam-search</code>는 빔 검색의 과정을 예로 들어 보여줍니다.
출력 어휘에 5개의 요소만 포함되어 있다고 가정합시다: $\mathcal{Y} = {A, B, C, D, E}$이며, 그중 하나는 “&lt;eos&gt;”입니다.
빔 크기를 2로 하고 출력 시퀀스의 최대 길이를 3으로 합시다.
타임 스텝 1에서
조건부 확률 $P(y_1 \mid \mathbf{c})$가 가장 높은 토큰이 $A$와 $C$라고 가정합니다.
타임 스텝 2에서 모든 $y_2 \in \mathcal{Y}$에 대해 다음을 계산합니다.</p>
<p>$$\begin{aligned}P(A, y_2 \mid \mathbf{c}) = P(A \mid \mathbf{c})P(y_2 \mid A, \mathbf{c}),\ P(C, y_2 \mid \mathbf{c}) = P(C \mid \mathbf{c})P(y_2 \mid C, \mathbf{c}),\end{aligned}$$</p>
<p>그리고 이 10개 값 중 가장 큰 두 개, 예를 들어 $P(A, B \mid \mathbf{c})$와 $P(C, E \mid \mathbf{c})$를 고릅니다.
그런 다음 타임 스텝 3에서 모든 $y_3 \in \mathcal{Y}$에 대해 다음을 계산합니다.</p>
<p>$$\begin{aligned}P(A, B, y_3 \mid \mathbf{c}) = P(A, B \mid \mathbf{c})P(y_3 \mid A, B, \mathbf{c}),\P(C, E, y_3 \mid \mathbf{c}) = P(C, E \mid \mathbf{c})P(y_3 \mid C, E, \mathbf{c}),\end{aligned}$$</p>
<p>그리고 이 10개 값 중 가장 큰 두 개, 예를 들어 $P(A, B, D \mid \mathbf{c})$와 $P(C, E, D \mid  \mathbf{c})$를 고릅니다.
결과적으로 (i) $A$, (ii) $C$, (iii) $A, B$, (iv) $C, E$, (v) $A, B, D$, (vi) $C, E, D$라는 6개의 후보 출력 시퀀스를 얻게 됩니다.</p>
<p>마지막으로, 이 6개 시퀀스를 기반으로 최종 후보 출력 시퀀스 집합을 얻습니다(예: “&lt;eos&gt;”를 포함한 그 이후 부분 폐기).
그런 다음 다음 점수를 최대화하는 출력 시퀀스를 선택합니다:</p>
<p>$$ \frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c});$$
:eqlabel:<code>eq_beam-search-score</code></p>
<p>여기서 $L$은 최종 후보 시퀀스의 길이이고 $\alpha$는 보통 0.75로 설정됩니다.
긴 시퀀스는 :eqref:<code>eq_beam-search-score</code>의 합산에서 더 많은 로그 항을 갖기 때문에, 분모의 $L^\alpha$ 항은 긴 시퀀스에 페널티를 줍니다.</p>
<p>빔 검색의 계산 비용은 $\mathcal{O}(k\left|\mathcal{Y}\right|T')$입니다.
이 결과는 그리디 검색과 전수 검색의 중간에 있습니다.
그리디 검색은 빔 크기를 1로 설정했을 때 발생하는 빔 검색의 특수한 경우로 취급될 수 있습니다.</p>
<h2 id="요약-summary-43"><a class="header" href="#요약-summary-43">요약 (Summary)</a></h2>
<p>시퀀스 검색 전략에는 그리디 검색, 전수 검색, 빔 검색이 포함됩니다.
빔 검색은 빔 크기의 유연한 선택을 통해 정확도와 계산 비용 사이의 트레이드오프를 제공합니다.</p>
<h2 id="연습-문제-exercises-56"><a class="header" href="#연습-문제-exercises-56">연습 문제 (Exercises)</a></h2>
<ol>
<li>전수 검색을 빔 검색의 특수한 유형으로 취급할 수 있습니까? 왜 그런가요 혹은 왜 아닌가요?</li>
<li>:numref:<code>sec_seq2seq</code>의 기계 번역 문제에 빔 검색을 적용하십시오. 빔 크기가 번역 결과와 예측 속도에 어떤 영향을 미칩니까?</li>
<li>우리는 :numref:<code>sec_rnn-scratch</code>에서 사용자 제공 접두사를 따르는 텍스트를 생성하기 위해 언어 모델링을 사용했습니다. 그것은 어떤 종류의 검색 전략을 사용합니까? 그것을 개선할 수 있습니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/338">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="주의-메커니즘과-트랜스포머-attention-mechanisms-and-transformers"><a class="header" href="#주의-메커니즘과-트랜스포머-attention-mechanisms-and-transformers">주의 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a></h1>
<p>:label:<code>chap_attention-and-transformers</code></p>
<p>딥러닝 붐의 초기에는 주로 다층 퍼셉트론, 합성곱 네트워크 및 순환 네트워크 아키텍처를 사용하여 생성된 결과에 의해 주도되었습니다.
놀랍게도 2010년대 딥러닝의 많은 돌파구를 뒷받침했던 모델 아키텍처는 거의 30년이 지났음에도 불구하고 이전 모델들에 비해 놀라울 정도로 거의 변하지 않았습니다.
ReLU 활성화, 잔차 레이어, 배치 정규화, 드롭아웃 및 적응형 학습률 스케줄과 같은 많은 새로운 방법론적 혁신이 대부분의 실무자들의 도구 세트에 포함되었지만, 핵심 기저 아키텍처는 고전적인 아이디어의 확장된 구현으로 명확하게 인식될 수 있었습니다.
대안적인 아이디어를 제안하는 수천 편의 논문에도 불구하고, 고전적인 합성곱 신경망(:numref:<code>chap_cnn</code>)과 유사한 모델은 컴퓨터 비전에서 <em>최첨단(state-of-the-art)</em> 상태를 유지했으며 Sepp Hochreiter의 원래 LSTM 순환 신경망 설계(:numref:<code>sec_lstm</code>)와 유사한 모델은 자연어 처리의 대부분 응용 분야를 지배했습니다.
틀림없이 그 시점까지 딥러닝의 급격한 출현은 주로 사용 가능한 컴퓨팅 리소스의 변화(GPU를 사용한 병렬 컴퓨팅 혁신 덕분)와 대규모 데이터 리소스의 가용성(저렴한 스토리지 및 인터넷 서비스 덕분)에 기인한 것으로 보였습니다.
이러한 요인들이 실제로 이 기술의 증가하는 힘 뒤에 있는 주요 동인으로 남아 있을 수 있지만, 우리는 마침내 지배적인 아키텍처 지형에서 큰 변화를 목격하고 있습니다.</p>
<p>현재 거의 모든 자연어 처리 작업을 위한 지배적인 모델은 Transformer 아키텍처를 기반으로 합니다.
자연어 처리의 새로운 작업이 주어지면 기본 첫 번째 접근 방식은 대규모 Transformer 기반 사전 훈련 모델(예: BERT :cite:<code>Devlin.Chang.Lee.ea.2018</code>, ELECTRA :cite:<code>clark2019electra</code>, RoBERTa :cite:<code>Liu.Ott.Goyal.ea.2019</code> 또는 Longformer :cite:<code>beltagy2020longformer</code>)을 가져와 필요에 따라 출력 레이어를 조정하고 다운스트림 작업을 위해 사용 가능한 데이터에 대해 모델을 미세 조정하는 것입니다.
OpenAI의 대규모 언어 모델을 중심으로 한 지난 몇 년 동안의 숨 가쁜 뉴스 보도에 관심을 가져왔다면 GPT-2 및 GPT-3 Transformer 기반 모델 :cite:<code>Radford.Wu.Child.ea.2019,brown2020language</code>을 중심으로 한 대화에 귀를 기울여 온 것입니다.
한편, 비전 Transformer는 이미지 인식, 객체 감지, 시맨틱 분할 및 초해상도 :cite:<code>Dosovitskiy.Beyer.Kolesnikov.ea.2021,liu2021swin</code>를 포함한 다양한 비전 작업을 위한 기본 모델로 부상했습니다.
Transformer는 음성 인식 :cite:<code>gulati2020conformer</code>, 강화 학습 :cite:<code>chen2021decision</code> 및 그래프 신경망 :cite:<code>dwivedi2020generalization</code>을 위한 경쟁력 있는 방법으로도 나타났습니다.</p>
<p>Transformer 모델 뒤에 숨겨진 핵심 아이디어는 <em>주의(attention) 메커니즘</em>으로, 원래 기계 번역 :cite:<code>Bahdanau.Cho.Bengio.2014</code>과 같은 시퀀스-투-시퀀스 응용 분야에 적용되는 인코더-디코더 RNN의 향상으로 구상된 혁신입니다.
기계 번역을 위한 최초의 시퀀스-투-시퀀스 모델 :cite:<code>Sutskever.Vinyals.Le.2014</code>에서 전체 입력이 인코더에 의해 단일 고정 길이 벡터로 압축되어 디코더로 공급되었음을 상기할 수 있을 것입니다.
주의 메커니즘 뒤에 숨겨진 직관은 입력을 압축하는 대신 디코더가 매 단계마다 입력 시퀀스를 다시 방문하는 것이 더 나을 수 있다는 것입니다.
더욱이 항상 동일한 입력 표현을 보는 대신 디코더가 특정 디코딩 단계에서 입력 시퀀스의 특정 부분에 선택적으로 집중해야 한다고 상상할 수 있습니다.
Bahdanau의 주의 메커니즘은 디코더가 각 디코딩 단계에서 입력의 다른 부분에 동적으로 <em>주의를 기울일(attend)</em> 수 있는 간단한 수단을 제공했습니다.
높은 수준의 아이디어는 인코더가 원래 입력 시퀀스와 동일한 길이의 표현을 생성할 수 있다는 것입니다.
그런 다음 디코딩 시에 디코더는 (어떤 제어 메커니즘을 통해) 각 타임 스텝에서 입력에 대한 표현의 가중 합으로 구성된 문맥 벡터를 입력으로 받을 수 있습니다.
직관적으로 가중치는 각 단계의 문맥이 각 입력 토큰에 "집중"하는 정도를 결정하며, 핵심은 이 가중치를 할당하는 프로세스를 미분 가능하게 만들어 다른 모든 신경망 파라미터와 함께 학습될 수 있도록 하는 것입니다.</p>
<p>처음에 이 아이디어는 이미 기계 번역 응용 분야를 지배하고 있던 순환 신경망에 대한 놀랍도록 성공적인 향상이었습니다.
이 모델들은 원래의 인코더-디코더 시퀀스-투-시퀀스 아키텍처보다 더 잘 수행되었습니다.
더욱이 연구자들은 주의 가중치 패턴을 검사함으로써 때때로 멋진 질적 통찰력이 나타난다는 점에 주목했습니다.
번역 작업에서 주의 모델은 타겟 언어의 해당 단어를 생성할 때 종종 교차 언어 동의어에 높은 주의 가중치를 할당했습니다.
예를 들어 "my feet hurt"라는 문장을 "j'ai mal au pieds"로 번역할 때, 신경망은 해당 프랑스어 단어 "pieds"를 생성할 때 "feet"의 표현에 높은 주의 가중치를 할당할 수 있습니다.
이러한 통찰력은 주의 모델이 "해석 가능성"을 부여한다는 주장을 자극했지만, 주의 가중치가 정확히 무엇을 의미하는지, 즉 어떻게 해석되어야 하는지는 여전히 모호한 연구 주제로 남아 있습니다.</p>
<p>그러나 주의 메커니즘은 곧 인코더-디코더 순환 신경망의 향상으로서의 유용성과 두드러진 입력을 선택하기 위한 추정된 유용성을 넘어 더 중요한 관심사로 부상했습니다.
:citet:<code>Vaswani.Shazeer.Parmar.ea.2017</code>는 기계 번역을 위해 순환 연결을 완전히 없애고 대신 입력 및 출력 토큰 간의 모든 관계를 캡처하기 위해 영리하게 배열된 주의 메커니즘에 의존하는 Transformer 아키텍처를 제안했습니다.
이 아키텍처는 놀랍도록 잘 수행되었으며, 2018년까지 Transformer는 대부분의 최첨단 자연어 처리 시스템에 나타나기 시작했습니다.
더욱이 동시에 자연어 처리의 지배적인 관행은 어떤 자기 지도 사전 훈련 목표를 최적화하기 위해 거대한 범용 배경 말뭉치에서 대규모 모델을 사전 훈련한 다음, 사용 가능한 다운스트림 데이터를 사용하여 이 모델을 미세 조정하는 것이 되었습니다.
Transformer와 전통적인 아키텍처 사이의 격차는 이 사전 훈련 패러다임에 적용될 때 특히 넓어졌으며, 따라서 Transformer의 부상은 현재 때때로 <em>파운데이션 모델(foundation models)</em> :cite:<code>bommasani2021opportunities</code>이라고 불리는 그러한 대규모 사전 훈련 모델의 부상과 일치했습니다.</p>
<p>이 장에서는 가장 기본적인 직관과 아이디어의 가장 단순한 인스턴스화부터 시작하여 주의 모델을 소개합니다.
그런 다음 Transformer 아키텍처, 비전 Transformer 및 현대 Transformer 기반 사전 훈련 모델의 지형으로 나아갑니다.</p>
<pre><code class="language-toc">:maxdepth: 2

queries-keys-values
attention-pooling
attention-scoring-functions
bahdanau-attention
multihead-attention
self-attention-and-positional-encoding
transformer
vision-transformer
large-pretraining-transformers
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow')
</code></pre>
<h1 id="쿼리-키-값-queries-keys-and-values"><a class="header" href="#쿼리-키-값-queries-keys-and-values">쿼리, 키, 값 (Queries, Keys, and Values)</a></h1>
<p>:label:<code>sec_queries-keys-values</code></p>
<p>지금까지 우리가 검토한 모든 네트워크는 입력이 잘 정의된 크기인 것에 결정적으로 의존했습니다.
예를 들어 ImageNet의 이미지는 $224 \times 224$ 픽셀 크기이며 CNN은 특히 이 크기에 맞춰져 있습니다.
자연어 처리에서도 RNN의 입력 크기는 잘 정의되어 있고 고정되어 있습니다. 가변 크기는 한 번에 하나의 토큰을 순차적으로 처리하거나 특별히 설계된 합성곱 커널을 통해 처리됩니다 :cite:<code>Kalchbrenner.Grefenstette.Blunsom.2014</code>.
이러한 접근 방식은 :numref:<code>sec_seq2seq</code>의 텍스트 변환과 같이 입력이 정보 내용이 변하는 진정한 가변 크기일 때 심각한 문제로 이어질 수 있습니다 :cite:<code>Sutskever.Vinyals.Le.2014</code>.
특히 긴 시퀀스의 경우 이미 생성되었거나 네트워크에서 확인된 모든 것을 추적하기가 상당히 어려워집니다. :citet:<code>yang2016neural</code>이 제안한 것과 같은 명시적인 추적 휴리스틱조차도 제한된 이점만 제공합니다.</p>
<p>이를 데이터베이스와 비교해 보십시오. 가장 단순한 형태의 데이터베이스는 키($k$)와 값($v$)의 모음입니다.
예를 들어, 우리의 데이터베이스 $\mathcal{D}$는 성이 키이고 이름이 값인 {("Zhang", "Aston"), ("Lipton", "Zachary"), ("Li", "Mu"), ("Smola", "Alex"), ("Hu", "Rachel"), ("Werness", "Brent")} 튜플로 구성될 수 있습니다.
우리는 $\mathcal{D}$에 대해 연산을 수행할 수 있습니다. 예를 들어 "Li"에 대한 정확한 쿼리($q$)는 값 "Mu"를 반환할 것입니다.
("Li", "Mu")가 $\mathcal{D}$의 레코드가 아니라면 유효한 답변이 없을 것입니다. 근사 매칭도 허용한다면 대신 ("Lipton", "Zachary")를 검색할 것입니다. 이 아주 간단하고 사소한 예제는 그럼에도 불구하고 우리에게 몇 가지 유용한 것들을 가르쳐 줍니다:</p>
<ul>
<li>우리는 데이터베이스 크기에 관계없이 유효하도록 ($k$,$v$) 쌍에 대해 작동하는 쿼리 $q$를 설계할 수 있습니다.</li>
<li>동일한 쿼리가 데이터베이스의 내용에 따라 다른 답변을 받을 수 있습니다.</li>
<li>큰 상태 공간(데이터베이스)에서 작동하기 위해 실행되는 "코드"는 상당히 간단할 수 있습니다(예: 정확한 매칭, 근사 매칭, 상위 $k$개).</li>
<li>작업을 효과적으로 만들기 위해 데이터베이스를 압축하거나 단순화할 필요가 없습니다.</li>
</ul>
<p>딥러닝을 설명하기 위한 목적이 아니었다면 분명히 여기서 간단한 데이터베이스를 소개하지 않았을 것입니다.
실제로 이것은 지난 10년 동안 딥러닝에 도입된 가장 흥미로운 개념 중 하나인 <em>주의(attention) 메커니즘</em>으로 이어집니다 :cite:<code>Bahdanau.Cho.Bengio.2014</code>.
기계 번역에 대한 구체적인 응용은 나중에 다룰 것입니다. 지금은 다음을 고려하십시오: $\mathcal{D} \stackrel{\textrm{def}}{=} {(\mathbf{k}_1, \mathbf{v}_1), \ldots (\mathbf{k}_m, \mathbf{v}_m)}$를 $m$개의 <em>키</em>와 <em>값</em> 튜플로 구성된 데이터베이스라고 합시다. 또한 $\mathbf{q}$를 <em>쿼리</em>라고 합시다. 그러면 $\mathcal{D}$에 대한 *주의(attention)*를 다음과 같이 정의할 수 있습니다.</p>
<p>$$\textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i,$$
:eqlabel:<code>eq_attention_pooling</code></p>
<p>여기서 $\alpha(\mathbf{q}, \mathbf{k}_i) \in \mathbb{R}$ ($i = 1, \ldots, m$)는 스칼라 주의 가중치입니다. 이 연산 자체는 일반적으로 *어텐션 풀링(attention pooling)*이라고 합니다. *주의(attention)*라는 이름은 이 연산이 가중치 $\alpha$가 유의미한(즉, 큰) 항들에 특별한 주의를 기울인다는 사실에서 유래했습니다. 이와 같이 $\mathcal{D}$에 대한 주의는 데이터베이스에 포함된 값들의 선형 결합을 생성합니다. 사실 이것은 단 하나의 가중치만 제외하고 모두 0인 특수한 경우로 위의 예제를 포함합니다. 우리는 몇 가지 특수한 경우를 가집니다:</p>
<ul>
<li>가중치 $\alpha(\mathbf{q}, \mathbf{k}_i)$가 0 이상입니다. 이 경우 주의 메커니즘의 출력은 값 $\mathbf{v}_i$들에 의해 생성된 볼록 원뿔(convex cone)에 포함됩니다.</li>
<li>가중치 $\alpha(\mathbf{q}, \mathbf{k}_i)$가 볼록 조합(convex combination)을 이룹니다. 즉, $\sum_i \alpha(\mathbf{q}, \mathbf{k}_i) = 1$이고 모든 $i$에 대해 $\alpha(\mathbf{q}, \mathbf{k}_i) \geq 0$입니다. 이것이 딥러닝에서 가장 일반적인 설정입니다.</li>
<li>가중치 중 정확히 하나만 1이고 나머지는 모두 0입니다. 이것은 전통적인 데이터베이스 쿼리와 유사합니다.</li>
<li>모든 가중치가 동일합니다. 즉, 모든 $i$에 대해 $\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{1}{m}$입니다. 이는 전체 데이터베이스에 걸쳐 평균을 내는 것과 같으며, 딥러닝에서는 평균 풀링(average pooling)이라고도 합니다.</li>
</ul>
<p>가중치의 합이 1이 되도록 보장하는 일반적인 전략은 다음을 통해 정규화하는 것입니다.</p>
<p>$$\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\alpha(\mathbf{q}, \mathbf{k}_i)}{{\sum_j} \alpha(\mathbf{q}, \mathbf{k}_j)}.$$</p>
<p>특히 가중치가 0 이상이 되도록 보장하기 위해 지수화(exponentiation)를 사용할 수 있습니다. 즉, 이제 <em>임의의</em> 함수 $a(\mathbf{q}, \mathbf{k})$를 선택한 다음 다항 모델에 사용되는 소프트맥스 연산을 다음과 같이 적용할 수 있습니다.</p>
<p>$$\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_j \exp(a(\mathbf{q}, \mathbf{k}_j))}. $$
:eqlabel:<code>eq_softmax_attention</code></p>
<p>이 연산은 모든 딥러닝 프레임워크에서 즉시 사용할 수 있습니다. 미분 가능하고 기울기가 결코 사라지지 않는데, 이는 모델에서 바람직한 속성입니다. 하지만 위에서 소개한 주의 메커니즘이 유일한 옵션은 아닙니다. 예를 들어 강화 학습 방법을 사용하여 훈련할 수 있는 미분 불가능한 주의 모델을 설계할 수 있습니다 :cite:<code>Mnih.Heess.Graves.ea.2014</code>. 예상할 수 있듯이 그러한 모델을 훈련하는 것은 매우 복잡합니다. 결과적으로 현대 주의 연구의 대부분은 :numref:<code>fig_qkv</code>에 설명된 프레임워크를 따릅니다. 따라서 우리는 이러한 미분 가능한 메커니즘 패밀리에 설명을 집중합니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/qkv.svg" alt="주의 메커니즘은 쿼리 $\mathbf{q}$와 키 $\mathbf{k}\mathit{i}$ 사이의 호환성에 따라 가중치가 파생되는 어텐션 풀링을 통해 값 $\mathbf{v}\mathit{i}$에 대한 선형 결합을 계산합니다." />
:label:<code>fig_qkv</code></p>
<p>상당히 놀라운 점은 키와 값의 집합에 대해 실행되는 실제 "코드", 즉 쿼리가 작동할 공간이 상당히 큼에도 불구하고 매우 간결할 수 있다는 것입니다. 이것은 학습해야 할 파라미터가 너무 많이 필요하지 않기 때문에 네트워크 레이어에 바람직한 속성입니다. 마찬가지로 편리한 점은 어텐션 풀링 연산이 수행되는 방식을 변경할 필요 없이 주의 메커니즘이 임의로 큰 데이터베이스에서 작동할 수 있다는 사실입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=2}">%%tab pytorch
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from jax import numpy as jnp
</code></pre>
<h2 id="시각화-visualization-1"><a class="header" href="#시각화-visualization-1">시각화 (Visualization)</a></h2>
<p>주의 메커니즘의 장점 중 하나는 특히 가중치가 0 이상이고 합이 1일 때 상당히 직관적일 수 있다는 것입니다. 이 경우 우리는 큰 가중치를 모델이 관련성 있는 구성 요소를 선택하는 방법으로 <em>해석</em>할 수 있습니다. 이것은 좋은 직관이지만, 그것이 단지 <em>직관</em>일 뿐이라는 것을 기억하는 것이 중요합니다. 그럼에도 불구하고 다양한 쿼리를 적용할 때 주어진 키 세트에 미치는 영향을 시각화하고 싶을 수 있습니다. 이 함수는 나중에 유용하게 사용될 것입니다.</p>
<p>따라서 <code>show_heatmaps</code> 함수를 정의합니다. 이 함수는 입력으로 (주의 가중치의) 행렬을 받는 것이 아니라 4개의 축을 가진 텐서를 받아 다양한 쿼리와 가중치 배열을 허용합니다. 결과적으로 입력 <code>matrices</code>는 (표시할 행 수, 표시할 열 수, 쿼리 수, 키 수) 모양을 갖습니다. 이는 나중에 Transformer를 설계하는 작동 방식을 시각화하고 싶을 때 유용할 것입니다.</p>
<pre><code class="language-{.python .input  n=17}">%%tab all
#@save
def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
                  cmap='Reds'):
    """행렬의 히트맵을 보여줍니다."""
    d2l.use_svg_display()
    num_rows, num_cols, _, _ = matrices.shape
    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
                                 sharex=True, sharey=True, squeeze=False)
    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
            if tab.selected('pytorch', 'mxnet', 'tensorflow'):
                pcm = ax.imshow(d2l.numpy(matrix), cmap=cmap)
            if tab.selected('jax'):
                pcm = ax.imshow(matrix, cmap=cmap)
            if i == num_rows - 1:
                ax.set_xlabel(xlabel)
            if j == 0:
                ax.set_ylabel(ylabel)
            if titles:
                ax.set_title(titles[j])
    fig.colorbar(pcm, ax=axes, shrink=0.6);
</code></pre>
<p>간단한 정상성 확인으로, 쿼리와 키가 동일할 때만 주의 가중치가 1인 경우를 나타내는 단위 행렬을 시각화해 보겠습니다.</p>
<pre><code class="language-{.python .input  n=20}">%%tab all
attention_weights = d2l.reshape(d2l.eye(10), (1, 1, 10, 10))
show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')
</code></pre>
<h2 id="요약-summary-44"><a class="header" href="#요약-summary-44">요약 (Summary)</a></h2>
<p>주의 메커니즘을 사용하면 많은 (키, 값) 쌍에서 데이터를 집계할 수 있습니다. 지금까지 우리의 논의는 데이터를 풀링하는 방법을 설명하는 꽤 추상적인 것이었습니다. 우리는 아직 그 신비로운 쿼리, 키, 값이 어디에서 발생할 수 있는지 설명하지 않았습니다. 여기서 약간의 직관이 도움이 될 수 있습니다: 예를 들어, 회귀 설정에서 쿼리는 회귀가 수행되어야 할 위치에 해당할 수 있습니다. 키는 과거 데이터가 관찰된 위치이고 값은 (회귀) 값 자체입니다. 이것은 우리가 다음 섹션에서 공부할 소위 나다라야-왓슨(Nadaraya--Watson) 추정기 :cite:<code>Nadaraya.1964,Watson.1964</code>입니다.</p>
<p>설계상 주의 메커니즘은 신경망이 집합에서 요소를 선택하고 표현들에 대해 관련 가중 합을 구성할 수 있게 하는 <em>미분 가능한</em> 제어 수단을 제공합니다.</p>
<h2 id="연습-문제-exercises-57"><a class="header" href="#연습-문제-exercises-57">연습 문제 (Exercises)</a></h2>
<ol>
<li>클래식 데이터베이스에서 사용되는 근사 (키, 쿼리) 매칭을 다시 구현하고 싶다면 어떤 주의 함수를 선택하시겠습니까?</li>
<li>주의 함수가 $a(\mathbf{q}, \mathbf{k}_i) = \mathbf{q}^\top \mathbf{k}_i$로 주어지고 $i = 1, \ldots, m$에 대해 $\mathbf{k}_i = \mathbf{v}<em>i$라고 가정합시다. :eqref:<code>eq_softmax_attention</code>의 소프트맥스 정규화를 사용할 때 키들에 대한 확률 분포를 $p(\mathbf{k}<em>i; \mathbf{q})$로 표시합시다. $\nabla</em>{\mathbf{q}} \mathop{\textrm{Attention}}(\mathbf{q}, \mathcal{D}) = \textrm{Cov}</em>{p(\mathbf{k}_i; \mathbf{q})}[\mathbf{k}_i]$임을 증명하십시오.</li>
<li>주의 메커니즘을 사용하여 미분 가능한 검색 엔진을 설계하십시오.</li>
<li>Squeeze and Excitation Networks :cite:<code>Hu.Shen.Sun.2018</code>의 설계를 검토하고 주의 메커니즘의 렌즈를 통해 해석해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/1596">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1592">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1710">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18024">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="유사성에-의한-어텐션-풀링-attention-pooling-by-similarity"><a class="header" href="#유사성에-의한-어텐션-풀링-attention-pooling-by-similarity">유사성에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></h1>
<p>:label:<code>sec_attention-pooling</code></p>
<p>이제 주의 메커니즘의 주요 구성 요소를 소개했으므로, 이를 다소 고전적인 설정, 즉 커널 밀도 추정을 통한 회귀 및 분류에서 사용해 봅시다 :cite:<code>Nadaraya.1964,Watson.1964</code>. 이 우회로는 단순히 추가 배경을 제공합니다: 전적으로 선택 사항이며 필요한 경우 건너뛸 수 있습니다.
핵심적으로 나다라야-왓슨(Nadaraya--Watson) 추정기는 쿼리 $\mathbf{q}$와 키 $\mathbf{k}$를 연관시키는 유사성 커널 $\alpha(\mathbf{q}, \mathbf{k})$에 의존합니다. 일반적인 커널들은 다음과 같습니다.</p>
<p>$$\begin{aligned}
\alpha(\mathbf{q}, \mathbf{k}) &amp; = \exp\left(-\frac{1}{2} |\mathbf{q} - \mathbf{k}|^2 \right) &amp;&amp; \textrm{가우시안;} \
\alpha(\mathbf{q}, \mathbf{k}) &amp; = 1 \textrm{ if } |\mathbf{q} - \mathbf{k}| \leq 1 &amp;&amp; \textrm{박스카(Boxcar);} \
\alpha(\mathbf{q}, \mathbf{k}) &amp; = \mathop{\mathrm{max}}\left(0, 1 - |\mathbf{q} - \mathbf{k}|\right) &amp;&amp; \textrm{에파네치니코프(Epanechikov).}<code> $$</code></p>
<p>우리가 선택할 수 있는 더 많은 선택지가 있습니다. 더 광범위한 검토와 커널 선택이 때때로 *파젠 윈도우(Parzen Windows)*라고도 불리는 커널 밀도 추정 :cite:<code>parzen1957consistent</code>과 어떻게 관련되는지는 <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)">위키피디아 문서</a>를 참조하십시오. 모든 커널은 휴리스틱하며 튜닝될 수 있습니다. 예를 들어 글로벌 기준뿐만 아니라 좌표별 기준으로 너비를 조정할 수 있습니다. 어쨌든 이들 모두는 회귀와 분류 모두에 대해 다음과 같은 방정식으로 이어집니다:</p>
<p>$$f(\mathbf{q}) = \sum_i \mathbf{v}_i \frac{\alpha(\mathbf{q}, \mathbf{k}_i)}{\sum_j \alpha(\mathbf{q}, \mathbf{k}_j)}.$$`</p>
<p>특성과 레이블에 대한 관찰 $(\mathbf{x}_i, y_i)$이 있는 (스칼라) 회귀의 경우, $\mathbf{v}_i = y_i$는 스칼라, $\mathbf{k}_i = \mathbf{x}_i$는 벡터이며, 쿼리 $\mathbf{q}$는 $f$가 평가되어야 할 새로운 위치를 나타냅니다. (다중 클래스) 분류의 경우 $y_i$의 원-핫 인코딩을 사용하여 $\mathbf{v}_i$를 얻습니다. 이 추정기의 편리한 속성 중 하나는 훈련이 필요 없다는 것입니다. 더욱이 데이터 양이 증가함에 따라 커널을 적절히 좁히면 이 접근 방식은 일관성이 있습니다 :cite:<code>mack1982weak</code>. 즉, 통계적으로 최적인 어떤 솔루션으로 수렴할 것입니다. 몇 가지 커널을 검사하는 것으로 시작해 봅시다.</p>
<pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
npx.set_np()
d2l.use_svg_display()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
import numpy as np

d2l.use_svg_display()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
import numpy as np

d2l.use_svg_display()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
import jax
from jax import numpy as jnp
from flax import linen as nn
</code></pre>
<h2 id="커널과-데이터-kernels-and-data"><a class="header" href="#커널과-데이터-kernels-and-data">[<strong>커널과 데이터 (Kernels and Data)</strong>]</a></h2>
<p>이 섹션에서 정의된 모든 커널 $\alpha(\mathbf{k}, \mathbf{q})$는 <em>평행 이동 및 회전 불변</em>입니다. 즉, $\mathbf{k}$와 $\mathbf{q}$를 같은 방식으로 이동하고 회전해도 $\alpha$의 값은 변하지 않습니다. 단순함을 위해 우리는 스칼라 인수 $k, q \in \mathbb{R}$을 선택하고 키 $k = 0$을 원점으로 잡습니다. 이는 다음을 산출합니다:</p>
<pre><code class="language-{.python .input}">%%tab all
# 몇 가지 커널 정의
def gaussian(x):
    return d2l.exp(-x**2 / 2)

def boxcar(x):
    return d2l.abs(x) &lt; 1.0

def constant(x):
    return 1.0 + 0 * x
 
if tab.selected('pytorch'):
    def epanechikov(x):
        return torch.max(1 - d2l.abs(x), torch.zeros_like(x))
if tab.selected('mxnet'):
    def epanechikov(x):
        return np.maximum(1 - d2l.abs(x), 0)
if tab.selected('tensorflow'):
    def epanechikov(x):
        return tf.maximum(1 - d2l.abs(x), 0)
if tab.selected('jax'):
    def epanechikov(x):
        return jnp.maximum(1 - d2l.abs(x), 0)
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
fig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))

kernels = (gaussian, boxcar, constant, epanechikov)
names = ('Gaussian', 'Boxcar', 'Constant', 'Epanechikov')
x = d2l.arange(-2.5, 2.5, 0.1)
for kernel, name, ax in zip(kernels, names, axes):
    if tab.selected('pytorch', 'mxnet', 'tensorflow'):
        ax.plot(d2l.numpy(x), d2l.numpy(kernel(x)))
    if tab.selected('jax'):
        ax.plot(x, kernel(x))
    ax.set_xlabel(name)

d2l.plt.show()
</code></pre>
<p>서로 다른 커널은 범위와 매끄러움에 대한 서로 다른 개념에 해당합니다. 예를 들어 박스카 커널은 거리 1(또는 달리 정의된 하이퍼파라미터) 이내의 관찰에만 무차별적으로 주의를 기울입니다.</p>
<p>나다라야-왓슨 추정이 작동하는 것을 보기 위해 몇 가지 훈련 데이터를 정의해 봅시다. 다음에서는 다음과 같은 종속성을 사용합니다.</p>
<p>$$y_i = 2\sin(x_i) + x_i + \epsilon,$$`</p>
<p>여기서 $\epsilon$은 평균 0과 단위 분산을 갖는 정규 분포에서 추출됩니다. 40개의 훈련 예제를 추출합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def f(x):
    return 2 * d2l.sin(x) + x

n = 40
if tab.selected('pytorch'):
    x_train, _ = torch.sort(d2l.rand(n) * 5)
    y_train = f(x_train) + d2l.randn(n)
if tab.selected('mxnet'):
    x_train = np.sort(d2l.rand(n) * 5, axis=None)
    y_train = f(x_train) + d2l.randn(n)
if tab.selected('tensorflow'):
    x_train = tf.sort(d2l.rand((n,1)) * 5, 0)
    y_train = f(x_train) + d2l.normal((n, 1))
if tab.selected('jax'):
    x_train = jnp.sort(jax.random.uniform(d2l.get_key(), (n,)) * 5)
    y_train = f(x_train) + jax.random.normal(d2l.get_key(), (n,))
x_val = d2l.arange(0, 5, 0.1)
y_val = f(x_val)
</code></pre>
<h2 id="나다라야-왓슨-회귀를-통한-어텐션-풀링-attention-pooling-via-nadaraya--watson-regression"><a class="header" href="#나다라야-왓슨-회귀를-통한-어텐션-풀링-attention-pooling-via-nadaraya--watson-regression">[<strong>나다라야-왓슨 회귀를 통한 어텐션 풀링 (Attention Pooling via Nadaraya--Watson Regression)</strong>]</a></h2>
<p>데이터와 커널이 준비되었으므로 필요한 것은 커널 회귀 추정치를 계산하는 함수뿐입니다. 우리는 또한 약간의 진단을 수행하기 위해 상대적인 커널 가중치를 얻고 싶습니다. 따라서 먼저 모든 훈련 특성(공변량) <code>x_train</code>과 모든 검증 특성 <code>x_val</code> 사이의 커널을 계산합니다. 이는 행렬을 산출하며, 이를 나중에 정규화합니다. 훈련 레이블 <code>y_train</code>과 곱하면 추정치를 얻습니다.</p>
<p>:eqref:<code>eq_attention_pooling</code>의 어텐션 풀링을 상기해 보십시오. 각 검증 특성을 쿼리로 하고, 각 훈련 특성-레이블 쌍을 키-값 쌍으로 합시다. 결과적으로 정규화된 상대 커널 가중치(아래의 <code>attention_w</code>)가 *주의 가중치(attention weights)*가 됩니다.</p>
<pre><code class="language-{.python .input}">%%tab all
def nadaraya_watson(x_train, y_train, x_val, kernel):
    dists = d2l.reshape(x_train, (-1, 1)) - d2l.reshape(x_val, (1, -1))
    # 각 열/행은 각 쿼리/키에 해당합니다
    k = d2l.astype(kernel(dists), d2l.float32)
    # 각 쿼리에 대한 키들에 대한 정규화
    attention_w = k / d2l.reduce_sum(k, 0)
    if tab.selected('pytorch'):
        y_hat = y_train@attention_w
    if tab.selected('mxnet'):
        y_hat = np.dot(y_train, attention_w)
    if tab.selected('tensorflow'):
        y_hat = d2l.transpose(d2l.transpose(y_train)@attention_w)
    if tab.selected('jax'):
        y_hat = y_train@attention_w
    return y_hat, attention_w
</code></pre>
<p>서로 다른 커널이 생성하는 추정치의 종류를 살펴봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
def plot(x_train, y_train, x_val, y_val, kernels, names, attention=False):
    fig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))
    for kernel, name, ax in zip(kernels, names, axes):
        y_hat, attention_w = nadaraya_watson(x_train, y_train, x_val, kernel)
        if attention:
            if tab.selected('pytorch', 'mxnet', 'tensorflow'):
                pcm = ax.imshow(d2l.numpy(attention_w), cmap='Reds')
            if tab.selected('jax'):
                pcm = ax.imshow(attention_w, cmap='Reds')
        else:
            ax.plot(x_val, y_hat)
            ax.plot(x_val, y_val, 'm--')
            ax.plot(x_train, y_train, 'o', alpha=0.5);
        ax.set_xlabel(name)
        if not attention:
            ax.legend(['y_hat', 'y'])
    if attention:
        fig.colorbar(pcm, ax=axes, shrink=0.7)
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
plot(x_train, y_train, x_val, y_val, kernels, names)
</code></pre>
<p>가장 눈에 띄는 것은 세 가지 사소하지 않은 커널(Gaussian, Boxcar, Epanechikov) 모두가 실제 함수에서 그리 멀지 않은 상당히 실행 가능한 추정치를 생성한다는 것입니다. 사소한 추정치 $f(x) = \frac{1}{n} \sum_i y_i$로 이어지는 상수(constant) 커널만이 다소 비현실적인 결과를 생성합니다. 주의 가중치를 좀 더 자세히 살펴봅시다:</p>
<pre><code class="language-{.python .input}">%%tab all
plot(x_train, y_train, x_val, y_val, kernels, names, attention=True)
</code></pre>
<p>시각화는 Gaussian, Boxcar, Epanechikov에 대한 추정치가 왜 매우 유사한지 명확하게 보여줍니다: 커널의 함수 형태가 다름에도 불구하고 매우 유사한 주의 가중치로부터 도출되기 때문입니다. 이것이 항상 그런 것인지에 대한 의문이 생깁니다.</p>
<h2 id="어텐션-풀링-조정하기-adapting-attention-pooling"><a class="header" href="#어텐션-풀링-조정하기-adapting-attention-pooling">[<strong>어텐션 풀링 조정하기 (Adapting Attention Pooling)</strong>]</a></h2>
<p>가우시안 커널을 다른 너비의 커널로 교체할 수 있습니다. 즉, $\alpha(\mathbf{q}, \mathbf{k}) = \exp\left(-\frac{1}{2 \sigma^2} |\mathbf{q} - \mathbf{k}|^2 \right)$를 사용할 수 있습니다. 여기서 $\sigma^2$은 커널의 너비를 결정합니다. 이것이 결과에 영향을 미치는지 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
sigmas = (0.1, 0.2, 0.5, 1)
names = ['Sigma ' + str(sigma) for sigma in sigmas]

def gaussian_with_width(sigma): 
    return (lambda x: d2l.exp(-x**2 / (2*sigma**2)))

kernels = [gaussian_with_width(sigma) for sigma in sigmas]
plot(x_train, y_train, x_val, y_val, kernels, names)
</code></pre>
<p>명백히 커널이 좁을수록 추정치가 덜 매끄럽습니다. 동시에 국소적인 변화에 더 잘 적응합니다. 해당 주의 가중치를 살펴봅시다.</p>
<pre><code class="language-{.python .input}">%%tab all
plot(x_train, y_train, x_val, y_val, kernels, names, attention=True)
</code></pre>
<p>예상대로 커널이 좁을수록 큰 주의 가중치의 범위가 좁아집니다. 또한 동일한 너비를 선택하는 것이 이상적이지 않을 수 있음이 분명합니다. 실제로 :citet:<code>Silverman86</code>은 국소 밀도에 의존하는 휴리스틱을 제안했습니다. 더 많은 그러한 "트릭"들이 제안되었습니다. 예를 들어 :citet:<code>norelli2022asif</code>는 교차 모달 이미지 및 텍스트 표현을 설계하기 위해 유사한 최근접 이웃 보간 기술을 사용했습니다.</p>
<p>예리한 독자는 왜 우리가 반세기 이상 된 방법에 대해 이렇게 깊이 있게 파고드는지 의아해할 수 있습니다. 첫째, 그것은 현대 주의 메커니즘의 가장 초기 전구체 중 하나입니다. 둘째, 시각화에 좋습니다. 셋째, 그리고 마찬가지로 중요한 것은 수작업으로 만든 주의 메커니즘의 한계를 보여줍니다. 훨씬 더 나은 전략은 쿼리와 키에 대한 표현을 학습함으로써 메커니즘을 <em>학습</em>하는 것입니다. 이것이 우리가 다음 섹션에서 착수할 작업입니다.</p>
<h2 id="요약-summary-45"><a class="header" href="#요약-summary-45">요약 (Summary)</a></h2>
<p>나다라야-왓슨 커널 회귀는 현재 주의 메커니즘의 초기 전구체입니다.
분류 또는 회귀를 위해 훈련이나 튜닝 없이 직접 사용할 수 있습니다.
주의 가중치는 쿼리와 키 사이의 유사성(또는 거리)에 따라, 그리고 얼마나 많은 유사한 관찰이 가능한지에 따라 할당됩니다.</p>
<h2 id="연습-문제-exercises-58"><a class="header" href="#연습-문제-exercises-58">연습 문제 (Exercises)</a></h2>
<ol>
<li>파젠 윈도우(Parzen windows) 밀도 추정치는 $\hat{p}(\mathbf{x}) = \frac{1}{n} \sum_i k(\mathbf{x}, \mathbf{x}_i)$로 주어집니다. 이진 분류의 경우 파젠 윈도우에 의해 얻은 함수 $\hat{p}(\mathbf{x}, y=1) - \hat{p}(\mathbf{x}, y=-1)$이 나다라야-왓슨 분류와 동일함을 증명하십시오.</li>
<li>나다라야-왓슨 회귀에서 커널 너비에 대한 좋은 값을 학습하기 위해 확률적 경사 하강법을 구현하십시오.
<ol>
<li>$(f(\mathbf{x_i}) - y_i)^2$를 직접 최소화하기 위해 위의 추정치를 사용하면 어떤 일이 발생합니까? 힌트: $y_i$는 $f$를 계산하는 데 사용되는 항의 일부입니다.</li>
<li>$f(\mathbf{x}_i)$ 추정치에서 $(\mathbf{x}_i, y_i)$를 제거하고 커널 너비에 대해 최적화하십시오. 여전히 과대적합이 관찰됩니까?</li>
</ol>
</li>
<li>모든 $\mathbf{x}$가 단위 구 위에 있다고 가정합시다. 즉, 모두 $|\mathbf{x}| = 1$을 만족합니다. 지수 내의 $|\mathbf{x} - \mathbf{x}_i|^2$ 항을 단순화할 수 있습니까? 힌트: 나중에 이것이 내적 주의(dot product attention)와 매우 밀접한 관련이 있음을 보게 될 것입니다.</li>
<li>:citet:<code>mack1982weak</code>가 나다라야-왓슨 추정이 일관됨을 증명했음을 상기하십시오. 데이터를 더 많이 얻을수록 주의 메커니즘의 스케일을 얼마나 빨리 줄여야 합니까? 답변에 대한 직관을 제공하십시오. 데이터의 차원성에 의존합니까? 어떻게 그렇습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/1598">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1599">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3866">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18026">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="어텐션-스코어-함수-attention-scoring-functions"><a class="header" href="#어텐션-스코어-함수-attention-scoring-functions">어텐션 스코어 함수 (Attention Scoring Functions)</a></h1>
<p>:label:<code>sec_attention-scoring-functions</code></p>
<p>:numref:<code>sec_attention-pooling</code>에서 우리는 쿼리와 키 사이의 상호 작용을 모델링하기 위해 가우시안 커널을 포함한 여러 가지 거리 기반 커널을 사용했습니다.
알고 보니 거리 함수는 내적(dot product)보다 계산 비용이 약간 더 많이 듭니다.
따라서 주의 가중치가 음수가 되지 않도록 보장하는 소프트맥스 연산과 함께,
계산이 더 간단한 :eqref:<code>eq_softmax_attention</code> 및 :numref:<code>fig_attention_output</code>의 <em>어텐션 스코어 함수(attention scoring functions)</em> $a$에 많은 노력이 기울여졌습니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/attention-output.svg" alt="주의 가중 평균으로 어텐션 풀링의 출력을 계산합니다. 여기서 가중치는 어텐션 스코어 함수 $\mathit{a}$와 소프트맥스 연산으로 계산됩니다." />
:label:<code>fig_attention_output</code></p>
<pre><code class="language-{.python .input}">%%tab mxnet
import math
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import math
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
import jax
import math
</code></pre>
<h2 id="내적-주의-dot-product-attention"><a class="header" href="#내적-주의-dot-product-attention">[<strong>내적 주의 (Dot Product Attention)</strong>]</a></h2>
<p>가우시안 커널의 주의 함수(지수화 제외)를 잠시 검토해 봅시다:</p>
<p>$$
a(\mathbf{q}, \mathbf{k}_i) = -\frac{1}{2} |\mathbf{q} - \mathbf{k}_i|^2  = \mathbf{q}^\top \mathbf{k}_i -\frac{1}{2} |\mathbf{k}_i|^2  -\frac{1}{2} |\mathbf{q}|^2.
$$</p>
<p>먼저, 마지막 항은 $\mathbf{q}$에만 의존한다는 점에 유의하십시오. 따라서 모든 $(\mathbf{q}, \mathbf{k}_i)$ 쌍에 대해 동일합니다.
:eqref:<code>eq_softmax_attention</code>에서와 같이 주의 가중치를 1로 정규화하면 이 항은 완전히 사라집니다.
둘째, 배치 및 레이어 정규화(나중에 논의됨)는 모두 잘 제한된, 종종 일정한 노름 $|\mathbf{k}_i|$을 갖는 활성화로 이어집니다.
예를 들어 키 $\mathbf{k}_i$가 레이어 정규화에 의해 생성된 경우입니다.
따라서 결과에 큰 변화 없이 $a$의 정의에서 이를 생략할 수 있습니다.</p>
<p>마지막으로, 지수 함수의 인수의 크기 정도를 제어해야 합니다.
쿼리 $\mathbf{q} \in \mathbb{R}^d$와 키 $\mathbf{k}_i \in \mathbb{R}^d$의 모든 요소가 평균이 0이고 분산이 1인 독립적이고 동일하게 추출된 확률 변수라고 가정합니다.
두 벡터 사이의 내적은 평균이 0이고 분산이 $d$입니다.
벡터 길이에 관계없이 내적의 분산이 1로 유지되도록 하기 위해, <em>스케일드 내적 주의(scaled dot product attention)</em> 스코어 함수를 사용합니다.
즉, 내적을 $1/\sqrt{d}$로 재조정합니다.
따라서 우리는 예를 들어 Transformer :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code>에서 사용되는 첫 번째로 흔히 사용되는 주의 함수에 도달합니다:</p>
<p>$$ a(\mathbf{q}, \mathbf{k}_i) = \mathbf{q}^\top \mathbf{k}_i / \sqrt{d}.$$
:eqlabel:<code>eq_dot_product_attention</code></p>
<p>주의 가중치 $\alpha$는 여전히 정규화가 필요하다는 점에 유의하십시오.
우리는 소프트맥스 연산을 사용하여 :eqref:<code>eq_softmax_attention</code>을 통해 이를 더욱 단순화할 수 있습니다:</p>
<p>$$\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(\mathbf{q}^\top \mathbf{k}<em>i / \sqrt{d})}{\sum</em>{j=1} \exp(\mathbf{q}^\top \mathbf{k}_j / \sqrt{d})}.$$
:eqlabel:<code>eq_attn-scoring-alpha</code></p>
<p>알고 보니 모든 인기 있는 주의 메커니즘은 소프트맥스를 사용하므로, 이 장의 나머지 부분에서는 그것으로 제한하겠습니다.</p>
<h2 id="편의-함수-convenience-functions"><a class="header" href="#편의-함수-convenience-functions">편의 함수 (Convenience Functions)</a></h2>
<p>주의 메커니즘을 효율적으로 배포하기 위해 몇 가지 함수가 필요합니다.
여기에는 가변 길이 문자열(자연어 처리에 흔함)을 처리하기 위한 도구와 미니배치에 대한 효율적인 평가를 위한 도구(배치 행렬 곱셈)가 포함됩니다.</p>
<h3 id="마스킹된-소프트맥스-연산-masked-softmax-operation"><a class="header" href="#마스킹된-소프트맥스-연산-masked-softmax-operation">[<strong>마스킹된 소프트맥스 연산 (Masked Softmax Operation)</strong>]</a></h3>
<p>주의 메커니즘의 가장 인기 있는 응용 중 하나는 시퀀스 모델입니다. 따라서 우리는 서로 다른 길이의 시퀀스를 처리할 수 있어야 합니다.
어떤 경우에는 그러한 시퀀스들이 동일한 미니배치에 포함될 수 있으며, 더 짧은 시퀀스에 대해 더미 토큰으로 패딩해야 합니다(:numref:<code>sec_machine_translation</code>의 예 참조).
이러한 특수 토큰은 의미를 전달하지 않습니다. 예를 들어 다음과 같은 세 문장이 있다고 가정해 봅시다:</p>
<pre><code>Dive  into  Deep    Learning 
Learn to    code    &lt;blank&gt;
Hello world &lt;blank&gt; &lt;blank&gt;
</code></pre>
<p>우리 주의 모델에서 공백을 원하지 않으므로, 단순히 $\sum_{i=1}^n \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}<em>i$를 실제 문장 길이인 $l \leq n$까지인 $\sum</em>{i=1}^l \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i$로 제한하면 됩니다.
이것은 매우 흔한 문제이므로 *마스킹된 소프트맥스 연산(masked softmax operation)*이라는 이름이 있습니다.</p>
<p>구현해 봅시다. 실제로 구현에서는 $i &gt; l$에 대해 $\mathbf{v}_i$의 값을 0으로 설정하여 아주 살짝 속임수를 씁니다.
게다가 실제로 기울기와 값에 대한 기여를 사라지게 하기 위해 주의 가중치를 $-10^{6}$과 같은 큰 음수로 설정합니다.
이는 선형 대수 커널과 연산자가 GPU에 고도로 최적화되어 있으며, 조건부(if then else) 문이 있는 코드보다는 계산에서 약간 낭비하는 것이 더 빠르기 때문입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def masked_softmax(X, valid_lens):  #@save
    """마지막 축의 요소를 마스킹하여 소프트맥스 연산을 수행합니다."""
    # X: 3D 텐서, valid_lens: 1D 또는 2D 텐서
    if valid_lens is None:
        return npx.softmax(X)
    else:
        shape = X.shape
        if valid_lens.ndim == 1:
            valid_lens = valid_lens.repeat(shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # 마지막 축에서 마스킹된 요소를 매우 큰 음수 값으로 교체합니다.
        # 이 값의 지수화 출력은 0이 됩니다.
        X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, True,
                              value=-1e6, axis=1)
        return npx.softmax(X).reshape(shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def masked_softmax(X, valid_lens):  #@save
    """마지막 축의 요소를 마스킹하여 소프트맥스 연산을 수행합니다."""
    # X: 3D 텐서, valid_lens: 1D 또는 2D 텐서 
    def _sequence_mask(X, valid_len, value=0):
        maxlen = X.size(1)
        mask = torch.arange((maxlen), dtype=torch.float32,
                            device=X.device)[None, :] &lt; valid_len[:, None]
        X[~mask] = value
        return X
    
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # 마지막 축에서 마스킹된 요소를 매우 큰 음수 값으로 교체합니다.
        # 이 값의 지수화 출력은 0이 됩니다.
        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def masked_softmax(X, valid_lens):  #@save
    """마지막 축의 요소를 마스킹하여 소프트맥스 연산을 수행합니다."""
    # X: 3D 텐서, valid_lens: 1D 또는 2D 텐서
    def _sequence_mask(X, valid_len, value=0):
        maxlen = X.shape[1]
        mask = tf.range(start=0, limit=maxlen, dtype=tf.float32)[
            None, :] &lt; tf.cast(valid_len[:, None], dtype=tf.float32)

        if len(X.shape) == 3:
            return tf.where(tf.expand_dims(mask, axis=-1), X, value)
        else:
            return tf.where(mask, X, value)
    
    if valid_lens is None:
        return tf.nn.softmax(X, axis=-1)
    else:
        shape = X.shape
        if len(valid_lens.shape) == 1:
            valid_lens = tf.repeat(valid_lens, repeats=shape[1])
            
        else:
            valid_lens = tf.reshape(valid_lens, shape=-1)
        # 마지막 축에서 마스킹된 요소를 매우 큰 음수 값으로 교체합니다.
        # 이 값의 지수화 출력은 0이 됩니다.
        X = _sequence_mask(tf.reshape(X, shape=(-1, shape[-1])), valid_lens,
                           value=-1e6)    
        return tf.nn.softmax(tf.reshape(X, shape=shape), axis=-1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def masked_softmax(X, valid_lens):  #@save
    """마지막 축의 요소를 마스킹하여 소프트맥스 연산을 수행합니다."""
    # X: 3D 텐서, valid_lens: 1D 또는 2D 텐서
    def _sequence_mask(X, valid_len, value=0):
        maxlen = X.shape[1]
        mask = jnp.arange((maxlen),
                          dtype=jnp.float32)[None, :] &lt; valid_len[:, None]
        return jnp.where(mask, X, value)

    if valid_lens is None:
        return nn.softmax(X, axis=-1)
    else:
        shape = X.shape
        if valid_lens.ndim == 1:
            valid_lens = jnp.repeat(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # 마지막 축에서 마스킹된 요소를 매우 큰 음수 값으로 교체합니다.
        # 이 값의 지수화 출력은 0이 됩니다.
        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)
        return nn.softmax(X.reshape(shape), axis=-1)
</code></pre>
<p>[<strong>이 함수가 어떻게 작동하는지 설명</strong>]하기 위해,
유효 길이가 각각 2와 3인 크기 $2 \times 4$의 두 예제 미니배치를 고려해 보십시오.
마스킹된 소프트맥스 연산의 결과로, 각 벡터 쌍에 대해 유효 길이를 벗어난 값은 모두 0으로 마스킹됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
masked_softmax(np.random.uniform(size=(2, 2, 4)), d2l.tensor([2, 3]))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
masked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([2, 3]))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
masked_softmax(jax.random.uniform(d2l.get_key(), (2, 2, 4)), jnp.array([2, 3]))
</code></pre>
<p>모든 예제의 두 벡터 각각에 대해 유효 길이를 지정하기 위해 더 세밀한 제어가 필요한 경우, 유효 길이의 2차원 텐서를 사용하면 됩니다. 이는 다음을 산출합니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
masked_softmax(np.random.uniform(size=(2, 2, 4)),
               d2l.tensor([[1, 3], [2, 4]]))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
masked_softmax(torch.rand(2, 2, 4), d2l.tensor([[1, 3], [2, 4]]))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
masked_softmax(tf.random.uniform((2, 2, 4)), tf.constant([[1, 3], [2, 4]]))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
masked_softmax(jax.random.uniform(d2l.get_key(), (2, 2, 4)),
               jnp.array([[1, 3], [2, 4]]))
</code></pre>
<h3 id="배치-행렬-곱셈-batch-matrix-multiplication"><a class="header" href="#배치-행렬-곱셈-batch-matrix-multiplication">배치 행렬 곱셈 (Batch Matrix Multiplication)</a></h3>
<p>:label:<code>subsec_batch_dot</code></p>
<p>또 다른 흔히 사용되는 연산은 행렬 배치를 서로 곱하는 것입니다. 이는 쿼리, 키, 값의 미니배치가 있을 때 유용합니다. 더 구체적으로 다음과 같다고 가정합니다.</p>
<p>$$\mathbf{Q} = [\mathbf{Q}_1, \mathbf{Q}_2, \ldots, \mathbf{Q}_n]  \in \mathbb{R}^{n \times a \times b}, <br />
\mathbf{K} = [\mathbf{K}_1, \mathbf{K}_2, \ldots, \mathbf{K}_n]  \in \mathbb{R}^{n \times b \times c}.
$$</p>
<p>그러면 배치 행렬 곱셈(BMM)은 다음과 같이 요소별 곱을 계산합니다.</p>
<p>$$\textrm{BMM}(\mathbf{Q}, \mathbf{K}) = [\mathbf{Q}_1 \mathbf{K}_1, \mathbf{Q}_2 \mathbf{K}_2, \ldots, \mathbf{Q}_n \mathbf{K}_n] \in \mathbb{R}^{n \times a \times c}.$$
:eqlabel:<code>eq_batch-matrix-mul</code></p>
<p>딥러닝 프레임워크에서 작동하는 것을 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
Q = d2l.ones((2, 3, 4))
K = d2l.ones((2, 4, 6))
d2l.check_shape(npx.batch_dot(Q, K), (2, 3, 6))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
Q = d2l.ones((2, 3, 4))
K = d2l.ones((2, 4, 6))
d2l.check_shape(torch.bmm(Q, K), (2, 3, 6))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
Q = d2l.ones((2, 3, 4))
K = d2l.ones((2, 4, 6))
d2l.check_shape(tf.matmul(Q, K).numpy(), (2, 3, 6))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
Q = d2l.ones((2, 3, 4))
K = d2l.ones((2, 4, 6))
d2l.check_shape(jax.lax.batch_matmul(Q, K), (2, 3, 6))
</code></pre>
<h2 id="스케일드-내적-주의-scaled-dot-product-attention"><a class="header" href="#스케일드-내적-주의-scaled-dot-product-attention">[<strong>스케일드 내적 주의 (Scaled Dot Product Attention)</strong>]</a></h2>
<p>:eqref:<code>eq_dot_product_attention</code>에서 소개된 내적 주의로 돌아가 봅시다.
일반적으로 쿼리와 키가 모두 동일한 벡터 길이 $d$를 가질 것을 요구하지만,
$\mathbf{q}^\top \mathbf{k}$를 두 공간 사이의 변환을 위해 적절히 선택된 행렬 $\mathbf{M}$에 대해 $\mathbf{q}^\top \mathbf{M} \mathbf{k}$로 대체함으로써 이를 쉽게 해결할 수 있습니다. 지금은 차원이 일치한다고 가정합니다.</p>
<p>실제로 우리는 효율성을 위해 $n$개의 쿼리와 $m$개의 키-값 쌍에 대한 주의를 계산하는 것과 같이 미니배치를 종종 생각합니다. 여기서 쿼리와 키의 길이는 $d$이고 값의 길이는 $v$입니다.
쿼리 $\mathbf Q\in\mathbb R^{n\times d}$, 키 $\mathbf K\in\mathbb R^{m\times d}$, 값 $\mathbf V\in\mathbb R^{m\times v}$의 스케일드 내적 주의는 다음과 같이 쓸 수 있습니다.</p>
<p>$$ \mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.$$
:eqlabel:<code>eq_softmax_QK_V</code></p>
<p>이를 미니배치에 적용할 때 :eqref:<code>eq_batch-matrix-mul</code>에서 소개한 배치 행렬 곱셈이 필요하다는 점에 유의하십시오. 다음 스케일드 내적 주의 구현에서는 모델 정규화를 위해 드롭아웃을 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class DotProductAttention(nn.Block):  #@save
    """스케일드 내적 주의."""
    def __init__(self, dropout):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    # queries의 모양: (batch_size, 쿼리 수, d)
    # keys의 모양: (batch_size, 키-값 쌍의 수, d)
    # values의 모양: (batch_size, 키-값 쌍의 수, 값 차원)
    # valid_lens의 모양: (batch_size,) 또는 (batch_size, 쿼리 수)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # keys의 마지막 두 차원을 바꾸기 위해 transpose_b=True로 설정합니다
        scores = npx.batch_dot(queries, keys, transpose_b=True) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return npx.batch_dot(self.dropout(self.attention_weights), values)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class DotProductAttention(nn.Module):  #@save
    """스케일드 내적 주의."""
    def __init__(self, dropout):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    # queries의 모양: (batch_size, 쿼리 수, d)
    # keys의 모양: (batch_size, 키-값 쌍의 수, d)
    # values의 모양: (batch_size, 키-값 쌍의 수, 값 차원)
    # valid_lens의 모양: (batch_size,) 또는 (batch_size, 쿼리 수)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # keys.transpose(1, 2)를 사용하여 keys의 마지막 두 차원을 바꿉니다
        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class DotProductAttention(tf.keras.layers.Layer):  #@save
    """스케일드 내적 주의."""
    def __init__(self, dropout):
        super().__init__()
        self.dropout = tf.keras.layers.Dropout(dropout)
        
    # queries의 모양: (batch_size, 쿼리 수, d)
    # keys의 모양: (batch_size, 키-값 쌍의 수, d)
    # values의 모양: (batch_size, 키-값 쌍의 수, 값 차원)
    # valid_lens의 모양: (batch_size,) 또는 (batch_size, 쿼리 수)
    def call(self, queries, keys, values, valid_lens=None, **kwargs):
        d = queries.shape[-1]
        scores = tf.matmul(queries, keys, transpose_b=True)/tf.math.sqrt(
            tf.cast(d, dtype=tf.float32))
        self.attention_weights = masked_softmax(scores, valid_lens)
        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class DotProductAttention(nn.Module):  #@save
    """스케일드 내적 주의."""
    dropout: float

    # queries의 모양: (batch_size, 쿼리 수, d)
    # keys의 모양: (batch_size, 키-값 쌍의 수, d)
    # values의 모양: (batch_size, 키-값 쌍의 수, 값 차원)
    # valid_lens의 모양: (batch_size,) 또는 (batch_size, 쿼리 수)
    @nn.compact
    def __call__(self, queries, keys, values, valid_lens=None,
                 training=False):
        d = queries.shape[-1]
        # keys.swapaxes(1, 2)를 사용하여 keys의 마지막 두 차원을 바꿉니다
        scores = queries@(keys.swapaxes(1, 2)) / math.sqrt(d)
        attention_weights = masked_softmax(scores, valid_lens)
        dropout_layer = nn.Dropout(self.dropout, deterministic=not training)
        return dropout_layer(attention_weights)@values, attention_weights
</code></pre>
<p>[<strong><code>DotProductAttention</code> 클래스가 어떻게 작동하는지 설명</strong>]하기 위해,
가산 주의(additive attention)에 대한 이전 장난감 예제의 것과 동일한 키, 값, 유효 길이를 사용합니다.
우리 예제의 목적을 위해 미니배치 크기가 2, 총 10개의 키와 값, 그리고 값의 차원이 4라고 가정합니다.
마지막으로 관찰당 유효 길이가 각각 2와 6이라고 가정합니다.
이를 감안할 때 출력이 $2 \times 1 \times 4$ 텐서, 즉 미니배치의 예제당 한 행이 될 것으로 기대합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
queries = d2l.normal(0, 1, (2, 1, 2))
keys = d2l.normal(0, 1, (2, 10, 2))
values = d2l.normal(0, 1, (2, 10, 4))
valid_lens = d2l.tensor([2, 6])

attention = DotProductAttention(dropout=0.5)
attention.initialize()
d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
queries = d2l.normal(0, 1, (2, 1, 2))
keys = d2l.normal(0, 1, (2, 10, 2))
values = d2l.normal(0, 1, (2, 10, 4))
valid_lens = d2l.tensor([2, 6])

attention = DotProductAttention(dropout=0.5)
attention.eval()
d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
queries = tf.random.normal(shape=(2, 1, 2))
keys = tf.random.normal(shape=(2, 10, 2))
values = tf.random.normal(shape=(2, 10, 4))
valid_lens = tf.constant([2, 6])

attention = DotProductAttention(dropout=0.5)
d2l.check_shape(attention(queries, keys, values, valid_lens, training=False),
                (2, 1, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
queries = jax.random.normal(d2l.get_key(), (2, 1, 2))
keys = jax.random.normal(d2l.get_key(), (2, 10, 2))
values = jax.random.normal(d2l.get_key(), (2, 10, 4))
valid_lens = d2l.tensor([2, 6])

attention = DotProductAttention(dropout=0.5)
(output, attention_weights), params = attention.init_with_output(
    d2l.get_key(), queries, keys, values, valid_lens)
print(output)
</code></pre>
<p>주의 가중치가 각각 두 번째와 여섯 번째 열 너머에서 실제로 사라지는지 확인해 봅시다(유효 길이를 2와 6으로 설정했기 때문입니다).</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
d2l.show_heatmaps(d2l.reshape(attention.attention_weights, (1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
d2l.show_heatmaps(d2l.reshape(attention_weights, (1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
</code></pre>
<h2 id="가산-주의-additive-attention"><a class="header" href="#가산-주의-additive-attention">[<strong>가산 주의 (Additive Attention)</strong>]</a></h2>
<p>:label:<code>subsec_additive-attention</code></p>
<p>쿼리 $\mathbf{q}$와 키 $\mathbf{k}$가 서로 다른 차원의 벡터인 경우, $\mathbf{q}^\top \mathbf{M} \mathbf{k}$를 통해 불일치를 해결하기 위해 행렬을 사용하거나, 스코어 함수로 가산 주의를 사용할 수 있습니다.
또 다른 이점은 이름에서 알 수 있듯이 주의가 가산적이라는 것입니다. 이는 약간의 계산 절약으로 이어질 수 있습니다.
쿼리 $\mathbf{q} \in \mathbb{R}^q$와 키 $\mathbf{k} \in \mathbb{R}^k$가 주어졌을 때, <em>가산 주의(additive attention)</em> 스코어 함수 :cite:<code>Bahdanau.Cho.Bengio.2014</code>는 다음과 같이 주어집니다.</p>
<p>$$a(\mathbf q, \mathbf k) = \mathbf w_v^\top \textrm{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},$$
:eqlabel:<code>eq_additive-attn</code></p>
<p>여기서 $\mathbf W_q\in\mathbb R^{h\times q}$, $\mathbf W_k\in\mathbb R^{h\times k}$, $\mathbf w_v\in\mathbb R^{h}$는 학습 가능한 파라미터입니다.
이 항은 비음수성과 정규화를 모두 보장하기 위해 소프트맥스에 공급됩니다.
:eqref:<code>eq_additive-attn</code>에 대한 동등한 해석은 쿼리와 키가 연결되어 단일 은닉층이 있는 MLP에 공급된다는 것입니다.
$\tanh$를 활성화 함수로 사용하고 편향 항을 비활성화하여 가산 주의를 다음과 같이 구현합니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class AdditiveAttention(nn.Block):  #@save
    """가산 주의."""
    def __init__(self, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        # flatten=False를 사용하여 마지막 축만 변환하여 다른 축의 모양이 동일하게 유지되도록 합니다
        self.W_k = nn.Dense(num_hiddens, use_bias=False, flatten=False)
        self.W_q = nn.Dense(num_hiddens, use_bias=False, flatten=False)
        self.w_v = nn.Dense(1, use_bias=False, flatten=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # 차원 확장 후, queries의 모양: (batch_size, 쿼리 수, 1, num_hiddens)
        # keys의 모양: (batch_size, 1, 키-값 쌍의 수, num_hiddens)
        # 브로드캐스팅을 사용하여 이들을 더합니다
        features = np.expand_dims(queries, axis=2) + np.expand_dims(
            keys, axis=1)
        features = np.tanh(features)
        # self.w_v의 출력은 하나뿐이므로 모양에서 마지막 1차원 항목을 제거합니다.
        # scores의 모양: (batch_size, 쿼리 수, 키-값 쌍의 수)
        scores = np.squeeze(self.w_v(features), axis=-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # values의 모양: (batch_size, 키-값 쌍의 수, 값 차원)
        return npx.batch_dot(self.dropout(self.attention_weights), values)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class AdditiveAttention(nn.Module):  #@save
    """가산 주의."""
    def __init__(self, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.LazyLinear(num_hiddens, bias=False)
        self.W_q = nn.LazyLinear(num_hiddens, bias=False)
        self.w_v = nn.LazyLinear(1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # 차원 확장 후, queries의 모양: (batch_size, 쿼리 수, 1, num_hiddens)
        # keys의 모양: (batch_size, 1, 키-값 쌍의 수, num_hiddens)
        # 브로드캐스팅을 사용하여 이들을 더합니다
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # self.w_v의 출력은 하나뿐이므로 모양에서 마지막 1차원 항목을 제거합니다.
        # scores의 모양: (batch_size, 쿼리 수, 키-값 쌍의 수)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # values의 모양: (batch_size, 키-값 쌍의 수, 값 차원)
        return torch.bmm(self.dropout(self.attention_weights), values)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class AdditiveAttention(tf.keras.layers.Layer):  #@save
    """가산 주의."""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super().__init__(**kwargs)
        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=False)
        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=False)
        self.w_v = tf.keras.layers.Dense(1, use_bias=False)
        self.dropout = tf.keras.layers.Dropout(dropout)
        
    def call(self, queries, keys, values, valid_lens, **kwargs):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # 차원 확장 후, queries의 모양: (batch_size, 쿼리 수, 1, num_hiddens)
        # keys의 모양: (batch_size, 1, 키-값 쌍의 수, num_hiddens)
        # 브로드캐스팅을 사용하여 이들을 더합니다
        features = tf.expand_dims(queries, axis=2) + tf.expand_dims(
            keys, axis=1)
        features = tf.nn.tanh(features)
        # self.w_v의 출력은 하나뿐이므로 모양에서 마지막 1차원 항목을 제거합니다.
        # scores의 모양: (batch_size, 쿼리 수, 키-값 쌍의 수)
        scores = tf.squeeze(self.w_v(features), axis=-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # values의 모양: (batch_size, 키-값 쌍의 수, 값 차원)
        return tf.matmul(self.dropout(
            self.attention_weights, **kwargs), values)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class AdditiveAttention(nn.Module):  #@save
    num_hiddens: int
    dropout: float

    def setup(self):
        self.W_k = nn.Dense(self.num_hiddens, use_bias=False)
        self.W_q = nn.Dense(self.num_hiddens, use_bias=False)
        self.w_v = nn.Dense(1, use_bias=False)

    @nn.compact
    def __call__(self, queries, keys, values, valid_lens, training=False):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # 차원 확장 후, queries의 모양: (batch_size, 쿼리 수, 1, num_hiddens)
        # keys의 모양: (batch_size, 1, 키-값 쌍의 수, num_hiddens)
        # 브로드캐스팅을 사용하여 이들을 더합니다
        features = jnp.expand_dims(queries, axis=2) + jnp.expand_dims(keys, axis=1)
        features = nn.tanh(features)
        # self.w_v의 출력은 하나뿐이므로 모양에서 마지막 1차원 항목을 제거합니다.
        # scores의 모양: (batch_size, 쿼리 수, 키-값 쌍의 수)
        scores = self.w_v(features).squeeze(-1)
        attention_weights = masked_softmax(scores, valid_lens)
        dropout_layer = nn.Dropout(self.dropout, deterministic=not training)
        # values의 모양: (batch_size, 키-값 쌍의 수, 값 차원)
        return dropout_layer(attention_weights)@values, attention_weights
</code></pre>
<p>[<strong><code>AdditiveAttention</code>이 어떻게 작동하는지 살펴봅시다.</strong>]
장난감 예제에서 쿼리, 키, 값의 크기를 각각 $(2, 1, 20)$, $(2, 10, 2)$, $(2, 10, 4)$로 선택합니다.
이는 이제 쿼리가 20차원이라는 점을 제외하고는 <code>DotProductAttention</code>에 대한 우리의 선택과 동일합니다.
마찬가지로 미니배치의 시퀀스에 대해 $(2, 6)$을 유효 길이로 선택합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
queries = d2l.normal(0, 1, (2, 1, 20))

attention = AdditiveAttention(num_hiddens=8, dropout=0.1)
attention.initialize()
d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
queries = d2l.normal(0, 1, (2, 1, 20))

attention = AdditiveAttention(num_hiddens=8, dropout=0.1)
attention.eval()
d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
queries = tf.random.normal(shape=(2, 1, 20))

attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,
                              dropout=0.1)
d2l.check_shape(attention(queries, keys, values, valid_lens, training=False),
                (2, 1, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
queries = jax.random.normal(d2l.get_key(), (2, 1, 20))
attention = AdditiveAttention(num_hiddens=8, dropout=0.1)
(output, attention_weights), params = attention.init_with_output(
    d2l.get_key(), queries, keys, values, valid_lens)
print(output)
</code></pre>
<p>주의 함수를 검토할 때 <code>DotProductAttention</code>의 결과와 질적으로 매우 유사한 동작을 보입니다.
즉, 선택된 유효 길이 $(2, 6)$ 내의 항만 0이 아닙니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
d2l.show_heatmaps(d2l.reshape(attention.attention_weights, (1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
d2l.show_heatmaps(d2l.reshape(attention_weights, (1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
</code></pre>
<h2 id="요약-summary-46"><a class="header" href="#요약-summary-46">요약 (Summary)</a></h2>
<p>이 섹션에서는 두 가지 핵심 어텐션 스코어 함수인 내적 주의와 가산 주의를 소개했습니다.
그들은 가변 길이의 시퀀스를 가로질러 집계하기 위한 효과적인 도구입니다.
특히 내적 주의는 현대 Transformer 아키텍처의 주류입니다.
쿼리와 키가 서로 다른 길이의 벡터인 경우 대신 가산 주의 스코어 함수를 사용할 수 있습니다.
이러한 레이어를 최적화하는 것은 최근 몇 년 동안의 핵심 발전 분야 중 하나입니다. 예를 들어 <a href="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html">NVIDIA의 Transformer 라이브러리</a>와 Megatron :cite:<code>shoeybi2019megatron</code>은 주의 메커니즘의 효율적인 변형에 결정적으로 의존합니다. 나중에 섹션에서 Transformer를 검토하면서 이에 대해 좀 더 자세히 알아볼 것입니다.</p>
<h2 id="연습-문제-exercises-59"><a class="header" href="#연습-문제-exercises-59">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>DotProductAttention</code> 코드를 수정하여 거리 기반 주의(distance-based attention)를 구현하십시오. 효율적인 구현을 위해 키의 제곱 노름 $|\mathbf{k}_i|^2$만 필요하다는 점에 유의하십시오.</li>
<li>차원을 조정하기 위해 행렬을 채택하여 서로 다른 차원의 쿼리와 키를 허용하도록 내적 주의를 수정하십시오.</li>
<li>계산 비용이 키, 쿼리, 값의 차원 및 수에 따라 어떻게 확장됩니까? 메모리 대역폭 요구 사항은 어떻습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/346">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1064">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3867">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18027">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<h1 id="바다나우-주의-메커니즘-the-bahdanau-attention-mechanism"><a class="header" href="#바다나우-주의-메커니즘-the-bahdanau-attention-mechanism">바다나우 주의 메커니즘 (The Bahdanau Attention Mechanism)</a></h1>
<p>:label:<code>sec_seq2seq_attention</code></p>
<p>:numref:<code>sec_seq2seq</code>에서 기계 번역을 접했을 때, 우리는 두 개의 RNN을 기반으로 시퀀스-투-시퀀스 학습을 위한 인코더-디코더 아키텍처를 설계했습니다 :cite:<code>Sutskever.Vinyals.Le.2014</code>.
구체적으로 RNN 인코더는 가변 길이 시퀀스를 <em>고정 모양</em>의 문맥 변수로 변환합니다.
그런 다음 RNN 디코더는 생성된 토큰과 문맥 변수를 기반으로 출력(타겟) 시퀀스를 토큰별로 생성합니다.</p>
<p>약간의 세부 사항을 추가하여 :numref:<code>fig_seq2seq_details</code>를 반복한 :numref:<code>fig_s2s_attention_state</code>를 상기해 보십시오.
관습적으로 RNN에서 소스 시퀀스에 대한 모든 관련 정보는 인코더에 의해 어떤 내부의 <em>고정 차원</em> 상태 표현으로 번역됩니다.
번역된 시퀀스를 생성하기 위해 디코더가 완전하고 독점적인 정보 소스로 사용하는 것이 바로 이 상태입니다.
다시 말해, 시퀀스-투-시퀀스 메커니즘은 중간 상태를 입력으로 사용된 문자열의 충분 통계량(sufficient statistic)으로 취급합니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/seq2seq-state.svg" alt="시퀀스-투-시퀀스 모델. 인코더에 의해 생성된 상태는 인코더와 디코더 간에 공유되는 유일한 정보 조각입니다." />
:label:<code>fig_s2s_attention_state</code></p>
<p>이것이 짧은 시퀀스에는 꽤 합리적이지만, 책의 장이나 아주 긴 문장과 같이 긴 시퀀스에는 불가능하다는 것이 분명합니다.
결국 머지않아 소스 시퀀스에서 중요한 모든 것을 저장하기에는 중간 표현에 단순히 충분한 "공간"이 없게 될 것입니다.
결과적으로 디코더는 길고 복잡한 문장을 번역하는 데 실패할 것입니다.
이를 처음 접한 사람 중 한 명은 손글씨 텍스트를 생성하기 위해 RNN을 설계하려고 시도한 :citet:<code>Graves.2013</code>였습니다.
소스 텍스트의 길이는 임의적이므로, 그들은 정렬이 한 방향으로만 이동하는 훨씬 더 긴 펜 자국과 텍스트 문자를 정렬하기 위해 미분 가능한 주의 모델을 설계했습니다.
이는 차례로 음성 인식의 디코딩 알고리즘(예: 은닉 마르코프 모델 :cite:<code>rabiner1993fundamentals</code>)을 활용합니다.</p>
<p>정렬하는 법을 배우는 아이디어에서 영감을 받아, :citet:<code>Bahdanau.Cho.Bengio.2014</code>는 단방향 정렬 제한이 <em>없는</em> 미분 가능한 주의 모델을 제안했습니다.
토큰을 예측할 때 모든 입력 토큰이 관련이 있는 것은 아니라면, 모델은 현재 예측과 관련이 있다고 간주되는 입력 시퀀스의 일부에만 정렬(또는 주의를 기울임)합니다.
이것은 다음 토큰을 생성하기 전에 현재 상태를 업데이트하는 데 사용됩니다.
설명은 꽤 평범해 보이지만, 이 *바다나우 주의 메커니즘(Bahdanau attention mechanism)*은 틀림없이 지난 10년 동안 딥러닝에서 가장 영향력 있는 아이디어 중 하나로 변모하여 Transformer :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code>와 많은 관련 새로운 아키텍처를 탄생시켰습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
from mxnet import init, np, npx
from mxnet.gluon import rnn, nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
import jax
</code></pre>
<h2 id="모델"><a class="header" href="#모델">모델</a></h2>
<p>우리는 :numref:<code>sec_seq2seq</code>의 시퀀스-투-시퀀스 아키텍처, 특히 :eqref:<code>eq_seq2seq_s_t</code>에 의해 도입된 표기법을 따릅니다.
핵심 아이디어는 소스 문장을 요약하는 문맥 변수 $\mathbf{c}$인 상태를 고정된 상태로 유지하는 대신, 원래 텍스트(인코더 은닉 상태 $\mathbf{h}<em>{t}$)와 이미 생성된 텍스트(디코더 은닉 상태 $\mathbf{s}</em>{t'-1}$) 모두의 함수로서 동적으로 업데이트하는 것입니다.
이는 임의의 디코딩 타임 스텝 $t'$ 이후에 업데이트되는 $\mathbf{c}_{t'}$를 산출합니다.
입력 시퀀스의 길이가 $T$라고 가정합시다. 이 경우 문맥 변수는 어텐션 풀링의 출력입니다.</p>
<p>$$\mathbf{c}<em>{t'} = \sum</em>{t=1}^{T} \alpha(\mathbf{s}<em>{t' - 1}, \mathbf{h}</em>{t}) \mathbf{h}_{t}.$$</p>
<p>우리는 $\mathbf{s}<em>{t' - 1}$을 쿼리로 사용하고, $\mathbf{h}</em>{t}$를 키와 값 모두로 사용했습니다.
$\mathbf{c}<em>{t'}$는 상태 $\mathbf{s}</em>{t'}$를 생성하고 새 토큰을 생성하는 데 사용됩니다: :eqref:<code>eq_seq2seq_s_t</code>를 참조하십시오.
특히 주의 가중치 $\alpha$는 :eqref:<code>eq_additive-attn</code>에 의해 정의된 가산 주의 스코어 함수를 사용하여 :eqref:<code>eq_attn-scoring-alpha</code>에서와 같이 계산됩니다.
주의를 사용하는 이 RNN 인코더-디코더 아키텍처는 :numref:<code>fig_s2s_attention_details</code>에 묘사되어 있습니다.
나중에 이 모델은 디코더에서 이미 생성된 토큰을 추가 문맥으로 포함하도록 수정되었습니다(즉, 주의 합계가 $T$에서 멈추지 않고 $t'-1$까지 진행됨). 예를 들어 음성 인식에 적용된 이 전략에 대한 설명은 :citet:<code>chan2015listen</code>을 참조하십시오.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/seq2seq-details-attention.svg" alt="바다나우 주의 메커니즘을 사용한 RNN 인코더-디코더 모델의 레이어들." />
:label:<code>fig_s2s_attention_details</code></p>
<h2 id="주의가-있는-디코더-정의하기-defining-the-decoder-with-attention"><a class="header" href="#주의가-있는-디코더-정의하기-defining-the-decoder-with-attention">주의가 있는 디코더 정의하기 (Defining the Decoder with Attention)</a></h2>
<p>주의가 있는 RNN 인코더-디코더를 구현하기 위해 디코더만 재정의하면 됩니다(주의 함수에서 생성된 기호를 생략하면 설계가 단순해집니다).
상당히 당연한 이름인 <code>AttentionDecoder</code> 클래스를 정의함으로써 [<strong>주의가 있는 디코더를 위한 기본 인터페이스</strong>]부터 시작하겠습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
class AttentionDecoder(d2l.Decoder):  #@save
    """주의 기반 디코더를 위한 기본 인터페이스."""
    def __init__(self):
        super().__init__()

    @property
    def attention_weights(self):
        raise NotImplementedError
</code></pre>
<p>우리는 <code>Seq2SeqAttentionDecoder</code> 클래스에서 [<strong>RNN 디코더를 구현</strong>]해야 합니다.
디코더의 상태는 다음과 같이 초기화됩니다:
(i) 모든 타임 스텝에서 인코더의 마지막 레이어의 은닉 상태(주의를 위한 키와 값으로 사용됨);
(ii) 최종 타임 스텝에서 모든 레이어에서의 인코더의 은닉 상태(디코더의 은닉 상태를 초기화하는 역할);
(iii) 어텐션 풀링에서 패딩 토큰을 제외하기 위한 인코더의 유효 길이.
각 디코딩 타임 스텝에서 이전 타임 스텝에서 얻은 디코더의 최종 레이어 은닉 상태가 주의 메커니즘의 쿼리로 사용됩니다.
주의 메커니즘의 출력과 입력 임베딩은 모두 연결되어 RNN 디코더의 입력으로 사용됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)
        self.dense = nn.Dense(vocab_size, flatten=False)
        self.initialize(init.Xavier())

    def init_state(self, enc_outputs, enc_valid_lens):
        # outputs 모양: (num_steps, batch_size, num_hiddens).
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        outputs, hidden_state = enc_outputs
        return (outputs.swapaxes(0, 1), hidden_state, enc_valid_lens)

    def forward(self, X, state):
        # enc_outputs 모양: (batch_size, num_steps, num_hiddens).
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        enc_outputs, hidden_state, enc_valid_lens = state
        # 출력 X 모양: (num_steps, batch_size, embed_size)
        X = self.embedding(X).swapaxes(0, 1)
        outputs, self._attention_weights = [], []
        for x in X:
            # query 모양: (batch_size, 1, num_hiddens)
            query = np.expand_dims(hidden_state[-1], axis=1)
            # context 모양: (batch_size, 1, num_hiddens)
            context = self.attention(
                query, enc_outputs, enc_outputs, enc_valid_lens)
            # 특성 차원에서 연결
            x = np.concatenate((context, np.expand_dims(x, axis=1)), axis=-1)
            # x를 (1, batch_size, embed_size + num_hiddens)로 재구성
            out, hidden_state = self.rnn(x.swapaxes(0, 1), hidden_state)
            hidden_state = hidden_state[0]
            outputs.append(out)
            self._attention_weights.append(self.attention.attention_weights)
        # 완전 연결 레이어 변환 후 outputs 모양:
        # (num_steps, batch_size, vocab_size)
        outputs = self.dense(np.concatenate(outputs, axis=0))
        return outputs.swapaxes(0, 1), [enc_outputs, hidden_state,
                                        enc_valid_lens]

    @property
    def attention_weights(self):
        return self._attention_weights
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(
            embed_size + num_hiddens, num_hiddens, num_layers,
            dropout=dropout)
        self.dense = nn.LazyLinear(vocab_size)
        self.apply(d2l.init_seq2seq)

    def init_state(self, enc_outputs, enc_valid_lens):
        # outputs 모양: (num_steps, batch_size, num_hiddens).
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        outputs, hidden_state = enc_outputs
        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)

    def forward(self, X, state):
        # enc_outputs 모양: (batch_size, num_steps, num_hiddens).
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        enc_outputs, hidden_state, enc_valid_lens = state
        # 출력 X 모양: (num_steps, batch_size, embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        outputs, self._attention_weights = [], []
        for x in X:
            # query 모양: (batch_size, 1, num_hiddens)
            query = torch.unsqueeze(hidden_state[-1], dim=1)
            # context 모양: (batch_size, 1, num_hiddens)
            context = self.attention(
                query, enc_outputs, enc_outputs, enc_valid_lens)
            # 특성 차원에서 연결
            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)
            # x를 (1, batch_size, embed_size + num_hiddens)로 재구성
            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)
            outputs.append(out)
            self._attention_weights.append(self.attention.attention_weights)
        # 완전 연결 레이어 변환 후 outputs 모양:
        # (num_steps, batch_size, vocab_size)
        outputs = self.dense(torch.cat(outputs, dim=0))
        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,
                                          enc_valid_lens]

    @property
    def attention_weights(self):
        return self._attention_weights
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0):
        super().__init__()
        self.attention = d2l.AdditiveAttention(num_hiddens, num_hiddens,
                                               num_hiddens, dropout)
        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)
        self.rnn = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells(
            [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)
             for _ in range(num_layers)]), return_sequences=True,
                                       return_state=True)
        self.dense = tf.keras.layers.Dense(vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens):
        # outputs 모양: (batch_size, num_steps, num_hiddens). 
        # 리스트 hidden_state의 길이는 num_layers이며, 그 요소의 모양은 (batch_size, num_hiddens)입니다
        outputs, hidden_state = enc_outputs
        return (tf.transpose(outputs, (1, 0, 2)), hidden_state,
                enc_valid_lens)

    def call(self, X, state, **kwargs):
        # 출력 enc_outputs 모양: # (batch_size, num_steps, num_hiddens)
        # 리스트 hidden_state의 길이는 num_layers이며, 그 요소의 모양은 (batch_size, num_hiddens)입니다
        enc_outputs, hidden_state, enc_valid_lens = state
        # 출력 X 모양: (num_steps, batch_size, embed_size)
        X = self.embedding(X)  # 입력 X 모양: (batch_size, num_steps)
        X = tf.transpose(X, perm=(1, 0, 2))
        outputs, self._attention_weights = [], []
        for x in X:
            # query 모양: (batch_size, 1, num_hiddens)
            query = tf.expand_dims(hidden_state[-1], axis=1)
            # context 모양: (batch_size, 1, num_hiddens)
            context = self.attention(query, enc_outputs, enc_outputs,
                                     enc_valid_lens, **kwargs)
            # 특성 차원에서 연결
            x = tf.concat((context, tf.expand_dims(x, axis=1)), axis=-1)
            out = self.rnn(x, hidden_state, **kwargs)
            hidden_state = out[1:]
            outputs.append(out[0])
            self._attention_weights.append(self.attention.attention_weights)
        # 완전 연결 레이어 변환 후 outputs 모양:
        # (batch_size, num_steps, vocab_size)
        outputs = self.dense(tf.concat(outputs, axis=1))
        return outputs, [enc_outputs, hidden_state, enc_valid_lens]

    @property
    def attention_weights(self):
        return self._attention_weights
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class Seq2SeqAttentionDecoder(nn.Module):
    vocab_size: int
    embed_size: int
    num_hiddens: int
    num_layers: int
    dropout: float = 0

    def setup(self):
        self.attention = d2l.AdditiveAttention(self.num_hiddens, self.dropout)
        self.embedding = nn.Embed(self.vocab_size, self.embed_size)
        self.dense = nn.Dense(self.vocab_size)
        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout=self.dropout)

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        # outputs 모양: (num_steps, batch_size, num_hiddens). 
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        outputs, hidden_state = enc_outputs
        # 주의 가중치는 상태의 일부로 반환됩니다. None으로 초기화합니다
        return (outputs.transpose(1, 0, 2), hidden_state, enc_valid_lens)

    @nn.compact
    def __call__(self, X, state, training=False):
        # enc_outputs 모양: (batch_size, num_steps, num_hiddens). 
        # hidden_state 모양: (num_layers, batch_size, num_hiddens)
        # 상태의 Attention 값 무시
        enc_outputs, hidden_state, enc_valid_lens = state
        # 출력 X 모양: (num_steps, batch_size, embed_size)
        X = self.embedding(X).transpose(1, 0, 2)
        outputs, attention_weights = [], []
        for x in X:
            # query 모양: (batch_size, 1, num_hiddens)
            query = jnp.expand_dims(hidden_state[-1], axis=1)
            # context 모양: (batch_size, 1, num_hiddens)
            context, attention_w = self.attention(query, enc_outputs,
                                                  enc_outputs, enc_valid_lens,
                                                  training=training)
            # 특성 차원에서 연결
            x = jnp.concatenate((context, jnp.expand_dims(x, axis=1)), axis=-1)
            # x를 (1, batch_size, embed_size + num_hiddens)로 재구성
            out, hidden_state = self.rnn(x.transpose(1, 0, 2), hidden_state,
                                         training=training)
            outputs.append(out)
            attention_weights.append(attention_w)

        # Flax sow API는 중간 변수를 캡처하는 데 사용됩니다
        self.sow('intermediates', 'dec_attention_weights', attention_weights)

        # 완전 연결 레이어 변환 후 outputs 모양:
        # (num_steps, batch_size, vocab_size)
        outputs = self.dense(jnp.concatenate(outputs, axis=0))
        return outputs.transpose(1, 0, 2), [enc_outputs, hidden_state,
                                            enc_valid_lens]
</code></pre>
<p>다음에서는 각각 7개 타임 스텝 길이의 4개 시퀀스 미니배치를 사용하여 주의가 있는 [<strong>구현된 디코더를 테스트</strong>]합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2
batch_size, num_steps = 4, 7
encoder = d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)
decoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,
                                  num_layers)
if tab.selected('mxnet'):
    X = d2l.zeros((batch_size, num_steps))
    state = decoder.init_state(encoder(X), None)
    output, state = decoder(X, state)
if tab.selected('pytorch'):
    X = d2l.zeros((batch_size, num_steps), dtype=torch.long)
    state = decoder.init_state(encoder(X), None)
    output, state = decoder(X, state)
if tab.selected('tensorflow'):
    X = tf.zeros((batch_size, num_steps))
    state = decoder.init_state(encoder(X, training=False), None)
    output, state = decoder(X, state, training=False)
if tab.selected('jax'):
    X = jnp.zeros((batch_size, num_steps), dtype=jnp.int32)
    state = decoder.init_state(encoder.init_with_output(d2l.get_key(),
                                                        X, training=False)[0],
                               None)
    (output, state), _ = decoder.init_with_output(d2l.get_key(), X,
                                                  state, training=False)
d2l.check_shape(output, (batch_size, num_steps, vocab_size))
d2l.check_shape(state[0], (batch_size, num_steps, num_hiddens))
d2l.check_shape(state[1][0], (batch_size, num_hiddens))
</code></pre>
<h2 id="훈련-training-20"><a class="header" href="#훈련-training-20">[<strong>훈련 (Training)</strong>]</a></h2>
<p>이제 새 디코더를 지정했으므로 :numref:<code>sec_seq2seq_training</code>과 유사하게 진행할 수 있습니다:
하이퍼파라미터를 지정하고, 일반 인코더와 주의가 있는 디코더를 인스턴스화하고, 기계 번역을 위해 이 모델을 훈련합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.MTFraEng(batch_size=128)
embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2
if tab.selected('mxnet', 'pytorch', 'jax'):
    encoder = d2l.Seq2SeqEncoder(
        len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)
    decoder = Seq2SeqAttentionDecoder(
        len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
if tab.selected('mxnet', 'pytorch'):
    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                        lr=0.005)
if tab.selected('jax'):
    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                        lr=0.005, training=True)
if tab.selected('mxnet', 'pytorch', 'jax'):
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        encoder = d2l.Seq2SeqEncoder(
            len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)
        decoder = Seq2SeqAttentionDecoder(
            len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
        model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                            lr=0.005)
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)
trainer.fit(model, data)
</code></pre>
<p>모델이 훈련된 후,
[<strong>몇 가지 영어 문장을 프랑스어로 번역</strong>]하고 BLEU 점수를 계산하는 데 사용합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
engs = ['go .', 'i lost .', 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    preds, _ = model.predict_step(
        data.build(engs, fras), d2l.try_gpu(), data.num_steps)
if tab.selected('jax'):
    preds, _ = model.predict_step(
        trainer.state.params, data.build(engs, fras), data.num_steps)
for en, fr, p in zip(engs, fras, preds):
    translation = []
    for token in data.tgt_vocab.to_tokens(p):
        if token == '&lt;eos&gt;':
            break
        translation.append(token)
    print(f'{en} =&gt; {translation}, bleu,'
          f'{d2l.bleu(" ".join(translation), fr, k=2):.3f}')
</code></pre>
<p>마지막 영어 문장을 번역할 때 [<strong>주의 가중치를 시각화</strong>]해 봅시다.
각 쿼리가 키-값 쌍에 대해 균일하지 않은 가중치를 할당하는 것을 볼 수 있습니다.
이는 각 디코딩 단계에서 입력 시퀀스의 서로 다른 부분이 어텐션 풀링에서 선택적으로 집계됨을 보여줍니다.</p>
<pre><code class="language-{.python .input}">%%tab all
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    _, dec_attention_weights = model.predict_step(
        data.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)
if tab.selected('jax'):
    _, (dec_attention_weights, _) = model.predict_step(
        trainer.state.params, data.build([engs[-1]], [fras[-1]]),
        data.num_steps, True)
attention_weights = d2l.concat(
    [step[0][0][0] for step in dec_attention_weights], 0)
attention_weights = d2l.reshape(attention_weights, (1, 1, -1, data.num_steps))
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
# 문장 끝 토큰을 포함하기 위해 1을 더합니다
d2l.show_heatmaps(
    attention_weights[:, :, :, :len(engs[-1].split()) + 1],
    xlabel='키 위치', ylabel='쿼리 위치')
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
# 문장 끝 토큰을 포함하기 위해 1을 더합니다
d2l.show_heatmaps(
    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),
    xlabel='키 위치', ylabel='쿼리 위치')
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# 문장 끝 토큰을 포함하기 위해 1을 더합니다
d2l.show_heatmaps(attention_weights[:, :, :, :len(engs[-1].split()) + 1],
                  xlabel='키 위치', ylabel='쿼리 위치')
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
# 문장 끝 토큰을 포함하기 위해 1을 더합니다
d2l.show_heatmaps(attention_weights[:, :, :, :len(engs[-1].split()) + 1],
                  xlabel='키 위치', ylabel='쿼리 위치')
</code></pre>
<h2 id="요약-summary-47"><a class="header" href="#요약-summary-47">요약 (Summary)</a></h2>
<p>토큰을 예측할 때 모든 입력 토큰이 관련이 있는 것은 아니라면, 바다나우 주의 메커니즘이 있는 RNN 인코더-디코더는 입력 시퀀스의 서로 다른 부분을 선택적으로 집계합니다. 이는 상태(문맥 변수)를 가산 주의 풀링의 출력으로 취급함으로써 달성됩니다.
RNN 인코더-디코더에서 바다나우 주의 메커니즘은 이전 타임 스텝의 디코더 은닉 상태를 쿼리로, 모든 타임 스텝의 인코더 은닉 상태를 키와 값 모두로 취급합니다.</p>
<h2 id="연습-문제-exercises-60"><a class="header" href="#연습-문제-exercises-60">연습 문제 (Exercises)</a></h2>
<ol>
<li>실험에서 GRU를 LSTM으로 교체하십시오.</li>
<li>가산 주의 스코어 함수를 스케일드 내적으로 교체하도록 실험을 수정하십시오. 그것이 훈련 효율성에 어떤 영향을 미칩니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/347">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1065">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3868">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18028">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<h1 id="멀티-헤드-어텐션-multi-head-attention"><a class="header" href="#멀티-헤드-어텐션-multi-head-attention">멀티 헤드 어텐션 (Multi-Head Attention)</a></h1>
<p>:label:<code>sec_multihead-attention</code></p>
<p>실제로 동일한 쿼리, 키, 값 세트가 주어졌을 때,
우리는 모델이 시퀀스 내의 다양한 범위(예: 짧은 범위 대 긴 범위)의 의존성을 캡처하는 것과 같이 동일한 주의 메커니즘의 서로 다른 행동들로부터 얻은 지식을 결합하기를 원할 수 있습니다.
따라서 우리의 주의 메커니즘이 쿼리, 키, 값의 서로 다른 표현 하위 공간을 공동으로 사용하도록 허용하는 것이 유익할 수 있습니다.</p>
<p>이를 위해 단일 어텐션 풀링을 수행하는 대신,
쿼리, 키, 값을 $h$개의 독립적으로 학습된 선형 투영(linear projection)으로 변환할 수 있습니다.
그런 다음 이 $h$개의 투영된 쿼리, 키, 값이 병렬로 어텐션 풀링에 공급됩니다.
마지막으로 $h$개의 어텐션 풀링 출력이 연결(concatenate)되고
다른 학습된 선형 투영으로 변환되어 최종 출력을 생성합니다.
이 설계를 *멀티 헤드 어텐션(multi-head attention)*이라고 하며, $h$개의 어텐션 풀링 출력 각각을 *헤드(head)*라고 합니다 :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code>.
학습 가능한 선형 변환을 수행하기 위해 완전 연결 레이어를 사용하는 멀티 헤드 어텐션을 :numref:<code>fig_multi-head-attention</code>에서 설명합니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/multi-head-attention.svg" alt="여러 헤드가 연결된 다음 선형 변환되는 멀티 헤드 어텐션." />
:label:<code>fig_multi-head-attention</code></p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
import math
from mxnet import autograd, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import math
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
import jax
</code></pre>
<h2 id="모델-1"><a class="header" href="#모델-1">모델</a></h2>
<p>멀티 헤드 어텐션의 구현을 제공하기 전에 이 모델을 수학적으로 공식화해 봅시다.
쿼리 $\mathbf q \in \mathbb R^{d_q}$, 키 $\mathbf k \in \mathbb R^{d_k}$, 값 $\mathbf v \in \mathbb R^{d_v}$가 주어지면,
각 어텐션 헤드 $\mathbf h_i$ ($i = 1, \ldots, h$)는 다음과 같이 계산됩니다.</p>
<p>$$\mathbf h_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},$$</p>
<p>여기서
$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$,
$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$,
$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$
는 학습 가능한 파라미터이고
$f$는 :numref:<code>sec_attention-scoring-functions</code>의 가산 주의 및 스케일드 내적 주의와 같은 어텐션 풀링입니다.
멀티 헤드 어텐션 출력은 $h$개 헤드의 연결에 대한 학습 가능한 파라미터 $\mathbf W_o\in\mathbb R^{p_o\times h p_v}$를 통한 또 다른 선형 변환입니다:</p>
<p>$$\mathbf W_o \begin{bmatrix}\mathbf h_1
\vdots
\\mathbf h_h
\end{bmatrix} \in \mathbb{R}^{p_o}.$$</p>
<p>이 설계를 바탕으로 각 헤드는 입력의 서로 다른 부분에 주의를 기울일 수 있습니다.
단순한 가중 평균보다 더 정교한 함수를 표현할 수 있습니다.</p>
<h2 id="구현-implementation"><a class="header" href="#구현-implementation">구현 (Implementation)</a></h2>
<p>우리 구현에서는 멀티 헤드 어텐션의 [<strong>각 헤드에 대해 스케일드 내적 주의를 선택</strong>]합니다.
계산 비용과 파라미터화 비용의 급격한 증가를 피하기 위해 $p_q = p_k = p_v = p_o / h$로 설정합니다.
쿼리, 키, 값에 대한 선형 변환의 출력 수를 $p_q h = p_k h = p_v h = p_o$로 설정하면 $h$개의 헤드를 병렬로 계산할 수 있음에 유의하십시오.
다음 구현에서 $p_o$는 <code>num_hiddens</code> 인수를 통해 지정됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class MultiHeadAttention(d2l.Module):  #@save
    """멀티 헤드 어텐션."""
    def __init__(self, num_hiddens, num_heads, dropout, use_bias=False,
                 **kwargs):
        super().__init__()
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
        self.W_k = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
        self.W_v = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
        self.W_o = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)

    def forward(self, queries, keys, values, valid_lens):
        # queries, keys, values의 모양: 
        # (batch_size, 쿼리 또는 키-값 쌍의 수, num_hiddens)
        # valid_lens의 모양: (batch_size,) 또는 (batch_size, 쿼리 수)
        # 전치 후 출력 queries, keys, values의 모양: 
        # (batch_size * num_heads, 쿼리 또는 키-값 쌍의 수, 
        # num_hiddens / num_heads)
        queries = self.transpose_qkv(self.W_q(queries))
        keys = self.transpose_qkv(self.W_k(keys))
        values = self.transpose_qkv(self.W_v(values))

        if valid_lens is not None:
            # axis 0에서 첫 번째 항목(스칼라 또는 벡터)을 num_heads번 복사하고, 
            # 그다음 항목을 복사하는 식으로 진행합니다
            valid_lens = valid_lens.repeat(self.num_heads, axis=0)

        # 출력 모양: (batch_size * num_heads, 쿼리 수, 
        # num_hiddens / num_heads)
        output = self.attention(queries, keys, values, valid_lens)
        
        # output_concat의 모양: (batch_size, 쿼리 수, num_hiddens)
        output_concat = self.transpose_output(output)
        return self.W_o(output_concat)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class MultiHeadAttention(d2l.Module):  #@save
    """멀티 헤드 어텐션."""
    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):
        super().__init__()
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)
        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)
        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)
        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)

    def forward(self, queries, keys, values, valid_lens):
        # queries, keys, values의 모양: 
        # (batch_size, 쿼리 또는 키-값 쌍의 수, num_hiddens)
        # valid_lens의 모양: (batch_size,) 또는 (batch_size, 쿼리 수)
        # 전치 후 출력 queries, keys, values의 모양: 
        # (batch_size * num_heads, 쿼리 또는 키-값 쌍의 수, 
        # num_hiddens / num_heads)
        queries = self.transpose_qkv(self.W_q(queries))
        keys = self.transpose_qkv(self.W_k(keys))
        values = self.transpose_qkv(self.W_v(values))

        if valid_lens is not None:
            # axis 0에서 첫 번째 항목(스칼라 또는 벡터)을 num_heads번 복사하고, 
            # 그다음 항목을 복사하는 식으로 진행합니다
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        # 출력 모양: (batch_size * num_heads, 쿼리 수, 
        # num_hiddens / num_heads)
        output = self.attention(queries, keys, values, valid_lens)
        # output_concat의 모양: (batch_size, 쿼리 수, num_hiddens)
        output_concat = self.transpose_output(output)
        return self.W_o(output_concat)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class MultiHeadAttention(d2l.Module):  #@save
    """멀티 헤드 어텐션."""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        super().__init__()
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=bias)
        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=bias)
        self.W_v = tf.keras.layers.Dense(num_hiddens, use_bias=bias)
        self.W_o = tf.keras.layers.Dense(num_hiddens, use_bias=bias)
    
    def call(self, queries, keys, values, valid_lens, **kwargs):
        # queries, keys, values의 모양: 
        # (batch_size, 쿼리 또는 키-값 쌍의 수, num_hiddens)
        # valid_lens의 모양: (batch_size,) 또는 (batch_size, 쿼리 수)
        # 전치 후 출력 queries, keys, values의 모양: 
        # (batch_size * num_heads, 쿼리 또는 키-값 쌍의 수, 
        # num_hiddens / num_heads)
        queries = self.transpose_qkv(self.W_q(queries))
        keys = self.transpose_qkv(self.W_k(keys))
        values = self.transpose_qkv(self.W_v(values))
        
        if valid_lens is not None:
            # axis 0에서 첫 번째 항목(스칼라 또는 벡터)을 num_heads번 복사하고, 
            # 그다음 항목을 복사하는 식으로 진행합니다
            valid_lens = tf.repeat(valid_lens, repeats=self.num_heads, axis=0)
            
        # 출력 모양: (batch_size * num_heads, 쿼리 수, 
        # num_hiddens / num_heads)
        output = self.attention(queries, keys, values, valid_lens, **kwargs)
        
        # output_concat의 모양: (batch_size, 쿼리 수, num_hiddens)
        output_concat = self.transpose_output(output)
        return self.W_o(output_concat)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class MultiHeadAttention(nn.Module):  #@save
    num_hiddens: int
    num_heads: int
    dropout: float
    bias: bool = False

    def setup(self):
        self.attention = d2l.DotProductAttention(self.dropout)
        self.W_q = nn.Dense(self.num_hiddens, use_bias=self.bias)
        self.W_k = nn.Dense(self.num_hiddens, use_bias=self.bias)
        self.W_v = nn.Dense(self.num_hiddens, use_bias=self.bias)
        self.W_o = nn.Dense(self.num_hiddens, use_bias=self.bias)

    @nn.compact
    def __call__(self, queries, keys, values, valid_lens, training=False):
        # queries, keys, values의 모양: 
        # (batch_size, 쿼리 또는 키-값 쌍의 수, num_hiddens)
        # valid_lens의 모양: (batch_size,) 또는 (batch_size, 쿼리 수)
        # 전치 후 출력 queries, keys, values의 모양: 
        # (batch_size * num_heads, 쿼리 또는 키-값 쌍의 수, 
        # num_hiddens / num_heads)
        queries = self.transpose_qkv(self.W_q(queries))
        keys = self.transpose_qkv(self.W_k(keys))
        values = self.transpose_qkv(self.W_v(values))

        if valid_lens is not None:
            # axis 0에서 첫 번째 항목(스칼라 또는 벡터)을 num_heads번 복사하고, 
            # 그다음 항목을 복사하는 식으로 진행합니다
            valid_lens = jnp.repeat(valid_lens, self.num_heads, axis=0)

        # 출력 모양: (batch_size * num_heads, 쿼리 수, 
        # num_hiddens / num_heads)
        output, attention_weights = self.attention(
            queries, keys, values, valid_lens, training=training)
        # output_concat의 모양: (batch_size, 쿼리 수, num_hiddens)
        output_concat = self.transpose_output(output)
        return self.W_o(output_concat), attention_weights
</code></pre>
<p>[<strong>여러 헤드의 병렬 계산</strong>]을 허용하기 위해 위의 <code>MultiHeadAttention</code> 클래스는 아래에 정의된 두 가지 전치(transposition) 메서드를 사용합니다.
구체적으로 <code>transpose_output</code> 메서드는 <code>transpose_qkv</code> 메서드의 연산을 반전시킵니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
@d2l.add_to_class(MultiHeadAttention)  #@save
def transpose_qkv(self, X):
    """여러 어텐션 헤드의 병렬 계산을 위한 전치."""
    # 입력 X의 모양: (batch_size, 쿼리 또는 키-값 쌍의 수, num_hiddens).
    # 출력 X의 모양: (batch_size, 쿼리 또는 키-값 쌍의 수, num_heads, num_hiddens / num_heads)
    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)
    # 출력 X의 모양: (batch_size, num_heads, 쿼리 또는 키-값 쌍의 수, num_hiddens / num_heads)
    X = X.transpose(0, 2, 1, 3)
    # 출력 모양: (batch_size * num_heads, 쿼리 또는 키-값 쌍의 수, num_hiddens / num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])

@d2l.add_to_class(MultiHeadAttention)  #@save
def transpose_output(self, X):
    """transpose_qkv 연산을 반전시킵니다."""
    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])
    X = X.transpose(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
@d2l.add_to_class(MultiHeadAttention)  #@save
def transpose_qkv(self, X):
    """여러 어텐션 헤드의 병렬 계산을 위한 전치."""
    # 입력 X의 모양: (batch_size, 쿼리 또는 키-값 쌍의 수, num_hiddens).
    # 출력 X의 모양: (batch_size, 쿼리 또는 키-값 쌍의 수, num_heads, num_hiddens / num_heads)
    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)
    # 출력 X의 모양: (batch_size, num_heads, 쿼리 또는 키-값 쌍의 수, num_hiddens / num_heads)
    X = X.permute(0, 2, 1, 3)
    # 출력 모양: (batch_size * num_heads, 쿼리 또는 키-값 쌍의 수, num_hiddens / num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])

@d2l.add_to_class(MultiHeadAttention)  #@save
def transpose_output(self, X):
    """transpose_qkv 연산을 반전시킵니다."""
    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
@d2l.add_to_class(MultiHeadAttention)  #@save
def transpose_qkv(self, X):
    """여러 어텐션 헤드의 병렬 계산을 위한 전치."""
    # 입력 X의 모양: (batch_size, 쿼리 또는 키-값 쌍의 수, num_hiddens).
    # 출력 X의 모양: (batch_size, 쿼리 또는 키-값 쌍의 수, num_heads, num_hiddens / num_heads)
    X = tf.reshape(X, shape=(X.shape[0], X.shape[1], self.num_heads, -1))
    # 출력 X의 모양: (batch_size, num_heads, 쿼리 또는 키-값 쌍의 수, num_hiddens / num_heads)
    X = tf.transpose(X, perm=(0, 2, 1, 3))
    # 출력 모양: (batch_size * num_heads, 쿼리 또는 키-값 쌍의 수, num_hiddens / num_heads)
    return tf.reshape(X, shape=(-1, X.shape[2], X.shape[3]))

@d2l.add_to_class(MultiHeadAttention)  #@save
def transpose_output(self, X):
    """transpose_qkv 연산을 반전시킵니다."""
    X = tf.reshape(X, shape=(-1, self.num_heads, X.shape[1], X.shape[2]))
    X = tf.transpose(X, perm=(0, 2, 1, 3))
    return tf.reshape(X, shape=(X.shape[0], X.shape[1], -1))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
@d2l.add_to_class(MultiHeadAttention)  #@save
def transpose_qkv(self, X):
    """여러 어텐션 헤드의 병렬 계산을 위한 전치."""
    # 입력 X의 모양: (batch_size, 쿼리 또는 키-값 쌍의 수, num_hiddens).
    # 출력 X의 모양: (batch_size, 쿼리 또는 키-값 쌍의 수, num_heads, num_hiddens / num_heads)
    X = X.reshape((X.shape[0], X.shape[1], self.num_heads, -1))
    # 출력 X의 모양: (batch_size, num_heads, 쿼리 또는 키-값 쌍의 수, num_hiddens / num_heads)
    X = jnp.transpose(X, (0, 2, 1, 3))
    # 출력 모양: (batch_size * num_heads, 쿼리 또는 키-값 쌍의 수, num_hiddens / num_heads)
    return X.reshape((-1, X.shape[2], X.shape[3]))

@d2l.add_to_class(MultiHeadAttention)  #@save
def transpose_output(self, X):
    """transpose_qkv 연산을 반전시킵니다."""
    X = X.reshape((-1, self.num_heads, X.shape[1], X.shape[2]))
    X = jnp.transpose(X, (0, 2, 1, 3))
    return X.reshape((X.shape[0], X.shape[1], -1))
</code></pre>
<p>키와 값이 동일한 장난감 예제를 사용하여 [<strong>구현된</strong>] <code>MultiHeadAttention</code> 클래스를 [<strong>테스트해 봅시다.</strong>]
결과적으로 멀티 헤드 어텐션 출력의 모양은 (<code>batch_size</code>, <code>num_queries</code>, <code>num_hiddens</code>)입니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)
batch_size, num_queries, num_kvpairs = 2, 4, 6
valid_lens = d2l.tensor([3, 2])
X = d2l.ones((batch_size, num_queries, num_hiddens))
Y = d2l.ones((batch_size, num_kvpairs, num_hiddens))
d2l.check_shape(attention(X, Y, Y, valid_lens),
                (batch_size, num_queries, num_hiddens))
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)
attention.initialize()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
                               num_hiddens, num_heads, 0.5)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
batch_size, num_queries, num_kvpairs = 2, 4, 6
valid_lens = d2l.tensor([3, 2])
X = d2l.ones((batch_size, num_queries, num_hiddens))
Y = d2l.ones((batch_size, num_kvpairs, num_hiddens))
d2l.check_shape(attention(X, Y, Y, valid_lens),
                (batch_size, num_queries, num_hiddens))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
batch_size, num_queries, num_kvpairs = 2, 4, 6
valid_lens = d2l.tensor([3, 2])
X = tf.ones((batch_size, num_queries, num_hiddens))
Y = tf.ones((batch_size, num_kvpairs, num_hiddens))
d2l.check_shape(attention(X, Y, Y, valid_lens, training=False),
                (batch_size, num_queries, num_hiddens))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
batch_size, num_queries, num_kvpairs = 2, 4, 6
valid_lens = d2l.tensor([3, 2])
X = d2l.ones((batch_size, num_queries, num_hiddens))
Y = d2l.ones((batch_size, num_kvpairs, num_hiddens))
d2l.check_shape(attention.init_with_output(d2l.get_key(), X, Y, Y, valid_lens,
                                           training=False)[0][0],
                (batch_size, num_queries, num_hiddens))
</code></pre>
<h2 id="요약-summary-48"><a class="header" href="#요약-summary-48">요약 (Summary)</a></h2>
<p>멀티 헤드 어텐션은 쿼리, 키, 값의 서로 다른 표현 하위 공간을 통해 동일한 어텐션 풀링의 지식을 결합합니다.
멀티 헤드 어텐션의 여러 헤드를 병렬로 계산하려면 적절한 텐서 조작이 필요합니다.</p>
<h2 id="연습-문제-exercises-61"><a class="header" href="#연습-문제-exercises-61">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 실험에서 여러 헤드의 주의 가중치를 시각화하십시오.</li>
<li>멀티 헤드 어텐션 기반의 훈련된 모델이 있고, 예측 속도를 높이기 위해 덜 중요한 어텐션 헤드를 프루닝(pruning)하고 싶다고 가정해 봅시다. 어텐션 헤드의 중요도를 측정하기 위해 실험을 어떻게 설계할 수 있을까요?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/1634">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1635">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3869">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18029">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<h1 id="셀프-어텐션과-위치-인코딩-self-attention-and-positional-encoding"><a class="header" href="#셀프-어텐션과-위치-인코딩-self-attention-and-positional-encoding">셀프 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></h1>
<p>:label:<code>sec_self-attention-and-positional-encoding</code></p>
<p>딥러닝에서 우리는 시퀀스를 인코딩하기 위해 종종 CNN이나 RNN을 사용합니다.
이제 주의 메커니즘을 염두에 두고, 매 단계마다 각 토큰이 고유한 쿼리, 키, 값을 갖도록 주의 메커니즘에 토큰 시퀀스를 공급한다고 상상해 보십시오.
여기서 다음 레이어에서 토큰의 표현 값을 계산할 때, 토큰은 (자신의 쿼리 벡터를 통해) 다른 토큰에 주의를 기울일 수 있습니다(그들의 키 벡터를 기반으로 매칭).
전체 쿼리-키 호환성 스코어 세트를 사용하여 각 토큰에 대해 다른 토큰들에 대한 적절한 가중 합을 구축함으로써 표현을 계산할 수 있습니다.
(디코더 단계가 인코더 단계에 주의를 기울이는 경우와 달리) 모든 토큰이 서로 다른 토큰에 주의를 기울이기 때문에, 이러한 아키텍처는 일반적으로 <em>셀프 어텐션(self-attention)</em> 모델 :cite:<code>Lin.Feng.Santos.ea.2017,Vaswani.Shazeer.Parmar.ea.2017</code>로 설명되며, 다른 곳에서는 <em>내부 주의(intra-attention)</em> 모델 :cite:<code>Cheng.Dong.Lapata.2016,Parikh.Tackstrom.Das.ea.2016,Paulus.Xiong.Socher.2017</code>로 설명되기도 합니다.
이 섹션에서는 시퀀스 순서에 대한 추가 정보를 사용하는 것을 포함하여 셀프 어텐션을 사용한 시퀀스 인코딩에 대해 논의할 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
import math
from mxnet import autograd, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import math
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import numpy as np
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
import jax
</code></pre>
<h2 id="셀프-어텐션-self-attention"><a class="header" href="#셀프-어텐션-self-attention">[<strong>셀프 어텐션 (Self-Attention)</strong>]</a></h2>
<p>입력 토큰 시퀀스 $\mathbf{x}_1, \ldots, \mathbf{x}_n$ (모든 $\mathbf{x}_i ∈ \mathbb{R}^d, 1 ≤ i ≤ n$)이 주어지면,
셀프 어텐션은 동일한 길이의 시퀀스 $\mathbf{y}_1, \ldots, \mathbf{y}_n$을 출력합니다. 여기서</p>
<p>$$\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d$$</p>
<p>이며, 이는 :eqref:<code>eq_attention_pooling</code>의 어텐션 풀링 정의에 따릅니다.
멀티 헤드 어텐션을 사용하여 다음 코드 스니펫은
모양이 (배치 크기, 타임 스텝 수 또는 토큰 단위 시퀀스 길이, $d$)인 텐서의 셀프 어텐션을 계산합니다.
출력 텐서는 동일한 모양을 갖습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
num_hiddens, num_heads = 100, 5
attention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)
batch_size, num_queries, valid_lens = 2, 4, d2l.tensor([3, 2])
X = d2l.ones((batch_size, num_queries, num_hiddens))
d2l.check_shape(attention(X, X, X, valid_lens),
                (batch_size, num_queries, num_hiddens))
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
num_hiddens, num_heads = 100, 5
attention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)
attention.initialize()
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
num_hiddens, num_heads = 100, 5
attention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
num_hiddens, num_heads = 100, 5
attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens,
                                   num_hiddens,
                                   num_hiddens, num_heads, 0.5)
</code></pre>
<pre><code class="language-{.python .input}">%%tab mxnet
batch_size, num_queries, valid_lens = 2, 4, d2l.tensor([3, 2])
X = d2l.ones((batch_size, num_queries, num_hiddens))
d2l.check_shape(attention(X, X, X, valid_lens),
                (batch_size, num_queries, num_hiddens))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
batch_size, num_queries, valid_lens = 2, 4, tf.constant([3, 2])
X = tf.ones((batch_size, num_queries, num_hiddens))
d2l.check_shape(attention(X, X, X, valid_lens, training=False),
                (batch_size, num_queries, num_hiddens))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
batch_size, num_queries, valid_lens = 2, 4, d2l.tensor([3, 2])
X = d2l.ones((batch_size, num_queries, num_hiddens))
d2l.check_shape(attention.init_with_output(d2l.get_key(), X, X, X, valid_lens,
                                           training=False)[0][0],
                (batch_size, num_queries, num_hiddens))
</code></pre>
<h2 id="cnn-rnn-셀프-어텐션-비교하기-comparing-cnns-rnns-and-self-attention"><a class="header" href="#cnn-rnn-셀프-어텐션-비교하기-comparing-cnns-rnns-and-self-attention">CNN, RNN, 셀프 어텐션 비교하기 (Comparing CNNs, RNNs, and Self-Attention)</a></h2>
<p>:label:<code>subsec_cnn-rnn-self-attention</code></p>
<p>$n$개 토큰 시퀀스를 동일한 길이의 다른 시퀀스로 매핑하는 아키텍처를 비교해 봅시다. 여기서 각 입력 또는 출력 토큰은 $d$차원 벡터로 표현됩니다.
구체적으로 CNN, RNN, 셀프 어텐션을 고려할 것입니다.
그들의 계산 복잡도, 순차적 연산(sequential operations), 최대 경로 길이를 비교할 것입니다.
순차적 연산은 병렬 계산을 방해하는 반면, 시퀀스 위치의 모든 조합 사이의 짧은 경로는 시퀀스 내의 장기 의존성을 학습하기 쉽게 만듭니다 :cite:<code>Hochreiter.Bengio.Frasconi.ea.2001</code>.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/cnn-rnn-self-attention.svg" alt="CNN(패딩 토큰 생략), RNN, 셀프 어텐션 아키텍처 비교." />
:label:<code>fig_cnn-rnn-self-attention</code></p>
<p>임의의 텍스트 시퀀스를 "1차원 이미지"로 간주합시다. 유사하게 1차원 CNN은 텍스트의 $n$-gram과 같은 지역적 특성을 처리할 수 있습니다.
길이 $n$의 시퀀스가 주어졌을 때 커널 크기가 $k$이고 입력 및 출력 채널 수가 모두 $d$인 합성곱 레이어를 고려해 보십시오.
합성곱 레이어의 계산 복잡도는 $\mathcal{O}(knd^2)$입니다.
:numref:<code>fig_cnn-rnn-self-attention</code>에서 볼 수 있듯이 CNN은 계층적이므로 $\mathcal{O}(1)$의 순차적 연산이 있고 최대 경로 길이는 $\mathcal{O}(n/k)$입니다.
예를 들어 $\mathbf{x}_1$과 $\mathbf{x}_5$는 :numref:<code>fig_cnn-rnn-self-attention</code>에서 커널 크기가 3인 2층 CNN의 수용 영역(receptive field) 내에 있습니다.</p>
<p>RNN의 은닉 상태를 업데이트할 때 $d \times d$ 가중치 행렬과 $d$차원 은닉 상태의 곱셈은 $\mathcal{O}(d^2)$의 계산 복잡도를 갖습니다.
시퀀스 길이가 $n$이므로 순환 레이어의 계산 복잡도는 $\mathcal{O}(nd^2)$입니다.
:numref:<code>fig_cnn-rnn-self-attention</code>에 따르면 병렬화할 수 없는 $\mathcal{O}(n)$의 순차적 연산이 있으며 최대 경로 길이 또한 $\mathcal{O}(n)$입니다.</p>
<p>셀프 어텐션에서 쿼리, 키, 값은 모두 $n \times d$ 행렬입니다.
:eqref:<code>eq_softmax_QK_V</code>의 스케일드 내적 주의를 고려해 보십시오. 여기서 $n \times d$ 행렬은 $d \times n$ 행렬과 곱해지고, 그 결과인 $n \times n$ 행렬은 다시 $n \times d$ 행렬과 곱해집니다.
결과적으로 셀프 어텐션은 $\mathcal{O}(n^2d)$의 계산 복잡도를 갖습니다.
:numref:<code>fig_cnn-rnn-self-attention</code>에서 알 수 있듯이, 각 토큰은 셀프 어텐션을 통해 다른 모든 토큰과 직접 연결됩니다.
따라서 계산은 $\mathcal{O}(1)$의 순차적 연산으로 병렬화될 수 있으며 최대 경로 길이 또한 $\mathcal{O}(1)$입니다.</p>
<p>요약하자면 CNN과 셀프 어텐션 모두 병렬 계산이 가능하며 셀프 어텐션이 가장 짧은 최대 경로 길이를 갖습니다.
그러나 시퀀스 길이에 대한 이차적인 계산 복잡도는 매우 긴 시퀀스에 대해 셀프 어텐션을 금지할 정도로 느리게 만듭니다.</p>
<h2 id="위치-인코딩-positional-encoding"><a class="header" href="#위치-인코딩-positional-encoding">[<strong>위치 인코딩 (Positional Encoding)</strong>]</a></h2>
<p>:label:<code>subsec_positional-encoding</code></p>
<p>시퀀스의 토큰을 하나씩 반복적으로 처리하는 RNN과 달리, 셀프 어텐션은 순차적 연산을 버리고 병렬 계산을 선호합니다.
셀프 어텐션 그 자체로는 시퀀스의 순서를 보존하지 않는다는 점에 유의하십시오.
모델이 입력 시퀀스가 도착한 순서를 아는 것이 정말 중요하다면 어떻게 해야 할까요?</p>
<p>토큰의 순서 정보를 보존하는 지배적인 접근 방식은 이를 각 토큰과 관련된 추가 입력으로 모델에 표현하는 것입니다.
이러한 입력을 *위치 인코딩(positional encodings)*이라고 하며, 학습되거나 사전(<em>a priori</em>)에 고정될 수 있습니다.
이제 사인 및 코사인 함수를 기반으로 한 고정 위치 인코딩을 위한 간단한 체계를 설명합니다 :cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code>.</p>
<p>입력 표현 $\mathbf{X} \in \mathbb{R}^{n \times d}$가 시퀀스의 $n$개 토큰에 대한 $d$차원 임베딩을 포함한다고 가정합시다.
위치 인코딩은 동일한 모양의 위치 임베딩 행렬 $\mathbf{P} \in \mathbb{R}^{n \times d}$를 사용하여 $\mathbf{X} + \mathbf{P}$를 출력합니다. 여기서 $i^\textrm{th}$번째 행과 $(2j)^\textrm{th}$번째 또는 $(2j + 1)^\textrm{th}$번째 열의 요소는 다음과 같습니다.</p>
<p>$$\begin{aligned}
p_{i, 2j} &amp;= \sin\left(\frac{i}{10000^{2j/d}}\right),\np_{i, 2j+1} &amp;= \cos\left(\frac{i}{10000^{2j/d}}\right).
\end{aligned}$$
:eqlabel:<code>eq_positional-encoding-def</code></p>
<p>처음 보기에 이 삼각 함수 설계는 이상해 보입니다.
이 설계에 대한 설명을 제공하기 전에 먼저 다음 <code>PositionalEncoding</code> 클래스에서 구현해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class PositionalEncoding(nn.Block):  #@save
    """위치 인코딩."""
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        # 충분히 긴 P 생성
        self.P = d2l.zeros((1, max_len, num_hiddens))
        X = d2l.arange(max_len).reshape(-1, 1) / np.power(
            10000, np.arange(0, num_hiddens, 2) / num_hiddens)
        self.P[:, :, 0::2] = np.sin(X)
        self.P[:, :, 1::2] = np.cos(X)

    def forward(self, X):
        X = X + self.P[:, :X.shape[1], :].as_in_ctx(X.ctx)
        return self.dropout(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class PositionalEncoding(nn.Module):  #@save
    """위치 인코딩."""
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        # 충분히 긴 P 생성
        self.P = d2l.zeros((1, max_len, num_hiddens))
        X = d2l.arange(max_len, dtype=torch.float32).reshape(
            -1, 1) / torch.pow(10000, torch.arange(
            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
        self.P[:, :, 0::2] = torch.sin(X)
        self.P[:, :, 1::2] = torch.cos(X)

    def forward(self, X):
        X = X + self.P[:, :X.shape[1], :].to(X.device)
        return self.dropout(X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class PositionalEncoding(tf.keras.layers.Layer):  #@save
    """위치 인코딩."""
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super().__init__()
        self.dropout = tf.keras.layers.Dropout(dropout)
        # 충분히 긴 P 생성
        self.P = np.zeros((1, max_len, num_hiddens))
        X = np.arange(max_len, dtype=np.float32).reshape(
            -1,1)/np.power(10000, np.arange(
            0, num_hiddens, 2, dtype=np.float32) / num_hiddens)
        self.P[:, :, 0::2] = np.sin(X)
        self.P[:, :, 1::2] = np.cos(X)
        
    def call(self, X, **kwargs):
        X = X + self.P[:, :X.shape[1], :]
        return self.dropout(X, **kwargs)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class PositionalEncoding(nn.Module):  #@save
    """위치 인코딩."""
    num_hiddens: int
    dropout: float
    max_len: int = 1000

    def setup(self):
        # 충분히 긴 P 생성
        self.P = d2l.zeros((1, self.max_len, self.num_hiddens))
        X = d2l.arange(self.max_len, dtype=jnp.float32).reshape(
            -1, 1) / jnp.power(10000, jnp.arange(
            0, self.num_hiddens, 2, dtype=jnp.float32) / self.num_hiddens)
        self.P = self.P.at[:, :, 0::2].set(jnp.sin(X))
        self.P = self.P.at[:, :, 1::2].set(jnp.cos(X))

    @nn.compact
    def __call__(self, X, training=False):
        # Flax sow API는 중간 변수를 캡처하는 데 사용됩니다
        self.sow('intermediates', 'P', self.P)
        X = X + self.P[:, :X.shape[1], :]
        return nn.Dropout(self.dropout)(X, deterministic=not training)
</code></pre>
<p>위치 임베딩 행렬 $\mathbf{P}$에서, [<strong>행은 시퀀스 내의 위치에 대응하고 열은 서로 다른 위치 인코딩 차원을 나타냅니다.</strong>]
아래 예제에서 위치 임베딩 행렬의 6번째와 7번째 열이 8번째와 9번째 열보다 더 높은 주파수를 가짐을 알 수 있습니다.
6번째와 7번째 열 사이의 오프셋(8번째와 9번째도 마찬가지)은 사인 및 코사인 함수의 교대 때문입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
encoding_dim, num_steps = 32, 60
pos_encoding = PositionalEncoding(encoding_dim, 0)
pos_encoding.initialize()
X = pos_encoding(np.zeros((1, num_steps, encoding_dim)))
P = pos_encoding.P[:, :X.shape[1], :]
d2l.plot(d2l.arange(num_steps), P[0, :, 6:10].T, xlabel='행 (위치)',
         figsize=(6, 2.5), legend=["Col %d" % d for d in d2l.arange(6, 10)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
encoding_dim, num_steps = 32, 60
pos_encoding = PositionalEncoding(encoding_dim, 0)
X = pos_encoding(d2l.zeros((1, num_steps, encoding_dim)))
P = pos_encoding.P[:, :X.shape[1], :]
d2l.plot(d2l.arange(num_steps), P[0, :, 6:10].T, xlabel='행 (위치)',
         figsize=(6, 2.5), legend=["Col %d" % d for d in d2l.arange(6, 10)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
encoding_dim, num_steps = 32, 60
pos_encoding = PositionalEncoding(encoding_dim, 0)
X = pos_encoding(tf.zeros((1, num_steps, encoding_dim)), training=False)
P = pos_encoding.P[:, :X.shape[1], :]
d2l.plot(np.arange(num_steps), P[0, :, 6:10].T, xlabel='행 (위치)',
         figsize=(6, 2.5), legend=["Col %d" % d for d in np.arange(6, 10)])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
encoding_dim, num_steps = 32, 60
pos_encoding = PositionalEncoding(encoding_dim, 0)
params = pos_encoding.init(d2l.get_key(), d2l.zeros((1, num_steps, encoding_dim)))
X, inter_vars = pos_encoding.apply(params, d2l.zeros((1, num_steps, encoding_dim)),
                                   mutable='intermediates')
P = inter_vars['intermediates']['P'][0]  # 중간 값 P 검색
P = P[:, :X.shape[1], :]
d2l.plot(d2l.arange(num_steps), P[0, :, 6:10].T, xlabel='행 (위치)',
         figsize=(6, 2.5), legend=["Col %d" % d for d in d2l.arange(6, 10)])
</code></pre>
<h3 id="절대-위치-정보-absolute-positional-information"><a class="header" href="#절대-위치-정보-absolute-positional-information">절대 위치 정보 (Absolute Positional Information)</a></h3>
<p>인코딩 차원을 따라 단조롭게 감소하는 주파수가 어떻게 절대 위치 정보와 관련되는지 확인하기 위해, $0, 1, \ldots, 7$의 [<strong>이진 표현</strong>]을 인쇄해 봅시다.
보시다시피 가장 낮은 비트, 두 번째로 낮은 비트, 세 번째로 낮은 비트는 각각 매 숫자, 매 두 숫자, 매 네 숫자마다 교대로 바뀝니다.</p>
<pre><code class="language-{.python .input}">%%tab all
for i in range(8):
    print(f'{i} 의 이진 표현: {i:&gt;03b}')
</code></pre>
<p>이진 표현에서 상위 비트는 하위 비트보다 주파수가 낮습니다.
마찬가지로 아래 히트맵에서 보여주듯이, [<strong>위치 인코딩은 삼각 함수를 사용하여 인코딩 차원을 따라 주파수를 감소시킵니다.</strong>]
출력이 부동 소수점 수이므로, 이러한 연속적인 표현은 이진 표현보다 공간 효율적입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
P = np.expand_dims(np.expand_dims(P[0, :, :], 0), 0)
d2l.show_heatmaps(P, xlabel='열 (인코딩 차원)',
                  ylabel='행 (위치)', figsize=(3.5, 4), cmap='Blues')
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
P = P[0, :, :].unsqueeze(0).unsqueeze(0)
d2l.show_heatmaps(P, xlabel='열 (인코딩 차원)',
                  ylabel='행 (위치)', figsize=(3.5, 4), cmap='Blues')
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
P = tf.expand_dims(tf.expand_dims(P[0, :, :], axis=0), axis=0)
d2l.show_heatmaps(P, xlabel='열 (인코딩 차원)',
                  ylabel='행 (위치)', figsize=(3.5, 4), cmap='Blues')
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
P = jnp.expand_dims(jnp.expand_dims(P[0, :, :], axis=0), axis=0)
d2l.show_heatmaps(P, xlabel='열 (인코딩 차원)',
                  ylabel='행 (위치)', figsize=(3.5, 4), cmap='Blues')
</code></pre>
<h3 id="상대-위치-정보-relative-positional-information"><a class="header" href="#상대-위치-정보-relative-positional-information">상대 위치 정보 (Relative Positional Information)</a></h3>
<p>절대 위치 정보를 캡처하는 것 외에도, 위의 위치 인코딩은 모델이 상대 위치에 따라 주의를 기울이는 법을 쉽게 배우도록 해 줍니다.
이는 임의의 고정된 위치 오프셋 $\delta$에 대해, 위치 $i + \delta$에서의 위치 인코딩이 위치 $i$에서의 선형 투영(linear projection)으로 표현될 수 있기 때문입니다.</p>
<p>이 투영은 수학적으로 설명될 수 있습니다.
$\omega_j = 1/10000^{2j/d}$라고 하면, :eqref:<code>eq_positional-encoding-def</code>의 임의의 $(p_{i, 2j}, p_{i, 2j+1})$ 쌍은 임의의 고정 오프셋 $\delta$에 대해 $(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$로 선형 투영될 수 있습니다.</p>
<p>$$\begin{aligned}
\begin{bmatrix} \cos(\delta \omega_j) &amp; \sin(\delta \omega_j) \  -\sin(\delta \omega_j) &amp; \cos(\delta \omega_j) \ \end{bmatrix}
\begin{bmatrix} p_{i, 2j} \  p_{i, 2j+1} \ \end{bmatrix}
=&amp;\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \ \end{bmatrix}
\=\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \  \cos\left((i+\delta) \omega_j\right) \ \end{bmatrix}
\=&amp;
\begin{bmatrix} p_{i+\delta, 2j} \  p_{i+\delta, 2j+1} \ \end{bmatrix},
\end{aligned}$$</p>
<p>여기서 $2\times 2$ 투영 행렬은 어떠한 위치 인덱스 $i$에도 의존하지 않습니다.</p>
<h2 id="요약-summary-49"><a class="header" href="#요약-summary-49">요약 (Summary)</a></h2>
<p>셀프 어텐션에서 쿼리, 키, 값은 모두 같은 곳에서 옵니다.
CNN과 셀프 어텐션 모두 병렬 계산이 가능하며 셀프 어텐션이 가장 짧은 최대 경로 길이를 갖습니다.
그러나 시퀀스 길이에 대한 이차적인 계산 복잡도는 매우 긴 시퀀스에 대해 셀프 어텐션을 금지할 정도로 느리게 만듭니다.
시퀀스 순서 정보를 사용하기 위해, 입력 표현에 위치 인코딩을 추가함으로써 절대적 또는 상대적 위치 정보를 주입할 수 있습니다.</p>
<h2 id="연습-문제-exercises-62"><a class="header" href="#연습-문제-exercises-62">연습 문제 (Exercises)</a></h2>
<ol>
<li>위치 인코딩이 있는 셀프 어텐션 레이어를 쌓아 시퀀스를 표현하는 심층 아키텍처를 설계한다고 가정해 봅시다. 가능한 문제는 무엇일까요?</li>
<li>학습 가능한 위치 인코딩 방법을 설계할 수 있습니까?</li>
<li>셀프 어텐션에서 비교되는 쿼리와 키 사이의 서로 다른 오프셋에 따라 서로 다른 학습된 임베딩을 할당할 수 있습니까? 힌트: 상대 위치 임베딩(relative position embeddings) :cite:<code>shaw2018self,huang2018music</code>을 참조할 수 있습니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/1651">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1652">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3870">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18030">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<h1 id="트랜스포머-아키텍처-the-transformer-architecture"><a class="header" href="#트랜스포머-아키텍처-the-transformer-architecture">트랜스포머 아키텍처 (The Transformer Architecture)</a></h1>
<p>:label:<code>sec_transformer</code></p>
<p>우리는 :numref:<code>subsec_cnn-rnn-self-attention</code>에서 CNN, RNN, 셀프 어텐션을 비교했습니다. 특히 셀프 어텐션은 병렬 계산과 가장 짧은 최대 경로 길이(maximum path length)라는 장점을 모두 가지고 있습니다. 따라서 셀프 어텐션을 사용하여 깊은 아키텍처를 설계하는 것은 매우 매력적입니다.</p>
<p>입력 표현을 위해 여전히 RNN에 의존했던 초기 셀프 어텐션 모델(:cite:<code>Cheng.Dong.Lapata.2016,Lin.Feng.Santos.ea.2017,Paulus.Xiong.Socher.2017</code>)과 달리, 트랜스포머(Transformer) 모델은 합성곱이나 순환 레이어 없이 오로지 어텐션 메커니즘에만 기반합니다(:cite:<code>Vaswani.Shazeer.Parmar.ea.2017</code>). 원래는 텍스트 데이터의 시퀀스 투 시퀀스(sequence-to-sequence) 학습을 위해 제안되었지만, 트랜스포머는 언어, 비전, 음성, 강화 학습 등 현대 딥러닝 응용 분야 전반에 걸쳐 널리 사용되고 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from d2l import mxnet as d2l
import math
from mxnet import autograd, init, np, npx
from mxnet.gluon import nn
import pandas as pd
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import math
import pandas as pd
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
from d2l import tensorflow as d2l
import numpy as np
import pandas as pd
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
from jax import numpy as jnp
import jax
import math
import pandas as pd
</code></pre>
<h2 id="모델-model-3"><a class="header" href="#모델-model-3">모델 (Model)</a></h2>
<p>인코더-디코더 아키텍처의 한 사례로서, 트랜스포머의 전체 아키텍처는 :numref:<code>fig_transformer</code>에 나와 있습니다. 보시는 바와 같이 트랜스포머는 인코더와 디코더로 구성됩니다. :numref:<code>fig_s2s_attention_details</code>의 시퀀스 투 시퀀스 학습을 위한 바다나우(Bahdanau) 어텐션과 대조적으로, 입력(소스) 및 출력(타겟) 시퀀스 임베딩은 셀프 어텐션 기반의 모듈을 쌓아 올린 인코더와 디코더에 공급되기 전에 포지셔널 인코딩(positional encoding)과 더해집니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/transformer.svg" alt="트랜스포머 아키텍처." />
:width:<code>320px</code>
:label:<code>fig_transformer</code></p>
<p>이제 :numref:<code>fig_transformer</code>에 있는 트랜스포머 아키텍처의 개요를 제공합니다. 높은 수준에서 볼 때, 트랜스포머 인코더는 여러 개의 동일한 레이어가 쌓인 형태이며, 각 레이어에는 두 개의 서브레이어($\textrm{sublayer}$라고 함)가 있습니다. 첫 번째는 멀티 헤드 셀프 어텐션 풀링이고, 두 번째는 포지션 와이즈(positionwise) 피드포워드 네트워크입니다. 구체적으로 인코더 셀프 어텐션에서 쿼리, 키, 값은 모두 이전 인코더 레이어의 출력에서 옵니다. :numref:<code>sec_resnet</code>의 ResNet 설계에서 영감을 받아, 두 서브레이어 주위에는 잔차 연결(residual connection)이 사용됩니다. 트랜스포머에서는 시퀀스의 모든 위치에 있는 임의의 입력 $\mathbf{x} \in \mathbb{R}^d$에 대해 $\textrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$여야 잔차 연결 $\mathbf{x} + \textrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$가 가능합니다. 잔차 연결을 통한 이 덧셈 직후에는 레이어 정규화(layer normalization)가 수행됩니다(:cite:<code>Ba.Kiros.Hinton.2016</code>). 결과적으로 트랜스포머 인코더는 입력 시퀀스의 각 위치에 대해 $d$차원 벡터 표현을 출력합니다.</p>
<p>트랜스포머 디코더 또한 잔차 연결과 레이어 정규화가 포함된 여러 개의 동일한 레이어 스택입니다. 인코더에서 설명한 두 서브레이어 외에도, 디코더는 이 두 레이어 사이에 인코더-디코더 어텐션이라고 알려진 세 번째 서브레이어를 삽입합니다. 인코더-디코더 어텐션에서 쿼리는 디코더의 셀프 어텐션 서브레이어 출력에서 오고, 키와 값은 트랜스포머 인코더의 출력에서 옵니다. 디코더 셀프 어텐션에서 쿼리, 키, 값은 모두 이전 디코더 레이어의 출력에서 옵니다. 그러나 디코더의 각 위치는 해당 위치까지의 디코더 내 모든 위치에만 어텐션을 수행할 수 있습니다. 이 <em>마스크된(masked)</em> 어텐션은 자기 회귀(autoregressive) 속성을 유지하여, 예측이 이미 생성된 출력 토큰에만 의존하도록 보장합니다.</p>
<p>우리는 이미 :numref:<code>sec_multihead-attention</code>에서 스케일드 닷 프로덕트(scaled dot product) 기반의 멀티 헤드 어텐션을, :numref:<code>subsec_positional-encoding</code>에서 포지셔널 인코딩을 설명하고 구현했습니다. 다음에서는 트랜스포머 모델의 나머지 부분을 구현할 것입니다.</p>
<h2 id="포지션-와이즈-피드포워드-네트워크-positionwise-feed-forward-networks"><a class="header" href="#포지션-와이즈-피드포워드-네트워크-positionwise-feed-forward-networks">포지션 와이즈 피드포워드 네트워크 (Positionwise Feed-Forward Networks)</a></h2>
<p>:label:<code>subsec_positionwise-ffn</code></p>
<p>포지션 와이즈 피드포워드 네트워크는 동일한 MLP를 사용하여 모든 시퀀스 위치에서의 표현을 변환합니다. 이것이 우리가 이를 <em>포지션 와이즈</em>라고 부르는 이유입니다. 아래 구현에서 (배치 크기, 타임스텝 수 또는 토큰 단위 시퀀스 길이, 은닉 유닛 수 또는 특성 차원) 형태의 입력 <code>X</code>는 2개 레이어의 MLP에 의해 (배치 크리, 타임스텝 수, <code>ffn_num_outputs</code>) 형태의 출력 텐서로 변환됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class PositionWiseFFN(nn.Block):  #@save
    """포지션 와이즈 피드포워드 네트워크."""
    def __init__(self, ffn_num_hiddens, ffn_num_outputs):
        super().__init__()
        self.dense1 = nn.Dense(ffn_num_hiddens, flatten=False,
                               activation='relu')
        self.dense2 = nn.Dense(ffn_num_outputs, flatten=False)

    def forward(self, X):
        return self.dense2(self.dense1(X))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class PositionWiseFFN(nn.Module):  #@save
    """포지션 와이즈 피드포워드 네트워크."""
    def __init__(self, ffn_num_hiddens, ffn_num_outputs):
        super().__init__()
        self.dense1 = nn.LazyLinear(ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.LazyLinear(ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class PositionWiseFFN(tf.keras.layers.Layer):  #@save
    """포지션 와이즈 피드포워드 네트워크."""
    def __init__(self, ffn_num_hiddens, ffn_num_outputs):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(ffn_num_hiddens)
        self.relu = tf.keras.layers.ReLU()
        self.dense2 = tf.keras.layers.Dense(ffn_num_outputs)

    def call(self, X):
        return self.dense2(self.relu(self.dense1(X)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class PositionWiseFFN(nn.Module):  #@save
    """포지션 와이즈 피드포워드 네트워크."""
    ffn_num_hiddens: int
    ffn_num_outputs: int

    def setup(self):
        self.dense1 = nn.Dense(self.ffn_num_hiddens)
        self.dense2 = nn.Dense(self.ffn_num_outputs)

    def __call__(self, X):
        return self.dense2(nn.relu(self.dense1(X)))
</code></pre>
<p>다음 예제는 [<strong>텐서의 가장 안쪽 차원이</strong>] 포지션 와이즈 피드포워드 네트워크의 출력 수로 변경됨을 보여줍니다. 동일한 MLP가 모든 위치에서 변환을 수행하므로, 이러한 모든 위치에서의 입력이 같으면 출력 또한 동일합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
ffn = PositionWiseFFN(4, 8)
ffn.initialize()
ffn(np.ones((2, 3, 4)))[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
ffn = PositionWiseFFN(4, 8)
ffn.eval()
ffn(d2l.ones((2, 3, 4)))[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
ffn = PositionWiseFFN(4, 8)
ffn(tf.ones((2, 3, 4)))[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
ffn = PositionWiseFFN(4, 8)
ffn.init_with_output(d2l.get_key(), jnp.ones((2, 3, 4)))[0][0]
</code></pre>
<h2 id="잔차-연결-및-레이어-정규화-residual-connection-and-layer-normalization"><a class="header" href="#잔차-연결-및-레이어-정규화-residual-connection-and-layer-normalization">잔차 연결 및 레이어 정규화 (Residual Connection and Layer Normalization)</a></h2>
<p>이제 :numref:<code>fig_transformer</code>의 "add &amp; norm" 구성 요소에 집중해 봅시다. 이 섹션의 시작 부분에서 설명했듯이, 이는 잔차 연결 직후에 레이어 정규화가 뒤따르는 구조입니다. 두 가지 모두 효과적인 깊은 아키텍처의 핵심입니다.</p>
<p>:numref:<code>sec_batch_norm</code>에서 우리는 배치 정규화(batch normalization)가 미니배치 내의 예제들에 걸쳐 어떻게 중심을 재조정하고 스케일을 조정하는지 설명했습니다. :numref:<code>subsec_layer-normalization-in-bn</code>에서 논의했듯이, 레이어 정규화는 특성 차원에 걸쳐 정규화한다는 점을 제외하면 배치 정규화와 동일하며, 따라서 스케일 독립성과 배치 크기 독립성의 이점을 누릴 수 있습니다. 컴퓨터 비전에서의 널리 퍼진 응용에도 불구하고, 배치 정규화는 입력이 종종 가변 길이 시퀀스인 자연어 처리 작업에서 레이어 정규화보다 실무적으로 덜 효과적인 경우가 많습니다.</p>
<p>다음 코드 스니펫은 [<strong>레이어 정규화와 배치 정규화에 의한 서로 다른 차원에 걸친 정규화를 비교합니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
ln = nn.LayerNorm()
ln.initialize()
bn = nn.BatchNorm()
bn.initialize()
X = d2l.tensor([[1, 2], [2, 3]])
# 훈련 모드에서 X로부터 평균과 분산을 계산합니다.
with autograd.record():
    print('layer norm:', ln(X), '\nbatch norm:', bn(X))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
ln = nn.LayerNorm(2)
bn = nn.LazyBatchNorm1d()
X = d2l.tensor([[1, 2], [2, 3]], dtype=torch.float32)
# 훈련 모드에서 X로부터 평균과 분산을 계산합니다.
print('layer norm:', ln(X), '\nbatch norm:', bn(X))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
ln = tf.keras.layers.LayerNormalization()
bn = tf.keras.layers.BatchNormalization()
X = tf.constant([[1, 2], [2, 3]], dtype=tf.float32)
print('layer norm:', ln(X), '\nbatch norm:', bn(X, training=True))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
ln = nn.LayerNorm()
bn = nn.BatchNorm()
X = d2l.tensor([[1, 2], [2, 3]], dtype=d2l.float32)
# 훈련 모드에서 X로부터 평균과 분산을 계산합니다.
print('layer norm:', ln.init_with_output(d2l.get_key(), X)[0],
      '\nbatch norm:', bn.init_with_output(d2l.get_key(), X,
                                           use_running_average=False)[0])
</code></pre>
<p>이제 [<strong>잔차 연결과 그 뒤를 잇는 레이어 정규화를 사용하여</strong>] <code>AddNorm</code> 클래스를 구현할 수 있습니다. 정규화를 위해 드롭아웃(dropout)도 적용됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class AddNorm(nn.Block):  #@save
    """잔차 연결과 그 뒤를 잇는 레이어 정규화."""
    def __init__(self, dropout):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm()

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class AddNorm(nn.Module):  #@save
    """잔차 연결과 그 뒤를 잇는 레이어 정규화."""
    def __init__(self, norm_shape, dropout):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(norm_shape)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class AddNorm(tf.keras.layers.Layer):  #@save
    """잔차 연결과 그 뒤를 잇는 레이어 정규화."""
    def __init__(self, norm_shape, dropout):
        super().__init__()
        self.dropout = tf.keras.layers.Dropout(dropout)
        self.ln = tf.keras.layers.LayerNormalization(norm_shape)

    def call(self, X, Y, **kwargs):
        return self.ln(self.dropout(Y, **kwargs) + X)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class AddNorm(nn.Module):  #@save
    """잔차 연결과 그 뒤를 잇는 레이어 정규화."""
    dropout: int

    @nn.compact
    def __call__(self, X, Y, training=False):
        return nn.LayerNorm()(
            nn.Dropout(self.dropout)(Y, deterministic=not training) + X)
</code></pre>
<p>잔차 연결은 두 입력의 모양이 같아야 하므로, [<strong>덧셈 연산 후의 출력 텐서도 같은 모양을 가집니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
add_norm = AddNorm(0.5)
add_norm.initialize()
shape = (2, 3, 4)
d2l.check_shape(add_norm(d2l.ones(shape), d2l.ones(shape)), shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
add_norm = AddNorm(4, 0.5)
shape = (2, 3, 4)
d2l.check_shape(add_norm(d2l.ones(shape), d2l.ones(shape)), shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# Normalized_shape은: [i for i in range(len(input.shape))][1:] 입니다.
add_norm = AddNorm([1, 2], 0.5)
shape = (2, 3, 4)
d2l.check_shape(add_norm(tf.ones(shape), tf.ones(shape), training=False),
                shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
add_norm = AddNorm(0.5)
shape = (2, 3, 4)
output, _ = add_norm.init_with_output(d2l.get_key(), d2l.ones(shape),
                                      d2l.ones(shape))
d2l.check_shape(output, shape)
</code></pre>
<h2 id="인코더-encoder-2"><a class="header" href="#인코더-encoder-2">인코더 (Encoder)</a></h2>
<p>:label:<code>subsec_transformer-encoder</code></p>
<p>트랜스포머 인코더를 조립하기 위한 모든 필수 구성 요소가 준비되었으므로, [<strong>인코더 내의 단일 레이어</strong>]를 구현하는 것부터 시작하겠습니다. 다음 <code>TransformerEncoderBlock</code> 클래스는 두 개의 서브레이어, 즉 멀티 헤드 셀프 어텐션과 포지션 와이즈 피드포워드 네트워크를 포함하며, 두 서브레이어 주위에는 잔차 연결과 그 뒤를 잇는 레이어 정규화가 사용됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class TransformerEncoderBlock(nn.Block):  #@save
    """트랜스포머 인코더 블록."""
    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,
                 use_bias=False):
        super().__init__()
        self.attention = d2l.MultiHeadAttention(
            num_hiddens, num_heads, dropout, use_bias)
        self.addnorm1 = AddNorm(dropout)
        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(dropout)

    def forward(self, X, valid_lens):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        return self.addnorm2(Y, self.ffn(Y))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class TransformerEncoderBlock(nn.Module):  #@save
    """트랜스포머 인코더 블록."""
    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,
                 use_bias=False):
        super().__init__()
        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,
                                                dropout, use_bias)
        self.addnorm1 = AddNorm(num_hiddens, dropout)
        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(num_hiddens, dropout)

    def forward(self, X, valid_lens):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        return self.addnorm2(Y, self.ffn(Y))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class TransformerEncoderBlock(tf.keras.layers.Layer):  #@save
    """트랜스포머 인코더 블록."""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_hiddens, num_heads, dropout, bias=False):
        super().__init__()
        self.attention = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            bias)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(norm_shape, dropout)

    def call(self, X, valid_lens, **kwargs):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens, **kwargs),
                          **kwargs)
        return self.addnorm2(Y, self.ffn(Y), **kwargs)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class TransformerEncoderBlock(nn.Module):  #@save
    """트랜스포머 인코더 블록."""
    num_hiddens: int
    ffn_num_hiddens: int
    num_heads: int
    dropout: float
    use_bias: bool = False

    def setup(self):
        self.attention = d2l.MultiHeadAttention(self.num_hiddens, self.num_heads,
                                                self.dropout, self.use_bias)
        self.addnorm1 = AddNorm(self.dropout)
        self.ffn = PositionWiseFFN(self.ffn_num_hiddens, self.num_hiddens)
        self.addnorm2 = AddNorm(self.dropout)

    def __call__(self, X, valid_lens, training=False):
        output, attention_weights = self.attention(X, X, X, valid_lens,
                                                   training=training)
        Y = self.addnorm1(X, output, training=training)
        return self.addnorm2(Y, self.ffn(Y), training=training), attention_weights
</code></pre>
<p>보시는 바와 같이, [<strong>트랜스포머 인코더의 어떤 레이어도 입력의 모양을 변경하지 않습니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
X = d2l.ones((2, 100, 24))
valid_lens = d2l.tensor([3, 2])
encoder_blk = TransformerEncoderBlock(24, 48, 8, 0.5)
encoder_blk.initialize()
d2l.check_shape(encoder_blk(X, valid_lens), X.shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
X = d2l.ones((2, 100, 24))
valid_lens = d2l.tensor([3, 2])
encoder_blk = TransformerEncoderBlock(24, 48, 8, 0.5)
encoder_blk.eval()
d2l.check_shape(encoder_blk(X, valid_lens), X.shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
X = tf.ones((2, 100, 24))
valid_lens = tf.constant([3, 2])
norm_shape = [i for i in range(len(X.shape))][1:]
encoder_blk = TransformerEncoderBlock(24, 24, 24, 24, norm_shape, 48, 8, 0.5)
d2l.check_shape(encoder_blk(X, valid_lens, training=False), X.shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X = jnp.ones((2, 100, 24))
valid_lens = jnp.array([3, 2])
encoder_blk = TransformerEncoderBlock(24, 48, 8, 0.5)
(output, _), _ = encoder_blk.init_with_output(d2l.get_key(), X, valid_lens,
                                              training=False)
d2l.check_shape(output, X.shape)
</code></pre>
<p>다음 [<strong>트랜스포머 인코더</strong>] 구현에서, 우리는 위에서 만든 <code>TransformerEncoderBlock</code> 클래스의 인스턴스를 <code>num_blks</code>개만큼 쌓습니다. 값이 항상 -1과 1 사이인 고정된 포지셔널 인코딩을 사용하므로, 입력 임베딩과 포지셔널 인코딩을 합치기 전에 스케일을 맞추기 위해 학습 가능한 입력 임베딩 값에 임베딩 차원의 제곱근을 곱합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class TransformerEncoder(d2l.Encoder):  #@save
    """트랜스포머 인코더."""
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,
                 num_heads, num_blks, dropout, use_bias=False):
        super().__init__()
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for _ in range(num_blks):
            self.blks.add(TransformerEncoderBlock(
                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))
        self.initialize()

    def forward(self, X, valid_lens):
        # 포지셔널 인코딩 값이 -1과 1 사이이므로, 임베딩 값들을 합치기 전에
        # 스케일을 맞추기 위해 임베딩 차원의 제곱근을 곱합니다.
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self.attention_weights = [None] * len(self.blks)
        for i, blk in enumerate(self.blks):
            X = blk(X, valid_lens)
            self.attention_weights[
                i] = blk.attention.attention.attention_weights
        return X
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class TransformerEncoder(d2l.Encoder):  #@save
    """트랜스포머 인코더."""
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,
                 num_heads, num_blks, dropout, use_bias=False):
        super().__init__()
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_blks):
            self.blks.add_module("block"+str(i), TransformerEncoderBlock(
                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))

    def forward(self, X, valid_lens):
        # 포지셔널 인코딩 값이 -1과 1 사이이므로, 임베딩 값들을 합치기 전에
        # 스케일을 맞추기 위해 임베딩 차원의 제곱근을 곱합니다.
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self.attention_weights = [None] * len(self.blks)
        for i, blk in enumerate(self.blks):
            X = blk(X, valid_lens)
            self.attention_weights[
                i] = blk.attention.attention.attention_weights
        return X
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class TransformerEncoder(d2l.Encoder):  #@save
    """트랜스포머 인코더."""
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_hiddens, num_heads,
                 num_blks, dropout, bias=False):
        super().__init__()
        self.num_hiddens = num_hiddens
        self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = [TransformerEncoderBlock(
            key_size, query_size, value_size, num_hiddens, norm_shape,
            ffn_num_hiddens, num_heads, dropout, bias) for _ in range(
            num_blks)]

    def call(self, X, valid_lens, **kwargs):
        # 포지셔널 인코딩 값이 -1과 1 사이이므로, 임베딩 값들을 합치기 전에
        # 스케일을 맞추기 위해 임베딩 차원의 제곱근을 곱합니다.
        X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(
            tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)
        self.attention_weights = [None] * len(self.blks)
        for i, blk in enumerate(self.blks):
            X = blk(X, valid_lens, **kwargs)
            self.attention_weights[
                i] = blk.attention.attention.attention_weights
        return X
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class TransformerEncoder(d2l.Encoder):  #@save
    """트랜스포머 인코더."""
    vocab_size: int
    num_hiddens:int
    ffn_num_hiddens: int
    num_heads: int
    num_blks: int
    dropout: float
    use_bias: bool = False

    def setup(self):
        self.embedding = nn.Embed(self.vocab_size, self.num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(self.num_hiddens,
                                                   self.dropout)
        self.blks = [TransformerEncoderBlock(self.num_hiddens,
                                             self.ffn_num_hiddens,
                                             self.num_heads,
                                             self.dropout, self.use_bias)
                     for _ in range(self.num_blks)]

    def __call__(self, X, valid_lens, training=False):
        # 포지셔널 인코딩 값이 -1과 1 사이이므로, 임베딩 값들을 합치기 전에
        # 스케일을 맞추기 위해 임베딩 차원의 제곱근을 곱합니다.
        X = self.embedding(X) * math.sqrt(self.num_hiddens)
        X = self.pos_encoding(X, training=training)
        attention_weights = [None] * len(self.blks)
        for i, blk in enumerate(self.blks):
            X, attention_w = blk(X, valid_lens, training=training)
            attention_weights[i] = attention_w
        # 중간 변수를 캡처하기 위해 Flax sow API가 사용됩니다.
        self.sow('intermediates', 'enc_attention_weights', attention_weights)
        return X
</code></pre>
<p>아래에서 [<strong>2개 레이어의 트랜스포머 인코더를 생성하기 위해</strong>] 하이퍼파라미터를 지정합니다. 트랜스포머 인코더 출력의 모양은 (배치 크기, 타임스텝 수, <code>num_hiddens</code>)입니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)
d2l.check_shape(encoder(np.ones((2, 100)), valid_lens), (2, 100, 24))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)
d2l.check_shape(encoder(d2l.ones((2, 100), dtype=torch.long), valid_lens),
                (2, 100, 24))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
encoder = TransformerEncoder(200, 24, 24, 24, 24, [1, 2], 48, 8, 2, 0.5)
d2l.check_shape(encoder(tf.ones((2, 100)), valid_lens, training=False),
                (2, 100, 24))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)
d2l.check_shape(encoder.init_with_output(d2l.get_key(),
                                         jnp.ones((2, 100), dtype=jnp.int32),
                                         valid_lens)[0],
                (2, 100, 24))
</code></pre>
<h2 id="디코더-decoder-2"><a class="header" href="#디코더-decoder-2">디코더 (Decoder)</a></h2>
<p>:numref:<code>fig_transformer</code>에 표시된 것처럼, [<strong>트랜스포머 디코더는 여러 개의 동일한 레이어로 구성됩니다.</strong>] 각 레이어는 아래의 <code>TransformerDecoderBlock</code> 클래스에서 구현되며, 세 개의 서브레이어(디코더 셀프 어텐션, 인코더-디코더 어텐션, 포지션 와이즈 피드포워드 네트워크)를 포함합니다. 이러한 서브레이어들은 주위에 잔차 연결과 그 뒤를 잇는 레이어 정규화를 사용합니다.</p>
<p>이 섹션의 앞부분에서 설명했듯이, 마스크된 멀티 헤드 디코더 셀프 어텐션(첫 번째 서브레이어)에서 쿼리, 키, 값은 모두 이전 디코더 레이어의 출력에서 옵니다. 시퀀스 투 시퀀스 모델을 훈련할 때 출력 시퀀스의 모든 위치(타임스텝)에 있는 토큰은 알려져 있습니다. 그러나 예측 중에는 출력 시퀀스가 토큰 단위로 생성되므로, 임의의 디코더 타임스텝에서 생성된 토큰들만 디코더 셀프 어텐션에서 사용될 수 있습니다. 디코더에서 자기 회귀를 유지하기 위해, 마스크된 셀프 어텐션은 임의의 쿼리가 해당 쿼리 위치까지의 디코더 내 위치들에만 어텐션을 수행하도록 <code>dec_valid_lens</code>를 지정합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class TransformerDecoderBlock(nn.Block):
    # 트랜스포머 디코더의 i번째 블록
    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):
        super().__init__()
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads,
                                                 dropout)
        self.addnorm1 = AddNorm(dropout)
        self.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads,
                                                 dropout)
        self.addnorm2 = AddNorm(dropout)
        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        self.addnorm3 = AddNorm(dropout)

    def forward(self, X, state):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # 훈련 중에는 임의의 출력 시퀀스의 모든 토큰이 동시에 처리되므로,
        # 초기화된 대로 state[2][self.i]는 None입니다. 예측 중에 토큰 단위로
        # 출력 시퀀스를 디코딩할 때, state[2][self.i]는 현재 타임스텝까지
        # i번째 블록에서 디코딩된 출력의 표현을 포함합니다.
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = np.concatenate((state[2][self.i], X), axis=1)
        state[2][self.i] = key_values

        if autograd.is_training():
            batch_size, num_steps, _ = X.shape
            # dec_valid_lens의 모양: (batch_size, num_steps), 여기서 모든
            # 행은 [1, 2, ..., num_steps] 입니다.
            dec_valid_lens = np.tile(np.arange(1, num_steps + 1, ctx=X.ctx),
                                     (batch_size, 1))
        else:
            dec_valid_lens = None
        # 셀프 어텐션
        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
        Y = self.addnorm1(X, X2)
        # 인코더-디코더 어텐션. enc_outputs의 모양:
        # (batch_size, num_steps, num_hiddens)
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z = self.addnorm2(Y, Y2)
        return self.addnorm3(Z, self.ffn(Z)), state
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class TransformerDecoderBlock(nn.Module):
    # 트랜스포머 디코더의 i번째 블록
    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):
        super().__init__()
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads,
                                                 dropout)
        self.addnorm1 = AddNorm(num_hiddens, dropout)
        self.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads,
                                                 dropout)
        self.addnorm2 = AddNorm(num_hiddens, dropout)
        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        self.addnorm3 = AddNorm(num_hiddens, dropout)

    def forward(self, X, state):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # 훈련 중에는 임의의 출력 시퀀스의 모든 토큰이 동시에 처리되므로,
        # 초기화된 대로 state[2][self.i]는 None입니다. 예측 중에 토큰 단위로
        # 출력 시퀀스를 디코딩할 때, state[2][self.i]는 현재 타임스텝까지
        # i번째 블록에서 디코딩된 출력의 표현을 포함합니다.
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = torch.cat((state[2][self.i], X), dim=1)
        state[2][self.i] = key_values
        if self.training:
            batch_size, num_steps, _ = X.shape
            # dec_valid_lens의 모양: (batch_size, num_steps), 여기서 모든
            # 행은 [1, 2, ..., num_steps] 입니다.
            dec_valid_lens = torch.arange(
                1, num_steps + 1, device=X.device).repeat(batch_size, 1)
        else:
            dec_valid_lens = None
        # 셀프 어텐션
        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
        Y = self.addnorm1(X, X2)
        # 인코더-디코더 어텐션. enc_outputs의 모양:
        # (batch_size, num_steps, num_hiddens)
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z = self.addnorm2(Y, Y2)
        return self.addnorm3(Z, self.ffn(Z)), state
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class TransformerDecoderBlock(tf.keras.layers.Layer):
    # 트랜스포머 디코더의 i번째 블록
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_hiddens, num_heads, dropout, i):
        super().__init__()
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.attention2 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm2 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        self.addnorm3 = AddNorm(norm_shape, dropout)

    def call(self, X, state, **kwargs):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # 훈련 중에는 임의의 출력 시퀀스의 모든 토큰이 동시에 처리되므로,
        # 초기화된 대로 state[2][self.i]는 None입니다. 예측 중에 토큰 단위로
        # 출력 시퀀스를 디코딩할 때, state[2][self.i]는 현재 타임스텝까지
        # i번째 블록에서 디코딩된 출력의 표현을 포함합니다.
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = tf.concat((state[2][self.i], X), axis=1)
        state[2][self.i] = key_values
        if kwargs["training"]:
            batch_size, num_steps, _ = X.shape
            # dec_valid_lens의 모양: (batch_size, num_steps), 여기서 모든
            # 행은 [1, 2, ..., num_steps] 입니다.
            dec_valid_lens = tf.repeat(
                tf.reshape(tf.range(1, num_steps + 1), 
                           shape=(-1, num_steps)), repeats=batch_size, axis=0)
        else:
            dec_valid_lens = None
        # 셀프 어텐션
        X2 = self.attention1(X, key_values, key_values, dec_valid_lens, 
                             **kwargs)
        Y = self.addnorm1(X, X2, **kwargs)
        # 인코더-디코더 어텐션. enc_outputs의 모양:
        # (batch_size, num_steps, num_hiddens)
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens,
                             **kwargs)
        Z = self.addnorm2(Y, Y2, **kwargs)
        return self.addnorm3(Z, self.ffn(Z), **kwargs), state
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class TransformerDecoderBlock(nn.Module):
    # 트랜스포머 디코더의 i번째 블록
    num_hiddens: int
    ffn_num_hiddens: int
    num_heads: int
    dropout: float
    i: int

    def setup(self):
        self.attention1 = d2l.MultiHeadAttention(self.num_hiddens,
                                                 self.num_heads,
                                                 self.dropout)
        self.addnorm1 = AddNorm(self.dropout)
        self.attention2 = d2l.MultiHeadAttention(self.num_hiddens,
                                                 self.num_heads,
                                                 self.dropout)
        self.addnorm2 = AddNorm(self.dropout)
        self.ffn = PositionWiseFFN(self.ffn_num_hiddens, self.num_hiddens)
        self.addnorm3 = AddNorm(self.dropout)

    def __call__(self, X, state, training=False):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # 훈련 중에는 임의의 출력 시퀀스의 모든 토큰이 동시에 처리되므로,
        # 초기화된 대로 state[2][self.i]는 None입니다. 예측 중에 토큰 단위로
        # 출력 시퀀스를 디코딩할 때, state[2][self.i]는 현재 타임스텝까지
        # i번째 블록에서 디코딩된 출력의 표현을 포함합니다.
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = jnp.concatenate((state[2][self.i], X), axis=1)
        state[2][self.i] = key_values
        if training:
            batch_size, num_steps, _ = X.shape
            # dec_valid_lens의 모양: (batch_size, num_steps), 여기서 모든
            # 행은 [1, 2, ..., num_steps] 입니다.
            dec_valid_lens = jnp.tile(jnp.arange(1, num_steps + 1),
                                      (batch_size, 1))
        else:
            dec_valid_lens = None
        # 셀프 어텐션
        X2, attention_w1 = self.attention1(X, key_values, key_values,
                                           dec_valid_lens, training=training)
        Y = self.addnorm1(X, X2, training=training)
        # 인코더-디코더 어텐션. enc_outputs의 모양:
        # (batch_size, num_steps, num_hiddens)
        Y2, attention_w2 = self.attention2(Y, enc_outputs, enc_outputs,
                                           enc_valid_lens, training=training)
        Z = self.addnorm2(Y, Y2, training=training)
        return self.addnorm3(Z, self.ffn(Z), training=training), state, attention_w1, attention_w2
</code></pre>
<p>인코더-디코더 어텐션에서의 스케일드 닷 프로덕트 연산과 잔차 연결에서의 덧셈 연산을 용이하게 하기 위해, [<strong>디코더의 특성 차원(<code>num_hiddens</code>)은 인코더의 특성 차원과 동일합니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
decoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)
decoder_blk.initialize()
X = np.ones((2, 100, 24))
state = [encoder_blk(X, valid_lens), valid_lens, [None]]
d2l.check_shape(decoder_blk(X, state)[0], X.shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
decoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)
X = d2l.ones((2, 100, 24))
state = [encoder_blk(X, valid_lens), valid_lens, [None]]
d2l.check_shape(decoder_blk(X, state)[0], X.shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
decoder_blk = TransformerDecoderBlock(24, 24, 24, 24, [1, 2], 48, 8, 0.5, 0)
X = tf.ones((2, 100, 24))
state = [encoder_blk(X, valid_lens), valid_lens, [None]]
d2l.check_shape(decoder_blk(X, state, training=False)[0], X.shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
decoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)
X = d2l.ones((2, 100, 24))
state = [encoder_blk.init_with_output(d2l.get_key(), X, valid_lens)[0][0],
         valid_lens, [None]]
d2l.check_shape(decoder_blk.init_with_output(d2l.get_key(), X, state)[0][0],
                X.shape)
</code></pre>
<p>이제 <code>TransformerDecoderBlock</code>의 <code>num_blks</code>개 인스턴스로 구성된 [<strong>전체 트랜스포머 디코더를 구축합니다.</strong>] 마지막에 완전 연결 레이어는 <code>vocab_size</code>개의 가능한 모든 출력 토큰에 대한 예측을 계산합니다. 디코더 셀프 어텐션 가중치와 인코더-디코더 어텐션 가중치는 나중에 시각화하기 위해 저장됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_blks, dropout):
        super().__init__()
        self.num_hiddens = num_hiddens
        self.num_blks = num_blks
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_blks):
            self.blks.add(TransformerDecoderBlock(
                num_hiddens, ffn_num_hiddens, num_heads, dropout, i))
        self.dense = nn.Dense(vocab_size, flatten=False)
        self.initialize()

    def init_state(self, enc_outputs, enc_valid_lens):
        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]

    def forward(self, X, state):
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state)
            # 디코더 셀프 어텐션 가중치
            self._attention_weights[0][
                i] = blk.attention1.attention.attention_weights
            # 인코더-디코더 어텐션 가중치
            self._attention_weights[1][
                i] = blk.attention2.attention.attention_weights
        return self.dense(X), state

    @property
    def attention_weights(self):
        return self._attention_weights
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_blks, dropout):
        super().__init__()
        self.num_hiddens = num_hiddens
        self.num_blks = num_blks
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_blks):
            self.blks.add_module("block"+str(i), TransformerDecoderBlock(
                num_hiddens, ffn_num_hiddens, num_heads, dropout, i))
        self.dense = nn.LazyLinear(vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens):
        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]

    def forward(self, X, state):
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state)
            # 디코더 셀프 어텐션 가중치
            self._attention_weights[0][
                i] = blk.attention1.attention.attention_weights
            # 인코더-디코더 어텐션 가중치
            self._attention_weights[1][
                i] = blk.attention2.attention.attention_weights
        return self.dense(X), state

    @property
    def attention_weights(self):
        return self._attention_weights
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_hiddens, num_heads,
                 num_blks, dropout):
        super().__init__()
        self.num_hiddens = num_hiddens
        self.num_blks = num_blks
        self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = [TransformerDecoderBlock(
            key_size, query_size, value_size, num_hiddens, norm_shape,
            ffn_num_hiddens, num_heads, dropout, i)
                     for i in range(num_blks)]
        self.dense = tf.keras.layers.Dense(vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens):
        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]

    def call(self, X, state, **kwargs):
        X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(
            tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)
        # 디코더에 2개의 어텐션 레이어
        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state, **kwargs)
            # 디코더 셀프 어텐션 가중치
            self._attention_weights[0][i] = (
                blk.attention1.attention.attention_weights)
            # 인코더-디코더 어텐션 가중치
            self._attention_weights[1][i] = (
                blk.attention2.attention.attention_weights)
        return self.dense(X), state

    @property
    def attention_weights(self):
        return self._attention_weights
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class TransformerDecoder(nn.Module):
    vocab_size: int
    num_hiddens: int
    ffn_num_hiddens: int
    num_heads: int
    num_blks: int
    dropout: float

    def setup(self):
        self.embedding = nn.Embed(self.vocab_size, self.num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(self.num_hiddens,
                                                   self.dropout)
        self.blks = [TransformerDecoderBlock(self.num_hiddens,
                                             self.ffn_num_hiddens,
                                             self.num_heads, self.dropout, i)
                     for i in range(self.num_blks)]
        self.dense = nn.Dense(self.vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens):
        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]

    def __call__(self, X, state, training=False):
        X = self.embedding(X) * jnp.sqrt(jnp.float32(self.num_hiddens))
        X = self.pos_encoding(X, training=training)
        attention_weights = [[None] * len(self.blks) for _ in range(2)]
        for i, blk in enumerate(self.blks):
            X, state, attention_w1, attention_w2 = blk(X, state,
                                                       training=training)
            # 디코더 셀프 어텐션 가중치
            attention_weights[0][i] = attention_w1
            # 인코더-디코더 어텐션 가중치
            attention_weights[1][i] = attention_w2
        # 중간 변수를 캡처하기 위해 Flax sow API가 사용됩니다.
        self.sow('intermediates', 'dec_attention_weights', attention_weights)
        return self.dense(X), state
</code></pre>
<h2 id="훈련-training-21"><a class="header" href="#훈련-training-21">훈련 (Training)</a></h2>
<p>트랜스포머 아키텍처를 따라 인코더-디코더 모델을 인스턴스화해 봅시다. 여기서는 트랜스포머 인코더와 트랜스포머 디코더 모두 4-헤드 어텐션을 사용하는 2개 레이어를 갖도록 지정합니다. :numref:<code>sec_seq2seq_training</code>에서와 같이, 영어-프랑스어 기계 번역 데이터셋에서 시퀀스 투 시퀀스 학습을 위해 트랜스포머 모델을 훈련합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
data = d2l.MTFraEng(batch_size=128)
num_hiddens, num_blks, dropout = 256, 2, 0.2
ffn_num_hiddens, num_heads = 64, 4
if tab.selected('tensorflow'):
    key_size, query_size, value_size = 256, 256, 256
    norm_shape = [2]
if tab.selected('pytorch', 'mxnet', 'jax'):
    encoder = TransformerEncoder(
        len(data.src_vocab), num_hiddens, ffn_num_hiddens, num_heads,
        num_blks, dropout)
    decoder = TransformerDecoder(
        len(data.tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,
        num_blks, dropout)
if tab.selected('mxnet', 'pytorch'):
    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                        lr=0.001)
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)
if tab.selected('jax'):
    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                        lr=0.001, training=True)
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)
if tab.selected('tensorflow'):
    with d2l.try_gpu():
        encoder = TransformerEncoder(
            len(data.src_vocab), key_size, query_size, value_size, num_hiddens,
            norm_shape, ffn_num_hiddens, num_heads, num_blks, dropout)
        decoder = TransformerDecoder(
            len(data.tgt_vocab), key_size, query_size, value_size, num_hiddens,
            norm_shape, ffn_num_hiddens, num_heads, num_blks, dropout)
        model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['&lt;pad&gt;'],
                            lr=0.001)
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)
trainer.fit(model, data)
</code></pre>
<p>훈련 후에는 트랜스포머 모델을 사용하여 [<strong>몇 가지 영어 문장을</strong>] 프랑스어로 번역하고 BLEU 점수를 계산합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
engs = ['go .', 'i lost .', 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
if tab.selected('pytorch', 'mxnet', 'tensorflow'):
    preds, _ = model.predict_step(
        data.build(engs, fras), d2l.try_gpu(), data.num_steps)
if tab.selected('jax'):
    preds, _ = model.predict_step(
        trainer.state.params, data.build(engs, fras), data.num_steps)
for en, fr, p in zip(engs, fras, preds):
    translation = []
    for token in data.tgt_vocab.to_tokens(p):
        if token == '&lt;eos&gt;':
            break
        translation.append(token)
    print(f'{en} =&gt; {translation}, bleu,'
          f'{d2l.bleu(" ".join(translation), fr, k=2):.3f}')
</code></pre>
<p>마지막 영어 문장을 프랑스어로 번역할 때 [<strong>트랜스포머 어텐션 가중치를 시각화해 봅시다.</strong>] 인코더 셀프 어텐션 가중치의 모양은 (인코더 레이어 수, 어텐션 헤드 수, <code>num_steps</code> 또는 쿼리 수, <code>num_steps</code> 또는 키-값 쌍의 수)입니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
_, dec_attention_weights = model.predict_step(
    data.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)
enc_attention_weights = d2l.concat(model.encoder.attention_weights, 0)
shape = (num_blks, num_heads, -1, data.num_steps)
enc_attention_weights = d2l.reshape(enc_attention_weights, shape)
d2l.check_shape(enc_attention_weights,
                (num_blks, num_heads, data.num_steps, data.num_steps))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
_, (dec_attention_weights, enc_attention_weights) = model.predict_step(
    trainer.state.params, data.build([engs[-1]], [fras[-1]]),
    data.num_steps, True)
enc_attention_weights = d2l.concat(enc_attention_weights, 0)
shape = (num_blks, num_heads, -1, data.num_steps)
enc_attention_weights = d2l.reshape(enc_attention_weights, shape)
d2l.check_shape(enc_attention_weights,
                (num_blks, num_heads, data.num_steps, data.num_steps))
</code></pre>
<p>인코더 셀프 어텐션에서 쿼리와 키는 모두 동일한 입력 시퀀스에서 옵니다. 패딩 토큰은 의미를 갖지 않으므로, 입력 시퀀스의 유효 길이가 지정되면 어떤 쿼리도 패딩 토큰의 위치에 어텐션을 수행하지 않습니다. 다음에서는 멀티 헤드 어텐션 가중치의 두 레이어가 행별로 표시됩니다. 각 헤드는 쿼리, 키, 값의 별도 표현 서브스페이스에 기반하여 독립적으로 어텐션을 수행합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, tensorflow, jax
d2l.show_heatmaps(
    enc_attention_weights, xlabel='키 위치', ylabel='쿼리 위치',
    titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
d2l.show_heatmaps(
    enc_attention_weights.cpu(), xlabel='키 위치',
    ylabel='쿼리 위치', titles=['Head %d' % i for i in range(1, 5)],
    figsize=(7, 3.5))
</code></pre>
<p>[<strong>디코더 셀프 어텐션 가중치와 인코더-디코더 어텐션 가중치를 시각화하려면 더 많은 데이터 조작이 필요합니다.</strong>] 예를 들어, 마스크된 어텐션 가중치를 0으로 채웁니다. 디코더 셀프 어텐션 가중치와 인코더-디코더 어텐션 가중치 모두 동일한 쿼리(시퀀스 시작 토큰 뒤에 출력 토큰들이 오고 필요한 경우 시퀀스 종료 토큰이 따름)를 갖는다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
dec_attention_weights_2d = [d2l.tensor(head[0]).tolist()
                            for step in dec_attention_weights
                            for attn in step for blk in attn for head in blk]
dec_attention_weights_filled = d2l.tensor(
    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)
dec_attention_weights = d2l.reshape(dec_attention_weights_filled, (
    -1, 2, num_blks, num_heads, data.num_steps))
dec_self_attention_weights, dec_inter_attention_weights = \
    dec_attention_weights.transpose(1, 2, 3, 0, 4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
dec_attention_weights_2d = [head[0].tolist()
                            for step in dec_attention_weights
                            for attn in step for blk in attn for head in blk]
dec_attention_weights_filled = d2l.tensor(
    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)
shape = (-1, 2, num_blks, num_heads, data.num_steps)
dec_attention_weights = d2l.reshape(dec_attention_weights_filled, shape)
dec_self_attention_weights, dec_inter_attention_weights = \
    dec_attention_weights.permute(1, 2, 3, 0, 4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
dec_attention_weights_2d = [head[0] for step in dec_attention_weights
                            for attn in step
                            for blk in attn for head in blk]
dec_attention_weights_filled = tf.convert_to_tensor(
    np.asarray(pd.DataFrame(dec_attention_weights_2d).fillna(
        0.0).values).astype(np.float32))
dec_attention_weights = tf.reshape(dec_attention_weights_filled, shape=(
    -1, 2, num_blks, num_heads, data.num_steps))
dec_self_attention_weights, dec_inter_attention_weights = tf.transpose(
    dec_attention_weights, perm=(1, 2, 3, 0, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
dec_attention_weights_2d = [head[0].tolist() for step in dec_attention_weights
                            for attn in step
                            for blk in attn for head in blk]
dec_attention_weights_filled = d2l.tensor(
    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)
dec_attention_weights = dec_attention_weights_filled.reshape(
    (-1, 2, num_blks, num_heads, data.num_steps))
dec_self_attention_weights, dec_inter_attention_weights = \
    dec_attention_weights.transpose(1, 2, 3, 0, 4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
d2l.check_shape(dec_self_attention_weights,
                (num_blks, num_heads, data.num_steps, data.num_steps))
d2l.check_shape(dec_inter_attention_weights,
                (num_blks, num_heads, data.num_steps, data.num_steps))
</code></pre>
<p>디코더 셀프 어텐션의 자기 회귀 속성 때문에, 어떤 쿼리도 쿼리 위치 이후의 키-값 쌍에 어텐션을 수행하지 않습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
d2l.show_heatmaps(
    dec_self_attention_weights[:, :, :, :],
    xlabel='키 위치', ylabel='쿼리 위치',
    titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))
</code></pre>
<p>인코더 셀프 어텐션의 경우와 유사하게, 입력 시퀀스의 지정된 유효 길이를 통해 [<strong>출력 시퀀스의 어떤 쿼리도 입력 시퀀스의 패딩 토큰에 어텐션을 수행하지 않습니다.</strong>]</p>
<pre><code class="language-{.python .input}">%%tab all
d2l.show_heatmaps(
    dec_inter_attention_weights, xlabel='키 위치',
    ylabel='쿼리 위치', titles=['Head %d' % i for i in range(1, 5)],
    figsize=(7, 3.5))
</code></pre>
<p>트랜스포머 아키텍처는 원래 시퀀스 투 시퀀스 학습을 위해 제안되었지만, 나중에 책에서 보게 되듯이 트랜스포머 인코더나 트랜스포머 디코더는 각각 개별적으로 서로 다른 딥러닝 작업에 자주 사용됩니다.</p>
<h2 id="요약-summary-50"><a class="header" href="#요약-summary-50">요약 (Summary)</a></h2>
<ul>
<li>트랜스포머는 인코더-디코더 아키텍처의 한 사례이지만, 실제로는 인코더나 디코더 중 하나만 개별적으로 사용될 수도 있습니다.</li>
<li>트랜스포머 아키텍처에서 멀티 헤드 셀프 어텐션은 입력 시퀀스와 출력 시퀀스를 표현하는 데 사용되지만, 디코더는 마스크된 버전을 통해 자기 회귀 속성을 유지해야 합니다.</li>
<li>트랜스포머의 잔차 연결과 레이어 정규화는 매우 깊은 모델을 훈련하는 데 중요합니다.</li>
<li>트랜스포머 모델의 포지션 와이즈 피드포워드 네트워크는 동일한 MLP를 사용하여 모든 시퀀스 위치의 표현을 변환합니다.</li>
</ul>
<h2 id="연습-문제-exercises-63"><a class="header" href="#연습-문제-exercises-63">연습 문제 (Exercises)</a></h2>
<ol>
<li>실험에서 더 깊은 트랜스포머를 훈련해 보십시오. 훈련 속도와 번역 성능에 어떤 영향을 미칩니까?</li>
<li>트랜스포머에서 스케일드 닷 프로덕트 어텐션을 가산(additive) 어텐션으로 바꾸는 것이 좋은 생각일까요? 왜 그렇습니까?</li>
<li>언어 모델링의 경우 트랜스포머 인코더, 디코더 중 무엇을 사용해야 할까요, 아니면 둘 다 사용해야 할까요? 이 방법을 어떻게 설계하시겠습니까?</li>
<li>입력 시퀀스가 매우 길면 트랜스포머가 어떤 어려움에 직면할 수 있습니까? 왜 그렇습니까?</li>
<li>트랜스포머의 계산 및 메모리 효율성을 어떻게 개선하시겠습니까? 힌트: :citet:<code>Tay.Dehghani.Bahri.ea.2020</code>의 서베이 논문을 참조할 수 있습니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/348">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1066">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/3871">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18031">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['pytorch', 'jax'])
</code></pre>
<h1 id="비전-트랜스포머-transformers-for-vision"><a class="header" href="#비전-트랜스포머-transformers-for-vision">비전 트랜스포머 (Transformers for Vision)</a></h1>
<p>:label:<code>sec_vision-transformer</code></p>
<p>트랜스포머 아키텍처는 처음에 기계 번역에 중점을 둔 시퀀스-투-시퀀스 학습을 위해 제안되었습니다.
그 후 트랜스포머는 다양한 자연어 처리 작업에서 선택되는 모델로 부상했습니다 :cite:<code>Radford.Narasimhan.Salimans.ea.2018,Radford.Wu.Child.ea.2019,brown2020language,Devlin.Chang.Lee.ea.2018,raffel2020exploring</code>.
그러나 컴퓨터 비전 분야에서는 지배적인 아키텍처가 CNN으로 남아 있었습니다 (:numref:<code>chap_modern_cnn</code>).
자연스럽게 연구자들은 트랜스포머 모델을 이미지 데이터에 적용하여 더 잘할 수 있을지 궁금해하기 시작했습니다.
이 질문은 컴퓨터 비전 커뮤니티에서 엄청난 관심을 불러일으켰습니다.
최근 :citet:<code>ramachandran2019stand</code>는 합성곱을 셀프 어텐션으로 대체하는 방안을 제안했습니다.
그러나 어텐션에서 특수한 패턴을 사용하기 때문에 하드웨어 가속기에서 모델을 확장하기 어렵습니다.
그 후 :citet:<code>cordonnier2020relationship</code>은 이론적으로 셀프 어텐션이 합성곱과 유사하게 동작하도록 학습할 수 있음을 증명했습니다.
경험적으로 이미지에서 $2 \times 2$ 패치를 입력으로 가져왔지만, 패치 크기가 작아 모델을 저해상도 이미지 데이터에만 적용할 수 있었습니다.</p>
<p>패치 크기에 대한 특정 제약 없이,
*비전 트랜스포머(Vision Transformers, ViTs)*는 이미지에서 패치를 추출하고 이를 트랜스포머 인코더에 공급하여 전역 표현을 얻으며, 이는 최종적으로 분류를 위해 변환됩니다 :cite:<code>Dosovitskiy.Beyer.Kolesnikov.ea.2021</code>.
주목할 점은 트랜스포머가 CNN보다 더 나은 확장성을 보여준다는 것입니다: 더 큰 데이터셋에서 더 큰 모델을 훈련할 때 비전 트랜스포머는 ResNet보다 성능이 월등히 뛰어납니다.
자연어 처리에서의 네트워크 아키텍처 설계 환경과 유사하게, 트랜스포머는 컴퓨터 비전에서도 게임 체인저가 되었습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from d2l import jax as d2l
from flax import linen as nn
import jax
from jax import numpy as jnp
</code></pre>
<h2 id="모델-2"><a class="header" href="#모델-2">모델</a></h2>
<p>:numref:<code>fig_vit</code>는 비전 트랜스포머의 모델 아키텍처를 묘사합니다.
이 아키텍처는 이미지를 패치화하는 줄기(stem), 다층 트랜스포머 인코더에 기반한 몸체(body), 그리고 전역 표현을 출력 레이블로 변환하는 머리(head)로 구성됩니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/vit.svg" alt="비전 트랜스포머 아키텍처. 이 예제에서 이미지는 9개의 패치로 분할됩니다. 특수 &quot;&lt;cls&gt;&quot; 토큰과 9개의 평탄화된 이미지 패치는 패치 임베딩과 $\mathit{n}$개의 트랜스포머 인코더 블록을 통해 각각 10개의 표현으로 변환됩니다. &quot;&lt;cls&gt;&quot; 표현은 출력 레이블로 추가 변환됩니다." />
:label:<code>fig_vit</code></p>
<p>높이 $h$, 너비 $w$, 채널 $c$인 입력 이미지를 고려해 보십시오.
패치 높이와 너비를 모두 $p$로 지정하면, 이미지는 $m = hw/p^2$개의 패치 시퀀스로 분할되며, 여기서 각 패치는 길이 $cp^2$의 벡터로 평탄화됩니다.
이런 식으로 이미지 패치는 트랜스포머 인코더에서 텍스트 시퀀스의 토큰과 유사하게 처리될 수 있습니다.
특수 "&lt;cls&gt;" (클래스) 토큰과 $m$개의 평탄화된 이미지 패치는 $m+1$개 벡터의 시퀀스로 선형 투영되고, 학습 가능한 위치 임베딩과 합산됩니다.
다층 트랜스포머 인코더는 $m+1$개의 입력 벡터를 동일한 수의 동일한 길이 출력 벡터 표현으로 변환합니다.
정규화 위치만 다를 뿐 :numref:<code>fig_transformer</code>의 원래 트랜스포머 인코더와 똑같이 작동합니다.
"&lt;cls&gt;" 토큰은 셀프 어텐션을 통해 모든 이미지 패치에 주의를 기울이므로(:numref:<code>fig_cnn-rnn-self-attention</code> 참조), 트랜스포머 인코더 출력에서의 그 표현은 출력 레이블로 추가 변환됩니다.</p>
<h2 id="패치-임베딩-patch-embedding"><a class="header" href="#패치-임베딩-patch-embedding">패치 임베딩 (Patch Embedding)</a></h2>
<p>비전 트랜스포머를 구현하기 위해 :numref:<code>fig_vit</code>의 패치 임베딩부터 시작하겠습니다.
이미지를 패치로 분할하고 평탄화된 패치를 선형적으로 투영하는 것은 단일 합성곱 연산으로 단순화할 수 있습니다.
여기서 커널 크기와 스트라이드 크기는 모두 패치 크기로 설정됩니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
class PatchEmbedding(nn.Module):
    def __init__(self, img_size=96, patch_size=16, num_hiddens=512):
        super().__init__()
        def _make_tuple(x):
            if not isinstance(x, (list, tuple)):
                return (x, x)
            return x
        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)
        self.num_patches = (img_size[0] // patch_size[0]) * (
            img_size[1] // patch_size[1])
        self.conv = nn.LazyConv2d(num_hiddens, kernel_size=patch_size,
                                  stride=patch_size)

    def forward(self, X):
        # 출력 모양: (배치 크기, 패치 수, 채널 수)
        return self.conv(X).flatten(2).transpose(1, 2)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class PatchEmbedding(nn.Module):
    img_size: int = 96
    patch_size: int = 16
    num_hiddens: int = 512

    def setup(self):
        def _make_tuple(x):
            if not isinstance(x, (list, tuple)):
                return (x, x)
            return x
        img_size, patch_size = _make_tuple(self.img_size), _make_tuple(self.patch_size)
        self.num_patches = (img_size[0] // patch_size[0]) * (
            img_size[1] // patch_size[1])
        self.conv = nn.Conv(self.num_hiddens, kernel_size=patch_size,
                            strides=patch_size, padding='SAME')

    def __call__(self, X):
        # 출력 모양: (배치 크기, 패치 수, 채널 수)
        X = self.conv(X)
        return X.reshape((X.shape[0], -1, X.shape[3]))
</code></pre>
<p>다음 예제에서는 높이와 너비가 <code>img_size</code>인 이미지를 입력으로 받아 패치 임베딩이 <code>(img_size//patch_size)**2</code>개의 패치를 출력하며, 이들은 길이 <code>num_hiddens</code>의 벡터로 선형 투영됩니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
img_size, patch_size, num_hiddens, batch_size = 96, 16, 512, 4
patch_emb = PatchEmbedding(img_size, patch_size, num_hiddens)
X = d2l.zeros(batch_size, 3, img_size, img_size)
d2l.check_shape(patch_emb(X),
                (batch_size, (img_size//patch_size)**2, num_hiddens))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
img_size, patch_size, num_hiddens, batch_size = 96, 16, 512, 4
patch_emb = PatchEmbedding(img_size, patch_size, num_hiddens)
X = d2l.zeros((batch_size, img_size, img_size, 3))
output, _ = patch_emb.init_with_output(d2l.get_key(), X)
d2l.check_shape(output, (batch_size, (img_size//patch_size)**2, num_hiddens))
</code></pre>
<h2 id="비전-트랜스포머-인코더-vision-transformer-encoder"><a class="header" href="#비전-트랜스포머-인코더-vision-transformer-encoder">비전 트랜스포머 인코더 (Vision Transformer Encoder)</a></h2>
<p>:label:<code>subsec_vit-encoder</code></p>
<p>비전 트랜스포머 인코더의 MLP는 원래 트랜스포머 인코더의 포지션와이즈 FFN과 약간 다릅니다(:numref:<code>subsec_positionwise-ffn</code> 참조).
첫째, 여기서 활성화 함수는 가우시안 오차 선형 유닛(GELU)을 사용합니다. 이는 ReLU의 더 부드러운 버전으로 간주될 수 있습니다 :cite:<code>Hendrycks.Gimpel.2016</code>.
둘째, 정규화를 위해 MLP의 각 완전 연결 레이어 출력에 드롭아웃이 적용됩니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
class ViTMLP(nn.Module):
    def __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):
        super().__init__()
        self.dense1 = nn.LazyLinear(mlp_num_hiddens)
        self.gelu = nn.GELU()
        self.dropout1 = nn.Dropout(dropout)
        self.dense2 = nn.LazyLinear(mlp_num_outputs)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x):
        return self.dropout2(self.dense2(self.dropout1(self.gelu(
            self.dense1(x)))))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class ViTMLP(nn.Module):
    mlp_num_hiddens: int
    mlp_num_outputs: int
    dropout: float = 0.5

    @nn.compact
    def __call__(self, x, training=False):
        x = nn.Dense(self.mlp_num_hiddens)(x)
        x = nn.gelu(x)
        x = nn.Dropout(self.dropout, deterministic=not training)(x)
        x = nn.Dense(self.mlp_num_outputs)(x)
        x = nn.Dropout(self.dropout, deterministic=not training)(x)
        return x
</code></pre>
<p>비전 트랜스포머 인코더 블록 구현은 :numref:<code>fig_vit</code>의 사전 정규화(pre-normalization) 설계를 따릅니다.
여기서 정규화는 멀티 헤드 어텐션 또는 MLP <em>바로 직전</em>에 적용됩니다.
잔차 연결 <em>직후</em>에 정규화가 배치되는 사후 정규화(:numref:<code>fig_transformer</code>의 "add &amp; norm")와 달리,
사전 정규화는 트랜스포머의 더 효과적이거나 효율적인 훈련으로 이어집니다 :cite:<code>baevski2018adaptive,wang2019learning,xiong2020layer</code>.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
class ViTBlock(nn.Module):
    def __init__(self, num_hiddens, norm_shape, mlp_num_hiddens,
                 num_heads, dropout, use_bias=False):
        super().__init__()
        self.ln1 = nn.LayerNorm(norm_shape)
        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,
                                                dropout, use_bias)
        self.ln2 = nn.LayerNorm(norm_shape)
        self.mlp = ViTMLP(mlp_num_hiddens, num_hiddens, dropout)

    def forward(self, X, valid_lens=None):
        X = X + self.attention(*([self.ln1(X)] * 3), valid_lens)
        return X + self.mlp(self.ln2(X))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class ViTBlock(nn.Module):
    num_hiddens: int
    mlp_num_hiddens: int
    num_heads: int
    dropout: float
    use_bias: bool = False

    def setup(self):
        self.attention = d2l.MultiHeadAttention(self.num_hiddens, self.num_heads,
                                                self.dropout, self.use_bias)
        self.mlp = ViTMLP(self.mlp_num_hiddens, self.num_hiddens, self.dropout)

    @nn.compact
    def __call__(self, X, valid_lens=None, training=False):
        X = X + self.attention(*([nn.LayerNorm()(X)] * 3),
                               valid_lens, training=training)[0]
        return X + self.mlp(nn.LayerNorm()(X), training=training)
</code></pre>
<p>:numref:<code>subsec_transformer-encoder</code>와 마찬가지로, 비전 트랜스포머 인코더 블록은 입력 모양을 변경하지 않습니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
X = d2l.ones((2, 100, 24))
encoder_blk = ViTBlock(24, 24, 48, 8, 0.5)
encoder_blk.eval()
d2l.check_shape(encoder_blk(X), X.shape)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
X = d2l.ones((2, 100, 24))
encoder_blk = ViTBlock(24, 48, 8, 0.5)
d2l.check_shape(encoder_blk.init_with_output(d2l.get_key(), X)[0], X.shape)
</code></pre>
<h2 id="종합하기-putting-it-all-together"><a class="header" href="#종합하기-putting-it-all-together">종합하기 (Putting It All Together)</a></h2>
<p>아래 비전 트랜스포머의 순방향 패스는 간단합니다.
먼저 입력 이미지가 <code>PatchEmbedding</code> 인스턴스에 공급되고,
그 출력은 "&lt;cls&gt;" 토큰 임베딩과 연결됩니다.
드롭아웃 전에 학습 가능한 위치 임베딩과 합산됩니다.
그런 다음 출력은 <code>ViTBlock</code> 클래스의 <code>num_blks</code> 인스턴스를 쌓는 트랜스포머 인코더에 공급됩니다.
마지막으로 "&lt;cls&gt;" 토큰의 표현은 네트워크 헤드에 의해 투영됩니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
class ViT(d2l.Classifier):
    """비전 트랜스포머."""
    def __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens,
                 num_heads, num_blks, emb_dropout, blk_dropout, lr=0.1,
                 use_bias=False, num_classes=10):
        super().__init__()
        self.save_hyperparameters()
        self.patch_embedding = PatchEmbedding(
            img_size, patch_size, num_hiddens)
        self.cls_token = nn.Parameter(d2l.zeros(1, 1, num_hiddens))
        num_steps = self.patch_embedding.num_patches + 1  # cls 토큰 추가
        # 위치 임베딩은 학습 가능합니다
        self.pos_embedding = nn.Parameter(
            torch.randn(1, num_steps, num_hiddens))
        self.dropout = nn.Dropout(emb_dropout)
        self.blks = nn.Sequential()
        for i in range(num_blks):
            self.blks.add_module(f"{i}", ViTBlock(
                num_hiddens, num_hiddens, mlp_num_hiddens,
                num_heads, blk_dropout, use_bias))
        self.head = nn.Sequential(nn.LayerNorm(num_hiddens),
                                  nn.Linear(num_hiddens, num_classes))

    def forward(self, X):
        X = self.patch_embedding(X)
        X = d2l.concat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)
        X = self.dropout(X + self.pos_embedding)
        for blk in self.blks:
            X = blk(X)
        return self.head(X[:, 0])
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
class ViT(d2l.Classifier):
    """비전 트랜스포머."""
    img_size: int
    patch_size: int
    num_hiddens: int
    mlp_num_hiddens: int
    num_heads: int
    num_blks: int
    emb_dropout: float
    blk_dropout: float
    lr: float = 0.1
    use_bias: bool = False
    num_classes: int = 10
    training: bool = False

    def setup(self):
        self.patch_embedding = PatchEmbedding(self.img_size, self.patch_size,
                                              self.num_hiddens)
        self.cls_token = self.param('cls_token', nn.initializers.zeros,
                                    (1, 1, self.num_hiddens))
        num_steps = self.patch_embedding.num_patches + 1  # cls 토큰 추가
        # 위치 임베딩은 학습 가능합니다
        self.pos_embedding = self.param('pos_embed', nn.initializers.normal(),
                                        (1, num_steps, self.num_hiddens))
        self.blks = [ViTBlock(self.num_hiddens, self.mlp_num_hiddens,
                              self.num_heads, self.blk_dropout, self.use_bias)
                    for _ in range(self.num_blks)]
        self.head = nn.Sequential([nn.LayerNorm(), nn.Dense(self.num_classes)])

    @nn.compact
    def __call__(self, X):
        X = self.patch_embedding(X)
        X = d2l.concat((jnp.tile(self.cls_token, (X.shape[0], 1, 1)), X), 1)
        X = nn.Dropout(emb_dropout, deterministic=not self.training)(X + self.pos_embedding)
        for blk in self.blks:
            X = blk(X, training=self.training)
        return self.head(X[:, 0])
</code></pre>
<h2 id="훈련-training-22"><a class="header" href="#훈련-training-22">훈련 (Training)</a></h2>
<p>Fashion-MNIST 데이터셋에서 비전 트랜스포머를 훈련하는 것은 :numref:<code>chap_modern_cnn</code>에서 CNN을 훈련하는 것과 같습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
img_size, patch_size = 96, 16
num_hiddens, mlp_num_hiddens, num_heads, num_blks = 512, 2048, 8, 2
emb_dropout, blk_dropout, lr = 0.1, 0.1, 0.1
model = ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads,
            num_blks, emb_dropout, blk_dropout, lr)
trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
data = d2l.FashionMNIST(batch_size=128, resize=(img_size, img_size))
trainer.fit(model, data)
</code></pre>
<h2 id="요약-및-토론-summary-and-discussion-8"><a class="header" href="#요약-및-토론-summary-and-discussion-8">요약 및 토론 (Summary and Discussion)</a></h2>
<p>Fashion-MNIST와 같은 작은 데이터셋의 경우 구현된 비전 트랜스포머가 :numref:<code>sec_resnet</code>의 ResNet보다 성능이 좋지 않음을 알 수 있습니다.
ImageNet 데이터셋(120만 개 이미지)에서도 비슷한 관찰을 할 수 있습니다.
이는 트랜스포머가 평행 이동 불변성 및 지역성과 같은 합성곱의 유용한 원칙이 <em>부족</em>하기 때문입니다(:numref:<code>sec_why-conv</code>).
그러나 더 큰 데이터셋(예: 3억 개 이미지)에서 더 큰 모델을 훈련할 때 상황이 바뀌어, 비전 트랜스포머가 이미지 분류에서 ResNet을 크게 능가하여 확장성에서 트랜스포머의 본질적인 우수성을 입증했습니다 :cite:<code>Dosovitskiy.Beyer.Kolesnikov.ea.2021</code>.
비전 트랜스포머의 도입은 이미지 데이터 모델링을 위한 네트워크 설계 환경을 변화시켰습니다.
이들은 곧 DeiT의 데이터 효율적인 훈련 전략을 통해 ImageNet 데이터셋에서도 효과적인 것으로 나타났습니다 :cite:<code>touvron2021training</code>.
그러나 셀프 어텐션의 이차 복잡도(:numref:<code>sec_self-attention-and-positional-encoding</code>)는 트랜스포머 아키텍처를 고해상도 이미지에 덜 적합하게 만듭니다.
컴퓨터 비전의 범용 백본 네트워크를 향하여, Swin Transformer는 이미지 크기에 대한 이차 계산 복잡도 문제를 해결하고(:numref:<code>subsec_cnn-rnn-self-attention</code>) 합성곱과 유사한 사전 지식(priors)을 복원하여, 트랜스포머의 적용 가능성을 이미지 분류를 넘어 다양한 컴퓨터 비전 작업으로 확장하고 최첨단 결과를 달성했습니다 :cite:<code>liu2021swin</code>.</p>
<h2 id="연습-문제-exercises-64"><a class="header" href="#연습-문제-exercises-64">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>img_size</code> 값은 훈련 시간에 어떤 영향을 줍니까?</li>
<li>"&lt;cls&gt;" 토큰 표현을 출력에 투영하는 대신, 평균화된 패치 표현을 투영하면 어떨까요? 이 변경 사항을 구현하고 정확도에 어떤 영향을 미치는지 확인하십시오.</li>
<li>비전 트랜스포머의 정확도를 높이기 위해 하이퍼파라미터를 수정할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/8943">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18032">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="트랜스포머를-사용한-대규모-사전-훈련-large-scale-pretraining-with-transformers"><a class="header" href="#트랜스포머를-사용한-대규모-사전-훈련-large-scale-pretraining-with-transformers">트랜스포머를 사용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></h1>
<p>:label:<code>sec_large-pretraining-transformers</code></p>
<p>지금까지 우리의 이미지 분류 및 기계 번역 실험에서,
모델은 특정 작업을 수행하기 위해 입력-출력 예제가 있는 데이터셋에서 <em>처음부터(from scratch)</em> 훈련되었습니다.
예를 들어 트랜스포머는 영어-프랑스어 쌍으로 훈련되어(:numref:<code>sec_transformer</code>) 입력 영어 텍스트를 프랑스어로 번역할 수 있습니다.
결과적으로 각 모델은 데이터 분포의 약간의 변화에도 민감한 *특정 전문가(specific expert)*가 됩니다 (:numref:<code>sec_environment-and-distribution-shift</code>).
더 잘 일반화된 모델, 또는 적응 없이 여러 작업을 수행할 수 있는 더 유능한 *제너럴리스트(generalist)*를 위해,
대규모 데이터에 대해 모델을 *사전 훈련(pretraining)*하는 것이 점점 보편화되고 있습니다.</p>
<p>사전 훈련을 위한 더 큰 데이터가 주어지면, 트랜스포머 아키텍처는 모델 크기와 훈련 컴퓨팅이 증가함에 따라 더 나은 성능을 발휘하여 우수한 <em>확장(scaling)</em> 동작을 보여줍니다.
구체적으로 트랜스포머 기반 언어 모델의 성능은 모델 파라미터 양, 훈련 토큰 및 훈련 컴퓨팅에 따라 거듭제곱 법칙으로 확장됩니다 :cite:<code>kaplan2020scaling</code>.
트랜스포머의 확장성은 더 큰 데이터에서 훈련된 더 큰 비전 트랜스포머(:numref:<code>sec_vision-transformer</code>에서 논의됨)의 크게 향상된 성능으로도 입증됩니다.
더 최근의 성공 사례로는 Atari 게임을 하고, 이미지에 캡션을 달고, 채팅하고, 로봇처럼 행동할 수 있는 <em>제너럴리스트</em> 모델인 Gato가 있습니다 :cite:<code>reed2022generalist</code>. Gato는 텍스트, 이미지, 관절 토크, 버튼 누름을 포함한 다양한 양식(modalities)에 대해 사전 훈련될 때 잘 확장되는 단일 트랜스포머입니다.
주목할 점은 이러한 모든 멀티모달 데이터가 평평한 토큰 시퀀스로 직렬화되어, 트랜스포머에 의해 텍스트 토큰(:numref:<code>sec_transformer</code>)이나 이미지 패치(:numref:<code>sec_vision-transformer</code>)와 유사하게 처리될 수 있다는 것입니다.</p>
<p>멀티모달 데이터에 대한 트랜스포머 사전 훈련의 놀라운 성공에 앞서, 트랜스포머는 풍부한 텍스트로 광범위하게 사전 훈련되었습니다.
원래 기계 번역을 위해 제안된 :numref:<code>fig_transformer</code>의 트랜스포머 아키텍처는 입력 시퀀스를 표현하기 위한 인코더와 타겟 시퀀스를 생성하기 위한 디코더로 구성됩니다.
주로 트랜스포머는 <em>인코더 전용(encoder-only)</em>, <em>인코더-디코더(encoder--decoder)</em>, *디코더 전용(decoder-only)*의 세 가지 다른 모드로 사용될 수 있습니다.
이 장을 마무리하기 위해, 이 세 가지 모드를 검토하고 트랜스포머 사전 훈련의 확장성을 설명할 것입니다.</p>
<h2 id="인코더-전용-encoder-only"><a class="header" href="#인코더-전용-encoder-only">인코더 전용 (Encoder-Only)</a></h2>
<p>트랜스포머 인코더만 사용되는 경우,
입력 토큰 시퀀스는 출력(예: 분류)으로 추가 투영될 수 있는 동일한 수의 표현으로 변환됩니다.
트랜스포머 인코더는 모든 입력 토큰이 서로에게 주의를 기울이는 셀프 어텐션 레이어로 구성됩니다.
예를 들어 :numref:<code>fig_vit</code>에 묘사된 비전 트랜스포머는 인코더 전용이며, 입력 이미지 패치 시퀀스를 특수 "&lt;cls&gt;" 토큰의 표현으로 변환합니다.
이 표현은 모든 입력 토큰에 의존하므로 분류 레이블로 추가 투영됩니다.
이 설계는 텍스트에 대해 사전 훈련된 초기 인코더 전용 트랜스포머인 BERT(Bidirectional Encoder Representations from Transformers)에서 영감을 받았습니다 :cite:<code>Devlin.Chang.Lee.ea.2018</code>.</p>
<h3 id="bert-사전-훈련-pretraining-bert"><a class="header" href="#bert-사전-훈련-pretraining-bert">BERT 사전 훈련 (Pretraining BERT)</a></h3>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/bert-encoder-only.svg" alt="왼쪽: 마스킹된 언어 모델링으로 BERT 사전 훈련. 마스킹된 &quot;love&quot; 토큰의 예측은 &quot;love&quot; 전후의 모든 입력 토큰에 의존합니다. 오른쪽: 트랜스포머 인코더의 주의 패턴. 세로축을 따른 각 토큰은 가로축을 따른 모든 입력 토큰에 주의를 기울입니다." />
:label:<code>fig_bert-encoder-only</code></p>
<p>BERT는 *마스킹된 언어 모델링(masked language modeling)*을 사용하여 텍스트 시퀀스에 대해 사전 훈련됩니다:
무작위로 마스킹된 토큰이 있는 입력 텍스트가 트랜스포머 인코더에 공급되어 마스킹된 토큰을 예측합니다.
:numref:<code>fig_bert-encoder-only</code>에 설명된 대로,
원래 텍스트 시퀀스 "I", "love", "this", "red", "car" 앞에 "&lt;cls&gt;" 토큰이 추가되고, "&lt;mask&gt;" 토큰이 "love"를 무작위로 대체합니다. 그런 다음 마스킹된 토큰 "love"와 그 예측 사이의 크로스 엔트로피 손실이 사전 훈련 중에 최소화됩니다.
트랜스포머 인코더의 주의 패턴(:numref:<code>fig_bert-encoder-only</code>의 오른쪽)에는 제약이 없으므로 모든 토큰이 서로에게 주의를 기울일 수 있음에 유의하십시오.
따라서 "love"의 예측은 시퀀스에서 그 전후의 입력 토큰에 의존합니다.
이것이 BERT가 "양방향 인코더"인 이유입니다.
수동 라벨링이 필요 없이 책과 위키피디아의 대규모 텍스트 데이터를 사용하여 BERT를 사전 훈련할 수 있습니다.</p>
<h3 id="bert-미세-조정-fine-tuning-bert"><a class="header" href="#bert-미세-조정-fine-tuning-bert">BERT 미세 조정 (Fine-Tuning BERT)</a></h3>
<p>사전 훈련된 BERT는 단일 텍스트 또는 텍스트 쌍을 포함하는 다운스트림 인코딩 작업에 *미세 조정(fine-tuned)*될 수 있습니다. 미세 조정 중에 무작위 파라미터가 있는 추가 레이어를 BERT에 추가할 수 있습니다: 이러한 파라미터와 사전 훈련된 BERT 파라미터는 다운스트림 작업의 훈련 데이터에 맞게 <em>업데이트</em>됩니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/bert-finetune-classification.svg" alt="감성 분석을 위한 BERT 미세 조정." />
:label:<code>fig_bert-finetune-classification</code></p>
<p>:numref:<code>fig_bert-finetune-classification</code>은 감성 분석을 위한 BERT의 미세 조정을 보여줍니다.
트랜스포머 인코더는 사전 훈련된 BERT로, 텍스트 시퀀스를 입력으로 받아 "&lt;cls&gt;" 표현(입력의 전역 표현)을 추가적인 완전 연결 레이어에 공급하여 감성을 예측합니다.
미세 조정 중에 감성 분석 데이터의 예측과 레이블 간의 크로스 엔트로피 손실이 기울기 기반 알고리즘을 통해 최소화되며, 여기서 추가 레이어는 처음부터 훈련되는 반면 BERT의 사전 훈련된 파라미터는 업데이트됩니다.
BERT는 감성 분석 이상의 일을 합니다.
2,500억 개의 훈련 토큰에서 3억 5천만 개의 파라미터를 가진 BERT가 학습한 일반 언어 표현은 단일 텍스트 분류, 텍스트 쌍 분류 또는 회귀, 텍스트 태깅, 질문 응답과 같은 자연어 작업의 최첨단 기술을 발전시켰습니다.</p>
<p>이러한 다운스트림 작업에 텍스트 쌍 이해가 포함되어 있음을 알 수 있습니다.
BERT 사전 훈련에는 한 문장이 다른 문장 바로 뒤에 오는지 예측하는 또 다른 손실이 있습니다.
그러나 이 손실은 나중에 동일한 크기의 BERT 변형인 RoBERTa를 2,000억 개의 토큰으로 사전 훈련할 때 덜 유용한 것으로 밝혀졌습니다 :cite:<code>Liu.Ott.Goyal.ea.2019</code>.
BERT의 다른 파생물들은 모델 아키텍처나 사전 훈련 목표를 개선했습니다. 예를 들어 ALBERT(파라미터 공유 강제) :cite:<code>lan2019albert</code>,
SpanBERT(텍스트 범위 표현 및 예측) :cite:<code>joshi2020spanbert</code>,
DistilBERT(지식 증류를 통한 경량화) :cite:<code>sanh2019distilbert</code>,
ELECTRA(대체된 토큰 감지) :cite:<code>clark2019electra</code>가 있습니다.
또한 BERT는 컴퓨터 비전에서의 트랜스포머 사전 훈련에 영감을 주었습니다. 예를 들어 비전 트랜스포머 :cite:<code>Dosovitskiy.Beyer.Kolesnikov.ea.2021</code>,
Swin Transformer :cite:<code>liu2021swin</code>,
MAE(masked autoencoders) :cite:<code>he2022masked</code>가 있습니다.</p>
<h2 id="인코더-디코더-encoder--decoder"><a class="header" href="#인코더-디코더-encoder--decoder">인코더-디코더 (Encoder--Decoder)</a></h2>
<p>트랜스포머 인코더는 일련의 입력 토큰을 동일한 수의 출력 표현으로 변환하므로, 인코더 전용 모드는 기계 번역처럼 임의의 길이의 시퀀스를 생성할 수 없습니다.
원래 기계 번역을 위해 제안된 대로, 트랜스포머 아키텍처에는 인코더 출력과 디코더 출력 모두에 조건부로 임의의 길이의 타겟 시퀀스를 토큰별로 자동 회귀적으로 예측하는 디코더가 장착될 수 있습니다:
(i) 인코더 출력에 대한 컨디셔닝을 위해 인코더-디코더 크로스 어텐션(:numref:<code>fig_transformer</code>의 디코더 멀티 헤드 어텐션)은 타겟 토큰이 <em>모든</em> 입력 토큰에 주의를 기울일 수 있게 합니다;
(ii) 디코더 출력에 대한 컨디셔닝은 소위 <em>인과(causal)</em> 어텐션(이 이름은 문헌에서 흔하지만 인과 관계에 대한 적절한 연구와는 관련이 거의 없기 때문에 오해의 소지가 있습니다) 패턴(:numref:<code>fig_transformer</code>의 디코더 마스킹된 멀티 헤드 어텐션)에 의해 달성됩니다. 여기서 타겟 토큰은 타겟 시퀀스의 <em>과거</em> 및 <em>현재</em> 토큰에만 주의를 기울일 수 있습니다.</p>
<p>인간이 라벨링한 기계 번역 데이터를 넘어 인코더-디코더 트랜스포머를 사전 훈련하기 위해,
BART :cite:<code>lewis2019bart</code>와 T5 :cite:<code>raffel2020exploring</code>는 대규모 텍스트 코퍼스에서 사전 훈련된 두 개의 동시에 제안된 인코더-디코더 트랜스포머입니다.
둘 다 사전 훈련 목표에서 원본 텍스트를 재구성하려고 시도하지만, 전자는 입력 노이즈(예: 마스킹, 삭제, 순열 및 회전)를 강조하고 후자는 포괄적인 절제 연구(ablation studies)를 통한 멀티태스크 통합을 강조합니다.</p>
<h3 id="t5-사전-훈련-pretraining-t5"><a class="header" href="#t5-사전-훈련-pretraining-t5">T5 사전 훈련 (Pretraining T5)</a></h3>
<p>사전 훈련된 트랜스포머 인코더-디코더의 예로서,
T5(Text-to-Text Transfer Transformer)는 많은 작업을 동일한 텍스트-투-텍스트 문제로 통합합니다:
모든 작업에 대해 인코더의 입력은 작업 설명(예: "Summarize", ":")과 작업 입력(예: 기사의 토큰 시퀀스)이 뒤따르는 것이고,
디코더는 작업 출력(예: 입력 기사를 요약하는 토큰 시퀀스)을 예측합니다.
텍스트-투-텍스트로 수행하기 위해, T5는 입력 텍스트에 조건부로 일부 타겟 텍스트를 생성하도록 훈련됩니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/t5-encoder-decoder.svg" alt="왼쪽: 연속적인 범위를 예측하여 T5 사전 훈련. 원래 문장은 &quot;I&quot;, &quot;love&quot;, &quot;this&quot;, &quot;red&quot;, &quot;car&quot;이며, 여기서 &quot;love&quot;는 특수 &quot;&lt;X&gt;&quot; 토큰으로 대체되고 연속적인 &quot;red&quot;, &quot;car&quot;는 특수 &quot;&lt;Y&gt;&quot; 토큰으로 대체됩니다. 타겟 시퀀스는 특수 &quot;&lt;Z&gt;&quot; 토큰으로 끝납니다. 오른쪽: 트랜스포머 인코더-디코더의 주의 패턴. 인코더 셀프 어텐션(아래쪽 정사각형)에서 모든 입력 토큰은 서로에게 주의를 기울입니다; 인코더-디코더 크로스 어텐션(위쪽 직사각형)에서 각 타겟 토큰은 모든 입력 토큰에 주의를 기울입니다; 디코더 셀프 어텐션(위쪽 삼각형)에서 각 타겟 토큰은 현재 및 과거 타겟 토큰에만 주의를 기울입니다(인과적)." />
:label:<code>fig_t5-encoder-decoder</code></p>
<p>임의의 원본 텍스트에서 입력과 출력을 얻기 위해,
T5는 연속적인 범위를 예측하도록 사전 훈련됩니다.
구체적으로 텍스트의 토큰은 무작위로 특수 토큰으로 대체되며, 여기서 각 연속적인 범위는 동일한 특수 토큰으로 대체됩니다.
:numref:<code>fig_t5-encoder-decoder</code>의 예를 고려해 보십시오. 여기서 원본 텍스트는 "I", "love", "this", "red", "car"입니다.
"love", "red", "car" 토큰은 무작위로 특수 토큰으로 대체됩니다.
"red"와 "car"는 연속적인 범위이므로 동일한 특수 토큰으로 대체됩니다.
결과적으로 입력 시퀀스는 "I", "&lt;X&gt;", "this", "&lt;Y&gt;"이고,
타겟 시퀀스는 "&lt;X&gt;", "love", "&lt;Y&gt;", "red", "car", "&lt;Z&gt;"이며,
여기서 "&lt;Z&gt;"는 끝을 표시하는 또 다른 특수 토큰입니다.
:numref:<code>fig_t5-encoder-decoder</code>에 표시된 것처럼, 디코더는 시퀀스 예측 중에 미래 토큰에 주의를 기울이는 것을 방지하기 위해 인과적 어텐션 패턴을 갖습니다.</p>
<p>T5에서 연속적인 범위를 예측하는 것은 손상된 텍스트를 재구성하는 것으로도 불립니다.
이 목표를 가지고 T5는 웹에서 가져온 깨끗한 영어 텍스트로 구성된 C4(Colossal Clean Crawled Corpus) 데이터의 1조 개 토큰으로 사전 훈련됩니다 :cite:<code>raffel2020exploring</code>.</p>
<h3 id="t5-미세-조정-fine-tuning-t5"><a class="header" href="#t5-미세-조정-fine-tuning-t5">T5 미세 조정 (Fine-Tuning T5)</a></h3>
<p>BERT와 마찬가지로 T5는 이 작업을 수행하기 위해 작업별 훈련 데이터에 대해 미세 조정(T5 파라미터 업데이트)되어야 합니다.
BERT 미세 조정과의 주요 차이점은 다음과 같습니다:
(i) T5 입력에는 작업 설명이 포함됩니다;
(ii) T5는 트랜스포머 디코더를 사용하여 임의 길이의 시퀀스를 생성할 수 있습니다;
(iii) 추가 레이어가 필요하지 않습니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/t5-finetune-summarization.svg" alt="텍스트 요약을 위한 T5 미세 조정. 작업 설명과 기사 토큰 모두 트랜스포머 인코더에 입력되어 요약을 예측합니다." />
:label:<code>fig_t5-finetune-summarization</code></p>
<p>:numref:<code>fig_t5-finetune-summarization</code>은 텍스트 요약을 예로 들어 T5 미세 조정을 설명합니다.
이 다운스트림 작업에서 작업 설명 토큰 "Summarize", ":" 뒤에 기사 토큰이 인코더에 입력됩니다.</p>
<p>미세 조정 후, 110억 개의 파라미터를 가진 T5(T5-11B)는 여러 인코딩(예: 분류) 및 생성(예: 요약) 벤치마크에서 최첨단 결과를 달성했습니다.
출시 이후 T5는 후속 연구에서 광범위하게 사용되었습니다.
예를 들어 스위치 트랜스포머(switch Transformers)는 더 나은 계산 효율성을 위해 파라미터의 하위 집합을 활성화하도록 T5를 기반으로 설계되었습니다 :cite:<code>fedus2022switch</code>.
Imagen이라는 텍스트-투-이미지 모델에서는 텍스트가 46억 개의 파라미터를 가진 고정된 T5 인코더(T5-XXL)에 입력됩니다 :cite:<code>saharia2022photorealistic</code>.
:numref:<code>fig_imagen</code>의 사실적인 텍스트-투-이미지 예제는 T5 인코더만으로도 미세 조정 없이 텍스트를 효과적으로 표현할 수 있음을 시사합니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/imagen.png" alt="T5의 텍스트 인코더를 사용하는 Imagen 모델의 텍스트-투-이미지 예제 (그림 출처: :citet:saharia2022photorealistic)." />
:width:<code>700px</code>
:label:<code>fig_imagen</code></p>
<h2 id="디코더-전용-decoder-only"><a class="header" href="#디코더-전용-decoder-only">디코더 전용 (Decoder-Only)</a></h2>
<p>우리는 인코더 전용 및 인코더-디코더 트랜스포머를 검토했습니다.
대안으로, 디코더 전용 트랜스포머는 :numref:<code>fig_transformer</code>에 묘사된 원래 인코더-디코더 아키텍처에서 전체 인코더와 인코더-디코더 크로스 어텐션이 있는 디코더 하위 레이어를 제거합니다.
오늘날 디코더 전용 트랜스포머는 대규모 언어 모델링(:numref:<code>sec_language-model</code>)의 <em>사실상</em> 아키텍처가 되었으며, 이는 자기 지도 학습을 통해 전 세계의 풍부한 라벨이 없는 텍스트 코퍼스를 활용합니다.</p>
<h3 id="gpt와-gpt-2"><a class="header" href="#gpt와-gpt-2">GPT와 GPT-2</a></h3>
<p>언어 모델링을 훈련 목표로 사용하여, GPT(generative pre-training) 모델은 트랜스포머 디코더를 백본으로 선택합니다 :cite:<code>Radford.Narasimhan.Salimans.ea.2018</code>.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/gpt-decoder-only.svg" alt="왼쪽: 언어 모델링으로 GPT 사전 훈련. 타겟 시퀀스는 한 토큰 이동된 입력 시퀀스입니다. &quot;&lt;bos&gt;&quot;와 &quot;&lt;eos&gt;&quot;는 각각 시퀀스의 시작과 끝을 표시하는 특수 토큰입니다. 오른쪽: 트랜스포머 디코더의 주의 패턴. 세로축을 따른 각 토큰은 가로축을 따른 과거 토큰에만 주의를 기울입니다(인과적)." />
:label:<code>fig_gpt-decoder-only</code></p>
<p>:numref:<code>subsec_partitioning-seqs</code>에 설명된 자동 회귀 언어 모델 훈련에 따라,
:numref:<code>fig_gpt-decoder-only</code>는 트랜스포머 인코더를 사용한 GPT 사전 훈련을 보여줍니다. 여기서 타겟 시퀀스는 한 토큰 이동된 입력 시퀀스입니다.
트랜스포머 디코더의 주의 패턴은 각 토큰이 과거 토큰에만 주의를 기울이도록 강제한다는 점에 유의하십시오(미래 토큰은 아직 선택되지 않았으므로 주의를 기울일 수 없습니다).</p>
<p>GPT는 1억 개의 파라미터를 가지고 있으며 개별 다운스트림 작업에 대해 미세 조정해야 합니다.
훨씬 더 큰 트랜스포머-디코더 언어 모델인 GPT-2가 1년 후에 소개되었습니다 :cite:<code>Radford.Wu.Child.ea.2019</code>.
GPT의 원래 트랜스포머 디코더와 비교하여, GPT-2에서는 사전 정규화(:numref:<code>subsec_vit-encoder</code>에서 논의됨)와 개선된 초기화 및 가중치 스케일링이 채택되었습니다.
40GB의 텍스트로 사전 훈련된 15억 파라미터 GPT-2는 <em>파라미터나 아키텍처 업데이트 없이</em> 언어 모델링 벤치마크에서 최첨단 결과와 여러 다른 작업에서 유망한 결과를 얻었습니다.</p>
<h3 id="gpt-3와-그-이후-gpt-3-and-beyond"><a class="header" href="#gpt-3와-그-이후-gpt-3-and-beyond">GPT-3와 그 이후 (GPT-3 and Beyond)</a></h3>
<p>GPT-2는 모델 업데이트 없이 여러 작업에 동일한 언어 모델을 사용할 수 있는 가능성을 보여주었습니다.
이는 기울기 계산을 통한 모델 업데이트가 필요한 미세 조정보다 계산 효율적입니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/gpt-3-xshot.svg" alt="언어 모델(트랜스포머 디코더)을 사용한 제로 샷, 원 샷, 퓨 샷 인컨텍스트 학습. 파라미터 업데이트가 필요하지 않습니다." />
:label:<code>fig_gpt-3-xshot</code></p>
<p>파라미터 업데이트 없이 언어 모델을 더 계산 효율적으로 사용하는 것을 설명하기 전에,
:numref:<code>sec_rnn-scratch</code>에서 언어 모델이 일부 접두사 텍스트 시퀀스에 조건부로 텍스트 시퀀스를 생성하도록 훈련될 수 있음을 상기하십시오.
따라서 사전 훈련된 언어 모델은 작업 설명, 작업별 입력-출력 예제 및 프롬프트(작업 입력)가 있는 입력 시퀀스에 조건부로 <em>파라미터 업데이트 없이</em> 시퀀스로서 작업 출력을 생성할 수 있습니다.
이 학습 패러다임을 *인컨텍스트 학습(in-context learning)*이라고 하며 :cite:<code>brown2020language</code>,
작업별 입력-출력 예제가 없거나, 하나 있거나, 몇 개 있을 때 각각 <em>제로 샷(zero-shot)</em>, <em>원 샷(one-shot)</em>, *퓨 샷(few-shot)*으로 더 분류될 수 있습니다 (:numref:<code>fig_gpt-3-xshot</code>).</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/gpt3-xshot-scaling.png" alt="모든 42개 정확도 기준 벤치마크에 대한 GPT-3의 종합 성능 (캡션 수정 및 그림 출처: :citet:brown2020language)." />
:width:<code>400px</code>
:label:<code>fig_gpt3-xshot-scaling</code></p>
<p>이 세 가지 설정은 GPT-3에서 테스트되었습니다 :cite:<code>brown2020language</code>.
GPT-3의 가장 큰 버전은 GPT-2보다 약 두 자릿수 더 큰 데이터 및 모델 크기를 사용합니다.
GPT-3는 교대 레이어에서 주의 패턴(:numref:<code>fig_gpt-decoder-only</code>의 오른쪽)이 더 희소하다는 점을 제외하고는 직계 전임자인 GPT-2와 동일한 트랜스포머 디코더 아키텍처를 사용합니다.
3,000억 개의 토큰으로 사전 훈련된 GPT-3는 더 큰 모델 크기에서 더 나은 성능을 발휘하며, 퓨 샷 성능이 가장 빠르게 증가합니다 (:numref:<code>fig_gpt3-xshot-scaling</code>).</p>
<p>후속 GPT-4 모델은 보고서에서 기술적 세부 사항을 완전히 공개하지 않았습니다 :cite:<code>openai2023gpt4</code>.
전임자들과 달리, GPT-4는 텍스트와 이미지를 모두 입력으로 받아 텍스트 출력을 생성할 수 있는 대규모 멀티모달 모델입니다.</p>
<h2 id="확장성-scalability"><a class="header" href="#확장성-scalability">확장성 (Scalability)</a></h2>
<p>:numref:<code>fig_gpt3-xshot-scaling</code>은 GPT-3 언어 모델에서 트랜스포머의 확장성을 경험적으로 보여줍니다.
언어 모델링의 경우, 트랜스포머의 확장성에 대한 더 포괄적인 경험적 연구를 통해 연구자들은 더 많은 데이터와 컴퓨팅으로 더 큰 트랜스포머를 훈련할 가능성을 보게 되었습니다 :cite:<code>kaplan2020scaling</code>.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/scaling-power-law.png" alt="모델 크기, 데이터셋 크기 및 훈련에 사용되는 컴퓨팅 양을 늘리면 트랜스포머 언어 모델 성능이 부드럽게 향상됩니다. 최적의 성능을 위해서는 세 가지 요소를 모두 함께 확장해야 합니다. 경험적 성능은 다른 두 요소에 의해 병목 현상이 발생하지 않을 때 각 개별 요소와 거듭제곱 법칙 관계를 갖습니다 (캡션 수정 및 그림 출처: :citet:kaplan2020scaling)." />
:width:<code>700px</code>
:label:<code>fig_scaling-power-law3</code></p>
<p>:numref:<code>fig_scaling-power-law3</code>에 표시된 것처럼,
모델 크기(임베딩 레이어를 제외한 파라미터 수), 데이터셋 크기(훈련 토큰 수) 및 훈련 컴퓨팅 양(PetaFLOP/s-days, 임베딩 레이어 제외)에 대한 성능에서 *거듭제곱 법칙 확장(power-law scaling)*을 관찰할 수 있습니다.
일반적으로 이 세 가지 요소를 모두 함께 늘리면 성능이 향상됩니다.
그러나 이들을 <em>어떻게</em> 함께 늘릴지는 여전히 논쟁의 여지가 있습니다 :cite:<code>hoffmann2022training</code>.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/scaling-sample-conv.png" alt="트랜스포머 언어 모델 훈련 실행 (그림 출처: :citet:kaplan2020scaling)." />
:width:<code>700px</code>
:label:<code>fig_scaling-sample-conv</code></p>
<p>성능 향상 외에도, 큰 모델은 작은 모델보다 더 나은 샘플 효율성을 누립니다. :numref:<code>fig_scaling-sample-conv</code>는 큰 모델이 작은 모델이 달성한 동일한 수준의 성능을 수행하기 위해 더 적은 훈련 샘플(처리된 토큰)이 필요하며, 성능이 컴퓨팅과 함께 부드럽게 확장됨을 보여줍니다.</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/scaling-gpt3.png" alt="GPT-3 성능(크로스 엔트로피 검증 손실)은 훈련에 사용된 컴퓨팅 양에 따른 거듭제곱 법칙 추세를 따릅니다. :citet:kaplan2020scaling에서 관찰된 거듭제곱 법칙 동작은 예측된 곡선에서 약간만 벗어나며 두 자릿수 더 계속됩니다. 임베딩 파라미터는 컴퓨팅 및 파라미터 수에서 제외됩니다 (캡션 수정 및 그림 출처: :citet:brown2020language)." />
:width:<code>250px</code>
:label:<code>fig_scaling-gpt3</code></p>
<p>:citet:<code>kaplan2020scaling</code>의 경험적 확장 동작은 후속 대규모 트랜스포머 모델에서 테스트되었습니다. 예를 들어 GPT-3는 :numref:<code>fig_scaling-gpt3</code>에서 두 자릿수 더 큰 규모로 이 가설을 뒷받침했습니다.</p>
<h2 id="대규모-언어-모델-large-language-models"><a class="header" href="#대규모-언어-모델-large-language-models">대규모 언어 모델 (Large Language Models)</a></h2>
<p>GPT 시리즈에서 트랜스포머의 확장성은 후속 대규모 언어 모델에 영감을 주었습니다.
GPT-2 트랜스포머 디코더는 2,700억 개의 훈련 토큰으로 5,300억 파라미터의 Megatron-Turing NLG :cite:<code>smith2022using</code>를 훈련하는 데 사용되었습니다. GPT-2 설계를 따라 3,000억 개의 토큰으로 사전 훈련된 2,800억 파라미터의 Gopher :cite:<code>rae2021scaling</code>는 다양한 작업에서 경쟁력 있는 성능을 보였습니다.
동일한 아키텍처를 상속하고 Gopher와 동일한 컴퓨팅 예산을 사용하여, Chinchilla :cite:<code>hoffmann2022training</code>는 훨씬 더 오래(1조 4천억 훈련 토큰) 훈련된 실질적으로 더 작은(700억 파라미터) 모델로, 파라미터 수보다 토큰 수에 더 중점을 두어 많은 작업에서 Gopher를 능가했습니다.
언어 모델링의 확장 라인을 계속 이어가며,
PaLM(Pathway Language Model) :cite:<code>chowdhery2022palm</code>은 7,800억 개의 토큰으로 사전 훈련된 수정된 설계를 가진 5,400억 파라미터의 트랜스포머 디코더로, BIG-Bench 벤치마크 :cite:<code>srivastava2022beyond</code>에서 평균 인간 성능을 능가했습니다. 그 후속 버전인 PaLM 2 :cite:<code>anil2023palm</code>는 데이터와 모델을 대략 1:1로 확장하고 다국어 및 추론 기능을 개선했습니다.
제너럴리스트(PaLM)를 추가로 훈련하는 Minerva :cite:<code>lewkowycz2022solving</code>와 일반 코퍼스에서 훈련되지 않은 Galactica :cite:<code>taylor2022galactica</code>와 같은 다른 대규모 언어 모델은 유망한 정량적 및 과학적 추론 능력을 보여주었습니다.</p>
<p>OPT(Open Pretrained Transformers) :cite:<code>zhang2022opt</code>, BLOOM :cite:<code> scao2022bloom</code>, FALCON :cite:<code>penedo2023refinedweb</code>과 같은 오픈 소스 릴리스는 대규모 언어 모델의 연구와 사용을 민주화했습니다.
추론 시간의 계산 효율성에 초점을 맞춘 오픈 소스 Llama 1 :cite:<code>touvron2023llama</code>은 일반적으로 사용되는 것보다 더 많은 토큰으로 훈련하여 훨씬 더 큰 모델을 능가했습니다. 업데이트된 Llama 2 :cite:<code>touvron2023llama2</code>는 사전 훈련 코퍼스를 40% 더 늘려 경쟁력 있는 비공개 소스 모델의 성능과 일치할 수 있는 제품 모델로 이어졌습니다.</p>
<p>:citet:<code>wei2022emergent</code>는 작은 모델에는 없지만 큰 모델에는 존재하는 대규모 언어 모델의 창발적 능력에 대해 논의했습니다.
그러나 단순히 모델 크기를 늘린다고 해서 본질적으로 모델이 인간의 지시를 더 잘 따르게 되는 것은 아닙니다.
:citet:<code>wei2021finetuned,sanh2021multitask</code>는 *지시(instructions)*를 통해 설명된 다양한 데이터셋에서 대규모 언어 모델을 미세 조정하면 보류된(held-out) 작업에 대한 제로 샷 성능을 향상시킬 수 있음을 발견했습니다.
<em>인간 피드백을 통한 강화 학습</em>을 사용하여 :citet:<code>ouyang2022training</code>은 다양한 지시 세트를 따르도록 GPT-3를 미세 조정했습니다.
미세 조정을 통해 언어 모델을 인간의 의도와 정렬하는 결과물인 InstructGPT를 따라 :cite:<code>ouyang2022training</code>,
<a href="https://chat.openai.com/">ChatGPT</a>는 인간과의 대화를 기반으로 인간과 유사한 응답(예: 코드 디버깅 및 창의적 글쓰기)을 생성할 수 있으며 많은 자연어 처리 작업을 제로 샷으로 수행할 수 있습니다 :cite:<code>qin2023chatgpt</code>.
:citet:<code>bai2022constitutional</code>는 인간 입력(예: 사람이 라벨링한 데이터)을 모델 출력으로 대체하여 지시 튜닝 프로세스를 부분적으로 자동화했습니다. 이는 <em>AI 피드백을 통한 강화 학습</em>으로도 알려져 있습니다.</p>
<p>대규모 언어 모델은 인컨텍스트 학습을 통해 모델이 원하는 작업을 수행하도록 유도하기 위해 텍스트 입력을 공식화하는 흥미로운 전망을 제공하며, 이는 *프롬프팅(prompting)*으로도 알려져 있습니다.
특히,
퓨 샷 "질문, 중간 추론 단계, 답변" 데모가 있는 인컨텍스트 학습 방법인 <em>생각의 사슬(chain-of-thought) 프롬프팅</em> :cite:<code>wei2022chain</code>은
수학적, 상식적, 상징적 추론 문제를 해결하기 위해 대규모 언어 모델의 복잡한 추론 능력을 끌어냅니다.
여러 추론 경로 샘플링 :cite:<code>wang2023self</code>, 퓨 샷 데모 다양화 :cite:<code>zhang2023automatic</code>,
복잡한 문제를 하위 문제로 줄이는 것 :cite:<code>zhou2023least</code>은 모두 추론 정확도를 향상시킬 수 있습니다.
실제로 각 답변 바로 앞에 "단계별로 생각해보자"와 같은 간단한 프롬프트를 사용하면,
대규모 언어 모델은 괜찮은 정확도로 <em>제로 샷</em> 생각의 사슬 추론을 수행할 수도 있습니다 :cite:<code>kojima2022large</code>.
텍스트와 이미지로 구성된 멀티모달 입력의 경우에도,
언어 모델은 텍스트 입력만 사용하는 것보다 더 높은 정확도로 멀티모달 생각의 사슬 추론을 수행할 수 있습니다 :cite:<code>zhang2023multicot</code>.</p>
<h2 id="요약-및-토론-summary-and-discussion-9"><a class="header" href="#요약-및-토론-summary-and-discussion-9">요약 및 토론 (Summary and Discussion)</a></h2>
<p>트랜스포머는 인코더 전용(예: BERT), 인코더-디코더(예: T5), 디코더 전용(예: GPT 시리즈)으로 사전 훈련되었습니다. 사전 훈련된 모델은 모델 업데이트(예: 미세 조정) 또는 업데이트 없이(예: 퓨 샷) 다른 작업을 수행하도록 조정될 수 있습니다. 트랜스포머의 확장성은 더 큰 모델, 더 많은 훈련 데이터, 더 많은 훈련 컴퓨팅에서 더 나은 성능을 얻을 수 있음을 시사합니다. 트랜스포머는 처음에 텍스트 데이터를 위해 설계되고 사전 훈련되었기 때문에 이 섹션은 자연어 처리 쪽으로 약간 기울어져 있습니다. 그럼에도 불구하고 위에서 논의된 모델들은 여러 양식에 걸친 더 최근 모델에서 종종 발견될 수 있습니다. 예를 들어,
(i) Chinchilla :cite:<code>hoffmann2022training</code>는 퓨 샷 학습을 위한 시각적 언어 모델인 Flamingo :cite:<code>alayrac2022flamingo</code>로 확장되었습니다;
(ii) GPT-2 :cite:<code>Radford.Wu.Child.ea.2019</code>와 비전 트랜스포머는 CLIP(Contrastive Language-Image Pre-training) :cite:<code>radford2021learning</code>에서 텍스트와 이미지를 인코딩하며, 이 이미지 및 텍스트 임베딩은 나중에 DALL-E 2 텍스트-투-이미지 시스템 :cite:<code>ramesh2022hierarchical</code>에 채택되었습니다. 아직 멀티모달 사전 훈련에서 트랜스포머 확장성에 대한 체계적인 연구는 없지만, Parti :cite:<code>yu2022scaling</code>라는 완전 트랜스포머 텍스트-투-이미지 모델은 양식 전반에 걸친 확장 가능성을 보여줍니다:
더 큰 Parti는 충실도 높은 이미지 생성과 내용이 풍부한 텍스트 이해 능력이 더 뛰어납니다 (:numref:<code>fig_parti</code>).</p>
<p><img src="chapter_attention-mechanisms-and-transformers/../img/parti.png" alt="증가하는 크기(350M, 750M, 3B, 20B)의 Parti 모델에 의해 동일한 텍스트에서 생성된 이미지 예제 (예제 출처: :citet:yu2022scaling)." />
:width:<code>700px</code>
:label:<code>fig_parti</code></p>
<h2 id="연습-문제-exercises-65"><a class="header" href="#연습-문제-exercises-65">연습 문제 (Exercises)</a></h2>
<ol>
<li>서로 다른 작업으로 구성된 미니배치를 사용하여 T5를 미세 조정하는 것이 가능합니까? 그 이유는 무엇입니까? GPT-2의 경우는 어떻습니까?</li>
<li>강력한 언어 모델이 주어졌을 때 어떤 응용 프로그램을 생각할 수 있습니까?</li>
<li>텍스트 분류를 수행하기 위해 추가 레이어를 추가하여 언어 모델을 미세 조정하라는 요청을 받았다고 가정해 보십시오. 어디에 추가하시겠습니까? 그 이유는 무엇입니까?</li>
<li>입력 시퀀스가 타겟 시퀀스 예측 내내 항상 사용 가능한 시퀀스-투-시퀀스 문제(예: 기계 번역)를 고려하십시오. 디코더 전용 트랜스포머로 모델링할 때의 한계는 무엇일 수 있습니까? 그 이유는 무엇입니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/9232">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="최적화-알고리즘-optimization-algorithms"><a class="header" href="#최적화-알고리즘-optimization-algorithms">최적화 알고리즘 (Optimization Algorithms)</a></h1>
<p>:label:<code>chap_optimization</code></p>
<p>지금까지 이 책을 순서대로 읽었다면 딥러닝 모델을 훈련하기 위해 이미 여러 최적화 알고리즘을 사용해 보셨을 것입니다.
그것들은 모델 파라미터를 계속 업데이트하고 훈련 세트에서 평가된 손실 함수의 값을 최소화할 수 있게 해주는 도구였습니다. 사실, 단순한 설정에서 목적 함수를 최소화하기 위한 블랙 박스 장치로 최적화를 다루는 데 만족하는 사람이라면 그러한 절차의 주문 배열(예: "SGD" 및 "Adam")이 존재한다는 사실에 만족할 수도 있습니다.</p>
<p>그러나 잘하려면 더 깊은 지식이 필요합니다.
최적화 알고리즘은 딥러닝에 중요합니다.
한편으로 복잡한 딥러닝 모델을 훈련하는 데는 몇 시간, 며칠, 심지어 몇 주가 걸릴 수 있습니다.
최적화 알고리즘의 성능은 모델의 훈련 효율성에 직접적인 영향을 미칩니다.
반면에 서로 다른 최적화 알고리즘의 원리와 하이퍼파라미터의 역할을 이해하면
딥러닝 모델의 성능을 향상시키기 위해 하이퍼파라미터를 목표에 맞게 조정할 수 있습니다.</p>
<p>이 장에서는 일반적인 딥러닝 최적화 알고리즘을 깊이 있게 탐구합니다.
딥러닝에서 발생하는 거의 모든 최적화 문제는 *비볼록(nonconvex)*입니다.
그럼에도 불구하고 <em>볼록(convex)</em> 문제의 맥락에서 알고리즘을 설계하고 분석하는 것은 매우 유익한 것으로 증명되었습니다.
이러한 이유로 이 장에는 볼록 최적화에 대한 입문서와 볼록 목적 함수에 대한 매우 간단한 확률적 경사 하강법 알고리즘에 대한 증명이 포함되어 있습니다.</p>
<pre><code class="language-toc">:maxdepth: 2

optimization-intro
convexity
gd
sgd
minibatch-sgd
momentum
adagrad
rmsprop
adadelta
adam
lr-scheduler
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="최적화와-딥러닝-optimization-and-deep-learning"><a class="header" href="#최적화와-딥러닝-optimization-and-deep-learning">최적화와 딥러닝 (Optimization and Deep Learning)</a></h1>
<p>:label:<code>sec_optimization-intro</code></p>
<p>이 섹션에서는 최적화와 딥러닝 간의 관계뿐만 아니라 딥러닝에서 최적화를 사용할 때의 과제에 대해 논의할 것입니다.
딥러닝 문제의 경우, 일반적으로 먼저 *손실 함수(loss function)*를 정의합니다. 손실 함수가 있으면 손실을 최소화하기 위해 최적화 알고리즘을 사용할 수 있습니다.
최적화에서 손실 함수는 종종 최적화 문제의 *목적 함수(objective function)*라고 불립니다. 전통과 관례에 따라 대부분의 최적화 알고리즘은 <em>최소화</em>와 관련이 있습니다. 목적 함수를 최대화해야 하는 경우 간단한 해결책이 있습니다: 목적 함수의 부호를 바꾸기만 하면 됩니다.</p>
<h2 id="최적화의-목표-goal-of-optimization"><a class="header" href="#최적화의-목표-goal-of-optimization">최적화의 목표 (Goal of Optimization)</a></h2>
<p>최적화가 딥러닝을 위한 손실 함수를 최소화하는 방법을 제공하지만, 본질적으로 최적화와 딥러닝의 목표는 근본적으로 다릅니다.
전자는 주로 목적 함수를 최소화하는 것과 관련이 있는 반면, 후자는 유한한 데이터가 주어졌을 때 적절한 모델을 찾는 것과 관련이 있습니다.
:numref:<code>sec_generalization_basics</code>에서 우리는 이 두 목표의 차이점에 대해 자세히 논의했습니다.
예를 들어,
훈련 오차와 일반화 오차는 일반적으로 다릅니다: 최적화 알고리즘의 목적 함수는 보통 훈련 데이터셋에 기반한 손실 함수이므로 최적화의 목표는 훈련 오차를 줄이는 것입니다.
그러나 딥러닝(또는 더 넓게는 통계적 추론)의 목표는 일반화 오차를 줄이는 것입니다.
후자를 달성하기 위해 우리는 훈련 오차를 줄이기 위해 최적화 알고리즘을 사용하는 것 외에도 과대적합(overfitting)에 주의를 기울여야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mpl_toolkits import mplot3d
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import numpy as np
from mpl_toolkits import mplot3d
import torch
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import numpy as np
from mpl_toolkits import mplot3d
import tensorflow as tf
</code></pre>
<p>앞서 언급한 서로 다른 목표를 설명하기 위해,
경험적 위험(empirical risk)과 위험(risk)을 고려해 봅시다.
:numref:<code>subsec_empirical-risk-and-risk</code>에서 설명한 대로,
경험적 위험은 훈련 데이터셋에서의 평균 손실인 반면,
위험은 전체 데이터 모집단에서의 기대 손실입니다.
아래에서 우리는 두 함수를 정의합니다:
위험 함수 <code>f</code>와 경험적 위험 함수 <code>g</code>입니다.
우리가 유한한 양의 훈련 데이터만 가지고 있다고 가정해 봅시다.
결과적으로, 여기서 <code>g</code>는 <code>f</code>보다 덜 매끄럽습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def f(x):
    return x * d2l.cos(np.pi * x)

def g(x):
    return f(x) + 0.2 * d2l.cos(5 * np.pi * x)
</code></pre>
<p>아래 그래프는 훈련 데이터셋에서의 경험적 위험의 최소값이 위험(일반화 오차)의 최소값과 다른 위치에 있을 수 있음을 보여줍니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def annotate(text, xy, xytext):  #@save
    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
                           arrowprops=dict(arrowstyle='-&gt;'))

x = d2l.arange(0.5, 1.5, 0.01)
d2l.set_figsize((4.5, 2.5))
d2l.plot(x, [f(x), g(x)], 'x', 'risk')
annotate('min of\nempirical risk', (1.0, -1.2), (0.5, -1.1))
annotate('min of risk', (1.1, -1.05), (0.95, -0.5))
</code></pre>
<h2 id="딥러닝에서의-최적화-과제-optimization-challenges-in-deep-learning"><a class="header" href="#딥러닝에서의-최적화-과제-optimization-challenges-in-deep-learning">딥러닝에서의 최적화 과제 (Optimization Challenges in Deep Learning)</a></h2>
<p>이 장에서는 모델의 일반화 오차보다는 목적 함수를 최소화하는 최적화 알고리즘의 성능에 구체적으로 초점을 맞출 것입니다.
:numref:<code>sec_linear_regression</code>에서 우리는 최적화 문제에서 해석적 해(analytical solutions)와 수치적 해(numerical solutions)를 구분했습니다.
딥러닝에서 대부분의 목적 함수는 복잡하며 해석적 해가 없습니다. 대신 우리는 수치적 최적화 알고리즘을 사용해야 합니다.
이 장의 최적화 알고리즘은 모두 이 범주에 속합니다.</p>
<p>딥러닝 최적화에는 많은 과제가 있습니다. 가장 골치 아픈 것 중 일부는 국소 최소값(local minima), 안장점(saddle points), 그리고 기울기 소실(vanishing gradients)입니다.
이들을 살펴봅시다.</p>
<h3 id="국소-최소값-local-minima"><a class="header" href="#국소-최소값-local-minima">국소 최소값 (Local Minima)</a></h3>
<p>임의의 목적 함수 $f(x)$에 대해,
$x$에서의 $f(x)$ 값이 $x$ 근처의 다른 어떤 점들에서의 $f(x)$ 값보다 작으면, $f(x)$는 국소 최소값이 될 수 있습니다.
$x$에서의 $f(x)$ 값이 전체 도메인에 걸쳐 목적 함수의 최소값이면,
$f(x)$는 전역 최소값(global minimum)입니다.</p>
<p>예를 들어, 다음과 같은 함수가 주어졌을 때</p>
<p>$$f(x) = x \cdot \textrm{cos}(\pi x) \textrm{ for } -1.0 \leq x \leq 2.0,$$</p>
<p>우리는 이 함수의 국소 최소값과 전역 최소값을 근사할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
x = d2l.arange(-1.0, 2.0, 0.01)
d2l.plot(x, [f(x), ], 'x', 'f(x)')
annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))
annotate('global minimum', (1.1, -0.95), (0.6, 0.8))
</code></pre>
<p>딥러닝 모델의 목적 함수는 보통 많은 국소 최적점(local optima)을 가집니다.
최적화 문제의 수치적 해가 국소 최적점 근처에 있을 때, 목적 함수 해의 기울기가 0에 접근하거나 0이 됨에 따라 최종 반복에 의해 얻어진 수치적 해는 목적 함수를 전역적으로가 아니라 <em>국소적으로</em>만 최소화할 수 있습니다.
어느 정도의 노이즈만이 파라미터를 국소 최소값에서 끄집어낼 수 있습니다. 사실, 이것은 미니배치에 대한 기울기의 자연스러운 변동이 파라미터를 국소 최소값에서 떼어낼 수 있는 미니배치 확률적 경사 하강법의 유익한 속성 중 하나입니다.</p>
<h3 id="안장점-saddle-points"><a class="header" href="#안장점-saddle-points">안장점 (Saddle Points)</a></h3>
<p>국소 최소값 외에도 안장점은 기울기가 사라지는 또 다른 이유입니다. *안장점(saddle point)*은 함수의 모든 기울기가 사라지지만 전역 최소값도 국소 최소값도 아닌 위치입니다.
함수 $f(x) = x^3$을 고려해 보십시오. 그 1계 및 2계 도함수는 $x=0$에서 사라집니다. 최적화는 이 지점에서 정체될 수 있으며, 비록 그곳이 최소값은 아닐지라도 말입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
x = d2l.arange(-2.0, 2.0, 0.01)
d2l.plot(x, [x**3], 'x', 'f(x)')
annotate('saddle point', (0, -0.2), (-0.52, -5.0))
</code></pre>
<p>고차원에서의 안장점은 아래 예제가 보여주는 것처럼 훨씬 더 교활합니다. 함수 $f(x, y) = x^2 - y^2$를 고려해 보십시오. 이 함수는 $(0, 0)$에서 안장점을 가집니다. 이는 $y$에 대해서는 최대값이고 $x$에 대해서는 최소값입니다. 더욱이, 그것은 말의 안장처럼 <em>보이는데</em>, 이것이 이 수학적 속성이 이름을 얻은 이유입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x, y = d2l.meshgrid(
    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))
z = x**2 - y**2

ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x.asnumpy(), y.asnumpy(), z.asnumpy(),
                  **{'rstride': 10, 'cstride': 10})
ax.plot([0], [0], [0], 'rx')
ticks = [-1, 0, 1]
d2l.plt.xticks(ticks)
d2l.plt.yticks(ticks)
ax.set_zticks(ticks)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y');
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch, tensorflow
x, y = d2l.meshgrid(
    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))
z = x**2 - y**2

ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})
ax.plot([0], [0], [0], 'rx')
ticks = [-1, 0, 1]
d2l.plt.xticks(ticks)
d2l.plt.yticks(ticks)
ax.set_zticks(ticks)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y');
</code></pre>
<p>우리는 함수의 입력이 $k$차원 벡터이고 그 출력이 스칼라라고 가정하므로, 그 헤시안(Hessian) 행렬은 $k$개의 고유값을 가질 것입니다. 함수의 해는 함수 기울기가 0인 위치에서 국소 최소값, 국소 최대값 또는 안장점이 될 수 있습니다:</p>
<ul>
<li>기울기가 0인 위치에서 함수의 헤시안 행렬의 고유값이 모두 양수일 때, 우리는 함수의 국소 최소값을 갖습니다.</li>
<li>기울기가 0인 위치에서 함수의 헤시안 행렬의 고유값이 모두 음수일 때, 우리는 함수의 국소 최대값을 갖습니다.</li>
<li>기울기가 0인 위치에서 함수의 헤시안 행렬의 고유값이 음수와 양수가 섞여 있을 때, 우리는 함수의 안장점을 갖습니다.</li>
</ul>
<p>고차원 문제의 경우 적어도 <em>일부</em> 고유값이 음수일 가능성은 상당히 높습니다. 이는 안장점을 국소 최소값보다 더 가능성 있게 만듭니다. 우리는 다음 섹션에서 볼록성(convexity)을 도입할 때 이 상황에 대한 몇 가지 예외를 논의할 것입니다. 요컨대, 볼록 함수는 헤시안의 고유값이 결코 음수가 아닌 함수들입니다. 하지만 슬프게도 대부분의 딥러닝 문제는 이 범주에 속하지 않습니다. 그럼에도 불구하고 이는 최적화 알고리즘을 연구하기 위한 훌륭한 도구입니다.</p>
<h3 id="기울기-소실-vanishing-gradients-1"><a class="header" href="#기울기-소실-vanishing-gradients-1">기울기 소실 (Vanishing Gradients)</a></h3>
<p>아마도 마주칠 수 있는 가장 교활한 문제는 기울기 소실입니다.
:numref:<code>subsec_activation-functions</code>에서 우리가 흔히 사용하는 활성화 함수와 그 도함수를 상기해 보십시오.
예를 들어, 우리가 $f(x) = \tanh(x)$ 함수를 최소화하고 싶고 마침 $x = 4$에서 시작하게 되었다고 가정해 봅시다. 보시다시피 $f$의 기울기는 거의 0에 가깝습니다.
더 구체적으로, $f'(x) = 1 - \tanh^2(x)$이므로 $f'(4) = 0.0013$입니다.
결과적으로 최적화는 진전을 이루기 전에 오랜 시간 동안 정체될 것입니다. 이것이 ReLU 활성화 함수가 도입되기 전에 딥러닝 모델을 훈련하는 것이 꽤 까다로웠던 이유 중 하나임이 밝혀졌습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
x = d2l.arange(-2.0, 5.0, 0.01)
d2l.plot(x, [d2l.tanh(x)], 'x', 'f(x)')
annotate('vanishing gradient', (4, 1), (2, 0.0))
</code></pre>
<p>우리가 보았듯이, 딥러닝을 위한 최적화는 도전 과제로 가득 차 있습니다. 다행히도 잘 작동하며 초보자도 사용하기 쉬운 견고한 알고리즘들이 존재합니다. 더욱이, 반드시 <em>최상의</em> 솔루션을 찾을 필요는 없습니다. 국소 최적점이나 그에 대한 근사해만으로도 여전히 매우 유용합니다.</p>
<h2 id="요약-summary-51"><a class="header" href="#요약-summary-51">요약 (Summary)</a></h2>
<ul>
<li>훈련 오차를 최소화하는 것이 일반화 오차를 최소화하기 위한 최상의 파라미터 세트를 찾는 것을 보장하지는 <em>않습니다</em>.</li>
<li>최적화 문제는 많은 국소 최소값을 가질 수 있습니다.</li>
<li>일반적으로 문제는 볼록하지 않으므로 훨씬 더 많은 안장점을 가질 수 있습니다.</li>
<li>기울기 소실은 최적화를 정체시킬 수 있습니다. 종종 문제의 재파라미터화(reparametrization)가 도움이 됩니다. 파라미터의 좋은 초기화도 유익할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-66"><a class="header" href="#연습-문제-exercises-66">연습 문제 (Exercises)</a></h2>
<ol>
<li>은닉층이 하나뿐이고 은닉층 차원이 $d$이며 단일 출력을 갖는 간단한 MLP를 고려해 보십시오. 임의의 국소 최소값에 대해 동일하게 행동하는 적어도 $d!$개의 등가 솔루션이 있음을 보여주십시오.</li>
<li>항목 $M_{ij} = M_{ji}$가 각각 어떤 확률 분포 $p_{ij}$에서 추출되는 대칭 무작위 행렬 $\mathbf{M}$이 있다고 가정합니다. 더욱이 $p_{ij}(x) = p_{ij}(-x)$, 즉 분포가 대칭이라고 가정합니다(자세한 내용은 예: :citet:<code>Wigner.1958</code> 참조).
<ol>
<li>고유값에 대한 분포도 대칭임을 증명하십시오. 즉, 임의의 고유벡터 $\mathbf{v}$에 대해 관련 고유값 $\lambda$가 $P(\lambda &gt; 0) = P(\lambda &lt; 0)$을 만족할 확률입니다.</li>
<li>왜 위의 내용이 $P(\lambda &gt; 0) = 0.5$를 의미하지는 <em>않을까요</em>?</li>
</ol>
</li>
<li>딥러닝 최적화와 관련된 또 다른 과제는 무엇이 있을까요?</li>
<li>(실제) 안장 위에 (실제) 공의 균형을 맞추고 싶다고 가정해 봅시다.
<ol>
<li>왜 이것이 어려울까요?</li>
<li>이 효과를 최적화 알고리즘에도 활용할 수 있을까요?</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/349">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/487">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/489">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="볼록성-convexity"><a class="header" href="#볼록성-convexity">볼록성 (Convexity)</a></h1>
<p>:label:<code>sec_convexity</code></p>
<p>볼록성은 최적화 알고리즘 설계에서 중요한 역할을 합니다.
이는 주로 그러한 맥락에서 알고리즘을 분석하고 테스트하기가 훨씬 쉽기 때문입니다.
즉, 알고리즘이 볼록 설정에서조차 성능이 좋지 않다면 일반적으로 다른 상황에서도 좋은 결과를 기대해서는 안 됩니다.
더욱이, 딥러닝의 최적화 문제는 일반적으로 비볼록(nonconvex)이지만, 국소 최소값 근처에서 볼록한 것의 몇 가지 특성을 나타내는 경우가 많습니다. 이는 :cite:<code>Izmailov.Podoprikhin.Garipov.ea.2018</code>과 같은 흥미로운 새로운 최적화 변형으로 이어질 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mpl_toolkits import mplot3d
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import numpy as np
from mpl_toolkits import mplot3d
import torch
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import numpy as np
from mpl_toolkits import mplot3d
import tensorflow as tf
</code></pre>
<h2 id="정의-definitions"><a class="header" href="#정의-definitions">정의 (Definitions)</a></h2>
<p>볼록 분석 전에 *볼록 집합(convex sets)*과 *볼록 함수(convex functions)*를 정의해야 합니다.
이들은 머신러닝에 흔히 적용되는 수학적 도구로 이어집니다.</p>
<h3 id="볼록-집합-convex-sets"><a class="header" href="#볼록-집합-convex-sets">볼록 집합 (Convex Sets)</a></h3>
<p>집합은 볼록성의 기초입니다. 간단히 말해서, 벡터 공간의 집합 $\mathcal{X}$는 임의의 $a, b \in \mathcal{X}$에 대해 $a$와 $b$를 잇는 선분도 $\mathcal{X}$에 있으면 <em>볼록</em>합니다. 수학적 용어로 이는 모든 $\lambda  [0, 1]$에 대해 다음을 의미합니다.</p>
<p>$$\lambda  a + (1-\lambda)  b \in \mathcal{X} \textrm{ whenever } a, b \in \mathcal{X}.$$</p>
<p>이것은 약간 추상적으로 들릴 수 있습니다. :numref:<code>fig_pacman</code>을 고려해 보십시오. 첫 번째 집합은 그 안에 포함되지 않는 선분이 존재하므로 볼록하지 않습니다. 다른 두 집합은 그런 문제가 없습니다.</p>
<p><img src="chapter_optimization/../img/pacman.svg" alt="첫 번째 집합은 비볼록하고 다른 두 개는 볼록합니다." />
:label:<code>fig_pacman</code></p>
<p>정의 자체는 그것으로 무언가를 할 수 없다면 특별히 유용하지 않습니다. 이 경우 :numref:<code>fig_convex_intersect</code>에 표시된 교집합을 살펴볼 수 있습니다. $\mathcal{X}$와 $\mathcal{Y}$가 볼록 집합이라고 가정합시다. 그러면 $\mathcal{X} \cap \mathcal{Y}$도 볼록합니다. 이를 확인하려면 임의의 $a, b \in \mathcal{X} \cap \mathcal{Y}$를 고려하십시오. $\mathcal{X}$와 $\mathcal{Y}$가 볼록하므로 $a$와 $b$를 잇는 선분은 $\mathcal{X}$와 $\mathcal{Y}$ 모두에 포함됩니다. 따라서 그들은 $\mathcal{X} \cap \mathcal{Y}$에도 포함되어야 하며, 이로써 정리가 증명됩니다.</p>
<p><img src="chapter_optimization/../img/convex-intersect.svg" alt="두 볼록 집합의 교집합은 볼록합니다." />
:label:<code>fig_convex_intersect</code></p>
<p>우리는 큰 노력 없이 이 결과를 강화할 수 있습니다: 볼록 집합 $\mathcal{X}<em>i$가 주어졌을 때, 그들의 교집합 $\cap</em>{i} \mathcal{X}_i$는 볼록합니다. 역이 참이 아님을 보려면 두 개의 분리된 집합 $\mathcal{X} \cap \mathcal{Y} = \emptyset$을 고려하십시오. 이제 $a \in \mathcal{X}$ 및 $b \in \mathcal{Y}$를 선택하십시오. $\mathcal{X} \cap \mathcal{Y} = \emptyset$이라고 가정했으므로 :numref:<code>fig_nonconvex</code>에서 $a$와 $b$를 잇는 선분은 $\mathcal{X}$에도 $\mathcal{Y}$에도 속하지 않는 일부 부분을 포함해야 합니다. 따라서 선분은 $\mathcal{X} \cup \mathcal{Y}$에도 있지 않으며, 이는 일반적으로 볼록 집합의 합집합이 볼록할 필요는 없음을 증명합니다.</p>
<p><img src="chapter_optimization/../img/nonconvex.svg" alt="두 볼록 집합의 합집합은 볼록할 필요가 없습니다." />
:label:<code>fig_nonconvex</code></p>
<p>일반적으로 딥러닝의 문제는 볼록 집합에서 정의됩니다. 예를 들어 실수들의 $d$차원 벡터 집합인 $\mathbb{R}^d$는 볼록 집합입니다(결국 $\mathbb{R}^d$의 임의의 두 점 사이의 직선은 $\mathbb{R}^d$에 남아 있습니다). 어떤 경우에는 ${\mathbf{x} | \mathbf{x} \in \mathbb{R}^d \textrm{ and } |\mathbf{x}| \leq r}$로 정의된 반지름 $r$의 공과 같이 길이가 제한된 변수들로 작업합니다.</p>
<h3 id="볼록-함수-convex-functions"><a class="header" href="#볼록-함수-convex-functions">볼록 함수 (Convex Functions)</a></h3>
<p>이제 볼록 집합이 있으므로 <em>볼록 함수</em> $f$를 도입할 수 있습니다.
볼록 집합 $\mathcal{X}$가 주어졌을 때, 함수 $f: \mathcal{X} \to \mathbb{R}$은 모든 $x, x' \in \mathcal{X}$와 모든 $\lambda  [0, 1]$에 대해 다음을 만족하면 <em>볼록</em>합니다.</p>
<p>$$\lambda f(x) + (1-\lambda) f(x') \geq f(\lambda x + (1-\lambda) x').$$</p>
<p>이를 설명하기 위해 몇 가지 함수를 플롯하고 어떤 함수가 요구 사항을 충족하는지 확인해 봅시다. 아래에서 볼록 함수와 비볼록 함수를 모두 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
f = lambda x: 0.5 * x**2  # 볼록
g = lambda x: d2l.cos(np.pi * x)  # 비볼록
h = lambda x: d2l.exp(0.5 * x)  # 볼록

x, segment = d2l.arange(-2, 2, 0.01), d2l.tensor([-1.5, 1])
d2l.use_svg_display()
_, axes = d2l.plt.subplots(1, 3, figsize=(9, 3))
for ax, func in zip(axes, [f, g, h]):
    d2l.plot([x, segment], [func(x), func(segment)], axes=ax)
</code></pre>
<p>예상대로 코사인 함수는 <em>비볼록</em>인 반면, 포물선과 지수 함수는 볼록합니다. 조건이 의미가 있으려면 $\mathcal{X}$가 볼록 집합이어야 한다는 요구 사항이 필요하다는 점에 유의하십시오. 그렇지 않으면 $f(\lambda x + (1-\lambda) x')$의 결과가 잘 정의되지 않을 수 있습니다.</p>
<h3 id="젠센-부등식-jensens-inequality"><a class="header" href="#젠센-부등식-jensens-inequality">젠센 부등식 (Jensen's Inequality)</a></h3>
<p>볼록 함수 $f$가 주어졌을 때,
가장 유용한 수학적 도구 중 하나는 *젠센 부등식(Jensen's inequality)*입니다.
이는 볼록성 정의의 일반화에 해당합니다.</p>
<p>$$\sum_i \alpha_i f(x_i)  \geq f\left(\sum_i \alpha_i x_i\right)    \textrm{ 및 }    E_X[f(X)]  \geq f\left(E_X[X]\right),$$
:eqlabel:<code>eq_jensens-inequality</code></p>
<p>여기서 $\alpha_i$는 $\sum_i \alpha_i = 1$을 만족하는 비음수 실수이고 $X$는 확률 변수입니다.
즉, 볼록 함수의 기대값은 기대값의 볼록 함수보다 작지 않으며, 후자는 일반적으로 더 단순한 식입니다.
첫 번째 부등식을 증명하기 위해 우리는 합계의 한 항씩 볼록성 정의를 반복적으로 적용합니다.</p>
<p>젠센 부등식의 일반적인 응용 중 하나는
더 복잡한 식을 더 단순한 식으로 경계 짓는 것입니다.
예를 들어,
그 응용은 부분적으로 관찰된 확률 변수의 로그 우도에 관한 것일 수 있습니다. 즉, 우리는 다음을 사용합니다.</p>
<p>$$E_{Y \sim P(Y)}[-\log P(X \mid Y)] \geq -\log P(X),$$</p>
<p>$\int P(Y) P(X \mid Y) dY = P(X)$이기 때문입니다.
이는 변분 방법(variational methods)에서 사용될 수 있습니다. 여기서 $Y$는 일반적으로 관찰되지 않은 확률 변수이고, $P(Y)$는 그것이 어떻게 분포되어 있을지에 대한 최선의 추측이며, $P(X)$는 $Y$가 통합되어 제거된 분포입니다. 예를 들어, 클러스터링에서 $Y$는 클러스터 레이블일 수 있고 $P(X \mid Y)$는 클러스터 레이블을 적용할 때의 생성 모델입니다.</p>
<h2 id="속성-properties"><a class="header" href="#속성-properties">속성 (Properties)</a></h2>
<p>볼록 함수는 유용한 속성을 많이 가지고 있습니다. 아래에서 흔히 사용되는 몇 가지를 설명합니다.</p>
<h3 id="국소-최소값은-전역-최소값이다-local-minima-are-global-minima"><a class="header" href="#국소-최소값은-전역-최소값이다-local-minima-are-global-minima">국소 최소값은 전역 최소값이다 (Local Minima Are Global Minima)</a></h3>
<p>무엇보다도, 볼록 함수의 국소 최소값은 전역 최소값이기도 합니다.
우리는 이를 다음과 같이 귀류법으로 증명할 수 있습니다.</p>
<p>볼록 집합 $\mathcal{X}$에서 정의된 볼록 함수 $f$를 고려하십시오.
$x^{\ast} \in \mathcal{X}$가 국소 최소값이라고 가정합시다:
$0 &lt; |x - x^{\ast}| \leq p$를 만족하는 $x \in \mathcal{X}$에 대해 $f(x^{\ast}) &lt; f(x)$가 성립하도록 하는 작은 양수 값 $p$가 존재합니다.</p>
<p>국소 최소값 $x^{\ast}$가 $f$의 전역 최소값이 아니라고 가정합시다:
$f(x') &lt; f(x^{\ast})$인 $x' \in \mathcal{X}$가 존재합니다.
또한 $0 &lt; |\lambda x^{\ast} + (1-\lambda) x' - x^{\ast}| \leq p$가 되도록
$\lambda = 1 - \frac{p}{|x^{\ast} - x'|}$와 같은
$\lambda \in [0, 1)$가 존재합니다.</p>
<p>그러나 볼록 함수의 정의에 따라 우리는 다음을 갖습니다.</p>
<p>$$\begin{aligned}
f(\lambda x^{\ast} + (1-\lambda) x') &amp;\leq \lambda f(x^{\ast}) + (1-\lambda) f(x') \
&amp;&lt; \lambda f(x^{\ast}) + (1-\lambda) f(x^{\ast}) \
&amp;= f(x^{\ast}),
\end{aligned}$$</p>
<p>이는 $x^{\ast}$가 국소 최소값이라는 우리의 진술과 모순됩니다.
따라서 $f(x') &lt; f(x^{\ast})$인 $x' \in \mathcal{X}$는 존재하지 않습니다. 국소 최소값 $x^{\ast}$는 전역 최소값이기도 합니다.</p>
<p>예를 들어, 볼록 함수 $f(x) = (x-1)^2$는 $x=1$에서 국소 최소값을 가지며, 이는 전역 최소값이기도 합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
f = lambda x: (x - 1) ** 2
d2l.set_figsize()
d2l.plot([x, segment], [f(x), f(segment)], 'x', 'f(x)')
</code></pre>
<p>볼록 함수의 국소 최소값이 전역 최소값이기도 하다는 사실은 매우 편리합니다.
이는 우리가 함수를 최소화할 때 "갇힐" 수 없음을 의미합니다.
하지만 이것이 전역 최소값이 하나 이상 존재할 수 없다는 것을 의미하거나 전역 최소값이 반드시 존재한다는 것을 의미하지는 않는다는 점에 유의하십시오. 예를 들어, 함수 $f(x) = \mathrm{max}(|x|-1, 0)$은 구간 $[-1, 1]$에서 최소값을 달성합니다. 반대로 함수 $f(x) = \exp(x)$는 $\mathbb{R}$에서 최소값을 달성하지 않습니다: $x \to -\infty$에 대해 $0$으로 점근하지만, $f(x) = 0$이 되는 $x$는 없습니다.</p>
<h3 id="볼록-함수의-하위-집합은-볼록하다-below-sets-of-convex-functions-are-convex"><a class="header" href="#볼록-함수의-하위-집합은-볼록하다-below-sets-of-convex-functions-are-convex">볼록 함수의 하위 집합은 볼록하다 (Below Sets of Convex Functions Are Convex)</a></h3>
<p>우리는 볼록 함수의 *하위 집합(below sets)*을 통해
볼록 집합을 편리하게 정의할 수 있습니다.
구체적으로,
볼록 집합 $\mathcal{X}$에서 정의된 볼록 함수 $f$가 주어졌을 때, 임의의 하위 집합</p>
<p>$$\mathcal{S}_b \stackrel{\textrm{def}}{=} {x | x \in \mathcal{X} \textrm{ 및 } f(x) \leq b}$$</p>
<p>은 볼록합니다.</p>
<p>이를 빠르게 증명해 봅시다. 임의의 $x, x' \in \mathcal{S}_b$에 대해 $\lambda \in [0, 1]$인 한 $\lambda x + (1-\lambda) x' \in \mathcal{S}_b$임을 보여야 합니다.
$f(x) \leq b$ 및 $f(x') \leq b$이므로, 볼록성의 정의에 의해 우리는 다음을 갖습니다.</p>
<p>$$f(\lambda x + (1-\lambda) x') \leq \lambda f(x) + (1-\lambda) f(x') \leq b.$$</p>
<h3 id="볼록성과-2계-도함수-convexity-and-second-derivatives"><a class="header" href="#볼록성과-2계-도함수-convexity-and-second-derivatives">볼록성과 2계 도함수 (Convexity and Second Derivatives)</a></h3>
<p>함수 $f: \mathbb{R}^n \rightarrow \mathbb{R}$의 2계 도함수가 존재할 때마다 $f$가 볼록한지 확인하는 것은 매우 쉽습니다.
우리가 해야 할 일은 $f$의 헤시안(Hessian)이 양의 반정부호(positive semidefinite)인지 확인하는 것입니다: $\nabla^2f \succeq 0$, 즉
헤시안 행렬 $\nabla^2f$를 $\mathbf{H}$로 표시할 때,
모든 $\mathbf{x} \in \mathbb{R}^n$에 대해
$\mathbf{x}^\top \mathbf{H}\mathbf{x} \geq 0$입니다.
예를 들어, 함수 $f(\mathbf{x}) = \frac{1}{2} |\mathbf{x}|^2$은 $\nabla^2 f = \mathbf{1}$, 즉 그 헤시안이 단위 행렬이므로 볼록합니다.</p>
<p>공식적으로, 두 번 미분 가능한 1차원 함수 $f: \mathbb{R} \rightarrow \mathbb{R}$은
그 2계 도함수 $f'' \geq 0$일 때만 볼록합니다. 임의의 두 번 미분 가능한 다차원 함수 $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$에 대해,
그 헤시안 $\nabla^2f \succeq 0$일 때만 볼록합니다.</p>
<p>먼저 1차원 사례를 증명해야 합니다.
$f$의 볼록성이 $f'' \geq 0$을 의미한다는 것을 보기 위해 우리는 다음 사실을 사용합니다.</p>
<p>$$\frac{1}{2} f(x + \epsilon) + \frac{1}{2} f(x - \epsilon) \geq f\left(\frac{x + \epsilon}{2} + \frac{x - \epsilon}{2}\right) = f(x).$$</p>
<p>2계 도함수가 유한 차분에 대한 극한으로 주어지므로 다음이 따릅니다.</p>
<p>$$f''(x) = \lim_{\epsilon \to 0} \frac{f(x+\epsilon) + f(x - \epsilon) - 2f(x)}{\epsilon^2} \geq 0.$$</p>
<p>$f'' \geq 0$이 $f$가 볼록함을 의미한다는 것을 보기 위해 $f'' \geq 0$이 $f'$이 단조 비감소 함수임을 의미한다는 사실을 사용합니다. $a &lt; x &lt; b$를 $\mathbb{R}$의 세 점이라고 합시다.
여기서 $x = (1-\lambda)a + \lambda b$이고 $\lambda \in (0, 1)$입니다.
평균값 정리에 따라, 다음을 만족하는 $\alpha \in [a, x]$ 및 $\beta \in [x, b]$가 존재합니다.</p>
<p>$$f'(\alpha) = \frac{f(x) - f(a)}{x-a} \textrm{ 및 } f'(\beta) = \frac{f(b) - f(x)}{b-x}.$$</p>
<p>단조성에 의해 $f'(\beta) \geq f'(\alpha)$이므로,</p>
<p>$$\frac{x-a}{b-a}f(b) + \frac{b-x}{b-a}f(a) \geq f(x).$$</p>
<p>$x = (1-\lambda)a + \lambda b$이므로,</p>
<p>$$\lambda f(b) + (1-\lambda)f(a) \geq f((1-\lambda)a + \lambda b),$$</p>
<p>이로써 볼록성이 증명됩니다.</p>
<p>둘째, 다차원 사례를 증명하기 전에 보조 정리가 필요합니다:
$f: \mathbb{R}^n \rightarrow \mathbb{R}$은
모든 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$에 대해</p>
<p>$$g(z) \stackrel{\textrm{def}}{=} f(z \mathbf{x} + (1-z)  \mathbf{y}) \textrm{ where } z \in [0,1]$$</p>
<p>이 볼록할 때만 볼록합니다.</p>
<p>$f$의 볼록성이 $g$가 볼록함을 의미한다는 것을 증명하기 위해, 모든 $a, b, \lambda \in [0, 1]$(따라서 $0 \leq \lambda a + (1-\lambda) b \leq 1$)에 대해 다음을 보일 수 있습니다.</p>
<p>$$\begin{aligned} &amp;g(\lambda a + (1-\lambda) b)<br />
=&amp;f\left(((\lambda a + (1-\lambda) b))\mathbf{x} + (1-\lambda a - (1-\lambda) b))\mathbf{y} \right)<br />
=&amp;f\left(\lambda (a \mathbf{x} + (1-a)  \mathbf{y})  + (1-\lambda) (b \mathbf{x} + (1-b)  \mathbf{y}) \right)<br />
\leq&amp; \lambda f(a \mathbf{x} + (1-a)  \mathbf{y})  + (1-\lambda) f(b \mathbf{x} + (1-b)  \mathbf{y}) \
=&amp; \lambda g(a) + (1-\lambda) g(b).
\end{aligned}$$</p>
<p>역을 증명하기 위해, 모든 $\lambda \in [0, 1]$에 대해 다음을 보일 수 있습니다.</p>
<p>$$\begin{aligned} &amp;f(\lambda \mathbf{x} + (1-\lambda) \mathbf{y})<br />
=&amp;g(\lambda \cdot 1 + (1-\lambda) \cdot 0)<br />
\leq&amp; \lambda g(1)  + (1-\lambda) g(0) \
=&amp; \lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}).
\end{aligned}$$</p>
<p>마지막으로, 위의 보조 정리와 1차원 사례의 결과를 사용하여 다차원 사례를 다음과 같이 증명할 수 있습니다.
다차원 함수 $f: \mathbb{R}^n \rightarrow \mathbb{R}$은 모든 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$에 대해 $z \in [0,1]$인 $g(z) \stackrel{\textrm{def}}{=} f(z \mathbf{x} + (1-z)  \mathbf{y})$가 볼록할 때만 볼록합니다.
1차원 사례에 따르면, 이는 모든 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$에 대해 $g'' = (\mathbf{x} - \mathbf{y})^\top \mathbf{H}(\mathbf{x} - \mathbf{y}) \geq 0$($\mathbf{H} \stackrel{\textrm{def}}{=} \nabla^2f$)일 때만 성립하며, 이는 양의 반정부호 행렬의 정의에 따라 $\mathbf{H} \succeq 0$과 동등합니다.</p>
<h2 id="제약-조건-constraints"><a class="header" href="#제약-조건-constraints">제약 조건 (Constraints)</a></h2>
<p>볼록 최적화의 좋은 속성 중 하나는 제약 조건을 효율적으로 처리할 수 있게 해 준다는 것입니다. 즉, 다음과 같은 형태의 <em>제약 조건이 있는 최적화(constrained optimization)</em> 문제를 풀 수 있게 해 줍니다.</p>
<p>$$\begin{aligned} \mathop{\textrm{minimize~}}_{\mathbf{x}} &amp; f(\mathbf{x}) \
\textrm{ subject to } &amp; c_i(\mathbf{x}) \leq 0 \textrm{ for all } i \in {1, \ldots, n},
\end{aligned}$$</p>
<p>여기서 $f$는 목적 함수이고 함수 $c_i$는 제약 함수입니다. 이것이 무엇을 하는지 보려면 $c_1(\mathbf{x}) = |\mathbf{x}|_2 - 1$인 경우를 고려해 보십시오. 이 경우 파라미터 $\mathbf{x}$는 단위 공(unit ball)으로 제약됩니다. 두 번째 제약 조건이 $c_2(\mathbf{x}) = \mathbf{v}^\top \mathbf{x} + b$라면, 이는 반공간(half-space)에 놓인 모든 $\mathbf{x}$에 대응합니다. 두 제약 조건을 동시에 만족하는 것은 공의 한 조각을 선택하는 것과 같습니다.</p>
<h3 id="라그랑지안-lagrangian"><a class="header" href="#라그랑지안-lagrangian">라그랑지안 (Lagrangian)</a></h3>
<p>일반적으로 제약 조건이 있는 최적화 문제를 푸는 것은 어렵습니다. 이를 다루는 한 가지 방법은 다소 간단한 직관을 가진 물리학에서 비롯되었습니다. 상자 안에 공이 있다고 상상해 보십시오. 공은 가장 낮은 곳으로 굴러갈 것이고 중력의 힘은 상자의 측면이 공에 가할 수 있는 힘과 균형을 이룰 것입니다. 요컨대, 목적 함수의 기울기(즉, 중력)는 제약 함수의 기울기(벽이 "밀어내는" 힘에 의해 공이 상자 안에 머물러야 함)에 의해 상쇄될 것입니다.
일부 제약 조건은 활성화되지 않을 수 있음에 유의하십시오:
공에 닿지 않는 벽은 공에 어떤 힘도 가할 수 없습니다.</p>
<p>라그랑지안 $L$의 유도를 건너뛰고, 위의 추론은
다음과 같은 안장점 최적화 문제로 표현될 수 있습니다.</p>
<p>$$L(\mathbf{x}, \alpha_1, \ldots, \alpha_n) = f(\mathbf{x}) + \sum_{i=1}^n \alpha_i c_i(\mathbf{x}) \textrm{ where } \alpha_i \geq 0.$$</p>
<p>여기서 변수 $\alpha_i$ ($i=1,\ldots,n$)는 제약 조건이 적절하게 적용되도록 보장하는 이른바 *라그랑주 승수(Lagrange multipliers)*입니다. 이들은 $c_i(\mathbf{x}) \leq 0$이 모든 $i$에 대해 보장되도록 충분히 크게 선택됩니다. 예를 들어, 자연스럽게 $c_i(\mathbf{x}) &lt; 0$인 임의의 $\mathbf{x}$에 대해 우리는 결국 $\alpha_i = 0$을 선택하게 될 것입니다. 더욱이, 이것은 모든 $\alpha_i$에 대해 $L$을 <em>최대화</em>하고 동시에 $\mathbf{x}$에 대해 $L$을 <em>최소화</em>하려는 안장점 최적화 문제입니다. 함수 $L(\mathbf{x}, \alpha_1, \ldots, \alpha_n)$에 어떻게 도달하는지 설명하는 풍부한 문헌이 있습니다. 우리의 목적을 위해서는 $L$의 안장점이 원래의 제약 조건이 있는 최적화 문제가 최적으로 해결되는 지점이라는 것을 아는 것으로 충분합니다.</p>
<h3 id="페널티-penalties"><a class="header" href="#페널티-penalties">페널티 (Penalties)</a></h3>
<p>제약 조건이 있는 최적화 문제를 적어도 <em>근사적으로</em> 만족시키는 한 가지 방법은 라그랑지안 $L$을 조정하는 것입니다.
$c_i(\mathbf{x}) \leq 0$을 만족시키는 대신 단순히 목적 함수 $f(x)$에 $\alpha_i c_i(\mathbf{x})$를 더합니다. 이는 제약 조건이 너무 심하게 위반되지 않도록 보장합니다.</p>
<p>사실, 우리는 줄곧 이 트릭을 사용해 왔습니다. :numref:<code>sec_weight_decay</code>의 가중치 감쇠(weight decay)를 고려해 보십시오. 여기서 우리는 $\mathbf{w}$가 너무 크게 자라지 않도록 보장하기 위해 목적 함수에 $\frac{\lambda}{2} |\mathbf{w}|^2$를 더합니다. 제약 조건이 있는 최적화 관점에서 보면, 이것이 어떤 반지름 $r$에 대해 $|\mathbf{w}|^2 - r^2 \leq 0$을 보장할 것임을 알 수 있습니다. $\lambda$ 값을 조정하면 $\mathbf{w}$의 크기를 변경할 수 있습니다.</p>
<p>일반적으로 페널티를 추가하는 것은 근사적인 제약 조건 만족을 보장하는 좋은 방법입니다. 실전에서 이것은 정확한 만족보다 훨씬 더 견고한 것으로 밝혀졌습니다. 더욱이, 비볼록 문제의 경우 볼록 사례에서 정확한 접근 방식을 매력적으로 만들었던 많은 속성(예: 최적성)이 더 이상 유지되지 않습니다.</p>
<h3 id="투영-projections"><a class="header" href="#투영-projections">투영 (Projections)</a></h3>
<p>제약 조건을 만족시키기 위한 대안 전략은 투영(projections)입니다. 다시 말하지만, 우리는 이전에 이를 접했습니다. 예를 들어 :numref:<code>sec_rnn-scratch</code>에서 기울기 클리핑(gradient clipping)을 다룰 때입니다. 거기서 우리는 다음을 통해 기울기의 길이가 $\theta$로 제한되도록 보장했습니다.</p>
<p>$$\mathbf{g} \leftarrow \mathbf{g} \cdot \mathrm{min}(1, \theta/|\mathbf{g}|).$$</p>
<p>이것은 기울기 $\mathbf{g}$를 반지름 $\theta$의 공 위로 <em>투영</em>한 것으로 밝혀졌습니다. 더 일반적으로, 볼록 집합 $\mathcal{X}$에 대한 투영은 다음과 같이 정의됩니다.</p>
<p>$$\textrm{Proj}<em>\mathcal{X}(\mathbf{x}) = \mathop{\mathrm{argmin}}</em>{\mathbf{x}' \in \mathcal{X}} |\mathbf{x} - \mathbf{x}'|,$$</p>
<p>이는 $\mathcal{X}$에서 $\mathbf{x}$와 가장 가까운 점입니다.</p>
<p><img src="chapter_optimization/../img/projections.svg" alt="볼록 투영." />
:label:<code>fig_projections</code></p>
<p>투영의 수학적 정의는 약간 추상적으로 들릴 수 있습니다. :numref:<code>fig_projections</code>은 이를 좀 더 명확하게 설명합니다. 여기에는 원과 다이아몬드라는 두 개의 볼록 집합이 있습니다.
두 집합 내부의 점(노란색)은 투영 중에 변경되지 않고 그대로 유지됩니다.
두 집합 외부의 점(검은색)은 원래 점(검은색)과 가장 가까운 집합 내부의 점(빨간색)으로 투영됩니다.
$\ell_2$ 공의 경우 이것이 방향을 바꾸지 않고 유지하지만, 다이아몬드의 사례에서 볼 수 있듯이 일반적으로는 그렇지 않을 수도 있습니다.</p>
<p>볼록 투영의 용도 중 하나는 희소 가중치 벡터(sparse weight vectors)를 계산하는 것입니다. 이 경우 가중치 벡터를 $\ell_1$ 공 위로 투영하는데,
이는 :numref:<code>fig_projections</code>의 다이아몬드 사례의 일반화된 버전입니다.</p>
<h2 id="요약-summary-52"><a class="header" href="#요약-summary-52">요약 (Summary)</a></h2>
<p>딥러닝의 맥락에서 볼록 함수의 주요 목적은 최적화 알고리즘에 동기를 부여하고 이를 자세히 이해하도록 돕는 것입니다. 다음에서는 경사 하강법과 확률적 경사 하강법이 그에 따라 어떻게 유도될 수 있는지 볼 것입니다.</p>
<ul>
<li>볼록 집합의 교집합은 볼록합니다. 합집합은 그렇지 않습니다.</li>
<li>볼록 함수의 기대값은 기대값의 볼록 함수보다 작지 않습니다(젠센 부등식).</li>
<li>두 번 미분 가능한 함수는 그 헤시안(2계 도함수 행렬)이 양의 반정부호일 때만 볼록합니다.</li>
<li>볼록 제약 조건은 라그랑지안을 통해 추가될 수 있습니다. 실전에서는 단순히 목적 함수에 페널티와 함께 추가할 수 있습니다.</li>
<li>투영은 볼록 집합에서 원래 점과 가장 가까운 점으로 매핑합니다.</li>
</ul>
<h2 id="연습-문제-exercises-67"><a class="header" href="#연습-문제-exercises-67">연습 문제 (Exercises)</a></h2>
<ol>
<li>집합 내의 점들 사이의 모든 선을 긋고 그 선들이 포함되는지 확인함으로써 집합의 볼록성을 확인하고 싶다고 가정합니다.
<ol>
<li>경계에 있는 점들만 확인하는 것으로 충분함을 증명하십시오.</li>
<li>집합의 정점(vertices)들만 확인하는 것으로 충분함을 증명하십시오.</li>
</ol>
</li>
<li>$\mathcal{B}_p[r] \stackrel{\textrm{def}}{=} {\mathbf{x} | \mathbf{x} \in \mathbb{R}^d \textrm{ 및 } |\mathbf{x}|_p \leq r}$를 $p$-노름을 사용한 반지름 $r$의 공이라고 합시다. 모든 $p \geq 1$에 대해 $\mathcal{B}_p[r]$이 볼록함을 증명하십시오.</li>
<li>볼록 함수 $f$와 $g$가 주어졌을 때, $\mathrm{max}(f, g)$도 볼록함을 보이십시오. $\mathrm{min}(f, g)$는 볼록하지 않음을 증명하십시오.</li>
<li>소프트맥스 함수의 정규화가 볼록함을 증명하십시오. 더 구체적으로 $f(x) = \log \sum_i \exp(x_i)$의 볼록성을 증명하십시오.</li>
<li>선형 부분 공간, 즉 $\mathcal{X} = {\mathbf{x} | \mathbf{W} \mathbf{x} = \mathbf{b}}$가 볼록 집합임을 증명하십시오.</li>
<li>$\mathbf{b} = \mathbf{0}$인 선형 부분 공간의 경우 투영 $\textrm{Proj}_\mathcal{X}$가 어떤 행렬 $\mathbf{M}$에 대해 $\mathbf{M} \mathbf{x}$로 쓰일 수 있음을 증명하십시오.</li>
<li>두 번 미분 가능한 볼록 함수 $f$에 대해 어떤 $\xi \in [0, \epsilon]$에 대해 $f(x + \epsilon) = f(x) + \epsilon f'(x) + \frac{1}{2} \epsilon^2 f''((x + \xi)$로 쓸 수 있음을 보이십시오.</li>
<li>볼록 집합 $\mathcal{X}$와 두 벡터 $\mathbf{x}$ 및 $\mathbf{y}$가 주어졌을 때, 투영이 거리를 결코 증가시키지 않음을 증명하십시오. 즉, $|\mathbf{x} - \mathbf{y}| \geq |\textrm{Proj}<em>\mathcal{X}(\mathbf{x}) - \textrm{Proj}</em>\mathcal{X}(\mathbf{y})|$입니다.</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/350">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="경사-하강법-gradient-descent"><a class="header" href="#경사-하강법-gradient-descent">경사 하강법 (Gradient Descent)</a></h1>
<p>:label:<code>sec_gd</code></p>
<p>이 섹션에서는 *경사 하강법(gradient descent)*의 기초가 되는 기본 개념을 소개할 것입니다.
딥러닝에서 직접 사용되는 경우는 드물지만, 경사 하강법을 이해하는 것은 확률적 경사 하강법 알고리즘을 이해하는 데 핵심입니다.
예를 들어, 최적화 문제는 지나치게 큰 학습률로 인해 발산할 수 있습니다. 이 현상은 이미 경사 하강법에서 볼 수 있습니다. 마찬가지로, 프리컨디셔닝(preconditioning)은 경사 하강법의 일반적인 기술이며 더 발전된 알고리즘으로 이어집니다.
간단한 특수 사례부터 시작해 봅시다.</p>
<h2 id="1차원-경사-하강법-one-dimensional-gradient-descent"><a class="header" href="#1차원-경사-하강법-one-dimensional-gradient-descent">1차원 경사 하강법 (One-Dimensional Gradient Descent)</a></h2>
<p>1차원 경사 하강법은 왜 경사 하강법 알고리즘이 목적 함수의 값을 줄일 수 있는지 설명하는 훌륭한 예입니다. 어떤 연속 미분 가능한 실수 값 함수 $f: ℝ&gt; ℝ$을 고려해 보십시오. 테일러 전개를 사용하면 다음을 얻습니다.</p>
<p>$$f(x + \epsilon) = f(x) + \epsilon f'(x) + \mathcal{O}(\epsilon^2).$$
:eqlabel:<code>gd-taylor</code></p>
<p>즉, 1차 근사에서 $f(x+\epsilon)$은 $x$에서의 함수 값 $f(x)$와 1계 도함수 $f'(x)$에 의해 주어집니다. 작은 $\epsilon$에 대해 음의 기울기 방향으로 이동하면 $f$가 감소할 것이라고 가정하는 것은 무리가 아닙니다. 단순함을 위해 고정된 단계 크기 $\eta &gt; 0$을 선택하고 $\epsilon = -\eta f'(x)$를 선택합니다. 이를 위의 테일러 전개에 대입하면 다음을 얻습니다.</p>
<p>$$f(x - \eta f'(x)) = f(x) - \eta f'^2(x) + \mathcal{O}(\eta^2 f'^2(x)).$$
:eqlabel:<code>gd-taylor-2</code></p>
<p>도함수 $f'(x) \neq 0$이 사라지지 않으면 $\eta f'^2(x)&gt;0$이므로 진전을 이룹니다. 게다가, 우리는 항상 고차 항이 무관해질 만큼 충분히 작은 $\eta$를 선택할 수 있습니다. 따라서 다음과 같이 도달합니다.</p>
<p>$$f(x - \eta f'(x)) \lessapprox f(x).$$</p>
<p>이것은 우리가</p>
<p>$$x \leftarrow x - \eta f'(x)$$</p>
<p>를 사용하여 $x$를 반복하면 함수 $f(x)$의 값이 감소할 수 있음을 의미합니다. 따라서 경사 하강법에서는 먼저 초기 값 $x$와 상수 $\eta &gt; 0$을 선택한 다음, 중지 조건(예: 기울기 $|f'(x)|$의 크기가 충분히 작아지거나 반복 횟수가 특정 값에 도달했을 때)에 도달할 때까지 이를 사용하여 $x$를 계속 반복합니다.</p>
<p>단순함을 위해 목적 함수 $f(x)=x^2$를 선택하여 경사 하강법을 구현하는 방법을 설명합니다. $x=0$이 $f(x)$를 최소화하는 해임을 알고 있지만, 여전히 이 단순한 함수를 사용하여 $x$가 어떻게 변하는지 관찰합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import numpy as np
import torch
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import numpy as np
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
def f(x):  # 목적 함수
    return x ** 2

def f_grad(x):  # 목적 함수의 기울기(도함수)
    return 2 * x
</code></pre>
<p>다음으로, $x=10$을 초기 값으로 사용하고 $\eta=0.2$라고 가정합니다. 경사 하강법을 사용하여 $x$를 10번 반복하면 결국 $x$의 값이 최적의 해에 접근하는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def gd(eta, f_grad):
    x = 10.0
    results = [x]
    for i in range(10):
        x -= eta * f_grad(x)
        results.append(float(x))
    print(f'epoch 10, x: {x:f}')
    return results

results = gd(0.2, f_grad)
</code></pre>
<p>$x$에 대한 최적화 과정은 다음과 같이 그릴 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def show_trace(results, f):
    n = max(abs(min(results)), abs(max(results)))
    f_line = d2l.arange(-n, n, 0.01)
    d2l.set_figsize()
    d2l.plot([f_line, results], [[f(x) for x in f_line], [
        f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])

show_trace(results, f)
</code></pre>
<h3 id="학습률-learning-rate"><a class="header" href="#학습률-learning-rate">학습률 (Learning Rate)</a></h3>
<p>:label:<code>subsec_gd-learningrate</code></p>
<p>학습률 $\eta$는 알고리즘 설계자가 설정할 수 있습니다. 너무 작은 학습률을 사용하면 $x$가 매우 느리게 업데이트되어 더 나은 해를 얻기 위해 더 많은 반복이 필요합니다. 그런 경우에 어떤 일이 일어나는지 보여주기 위해, $\eta = 0.05$일 때 동일한 최적화 문제에서의 과정을 고려해 보십시오. 보시다시피 10단계 후에도 우리는 여전히 최적의 해에서 매우 멀리 떨어져 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
show_trace(gd(0.05, f_grad), f)
</code></pre>
<p>반대로 지나치게 높은 학습률을 사용하면 $\left|\eta f'(x)\right|$가 1차 테일러 전개 공식에 비해 너무 클 수 있습니다. 즉, :eqref:<code>gd-taylor-2</code>의 항 $\mathcal{O}(\eta^2 f'^2(x))$가 중요해질 수 있습니다. 이 경우 $x$의 반복이 $f(x)$의 값을 낮출 수 있다고 보장할 수 없습니다. 예를 들어 학습률을 $\eta=1.1$로 설정하면 $x$가 최적의 해 $x=0$을 지나치고 점차 발산합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
show_trace(gd(1.1, f_grad), f)
</code></pre>
<h3 id="국소-최소값-local-minima-1"><a class="header" href="#국소-최소값-local-minima-1">국소 최소값 (Local Minima)</a></h3>
<p>비볼록(nonconvex) 함수의 경우 어떤 일이 일어나는지 설명하기 위해 어떤 상수 $c$에 대해 $f(x) = x \cdot \cos(cx)$인 경우를 고려해 보십시오. 이 함수는 무한히 많은 국소 최소값을 가집니다. 학습률 선택과 문제의 조건이 얼마나 좋은지에 따라 우리는 많은 해 중 하나에 도달할 수 있습니다. 아래 예제는 (비현실적으로) 높은 학습률이 좋지 않은 국소 최소값으로 어떻게 이어지는지 보여줍니다.</p>
<pre><code class="language-{.python .input}">#@tab all
c = d2l.tensor(0.15 * np.pi)

def f(x):  # 목적 함수
    return x * d2l.cos(c * x)

def f_grad(x):  # 목적 함수의 기울기
    return d2l.cos(c * x) - c * x * d2l.sin(c * x)

show_trace(gd(2, f_grad), f)
</code></pre>
<h2 id="다변량-경사-하강법-multivariate-gradient-descent"><a class="header" href="#다변량-경사-하강법-multivariate-gradient-descent">다변량 경사 하강법 (Multivariate Gradient Descent)</a></h2>
<p>이제 일변량 사례에 대해 더 나은 직관을 얻었으므로 $\mathbf{x} = [x_1, x_2, \ldots, x_d]^⊥$인 상황을 고려해 봅시다. 즉, 목적 함수 $f: ℝ&gt;^d 	o ℝ$은 벡터를 스칼라로 매핑합니다. 그에 대응하여 기울기도 다변량입니다. 이는 $d$개의 편미분으로 구성된 벡터입니다.</p>
<p>$$\nabla f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_d}\bigg]^⊥.$$</p>
<p>기울기의 각 편미분 요소 $\partial f(\mathbf{x})/\partial x_i$는 입력 $x_i$에 대한 $\mathbf{x}$에서의 $f$의 변화율을 나타냅니다. 이전 일변량 사례와 마찬가지로 다변량 함수에 대한 해당 테일러 근사를 사용하여 무엇을 해야 할지 감을 잡을 수 있습니다. 특히 다음을 얻습니다.</p>
<p>$$f(\mathbf{x} + \boldsymbol{\epsilon}) = f(\mathbf{x}) + \boldsymbol{\epsilon}^\top \nabla f(\mathbf{x}) + \mathcal{O}(|\boldsymbol{\epsilon}|^2).$$
:eqlabel:<code>gd-multi-taylor</code></p>
<p>즉, $\boldsymbol{\epsilon}$의 2차 항까지 가장 가파른 하강 방향은 음의 기울기 $-\nabla f(\mathbf{x})$에 의해 주어집니다. 적절한 학습률 $\eta &gt; 0$을 선택하면 전형적인 경사 하강법 알고리즘이 생성됩니다.</p>
<p>$$\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f(\mathbf{x}).$$</p>
<p>알고리즘이 실제로 어떻게 작동하는지 확인하기 위해 2차원 벡터 $\mathbf{x} = [x_1, x_2]^⊥$를 입력으로 하고 스칼라를 출력으로 하는 목적 함수 $f(\mathbf{x})=x_1^2+2x_2^2$를 구성해 봅시다. 기울기는 $\nabla f(\mathbf{x}) = [2x_1, 4x_2]^⊥$로 주어집니다. 초기 위치 $[-5, -2]$에서 경사 하강법에 의한 $\mathbf{x}$의 궤적을 관찰할 것입니다.</p>
<p>우선 두 개의 보조 함수가 더 필요합니다. 첫 번째는 업데이트 함수를 사용하여 초기 값에 20번 적용합니다. 두 번째 보조 함수는 $\mathbf{x}$의 궤적을 시각화합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def train_2d(trainer, steps=20, f_grad=None):  #@save
    """맞춤형 트레이너로 2D 목적 함수를 최적화합니다."""
    # `s1` 및 `s2`는 모멘텀, adagrad, RMSProp에서 사용될 내부 상태 변수입니다
    x1, x2, s1, s2 = -5, -2, 0, 0
    results = [(x1, x2)]
    for i in range(steps):
        if f_grad:
            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)
        else:
            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)
        results.append((x1, x2))
    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')
    return results
</code></pre>
<pre><code class="language-{.python .input}">#@tab mxnet
def show_trace_2d(f, results):  #@save
    """최적화 중 2D 변수의 궤적을 보여줍니다."""
    d2l.set_figsize()
    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')
    x1, x2 = d2l.meshgrid(d2l.arange(-55, 1, 1),
                          d2l.arange(-30, 1, 1))
    x1, x2 = x1.asnumpy()*0.1, x2.asnumpy()*0.1
    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')
    d2l.plt.xlabel('x1')
    d2l.plt.ylabel('x2')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def show_trace_2d(f, results):  #@save
    """최적화 중 2D 변수의 궤적을 보여줍니다."""
    d2l.set_figsize()
    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')
    x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),
                          d2l.arange(-3.0, 1.0, 0.1))
    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')
    d2l.plt.xlabel('x1')
    d2l.plt.ylabel('x2')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def show_trace_2d(f, results):  #@save
    """최적화 중 2D 변수의 궤적을 보여줍니다."""
    d2l.set_figsize()
    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')
    x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),
                          d2l.arange(-3.0, 1.0, 0.1), indexing='ij')
    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')
    d2l.plt.xlabel('x1')
    d2l.plt.ylabel('x2')
</code></pre>
<p>다음으로 학습률 $\eta = 0.1$에 대한 최적화 변수 $\mathbf{x}$의 궤적을 관찰합니다. 20단계 후에 $\mathbf{x}$의 값이 $[0, 0]$의 최소값에 접근하는 것을 볼 수 있습니다. 진행은 다소 느리지만 상당히 잘 작동합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def f_2d(x1, x2):  # 목적 함수
    return x1 ** 2 + 2 * x2 ** 2

def f_2d_grad(x1, x2):  # 목적 함수의 기울기
    return (2 * x1, 4 * x2)

def gd_2d(x1, x2, s1, s2, f_grad):
    g1, g2 = f_grad(x1, x2)
    return (x1 - eta * g1, x2 - eta * g2, 0, 0)

eta = 0.1
show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))
</code></pre>
<h2 id="적응형-방법-adaptive-methods"><a class="header" href="#적응형-방법-adaptive-methods">적응형 방법 (Adaptive Methods)</a></h2>
<p>:numref:<code>subsec_gd-learningrate</code>에서 볼 수 있듯이 학습률 $\eta$를 "딱 맞게" 맞추는 것은 까다롭습니다. 너무 작게 잡으면 진전이 거의 없습니다. 너무 크게 잡으면 해가 진동하고 최악의 경우 발산할 수도 있습니다. $\eta$를 자동으로 결정하거나 학습률을 전혀 선택할 필요가 없게 할 수 있다면 어떨까요?
목적 함수의 값과 기울기뿐만 아니라 그 *곡률(curvature)*까지 살펴보는 2차 방법이 이 경우에 도움이 될 수 있습니다. 이러한 방법은 계산 비용 때문에 딥러닝에 직접 적용할 수는 없지만, 아래에 설명된 알고리즘의 바람직한 속성을 많이 모방하는 고급 최적화 알고리즘을 설계하는 방법에 대한 유용한 직관을 제공합니다.</p>
<h3 id="뉴턴-방법-newtons-method"><a class="header" href="#뉴턴-방법-newtons-method">뉴턴 방법 (Newton's Method)</a></h3>
<p>어떤 함수 $f: ℝ&gt;^d 	o ℝ$의 테일러 전개를 검토해 보면 첫 번째 항 이후에 멈출 필요가 없습니다. 사실 다음과 같이 쓸 수 있습니다.</p>
<p>$$f(\mathbf{x} + \boldsymbol{\epsilon}) = f(\mathbf{x}) + \boldsymbol{\epsilon}^\top \nabla f(\mathbf{x}) + \frac{1}{2} \boldsymbol{\epsilon}^\top \nabla^2 f(\mathbf{x}) \boldsymbol{\epsilon} + \mathcal{O}(|\boldsymbol{\epsilon}|^3).$$
:eqlabel:<code>gd-hot-taylor</code></p>
<p>번거로운 표기법을 피하기 위해 $\mathbf{H} \stackrel{\textrm{def}}{=} \nabla^2 f(\mathbf{x})$를 $f$의 헤시안(Hessian)으로 정의합니다. 이는 $d \times d$ 행렬입니다. 작은 $d$와 단순한 문제의 경우 $\mathbf{H}$는 계산하기 쉽습니다. 반면 심층 신경망의 경우 $\mathcal{O}(d^2)$ 항목을 저장하는 비용 때문에 $\mathbf{H}$가 엄청나게 클 수 있습니다. 더욱이 역전파를 통해 계산하기에 너무 비쌀 수 있습니다. 지금은 그러한 고려 사항을 무시하고 어떤 알고리즘을 얻게 될지 살펴봅시다.</p>
<p>결국 $f$의 최소값은 $\nabla f = 0$을 만족합니다.
:numref:<code>subsec_calculus-grad</code>의 미적분 규칙에 따라, $\boldsymbol{\epsilon}$에 대해 :eqref:<code>gd-hot-taylor</code>의 도함수를 취하고 고차 항을 무시하면 다음과 같이 도달합니다.</p>
<p>$$\nabla f(\mathbf{x}) + \mathbf{H} \boldsymbol{\epsilon} = 0 \textrm{ 이고 따라서 }
\boldsymbol{\epsilon} = -\mathbf{H}^{-1} \nabla f(\mathbf{x}).$$</p>
<p>즉, 최적화 문제의 일부로 헤시안 $\mathbf{H}$를 반전시켜야 합니다.</p>
<p>간단한 예로 $f(x) = \frac{1}{2} x^2$의 경우 $\nabla f(x) = x$ 및 $\mathbf{H} = 1$입니다. 따라서 모든 $x$에 대해 $\epsilon = -x$를 얻습니다. 즉, 조정할 필요 없이 <em>단 한 번의</em> 단계로 완벽하게 수렴하기에 충분합니다! 아쉽게도 여기서는 운이 좋았습니다. $f(x+\epsilon)= \frac{1}{2} x^2 + \epsilon x + \frac{1}{2} \epsilon^2$이므로 테일러 전개가 정확했기 때문입니다.</p>
<p>다른 문제에서는 어떻게 되는지 봅시다.
어떤 상수 $c$에 대해 볼록 쌍곡선 코사인 함수 $f(x) = \cosh(cx)$가 주어졌을 때, $x=0$에서의 전역 최소값이 몇 번의 반복 후에 도달하는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
c = d2l.tensor(0.5)

def f(x):  # 목적 함수
    return d2l.cosh(c * x)

def f_grad(x):  # 목적 함수의 기울기
    return c * d2l.sinh(c * x)

def f_hess(x):  # 목적 함수의 헤시안
    return c**2 * d2l.cosh(c * x)

def newton(eta=1):
    x = 10.0
    results = [x]
    for i in range(10):
        x -= eta * f_grad(x) / f_hess(x)
        results.append(float(x))
    print('epoch 10, x:', x)
    return results

show_trace(newton(), f)
</code></pre>
<p>이제 $f(x) = x \cos(c x)$와 같은 <em>비볼록</em> 함수를 고려해 봅시다. 결국 뉴턴 방법에서는 헤시안으로 나누게 됩니다. 이는 2계 도함수가 <em>음수</em>이면 $f$의 값이 <em>증가</em>하는 방향으로 걸어갈 수 있음을 의미합니다.
그것은 알고리즘의 치명적인 결함입니다.
실제로 어떤 일이 일어나는지 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
c = d2l.tensor(0.15 * np.pi)

def f(x):  # 목적 함수
    return x * d2l.cos(c * x)

def f_grad(x):  # 목적 함수의 기울기
    return d2l.cos(c * x) - c * x * d2l.sin(c * x)

def f_hess(x):  # 목적 함수의 헤시안
    return - 2 * c * d2l.sin(c * x) - x * c**2 * d2l.cos(c * x)

show_trace(newton(), f)
</code></pre>
<p>이것은 아주 잘못되었습니다. 어떻게 고칠 수 있을까요? 한 가지 방법은 헤시안의 절댓값을 취하여 헤시안을 "수정"하는 것입니다. 또 다른 전략은 학습률을 다시 도입하는 것입니다. 이것은 목적을 달성하지 못하는 것처럼 보일 수 있지만 꼭 그렇지는 않습니다. 2차 정보를 가지면 곡률이 클 때마다 주의하고 목적 함수가 더 평평할 때마다 더 긴 단계를 밟을 수 있습니다.
학습률을 약간 작게 하여(예: $\eta = 0.5$) 어떻게 작동하는지 봅시다. 보시다시피 상당히 효율적인 알고리즘을 갖게 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
show_trace(newton(0.5), f)
</code></pre>
<h3 id="수렴-분석-convergence-analysis"><a class="header" href="#수렴-분석-convergence-analysis">수렴 분석 (Convergence Analysis)</a></h3>
<p>우리는 2계 도함수가 0이 아닌($f''&gt;0$) 어떤 볼록하고 세 번 미분 가능한 목적 함수 $f$에 대해서만 뉴턴 방법의 수렴 속도를 분석합니다. 다변량 증명은 아래의 1차원 주장을 직접적으로 확장한 것이며 직관 측면에서 큰 도움이 되지 않으므로 생략합니다.</p>
<p>$k^\textrm{th}$ 반복에서의 $x$ 값을 $x^{(k)}$라고 하고, $k^\textrm{th}$ 반복에서의 최적성으로부터의 거리를 $e^{(k)} \stackrel{\textrm{def}}{=} x^{(k)} - x^<em>$라고 합시다. 테일러 전개에 의해 조건 $f'(x^</em>) = 0$은 다음과 같이 쓰일 수 있습니다.</p>
<p>$$0 = f'(x^{(k)} - e^{(k)}) = f'(x^{(k)}) - e^{(k)} f''(x^{(k)}) + \frac{1}{2} (e^{(k)})^2 f'''(\xi^{(k)}),$$</p>
<p>이는 어떤 $\xi^{(k)} \in [x^{(k)} - e^{(k)}, x^{(k)}]$에 대해 성립합니다. 위의 전개를 $f''(x^{(k)})$로 나누면 다음을 얻습니다.</p>
<p>$$e^{(k)} - \frac{f'(x^{(k)})}{f''(x^{(k)})} = \frac{1}{2} (e^{(k)})^2 \frac{f'''(\xi^{(k)})}{f''(x^{(k)})}.$$</p>
<p>우리는 업데이트 $x^{(k+1)} = x^{(k)} - f'(x^{(k)}) / f''(x^{(k)})$를 가짐을 상기하십시오.
이 업데이트 방정식을 대입하고 양변의 절댓값을 취하면 다음을 얻습니다.</p>
<p>$$\left|e^{(k+1)}\right| = \frac{1}{2}(e^{(k)})^2 \frac{\left|f'''(\xi^{(k)})\right|}{f''(x^{(k)})}.$$</p>
<p>결과적으로, 유계인 $\left|f'''(\xi^{(k)})\right| / (2f''(x^{(k)})) \leq c$인 영역에 있을 때마다 우리는 이차적으로 감소하는 오차를 갖습니다.</p>
<p>$$\left|e^{(k+1)}\right| \leq c (e^{(k)})^2.$$</p>
<p>참고로, 최적화 연구자들은 이를 <em>선형(linear)</em> 수렴이라고 부르는 반면, $\left|e^{(k+1)}\right| \leq \alpha \left|e^{(k)}\right|$와 같은 조건은 <em>상수(constant)</em> 수렴 속도라고 부릅니다.
이 분석에는 몇 가지 주의 사항이 있습니다.
첫째, 우리는 언제 빠른 수렴 영역에 도달할지 정말로 알 수 없습니다. 대신 그곳에 도달하면 수렴이 매우 빠를 것이라는 것만 압니다. 둘째, 이 분석은 $f$가 고차 도함수까지 잘 작동할 것을 요구합니다. 이는 $f$가 값이 어떻게 변할 수 있는지 측면에서 "놀라운" 속성을 갖지 않도록 보장하는 것으로 귀결됩니다.</p>
<h3 id="프리컨디셔닝-preconditioning"><a class="header" href="#프리컨디셔닝-preconditioning">프리컨디셔닝 (Preconditioning)</a></h3>
<p>전체 헤시안을 계산하고 저장하는 것이 매우 비싸다는 것은 그리 놀라운 일이 아닙니다. 따라서 대안을 찾는 것이 바람직합니다. 상황을 개선하는 한 가지 방법은 *프리컨디셔닝(preconditioning)*입니다. 헤시안 전체를 계산하는 것을 피하고 <em>대각</em> 항목만 계산합니다. 이는 다음과 같은 업데이트 알고리즘으로 이어집니다.</p>
<p>$$\mathbf{x} \leftarrow \mathbf{x} - \eta \textrm{diag}(\mathbf{H})^{-1} \nabla f(\mathbf{x}).$$</p>
<p>이것이 전체 뉴턴 방법만큼 좋지는 않지만, 사용하지 않는 것보다는 훨씬 낫습니다.
왜 이것이 좋은 아이디어일 수 있는지 이해하기 위해 한 변수는 밀리미터 단위의 높이를 나타내고 다른 변수는 킬로미터 단위의 높이를 나타내는 상황을 고려해 보십시오. 두 변수 모두 자연스러운 척도가 미터라고 가정하면 파라미터화에서 끔찍한 불일치가 발생합니다. 다행히 프리컨디셔닝을 사용하면 이를 제거할 수 있습니다. 경사 하강법과 함께 프리컨디셔닝을 효과적으로 수행하는 것은 각 변수(벡터 $\mathbf{x}$의 좌표)에 대해 다른 학습률을 선택하는 것과 같습니다.
나중에 보게 되겠지만, 프리컨디셔닝은 확률적 경사 하강법 최적화 알고리즘의 혁신 중 일부를 주도합니다.</p>
<h3 id="라인-검색을-사용한-경사-하강법-gradient-descent-with-line-search"><a class="header" href="#라인-검색을-사용한-경사-하강법-gradient-descent-with-line-search">라인 검색을 사용한 경사 하강법 (Gradient Descent with Line Search)</a></h3>
<p>경사 하강법의 주요 문제 중 하나는 목표를 지나치거나 진전이 불충분할 수 있다는 것입니다. 이 문제에 대한 간단한 수정은 경사 하강법과 함께 라인 검색(line search)을 사용하는 것입니다. 즉, $\nabla f(\mathbf{x})$에 의해 주어진 방향을 사용한 다음 어떤 학습률 $\eta$가 $f(\mathbf{x} - \eta \nabla f(\mathbf{x}))$를 최소화하는지에 대해 이진 검색을 수행합니다.</p>
<p>이 알고리즘은 빠르게 수렴합니다(분석 및 증명은 예: :citet:<code>Boyd.Vandenberghe.2004</code> 참조). 그러나 딥러닝의 목적을 위해 이것은 그다지 실용적이지 않습니다. 라인 검색의 각 단계마다 전체 데이터셋에서 목적 함수를 평가해야 하기 때문입니다. 이를 수행하기에는 비용이 너무 많이 듭니다.</p>
<h2 id="요약-summary-53"><a class="header" href="#요약-summary-53">요약 (Summary)</a></h2>
<ul>
<li>학습률은 중요합니다. 너무 크면 발산하고 너무 작으면 진전이 없습니다.</li>
<li>경사 하강법은 국소 최소값에 갇힐 수 있습니다.</li>
<li>고차원에서는 학습률을 조정하는 것이 복잡합니다.</li>
<li>프리컨디셔닝은 스케일 조정에 도움이 될 수 있습니다.</li>
<li>뉴턴 방법은 볼록 문제에서 제대로 작동하기 시작하면 훨씬 더 빠릅니다.</li>
<li>비볼록 문제에 대해 아무런 조정 없이 뉴턴 방법을 사용하는 것을 주의하십시오.</li>
</ul>
<h2 id="연습-문제-exercises-68"><a class="header" href="#연습-문제-exercises-68">연습 문제 (Exercises)</a></h2>
<ol>
<li>경사 하강법에 대해 다양한 학습률과 목적 함수로 실험해 보십시오.</li>
<li>구간 $[a, b]$에서 볼록 함수를 최소화하기 위해 라인 검색을 구현하십시오.
<ol>
<li>이진 검색을 위해, 즉 $[a, (a+b)/2]$를 선택할지 $[(a+b)/2, b]$를 선택할지 결정하기 위해 도함수가 필요합니까?</li>
<li>알고리즘의 수렴 속도는 얼마나 빠릅니까?</li>
<li>알고리즘을 구현하고 $\log (\exp(x) + \exp(-2x -3))$를 최소화하는 데 적용하십시오.</li>
</ol>
</li>
<li>경사 하강법이 매우 느린 $\mathbb{R}^2$에 정의된 목적 함수를 설계하십시오. 힌트: 다른 좌표의 스케일을 다르게 조정하십시오.</li>
<li>프리컨디셔닝을 사용하여 뉴턴 방법의 경량 버전을 구현하십시오.
<ol>
<li>대각 헤시안을 프리컨디셔너로 사용하십시오.</li>
<li>실제(잠재적으로 부호가 있는) 값보다는 그것의 절댓값을 사용하십시오.</li>
<li>위의 문제에 이를 적용하십시오.</li>
</ol>
</li>
<li>위의 알고리즘을 여러 목적 함수(볼록하거나 그렇지 않거나)에 적용하십시오. 좌표를 $45$도 회전하면 어떻게 됩니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/351">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="확률적-경사-하강법-stochastic-gradient-descent"><a class="header" href="#확률적-경사-하강법-stochastic-gradient-descent">확률적 경사 하강법 (Stochastic Gradient Descent)</a></h1>
<p>:label:<code>sec_sgd</code></p>
<p>이전 장에서 우리는 훈련 절차에서 확률적 경사 하강법을 계속 사용했지만, 이것이 왜 작동하는지 설명하지 않았습니다.
이에 대해 설명하기 위해
우리는 :numref:<code>sec_gd</code>에서 경사 하강법의 기본 원리를 설명했습니다.
이 섹션에서는 계속해서
*확률적 경사 하강법(stochastic gradient descent)*에 대해 더 자세히 논의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
</code></pre>
<h2 id="확률적-경사-업데이트-stochastic-gradient-updates"><a class="header" href="#확률적-경사-업데이트-stochastic-gradient-updates">확률적 경사 업데이트 (Stochastic Gradient Updates)</a></h2>
<p>딥러닝에서 목적 함수는 일반적으로 훈련 데이터셋의 각 예제에 대한 손실 함수의 평균입니다.
$n$개의 예제로 구성된 훈련 데이터셋이 주어졌을 때,
우리는 $f_i(\mathbf{x})$를 인덱스 $i$의 훈련 예제에 대한 손실 함수라고 가정합니다.
여기서 $\mathbf{x}$는 파라미터 벡터입니다.
그러면 우리는 다음과 같은 목적 함수에 도달합니다.</p>
<p>$$f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n f_i(\mathbf{x}).$$</p>
<p>$\mathbf{x}$에서의 목적 함수의 기울기는 다음과 같이 계산됩니다.</p>
<p>$$\nabla f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x}).$$</p>
<p>경사 하강법을 사용하는 경우, 각 독립 변수 반복에 대한 계산 비용은 $\mathcal{O}(n)$이며, 이는 $n$에 따라 선형적으로 증가합니다. 따라서 훈련 데이터셋이 클수록 반복당 경사 하강법 비용이 더 높아집니다.</p>
<p>확률적 경사 하강법(SGD)은 각 반복에서의 계산 비용을 줄입니다. 확률적 경사 하강법의 각 반복에서 우리는 데이터 예제에 대해 인덱스 $i\in{1,\ldots, n}$를 무작위로 균일하게 샘플링하고 $\mathbf{x}$를 업데이트하기 위해 기울기 $\nabla f_i(\mathbf{x})$를 계산합니다.</p>
<p>$$\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f_i(\mathbf{x}),$$</p>
<p>여기서 $\eta$는 학습률입니다. 각 반복에 대한 계산 비용이 경사 하강법의 $\mathcal{O}(n)$에서 상수 $\mathcal{O}(1)$로 떨어지는 것을 볼 수 있습니다. 또한 확률적 기울기 $\nabla f_i(\mathbf{x})$는 전체 기울기 $\nabla f(\mathbf{x})$의 편향되지 않은 추정치(unbiased estimate)임을 강조하고 싶습니다.</p>
<p>$$\mathbb{E}<em>i \nabla f_i(\mathbf{x}) = \frac{1}{n} \sum</em>{i = 1}^n \nabla f_i(\mathbf{x}) = \nabla f(\mathbf{x}).$$</p>
<p>이는 평균적으로 확률적 기울기가 기울기의 좋은 추정치임을 의미합니다.</p>
<p>이제 확률적 경사 하강법을 시뮬레이션하기 위해 기울기에 평균이 0이고 분산이 1인 무작위 노이즈를 추가하여 경사 하강법과 비교할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def f(x1, x2):  # 목적 함수
    return x1 ** 2 + 2 * x2 ** 2

def f_grad(x1, x2):  # 목적 함수의 기울기
    return 2 * x1, 4 * x2
</code></pre>
<pre><code class="language-{.python .input}">#@tab mxnet
def sgd(x1, x2, s1, s2, f_grad):
    g1, g2 = f_grad(x1, x2)
    # 노이즈가 있는 기울기 시뮬레이션
    g1 += d2l.normal(0.0, 1, (1,))
    g2 += d2l.normal(0.0, 1, (1,))
    eta_t = eta * lr()
    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def sgd(x1, x2, s1, s2, f_grad):
    g1, g2 = f_grad(x1, x2)
    # 노이즈가 있는 기울기 시뮬레이션
    g1 += torch.normal(0.0, 1, (1,)).item()
    g2 += torch.normal(0.0, 1, (1,)).item()
    eta_t = eta * lr()
    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def sgd(x1, x2, s1, s2, f_grad):
    g1, g2 = f_grad(x1, x2)
    # 노이즈가 있는 기울기 시뮬레이션
    g1 += d2l.normal([1], 0.0, 1)
    g2 += d2l.normal([1], 0.0, 1)
    eta_t = eta * lr()
    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
def constant_lr():
    return 1

eta = 0.1
lr = constant_lr  # 일정한 학습률
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))
</code></pre>
<p>보시다시피 확률적 경사 하강법에서 변수의 궤적은 :numref:<code>sec_gd</code>의 경사 하강법에서 관찰한 것보다 훨씬 더 시끄럽습니다. 이는 기울기의 확률적 특성 때문입니다. 즉, 최소값 근처에 도착하더라도 $\eta \nabla f_i(\mathbf{x})$를 통해 주입된 순간적인 기울기의 불확실성에 여전히 영향을 받습니다. 50단계 후에도 품질은 여전히 좋지 않습니다. 설상가상으로 추가 단계 후에도 개선되지 않습니다(이를 확인하기 위해 더 많은 단계로 실험해 보시기 바랍니다). 이것은 우리에게 유일한 대안을 남깁니다: 학습률 $\eta$를 변경하는 것입니다. 그러나 이것을 너무 작게 선택하면 초기에 의미 있는 진전을 이루지 못할 것입니다. 반면에 너무 크게 선택하면 위에서 본 것처럼 좋은 솔루션을 얻지 못할 것입니다. 이러한 상충되는 목표를 해결하는 유일한 방법은 최적화가 진행됨에 따라 학습률을 <em>동적으로</em> 줄이는 것입니다.</p>
<p>이것이 <code>sgd</code> 단계 함수에 학습률 함수 <code>lr</code>을 추가하는 이유이기도 합니다. 위의 예에서 <code>lr</code> 함수를 상수로 설정했기 때문에 학습률 스케줄링을 위한 기능은 휴면 상태입니다.</p>
<h2 id="동적-학습률-dynamic-learning-rate"><a class="header" href="#동적-학습률-dynamic-learning-rate">동적 학습률 (Dynamic Learning Rate)</a></h2>
<p>$\eta$를 시간 의존적 학습률 $\eta(t)$로 대체하면 최적화 알고리즘의 수렴 제어 복잡성이 증가합니다. 특히 $\eta$가 얼마나 빨리 감소해야 하는지 파악해야 합니다. 너무 빠르면 조기에 최적화를 중단하게 됩니다. 너무 천천히 줄이면 최적화에 너무 많은 시간을 낭비하게 됩니다. 다음은 시간에 따라 $\eta$를 조정하는 데 사용되는 몇 가지 기본 전략입니다(나중에 고급 전략에 대해 논의할 것입니다).</p>
<p>$$
\begin{aligned}
\eta(t) &amp; = \eta_i \textrm{ if } t_i \leq t \leq t_{i+1}  &amp;&amp; \textrm{piecewise constant} \
\eta(t) &amp; = \eta_0 \cdot e^{-\lambda t} &amp;&amp; \textrm{exponential decay} \
\eta(t) &amp; = \eta_0 \cdot (\beta t + 1)^{-\alpha} &amp;&amp; \textrm{polynomial decay}
\end{aligned}
$$</p>
<p>첫 번째 <em>구간별 상수(piecewise constant)</em> 시나리오에서는 예를 들어 최적화 진전이 멈출 때마다 학습률을 줄입니다. 이는 딥 네트워크를 훈련하기 위한 일반적인 전략입니다. 대안으로 *지수 감쇠(exponential decay)*를 통해 훨씬 더 공격적으로 줄일 수 있습니다. 불행히도 이는 종종 알고리즘이 수렴하기 전에 조기 중지로 이어집니다. 인기 있는 선택은 $\alpha = 0.5$인 *다항식 감쇠(polynomial decay)*입니다. 볼록 최적화의 경우 이 속도가 잘 작동한다는 것을 보여주는 여러 증명이 있습니다.</p>
<p>지수 감쇠가 실제로 어떻게 보이는지 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
def exponential_lr():
    # 이 함수 외부에서 정의되고 내부에서 업데이트되는 전역 변수
    global t
    t += 1
    return math.exp(-0.1 * t)

t = 1
lr = exponential_lr
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))
</code></pre>
<p>예상대로 파라미터의 분산이 상당히 줄어듭니다. 그러나 이는 최적의 해 $\mathbf{x} = (0, 0)$으로 수렴하지 못하는 비용을 치르게 됩니다. 1000번의 반복 단계 후에도 우리는 여전히 최적의 해에서 매우 멀리 떨어져 있습니다. 실제로 알고리즘은 전혀 수렴하지 못합니다. 반면 학습률이 단계 수의 역제곱근으로 감소하는 다항식 감쇠를 사용하면 50단계 만에 수렴이 더 좋아집니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def polynomial_lr():
    # 이 함수 외부에서 정의되고 내부에서 업데이트되는 전역 변수
    global t
    t += 1
    return (1 + 0.1 * t) ** (-0.5)

t = 1
lr = polynomial_lr
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))
</code></pre>
<p>학습률을 설정하는 방법에 대한 더 많은 선택지가 존재합니다. 예를 들어, 작은 속도로 시작한 다음 급격히 높이고 다시 천천히 줄일 수 있습니다. 더 작은 학습률과 더 큰 학습률을 번갈아 사용할 수도 있습니다. 그러한 스케줄은 매우 다양합니다. 지금은 포괄적인 이론적 분석이 가능한 학습률 스케줄, 즉 볼록 설정에서의 학습률에 초점을 맞춰 보겠습니다. 일반적인 비볼록 문제의 경우 의미 있는 수렴 보장을 얻기가 매우 어렵습니다. 일반적으로 비선형 비볼록 문제를 최소화하는 것은 NP hard이기 때문입니다. 설문 조사는 예: Tibshirani 2015의 훌륭한 <a href="https://www.stat.cmu.edu/%7Eryantibs/convexopt-F15/lectures/26-nonconvex.pdf">강의 노트</a>를 참조하십시오.</p>
<h2 id="볼록-목적-함수에-대한-수렴-분석-convergence-analysis-for-convex-objectives"><a class="header" href="#볼록-목적-함수에-대한-수렴-분석-convergence-analysis-for-convex-objectives">볼록 목적 함수에 대한 수렴 분석 (Convergence Analysis for Convex Objectives)</a></h2>
<p>볼록 목적 함수에 대한 확률적 경사 하강법의 다음 수렴 분석은 선택 사항이며 주로 문제에 대한 더 많은 직관을 전달하는 역할을 합니다.
우리는 가장 간단한 증명 중 하나로 제한합니다 :cite:<code>Nesterov.Vial.2000</code>.
예를 들어 목적 함수가 특히 잘 작동할 때 훨씬 더 발전된 증명 기술이 존재합니다.</p>
<p>목적 함수 $f(\boldsymbol{\xi}, \mathbf{x})$가 모든 $\boldsymbol{\xi}$에 대해 $\mathbf{x}$에서 볼록하다고 가정합니다.
더 구체적으로,
우리는 확률적 경사 하강법 업데이트를 고려합니다:</p>
<p>$$\mathbf{x}<em>{t+1} = \mathbf{x}</em>{t} - \eta_t \partial_\mathbf{x} f(\boldsymbol{\xi}_t, \mathbf{x}),$$</p>
<p>여기서 $f(\boldsymbol{\xi}_t, \mathbf{x})$는
단계 $t$에서 어떤 분포로부터 추출된 훈련 예제 $\boldsymbol{\xi}_t$에 대한 목적 함수이고
$\mathbf{x}$는 모델 파라미터입니다.
다음을</p>
<p>$$R(\mathbf{x}) = E_{\boldsymbol{\xi}}[f(\boldsymbol{\xi}, \mathbf{x})]$$</p>
<p>기대 위험(expected risk)으로, $R^<em>$를 $\mathbf{x}$에 대한 최소값으로 표시합니다. 마지막으로 $\mathbf{x}^</em>$를 최소화자(minimizer)라고 합시다($\mathbf{x}$가 정의된 도메인 내에 존재한다고 가정). 이 경우 시간 $t$에서의 현재 파라미터 $\mathbf{x}_t$와 위험 최소화자 $\mathbf{x}^*$ 사이의 거리를 추적하고 시간이 지남에 따라 개선되는지 확인할 수 있습니다.</p>
<p>$$\begin{aligned}    &amp;|\mathbf{x}<em>{t+1} - \mathbf{x}^*|^2 \ =&amp; |\mathbf{x}</em>{t} - \eta_t \partial_\mathbf{x} f(\boldsymbol{\xi}<em>t, \mathbf{x}) - \mathbf{x}^*|^2 \    =&amp; |\mathbf{x}</em>{t} - \mathbf{x}^<em>|^2 + \eta_t^2 |\partial_\mathbf{x} f(\boldsymbol{\xi}_t, \mathbf{x})|^2 - 2 \eta_t    \left\langle \mathbf{x}_t - \mathbf{x}^</em>, \partial_\mathbf{x} f(\boldsymbol{\xi}_t, \mathbf{x})\right\rangle.   \end{aligned}$$
:eqlabel:<code>eq_sgd-xt+1-xstar</code></p>
<p>우리는 확률적 기울기 $\partial_\mathbf{x} f(\boldsymbol{\xi}_t, \mathbf{x})$의 $\ell_2$ 노름이 어떤 상수 $L$에 의해 제한된다고 가정하므로 다음을 갖습니다.</p>
<p>$$\eta_t^2 |\partial_\mathbf{x} f(\boldsymbol{\xi}_t, \mathbf{x})|^2 \leq \eta_t^2 L^2.$$
:eqlabel:<code>eq_sgd-L</code></p>
<p>우리는 $\mathbf{x}_t$와 $\mathbf{x}^*$ 사이의 거리가 <em>기대치에서</em> 어떻게 변하는지에 가장 관심이 있습니다. 사실 어떤 특정 단계 시퀀스에 대해서는 우리가 마주치는 $\boldsymbol{\xi}_t$에 따라 거리가 증가할 수도 있습니다. 따라서 내적을 제한해야 합니다.
어떤 볼록 함수 $f$에 대해서도 모든 $\mathbf{x}$와 $\mathbf{y}$에 대해
$f(\mathbf{y}) \geq f(\mathbf{x}) + \langle f'(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle$이 성립하므로,
볼록성에 의해 우리는 다음을 갖습니다.</p>
<p>$$f(\boldsymbol{\xi}_t, \mathbf{x}^<em>) \geq f(\boldsymbol{\xi}_t, \mathbf{x}_t) + \left\langle \mathbf{x}^</em> - \mathbf{x}<em>t, \partial</em>{\mathbf{x}} f(\boldsymbol{\xi}_t, \mathbf{x}_t) \right\rangle.$$
:eqlabel:<code>eq_sgd-f-xi-xstar</code></p>
<p>부등식 :eqref:<code>eq_sgd-L</code>과 :eqref:<code>eq_sgd-f-xi-xstar</code>를 :eqref:<code>eq_sgd-xt+1-xstar</code>에 대입하면 다음과 같이 시간 $t+1$에서의 파라미터 간 거리에 대한 경계를 얻습니다.</p>
<p>$$|\mathbf{x}<em>{t} - \mathbf{x}^*|^2 - |\mathbf{x}</em>{t+1} - \mathbf{x}^<em>|^2 \geq 2 \eta_t (f(\boldsymbol{\xi}_t, \mathbf{x}_t) - f(\boldsymbol{\xi}_t, \mathbf{x}^</em>)) - \eta_t^2 L^2.$$
:eqlabel:<code>eqref_sgd-xt-diff</code></p>
<p>이것은 현재 손실과 최적 손실 간의 차이가 $\eta_t L^2/2$보다 큰 한 진전을 이룬다는 것을 의미합니다. 이 차이는 0으로 수렴해야 하므로 학습률 $\eta_t$도 <em>사라져야</em> 합니다.</p>
<p>다음으로 :eqref:<code>eqref_sgd-xt-diff</code>에 대해 기대값을 취합니다. 이것은 다음을 산출합니다.</p>
<p>$$E\left[|\mathbf{x}<em>{t} - \mathbf{x}^*|^2\right] - E\left[|\mathbf{x}</em>{t+1} - \mathbf{x}^<em>|^2\right] \geq 2 \eta_t [E[R(\mathbf{x}_t)] - R^</em>] -  \eta_t^2 L^2.$$</p>
<p>마지막 단계는 $t \in {1, \ldots, T}$에 대한 부등식을 합산하는 것을 포함합니다. 합이 텔레스코핑(telescoping)되고 낮은 항을 삭제함으로써 다음을 얻습니다.</p>
<p>$$|\mathbf{x}<em>1 - \mathbf{x}^*|^2 \geq 2 \left (\sum</em>{t=1}^T   \eta_t \right) [E[R(\mathbf{x}<em>t)] - R^*] - L^2 \sum</em>{t=1}^T \eta_t^2.$$
:eqlabel:<code>eq_sgd-x1-xstar</code></p>
<p>우리는 $\mathbf{x}_1$이 주어졌으므로 기대값을 삭제할 수 있다는 점을 이용했습니다. 마지막으로 다음을 정의합니다.</p>
<p>$$\bar{\mathbf{x}} \stackrel{\textrm{def}}{=} \frac{\sum_{t=1}^T \eta_t \mathbf{x}<em>t}{\sum</em>{t=1}^T \eta_t}.$$</p>
<p>다음이 성립하므로</p>
<p>$$E\left(\frac{\sum_{t=1}^T \eta_t R(\mathbf{x}<em>t)}{\sum</em>{t=1}^T \eta_t}\right) = \frac{\sum_{t=1}^T \eta_t E[R(\mathbf{x}<em>t)]}{\sum</em>{t=1}^T \eta_t} = E[R(\mathbf{x}_t)],$$</p>
<p>젠센 부등식(:eqref:<code>eq_jensens-inequality</code>에서 $i=t$, $\alpha_i = \eta_t/\sum_{t=1}^T \eta_t$로 설정)과 $R$의 볼록성에 의해 $E[R(\mathbf{x}_t)] \geq E[R(\bar{\mathbf{x}})]$가 따르며, 따라서</p>
<p>$$\sum_{t=1}^T \eta_t E[R(\mathbf{x}<em>t)] \geq \sum</em>{t=1}^T \eta_t  E\left[R(\bar{\mathbf{x}})\right].$$</p>
<p>이를 부등식 :eqref:<code>eq_sgd-x1-xstar</code>에 대입하면 경계를 얻습니다.</p>
<p>$$
\left[E[\bar{\mathbf{x}}]\right] - R^* \leq \frac{r^2 + L^2 \sum_{t=1}^T \eta_t^2}{2 \sum_{t=1}^T \eta_t},
$$</p>
<p>여기서 $r^2 \stackrel{\textrm{def}}{=} |\mathbf{x}_1 - \mathbf{x}^*|^2$는 초기 파라미터 선택과 최종 결과 간의 거리에 대한 경계입니다. 요컨대, 수렴 속도는
확률적 기울기의 노름이 어떻게 제한되는지($L$)와 초기 파라미터 값이 최적성에서 얼마나 멀리 떨어져 있는지($r$)에 따라 달라집니다. 경계는 $\mathbf{x}_T$가 아니라 $\bar{\mathbf{x}}$에 대한 것임에 유의하십시오. $\bar{\mathbf{x}}$는 최적화 경로의 평활화된 버전이기 때문입니다.
$r, L, T$를 알고 있다면 학습률 $\eta = r/(L \sqrt{T})$를 선택할 수 있습니다. 이것은 상한 $rL/\sqrt{T}$를 산출합니다. 즉, 우리는 $\mathcal{O}(1/\sqrt{T})$의 속도로 최적의 해에 수렴합니다.</p>
<h2 id="확률적-기울기와-유한-샘플-stochastic-gradients-and-finite-samples"><a class="header" href="#확률적-기울기와-유한-샘플-stochastic-gradients-and-finite-samples">확률적 기울기와 유한 샘플 (Stochastic Gradients and Finite Samples)</a></h2>
<p>지금까지 우리는 확률적 경사 하강법에 대해 이야기할 때 약간 느슨하게 다루었습니다. 우리는 어떤 분포 $p(x, y)$에서 인스턴스 $x_i$, 일반적으로 레이블 $y_i$를 추출하고 이를 사용하여 모델 파라미터를 어떤 방식으로 업데이트한다고 가정했습니다. 특히 유한 샘플 크기에 대해 우리는 단순히 이산 분포 $p(x, y) = \frac{1}{n} \sum_{i=1}^n \delta_{x_i}(x) \delta_{y_i}(y)$
(어떤 함수 $\delta_{x_i}$ 및 $\delta_{y_i}$에 대해)가
그 위에서 확률적 경사 하강법을 수행할 수 있게 한다고 주장했습니다.</p>
<p>그러나 이것은 우리가 실제로 한 일이 아닙니다. 현재 섹션의 장난감 예제에서는 단순히 확률적이지 않은 기울기에 노이즈를 추가했습니다. 즉, 쌍 $(x_i, y_i)$가 있는 척했습니다. 여기서 이것이 정당하다는 것이 밝혀졌습니다(자세한 논의는 연습 문제 참조). 더 골치 아픈 것은 이전의 모든 논의에서 우리가 분명히 이렇게 하지 않았다는 것입니다. 대신 우리는 모든 인스턴스를 <em>정확히 한 번</em> 반복했습니다. 이것이 왜 바람직한지 보려면 반대의 경우, 즉 이산 분포에서 *복원 추출(replacement)*로 $n$개의 관찰을 샘플링한다고 생각해 보십시오. 무작위로 요소 $i$를 선택할 확률은 $1/n$입니다. 따라서 그것을 <em>적어도</em> 한 번 선택할 확률은 다음과 같습니다.</p>
<p>$$P(\textrm{choose~} i) = 1 - P(\textrm{omit~} i) = 1 - (1-1/n)^n \approx 1-e^{-1} \approx 0.63.$$</p>
<p>유사한 추론은 어떤 샘플(즉, 훈련 예제)을 <em>정확히 한 번</em> 선택할 확률이 다음과 같이 주어짐을 보여줍니다.</p>
<p>$${n \choose 1} \frac{1}{n} \left(1-\frac{1}{n}\right)^{n-1} = \frac{n}{n-1} \left(1-\frac{1}{n}\right)^{n} \approx e^{-1} \approx 0.37.$$</p>
<p>복원 추출로 샘플링하면 *비복원 추출(without replacement)*로 샘플링하는 것에 비해 분산이 증가하고 데이터 효율성이 감소합니다. 따라서 실제로는 후자를 수행합니다(그리고 이것이 이 책 전체의 기본 선택입니다). 마지막으로 훈련 데이터셋을 통한 반복적인 패스는 <em>다른</em> 무작위 순서로 순회한다는 점에 유의하십시오.</p>
<h2 id="요약-summary-54"><a class="header" href="#요약-summary-54">요약 (Summary)</a></h2>
<ul>
<li>볼록 문제의 경우 광범위한 학습률 선택에 대해 확률적 경사 하강법이 최적의 해로 수렴함을 증명할 수 있습니다.</li>
<li>딥러닝의 경우 일반적으로 그렇지 않습니다. 그러나 볼록 문제 분석은 최적화에 접근하는 방법, 즉 학습률을 점진적으로 줄이되 너무 빨리 줄이지 않는 방법에 대한 유용한 통찰력을 제공합니다.</li>
<li>학습률이 너무 작거나 너무 클 때 문제가 발생합니다. 실제로 적절한 학습률은 종종 여러 번의 실험 후에야 발견됩니다.</li>
<li>훈련 데이터셋에 더 많은 예제가 있는 경우 경사 하강법의 각 반복을 계산하는 데 더 많은 비용이 들기 때문에 이 경우 확률적 경사 하강법이 선호됩니다.</li>
<li>확률적 경사 하강법에 대한 최적성 보장은 비볼록 사례에서 일반적으로 사용할 수 없습니다. 확인해야 할 국소 최소값의 수가 기하급수적일 수 있기 때문입니다.</li>
</ul>
<h2 id="연습-문제-exercises-69"><a class="header" href="#연습-문제-exercises-69">연습 문제 (Exercises)</a></h2>
<ol>
<li>확률적 경사 하강법에 대해 다양한 학습률 스케줄과 다양한 반복 횟수로 실험해 보십시오. 특히 반복 횟수의 함수로 최적의 해 $(0, 0)$으로부터의 거리를 플롯하십시오.</li>
<li>함수 $f(x_1, x_2) = x_1^2 + 2 x_2^2$에 대해 기울기에 정규 노이즈를 추가하는 것은 $\mathbf{x}$가 정규 분포에서 추출되는 손실 함수 $f(\mathbf{x}, \mathbf{w}) = (x_1 - w_1)^2 + 2 (x_2 - w_2)^2$를 최소화하는 것과 동등함을 증명하십시오.</li>
<li>${(x_1, y_1), \ldots, (x_n, y_n)}$에서 복원 추출로 샘플링할 때와 비복원 추출로 샘플링할 때 확률적 경사 하강법의 수렴을 비교하십시오.</li>
<li>어떤 기울기(또는 그와 관련된 어떤 좌표)가 다른 모든 기울기보다 일관되게 크다면 확률적 경사 하강법 솔버를 어떻게 변경하겠습니까?</li>
<li>$f(x) = x^2 (1 + \sin x)$라고 가정합니다. $f$는 몇 개의 국소 최소값을 가집니까? $f$를 최소화하기 위해 모든 국소 최소값을 평가해야 하도록 $f$를 변경할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/352">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/497">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1067">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="미니배치-확률적-경사-하강법-minibatch-stochastic-gradient-descent-1"><a class="header" href="#미니배치-확률적-경사-하강법-minibatch-stochastic-gradient-descent-1">미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></h1>
<p>:label:<code>sec_minibatch_sgd</code></p>
<p>지금까지 우리는 경사 기반 학습에 대한 접근 방식에서 두 가지 극단을 만났습니다. :numref:<code>sec_gd</code>는 전체 데이터셋을 사용하여 기울기를 계산하고 파라미터를 한 번에 한 번씩 업데이트합니다. 반대로 :numref:<code>sec_sgd</code>는 한 번에 하나의 훈련 예제를 처리하여 진전을 이룹니다.
이들 중 어느 것도 단점이 있습니다.
경사 하강법은 데이터가 매우 유사할 때 특히 <em>데이터 효율적</em>이지 않습니다.
확률적 경사 하강법은 CPU와 GPU가 벡터화의 전체 능력을 활용할 수 없기 때문에 특히 <em>계산 효율적</em>이지 않습니다.
이는 그 사이에 무언가가 있을 수 있음을 시사하며, 사실 그것이 우리가 지금까지 논의한 예제에서 사용해 온 것입니다.</p>
<h2 id="벡터화와-캐시-vectorization-and-caches"><a class="header" href="#벡터화와-캐시-vectorization-and-caches">벡터화와 캐시 (Vectorization and Caches)</a></h2>
<p>미니배치를 사용하기로 결정하는 핵심에는 계산 효율성이 있습니다. 이는 다중 GPU 및 다중 서버로의 병렬화를 고려할 때 가장 쉽게 이해됩니다. 이 경우 각 GPU에 적어도 하나의 이미지를 보내야 합니다. 서버당 8개의 GPU와 16개의 서버가 있다면 이미 미니배치 크기가 128보다 작지 않아야 합니다.</p>
<p>단일 GPU 또는 CPU의 경우 상황은 조금 더 미묘합니다. 이러한 장치에는 여러 유형의 메모리, 종종 여러 유형의 계산 장치 및 이들 간의 다양한 대역폭 제약 조건이 있습니다.
예를 들어 CPU에는 소수의 레지스터가 있고 그다음 L1, L2, 그리고 어떤 경우에는 L3 캐시(다른 프로세서 코어 간에 공유됨)가 있습니다.
이러한 캐시는 크기와 대기 시간이 증가합니다(동시에 대역폭은 감소합니다).
프로세서가 메인 메모리 인터페이스가 제공할 수 있는 것보다 훨씬 더 많은 연산을 수행할 수 있다고 말하는 것으로 충분합니다.</p>
<p>첫째, 16 코어와 AVX-512 벡터화가 있는 2GHz CPU는 초당 최대 $2 \cdot 10^9 \cdot 16 \cdot 32 = 10^{12}$ 바이트를 처리할 수 있습니다. GPU의 기능은 이 수치를 100배 쉽게 초과합니다. 반면, 중급 서버 프로세서는 100GB/s 대역폭을 넘지 못할 수 있습니다. 즉, 프로세서를 계속 공급하는 데 필요한 것의 1/10 미만입니다. 설상가상으로 모든 메모리 액세스가 동일하게 생성되는 것은 아닙니다. 메모리 인터페이스는 일반적으로 64비트 너비 이상(예: GPU에서는 최대 384비트)이므로 단일 바이트를 읽으면 훨씬 더 넓은 액세스 비용이 발생합니다.</p>
<p>둘째, 첫 번째 액세스에는 상당한 오버헤드가 발생하는 반면 순차적 액세스는 비교적 저렴합니다(이것을 종종 버스트 읽기라고 함). 여러 소켓, 칩렛 및 기타 구조가 있을 때 캐싱과 같이 염두에 두어야 할 다른 많은 것들이 있습니다.
더 깊이 있는 논의는 이 <a href="https://en.wikipedia.org/wiki/Cache_hierarchy">위키백과 문서</a>를 참조하십시오.</p>
<p>이러한 제약을 완화하는 방법은 프로세서에 데이터를 공급할 만큼 충분히 빠른 CPU 캐시 계층 구조를 사용하는 것입니다. 이것이 딥러닝에서 배치 처리의 <em>원동력</em>입니다. 문제를 단순하게 유지하기 위해 행렬-행렬 곱셈, 예를 들어 $\mathbf{A} = \mathbf{B}\mathbf{C}$를 고려해 봅시다. $\mathbf{A}$를 계산하기 위한 여러 가지 옵션이 있습니다. 예를 들어 다음을 시도할 수 있습니다.</p>
<ol>
<li>$\mathbf{A}<em>{ij} = \mathbf{B}</em>{i,:} \mathbf{C}_{:,j}$를 계산할 수 있습니다. 즉, 내적을 통해 요소별로 계산할 수 있습니다.</li>
<li>$\mathbf{A}<em>{:,j} = \mathbf{B} \mathbf{C}</em>{:,j}$를 계산할 수 있습니다. 즉, 한 번에 한 열씩 계산할 수 있습니다. 마찬가지로 $\mathbf{A}$를 한 번에 한 행 $\mathbf{A}_{i,:}$씩 계산할 수도 있습니다.</li>
<li>단순히 $\mathbf{A} = \mathbf{B} \mathbf{C}$를 계산할 수 있습니다.</li>
<li>$\mathbf{B}$와 $\mathbf{C}$를 더 작은 블록 행렬로 나누고 한 번에 한 블록씩 $\mathbf{A}$를 계산할 수 있습니다.</li>
</ol>
<p>첫 번째 옵션을 따르면 $\mathbf{A}<em>{ij}$ 요소를 계산하고 싶을 때마다 행 벡터 하나와 열 벡터 하나를 CPU로 복사해야 합니다. 더 나쁜 것은 행렬 요소가 순차적으로 정렬되어 있다는 사실 때문에 메모리에서 읽을 때 두 벡터 중 하나에 대해 많은 분리된 위치에 액세스해야 한다는 것입니다. 두 번째 옵션이 훨씬 더 유리합니다. 그 안에서 $\mathbf{B}$를 계속 순회하는 동안 열 벡터 $\mathbf{C}</em>{:,j}$를 CPU 캐시에 유지할 수 있습니다. 이것은 메모리 대역폭 요구 사항을 절반으로 줄이고 그에 상응하여 더 빠른 액세스를 제공합니다. 물론 옵션 3이 가장 바람직합니다. 불행히도 대부분의 행렬은 캐시에 완전히 들어가지 않을 수 있습니다(이것이 우리가 논의하고 있는 것입니다). 그러나 옵션 4는 실질적으로 유용한 대안을 제공합니다. 행렬의 블록을 캐시로 이동하고 로컬에서 곱할 수 있습니다. 최적화된 라이브러리가 우리를 위해 이것을 처리합니다. 실제로 이러한 연산이 얼마나 효율적인지 살펴봅시다.</p>
<p>계산 효율성 외에도 Python과 딥러닝 프레임워크 자체에서 발생하는 오버헤드도 상당합니다. 명령을 실행할 때마다 Python 인터프리터가 MXNet 엔진에 명령을 보내고, 엔진은 이를 계산 그래프에 삽입하고 스케줄링 중에 처리해야 한다는 것을 기억하십시오. 이러한 오버헤드는 꽤 해로울 수 있습니다. 요컨대, 가능할 때마다 벡터화(및 행렬)를 사용하는 것이 좋습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
import time
npx.set_np()

A = np.zeros((256, 256))
B = np.random.normal(0, 1, (256, 256))
C = np.random.normal(0, 1, (256, 256))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import numpy as np
import time
import torch
from torch import nn

A = torch.zeros(256, 256)
B = torch.randn(256, 256)
C = torch.randn(256, 256)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import numpy as np
import tensorflow as tf
import time

A = tf.Variable(d2l.zeros((256, 256)))
B = tf.Variable(d2l.normal([256, 256], 0, 1))
C = tf.Variable(d2l.normal([256, 256], 0, 1))
</code></pre>
<p>책의 나머지 부분에서 실행 시간을 자주 벤치마킹할 것이므로 타이머를 정의해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
class Timer:  #@save
    """여러 실행 시간을 기록합니다."""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """타이머를 시작합니다."""
        self.tik = time.time()

    def stop(self):
        """타이머를 중지하고 시간을 목록에 기록합니다."""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """평균 시간을 반환합니다."""
        return sum(self.times) / len(self.times)

    def sum(self):
        """시간의 합을 반환합니다."""
        return sum(self.times)

    def cumsum(self):
        """누적 시간을 반환합니다."""
        return np.array(self.times).cumsum().tolist()

timer = Timer()
</code></pre>
<p>요소별 할당은 단순히 $\mathbf{B}$와 $\mathbf{C}$의 모든 행과 열을 반복하여 값을 $\mathbf{A}$에 할당합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 한 번에 한 요소씩 A = BC 계산
timer.start()
for i in range(256):
    for j in range(256):
        A[i, j] = np.dot(B[i, :], C[:, j])
A.wait_to_read()
timer.stop()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 한 번에 한 요소씩 A = BC 계산
timer.start()
for i in range(256):
    for j in range(256):
        A[i, j] = torch.dot(B[i, :], C[:, j])
timer.stop()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 한 번에 한 요소씩 A = BC 계산
timer.start()
for i in range(256):
    for j in range(256):
        A[i, j].assign(tf.tensordot(B[i, :], C[:, j], axes=1))
timer.stop()
</code></pre>
<p>더 빠른 전략은 열별 할당을 수행하는 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 한 번에 한 열씩 A = BC 계산
timer.start()
for j in range(256):
    A[:, j] = np.dot(B, C[:, j])
A.wait_to_read()
timer.stop()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 한 번에 한 열씩 A = BC 계산
timer.start()
for j in range(256):
    A[:, j] = torch.mv(B, C[:, j])
timer.stop()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
timer.start()
for j in range(256):
    A[:, j].assign(tf.tensordot(B, C[:, j], axes=1))
timer.stop()
</code></pre>
<p>마지막으로 가장 효과적인 방법은 전체 연산을 한 블록으로 수행하는 것입니다.
두 행렬 $\mathbf{B} \in \mathbb{R}^{m \times n}$ 및 $\mathbf{C} \in \mathbb{R}^{n \times p}$를 곱하는 데는 스칼라 곱셈과 덧셈을 별도의 연산으로 계산할 때(실제로는 융합됨) 약 $2mnp$ 부동 소수점 연산이 소요됩니다.
따라서 두 개의 $256 \times 256$ 행렬을 곱하는 데는 $0.03$ 십억 번의 부동 소수점 연산이 소요됩니다.
각각의 연산 속도가 어떤지 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 한 번에 A = BC 계산
timer.start()
A = np.dot(B, C)
A.wait_to_read()
timer.stop()

gigaflops = [0.03 / i for i in timer.times]
print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, ')
      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 한 번에 A = BC 계산
timer.start()
A = torch.mm(B, C)
timer.stop()

gigaflops = [0.03 / i for i in timer.times]
print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, ')
      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
timer.start()
A.assign(tf.tensordot(B, C, axes=1))
timer.stop()

gigaflops = [0.03 / i for i in timer.times]
print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, ')
      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')
</code></pre>
<h2 id="미니배치-minibatches"><a class="header" href="#미니배치-minibatches">미니배치 (Minibatches)</a></h2>
<p>:label:<code>sec_minibatches</code></p>
<p>과거에는 파라미터를 업데이트하기 위해 단일 관찰이 아닌 데이터의 <em>미니배치</em>를 읽는 것을 당연하게 여겼습니다. 이제 그에 대한 간단한 정당성을 제시합니다. 단일 관찰을 처리하려면 많은 단일 행렬-벡터(또는 벡터-벡터) 곱셈을 수행해야 하는데, 이는 꽤 비싸고 기본 딥러닝 프레임워크를 대신하여 상당한 오버헤드를 발생시킵니다. 이는 데이터에 적용할 때(종종 추론이라고 함) 네트워크를 평가할 때와 파라미터를 업데이트하기 위해 기울기를 계산할 때 모두 적용됩니다. 즉, $\mathbf{w} \leftarrow \mathbf{w} - \eta_t \mathbf{g}_t$를 수행할 때마다 적용됩니다. 여기서</p>
<p>$$\mathbf{g}<em>t = \partial</em>{\mathbf{w}} f(\mathbf{x}_{t}, \mathbf{w})$$</p>
<p>우리는 한 번에 관찰의 미니배치에 적용함으로써 이 연산의 <em>계산</em> 효율성을 높일 수 있습니다. 즉, 단일 관찰에 대한 기울기 $\mathbf{g}_t$를 작은 배치에 대한 것으로 대체합니다.</p>
<p>$$\mathbf{g}<em>t = \partial</em>{\mathbf{w}} \frac{1}{|\mathcal{B}<em>t|} \sum</em>{i \in \mathcal{B}<em>t} f(\mathbf{x}</em>{i}, \mathbf{w})$$</p>
<p>이것이 $\mathbf{g}_t$의 통계적 속성에 어떤 영향을 미치는지 봅시다: $\mathbf{x}_t$와 미니배치 $\mathcal{B}_t$의 모든 요소가 훈련 세트에서 무작위로 균일하게 추출되므로 기울기의 기대값은 변경되지 않습니다. 반면 분산은 상당히 감소합니다. 미니배치 기울기는 평균화되는 $b \stackrel{\textrm{def}}{=} |\mathcal{B}_t|$개의 독립적인 기울기로 구성되므로 표준 편차는 $b^{-\frac{1}{2}}$의 인수만큼 감소합니다. 이것 자체로는 좋은 일입니다. 업데이트가 전체 기울기와 더 안정적으로 정렬됨을 의미하기 때문입니다.</p>
<p>순진하게 이것은 큰 미니배치 $\mathcal{B}_t$를 선택하는 것이 보편적으로 바람직하다는 것을 나타냅니다. 아쉽게도 어느 시점 이후에는 표준 편차의 추가 감소가 계산 비용의 선형 증가에 비해 미미합니다. 실제로는 GPU 메모리에 맞으면서도 우수한 계산 효율성을 제공할 만큼 충분히 큰 미니배치를 선택합니다. 절감 효과를 설명하기 위해 코드를 살펴봅시다. 여기서는 동일한 행렬-행렬 곱셈을 수행하지만 이번에는 한 번에 64개의 열로 "미니배치"로 나눕니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
timer.start()
for j in range(0, 256, 64):
    A[:, j:j+64] = np.dot(B, C[:, j:j+64])
timer.stop()
print(f'performance in Gigaflops: block {0.03 / timer.times[3]:.3f}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
timer.start()
for j in range(0, 256, 64):
    A[:, j:j+64] = torch.mm(B, C[:, j:j+64])
timer.stop()
print(f'performance in Gigaflops: block {0.03 / timer.times[3]:.3f}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
timer.start()
for j in range(0, 256, 64):
    A[:, j:j+64].assign(tf.tensordot(B, C[:, j:j+64], axes=1))
timer.stop()
print(f'performance in Gigaflops: block {0.03 / timer.times[3]:.3f}')
</code></pre>
<p>보시다시피, 미니배치에서의 계산은 본질적으로 전체 행렬에서의 계산만큼 효율적입니다. 주의할 점이 있습니다. :numref:<code>sec_batch_norm</code>에서 우리는 미니배치의 분산 양에 크게 의존하는 유형의 정규화를 사용했습니다. 후자를 늘리면 분산이 감소하고 그에 따라 배치 정규화로 인한 노이즈 주입의 이점도 감소합니다. 적절한 항을 다시 스케일링하고 계산하는 방법에 대한 세부 정보는 예: :citet:<code>Ioffe.2017</code>를 참조하십시오.</p>
<h2 id="데이터셋-읽기-reading-the-dataset-3"><a class="header" href="#데이터셋-읽기-reading-the-dataset-3">데이터셋 읽기 (Reading the Dataset)</a></h2>
<p>미니배치가 데이터에서 어떻게 효율적으로 생성되는지 살펴봅시다. 다음에서는 이 최적화 알고리즘들을 비교하기 위해 NASA에서 개발한 <a href="https://archive.ics.uci.edu/dataset/291/airfoil+self+noise">다양한 항공기의 날개 소음</a>을 테스트하는 데이터셋을 사용합니다. 편의상 처음 $1,500$개의 예제만 사용합니다. 데이터는 전처리를 위해 백색화(whitened)됩니다. 즉, 평균을 제거하고 좌표당 분산을 $1$로 다시 스케일링합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')

#@save
def get_data_ch11(batch_size=10, n=1500):
    data = np.genfromtxt(d2l.download('airfoil'),
                         dtype=np.float32, delimiter='\t')
    data = (data - data.mean(axis=0)) / data.std(axis=0)
    data_iter = d2l.load_array(
        (data[:n, :-1], data[:n, -1]), batch_size, is_train=True)
    return data_iter, data.shape[1]-1
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')

#@save
def get_data_ch11(batch_size=10, n=1500):
    data = np.genfromtxt(d2l.download('airfoil'),
                         dtype=np.float32, delimiter='\t')
    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))
    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),
                               batch_size, is_train=True)
    return data_iter, data.shape[1]-1
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
#@save
d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')

#@save
def get_data_ch11(batch_size=10, n=1500):
    data = np.genfromtxt(d2l.download('airfoil'),
                         dtype=np.float32, delimiter='\t')
    data = (data - data.mean(axis=0)) / data.std(axis=0)
    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),
                               batch_size, is_train=True)
    return data_iter, data.shape[1]-1
</code></pre>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-8"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-8">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>:numref:<code>sec_linear_scratch</code>의 미니배치 확률적 경사 하강법 구현을 상기하십시오. 다음에서는 약간 더 일반적인 구현을 제공합니다. 편의상 이 장의 뒷부분에서 소개되는 다른 최적화 알고리즘과 동일한 호출 서명을 갖습니다. 구체적으로 상태 입력 <code>states</code>를 추가하고 하이퍼파라미터를 <code>hyperparams</code> 딕셔너리에 넣습니다. 또한 훈련 함수에서 각 미니배치 예제의 손실을 평균화하므로 최적화 알고리즘의 기울기를 배치 크기로 나눌 필요가 없습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def sgd(params, states, hyperparams):
    for p in params:
        p[:] -= hyperparams['lr'] * p.grad
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def sgd(params, states, hyperparams):
    for p in params:
        p.data.sub_(hyperparams['lr'] * p.grad)
        p.grad.data.zero_()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def sgd(params, grads, states, hyperparams):
    for param, grad in zip(params, grads):
        param.assign_sub(hyperparams['lr']*grad)
</code></pre>
<p>다음으로, 나중에 소개될 다른 최적화 알고리즘의 사용을 용이하게 하기 위해 일반 훈련 함수를 구현합니다. 선형 회귀 모델을 초기화하고 미니배치 확률적 경사 하강법 및 이후에 소개될 다른 알고리즘으로 모델을 훈련하는 데 사용할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def train_ch11(trainer_fn, states, hyperparams, data_iter,
               feature_dim, num_epochs=2):
    # 초기화
    w = np.random.normal(scale=0.01, size=(feature_dim, 1))
    b = np.zeros(1)
    w.attach_grad()
    b.attach_grad()
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    # 훈련
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[0, num_epochs], ylim=[0.22, 0.35])
    n, timer = 0, d2l.Timer()
    for _ in range(num_epochs):
        for X, y in data_iter:
            with autograd.record():
                l = loss(net(X), y).mean()
            l.backward()
            trainer_fn([w, b], states, hyperparams)
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                animator.add(n/X.shape[0]/len(data_iter),
                             (d2l.evaluate_loss(net, data_iter, loss),))
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')
    return timer.cumsum(), animator.Y[0]
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def train_ch11(trainer_fn, states, hyperparams, data_iter,
               feature_dim, num_epochs=2):
    # 초기화
    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),
                     requires_grad=True)
    b = torch.zeros((1), requires_grad=True)
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    # 훈련
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[0, num_epochs], ylim=[0.22, 0.35])
    n, timer = 0, d2l.Timer()
    for _ in range(num_epochs):
        for X, y in data_iter:
            l = loss(net(X), y).mean()
            l.backward()
            trainer_fn([w, b], states, hyperparams)
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                animator.add(n/X.shape[0]/len(data_iter),
                             (d2l.evaluate_loss(net, data_iter, loss),))
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')
    return timer.cumsum(), animator.Y[0]
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
#@save
def train_ch11(trainer_fn, states, hyperparams, data_iter,
               feature_dim, num_epochs=2):
    # 초기화
    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1),
                                   mean=0, stddev=0.01),trainable=True)
    b = tf.Variable(tf.zeros(1), trainable=True)

    # 훈련
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[0, num_epochs], ylim=[0.22, 0.35])
    n, timer = 0, d2l.Timer()

    for _ in range(num_epochs):
        for X, y in data_iter:
          with tf.GradientTape() as g:
            l = tf.math.reduce_mean(loss(net(X), y))

          dw, db = g.gradient(l, [w, b])
          trainer_fn([w, b], [dw, db], states, hyperparams)
          n += X.shape[0]
          if n % 200 == 0:
              timer.stop()
              p = n/X.shape[0]
              q = p/tf.data.experimental.cardinality(data_iter).numpy()
              r = (d2l.evaluate_loss(net, data_iter, loss),)
              animator.add(q, r)
              timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')
    return timer.cumsum(), animator.Y[0]
</code></pre>
<p>배치 경사 하강법에 대한 최적화가 어떻게 진행되는지 봅시다. 미니배치 크기를 1500(즉, 총 예제 수)으로 설정하면 됩니다. 결과적으로 모델 파라미터는 에폭당 한 번만 업데이트됩니다. 진전이 거의 없습니다. 실제로 6단계 후에 진전이 멈춥니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def train_sgd(lr, batch_size, num_epochs=2):
    data_iter, feature_dim = get_data_ch11(batch_size)
    return train_ch11(
        sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)

gd_res = train_sgd(1, 1500, 10)
</code></pre>
<p>배치 크기가 1일 때 최적화를 위해 확률적 경사 하강법을 사용합니다. 구현의 단순성을 위해 일정한(작지만) 학습률을 선택했습니다. 확률적 경사 하강법에서는 예제가 처리될 때마다 모델 파라미터가 업데이트됩니다. 우리의 경우 에폭당 1500번의 업데이트에 해당합니다. 보시다시피 목적 함수 값의 감소는 한 에폭 후에 느려집니다. 두 절차 모두 한 에폭 내에 1500개의 예제를 처리했지만, 우리 실험에서 확률적 경사 하강법은 경사 하강법보다 더 많은 시간을 소비했습니다. 이는 확률적 경사 하강법이 파라미터를 더 자주 업데이트하고 한 번에 하나의 관찰을 처리하는 것이 덜 효율적이기 때문입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
sgd_res = train_sgd(0.005, 1)
</code></pre>
<p>마지막으로 배치 크기가 100일 때 최적화를 위해 미니배치 확률적 경사 하강법을 사용합니다. 에폭당 필요한 시간은 확률적 경사 하강법에 필요한 시간과 배치 경사 하강법에 필요한 시간보다 짧습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
mini1_res = train_sgd(.4, 100)
</code></pre>
<p>배치 크기를 10으로 줄이면 각 배치의 작업 부하가 실행 효율성이 떨어지기 때문에 각 에폭에 걸리는 시간이 늘어납니다.</p>
<pre><code class="language-{.python .input}">#@tab all
mini2_res = train_sgd(.05, 10)
</code></pre>
<p>이제 이전 네 가지 실험에 대한 시간 대 손실을 비교할 수 있습니다. 보시다시피 확률적 경사 하강법은 처리된 예제 수 측면에서 GD보다 빠르게 수렴하지만, 예제별로 기울기를 계산하는 것이 효율적이지 않기 때문에 GD보다 동일한 손실에 도달하는 데 더 많은 시간을 사용합니다. 미니배치 확률적 경사 하강법은 수렴 속도와 계산 효율성을 절충할 수 있습니다. 미니배치 크기 10은 확률적 경사 하강법보다 효율적입니다. 미니배치 크기 100은 런타임 측면에서 GD보다 성능이 뛰어납니다.</p>
<pre><code class="language-{.python .input}">#@tab all
d2l.set_figsize([6, 3])
d2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),
         'time (sec)', 'loss', xlim=[1e-2, 10],
         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])
d2l.plt.gca().set_xscale('log')
</code></pre>
<h2 id="간결한-구현-concise-implementation-5"><a class="header" href="#간결한-구현-concise-implementation-5">간결한 구현 (Concise Implementation)</a></h2>
<p>Gluon에서는 <code>Trainer</code> 클래스를 사용하여 최적화 알고리즘을 호출할 수 있습니다. 이것은 일반 훈련 함수를 구현하는 데 사용됩니다. 현재 장 전체에서 이것을 사용할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def train_concise_ch11(tr_name, hyperparams, data_iter, num_epochs=2):
    # 초기화
    net = nn.Sequential()
    net.add(nn.Dense(1))
    net.initialize(init.Normal(sigma=0.01))
    trainer = gluon.Trainer(net.collect_params(), tr_name, hyperparams)
    loss = gluon.loss.L2Loss()
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[0, num_epochs], ylim=[0.22, 0.35])
    n, timer = 0, d2l.Timer()
    for _ in range(num_epochs):
        for X, y in data_iter:
            with autograd.record():
                l = loss(net(X), y)
            l.backward()
            trainer.step(X.shape[0])
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                animator.add(n/X.shape[0]/len(data_iter),
                             (d2l.evaluate_loss(net, data_iter, loss),))
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):
    # 초기화
    net = nn.Sequential(nn.Linear(5, 1))
    def init_weights(module):
        if type(module) == nn.Linear:
            torch.nn.init.normal_(module.weight, std=0.01)
    net.apply(init_weights)

    optimizer = trainer_fn(net.parameters(), **hyperparams)
    loss = nn.MSELoss(reduction='none')
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[0, num_epochs], ylim=[0.22, 0.35])
    n, timer = 0, d2l.Timer()
    for _ in range(num_epochs):
        for X, y in data_iter:
            optimizer.zero_grad()
            out = net(X)
            y = y.reshape(out.shape)
            l = loss(out, y)
            l.mean().backward()
            optimizer.step()
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                # `MSELoss`는 1/2 인자 없이 제곱 오차를 계산합니다
                animator.add(n/X.shape[0]/len(data_iter),
                             (d2l.evaluate_loss(net, data_iter, loss) / 2,))
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
#@save
def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=2):
    # 초기화
    net = tf.keras.Sequential()
    net.add(tf.keras.layers.Dense(1,
            kernel_initializer=tf.random_normal_initializer(stddev=0.01)))
    optimizer = trainer_fn(**hyperparams)
    loss = tf.keras.losses.MeanSquaredError()
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[0, num_epochs], ylim=[0.22, 0.35])
    n, timer = 0, d2l.Timer()
    for _ in range(num_epochs):
        for X, y in data_iter:
            with tf.GradientTape() as g:
                out = net(X)
                l = loss(y, out)
                params = net.trainable_variables
                grads = g.gradient(l, params)
            optimizer.apply_gradients(zip(grads, params))
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                p = n/X.shape[0]
                q = p/tf.data.experimental.cardinality(data_iter).numpy()
                # `MeanSquaredError`는 1/2 인자 없이 제곱 오차를 계산합니다
                r = (d2l.evaluate_loss(net, data_iter, loss) / 2,)
                animator.add(q, r)
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')
</code></pre>
<p>Gluon을 사용하여 마지막 실험을 반복하면 동일한 동작이 나타납니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
data_iter, _ = get_data_ch11(10)
train_concise_ch11('sgd', {'learning_rate': 0.05}, data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
data_iter, _ = get_data_ch11(10)
trainer = torch.optim.SGD
train_concise_ch11(trainer, {'lr': 0.01}, data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
data_iter, _ = get_data_ch11(10)
trainer = tf.keras.optimizers.SGD
train_concise_ch11(trainer, {'learning_rate': 0.05}, data_iter)
</code></pre>
<h2 id="요약-summary-55"><a class="header" href="#요약-summary-55">요약 (Summary)</a></h2>
<ul>
<li>벡터화는 딥러닝 프레임워크에서 발생하는 오버헤드를 줄이고 CPU 및 GPU에서 더 나은 메모리 지역성 및 캐싱으로 인해 코드를 더 효율적으로 만듭니다.</li>
<li>확률적 경사 하강법에서 발생하는 통계적 효율성과 한 번에 대규모 배치 데이터를 처리하여 발생하는 계산 효율성 사이에는 상충 관계가 있습니다.</li>
<li>미니배치 확률적 경사 하강법은 계산 및 통계적 효율성이라는 두 가지 장점을 모두 제공합니다.</li>
<li>미니배치 확률적 경사 하강법에서는 훈련 데이터의 무작위 순열에 의해 얻은 데이터 배치를 처리합니다(즉, 각 관찰은 에폭당 한 번만 처리되지만 무작위 순서로 처리됨).</li>
<li>훈련 중에 학습률을 감소시키는 것이 좋습니다.</li>
<li>일반적으로 미니배치 확률적 경사 하강법은 클록 시간 측면에서 더 작은 위험으로 수렴하는 데 있어 확률적 경사 하강법 및 경사 하강법보다 빠릅니다.</li>
</ul>
<h2 id="연습-문제-exercises-70"><a class="header" href="#연습-문제-exercises-70">연습 문제 (Exercises)</a></h2>
<ol>
<li>배치 크기와 학습률을 수정하고 목적 함수 값의 감소율과 각 에폭에서 소비되는 시간을 관찰하십시오.</li>
<li>MXNet 문서를 읽고 <code>Trainer</code> 클래스의 <code>set_learning_rate</code> 함수를 사용하여 각 에폭 후에 미니배치 확률적 경사 하강법의 학습률을 이전 값의 1/10로 줄이십시오.</li>
<li>미니배치 확률적 경사 하강법을 훈련 세트에서 *복원 추출(samples with replacement)*하는 변형과 비교하십시오. 어떻게 됩니까?</li>
<li>사악한 지니가 당신에게 말하지 않고 데이터셋을 복제합니다(즉, 각 관찰이 두 번 발생하고 데이터셋이 원래 크기의 두 배로 커지지만 아무도 당신에게 말하지 않았습니다). 확률적 경사 하강법, 미니배치 확률적 경사 하강법 및 경사 하강법의 동작은 어떻게 변합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/353">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1068">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1069">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="모멘텀-momentum"><a class="header" href="#모멘텀-momentum">모멘텀 (Momentum)</a></h1>
<p>:label:<code>sec_momentum</code></p>
<p>:numref:<code>sec_sgd</code>에서 우리는 확률적 경사 하강법을 수행할 때, 즉 기울기의 노이즈가 섞인 변형만 사용할 수 있는 최적화를 수행할 때 어떤 일이 일어나는지 검토했습니다. 특히 노이즈가 있는 기울기의 경우 노이즈에 직면하여 학습률을 선택할 때 각별히 주의해야 한다는 점을 알아차렸습니다. 너무 빨리 줄이면 수렴이 멈춥니다. 너무 관대하면 노이즈가 계속해서 최적성에서 멀어지게 만들기 때문에 충분히 좋은 솔루션으로 수렴하지 못합니다.</p>
<h2 id="기초-basics-1"><a class="header" href="#기초-basics-1">기초 (Basics)</a></h2>
<p>이 섹션에서는 특히 실전에서 흔히 볼 수 있는 특정 유형의 최적화 문제에 대해 더 효과적인 최적화 알고리즘을 탐구할 것입니다.</p>
<h3 id="누적-평균-leaky-averages"><a class="header" href="#누적-평균-leaky-averages">누적 평균 (Leaky Averages)</a></h3>
<p>이전 섹션에서는 계산을 가속화하기 위한 수단으로 미니배치 SGD를 논의했습니다. 또한 기울기를 평균화하면 분산의 양이 줄어든다는 좋은 부수 효과가 있었습니다. 미니배치 확률적 경사 하강법은 다음과 같이 계산될 수 있습니다.</p>
<p>$$\mathbf{g}<em>{t, t-1} = \partial</em>{\mathbf{w}} \frac{1}{|\mathcal{B}<em>t|} \sum</em>{i \in \mathcal{B}<em>t} f(\mathbf{x}</em>{i}, \mathbf{w}_{t-1}) = \frac{1}{|\mathcal{B}<em>t|} \sum</em>{i \in \mathcal{B}<em>t} \mathbf{h}</em>{i, t-1}.
$$</p>
<p>표기법을 단순하게 유지하기 위해, 여기서 $\mathbf{h}<em>{i, t-1} = \partial</em>{\mathbf{w}} f(\mathbf{x}<em>i, \mathbf{w}</em>{t-1})$를 $t-1$ 시간에 업데이트된 가중치를 사용하여 샘플 $i$에 대한 확률적 경사 하강법으로 사용했습니다. 미니배치에서 기울기를 평균화하는 것을 넘어 분산 감소의 효과를 누릴 수 있다면 좋을 것입니다. 이 작업을 수행하는 한 가지 옵션은 기울기 계산을 "누적 평균(leaky average)"으로 대체하는 것입니다.</p>
<p>$$\mathbf{v}<em>t = \beta \mathbf{v}</em>{t-1} + \mathbf{g}_{t, t-1}$$</p>
<p>어떤 $\beta \in (0, 1)$에 대해서입니다. 이는 효과적으로 순간적인 기울기를 여러 <em>과거</em> 기울기에 대해 평균화된 것으로 대체합니다. $\mathbf{v}$는 *속도(velocity)*라고 불립니다. 이는 목적 함수 지형을 따라 굴러가는 무거운 공이 과거의 힘을 통합하는 방식과 유사하게 과거 기울기를 누적합니다. 무슨 일이 일어나고 있는지 더 자세히 보기 위해 $\mathbf{v}_t$를 재귀적으로 다음과 같이 확장해 봅시다.</p>
<p>$$\begin{aligned}
\mathbf{v}<em>t = \beta^2 \mathbf{v}</em>{t-2} + \beta \mathbf{g}<em>{t-1, t-2} + \mathbf{g}</em>{t, t-1}
= \ldots, = \sum_{\tau = 0}^{t-1} \beta^{\tau} \mathbf{g}_{t-\tau, t-\tau-1}.
\end{aligned}$$</p>
<p>큰 $\beta$는 장기 평균에 해당하고, 작은 $\beta$는 기울기 방법에 비해 약간의 보정만 하는 것에 해당합니다. 새로운 기울기 대체물은 더 이상 특정 인스턴스에서 가장 가파른 하강 방향을 가리키지 않고 대신 과거 기울기의 가중 평균 방향을 가리킵니다. 이를 통해 실제로 기울기를 계산하는 비용 없이 배치에 대해 평균화하는 대부분의 이점을 실현할 수 있습니다. 나중에 이 평균화 절차를 더 자세히 살펴볼 것입니다.</p>
<p>위의 추론은 이제 모멘텀이 있는 기울기와 같은 <em>가속된(accelerated)</em> 기울기 방법으로 알려진 것의 기초를 형성했습니다. 이들은 최적화 문제가 조건이 나쁜 경우(즉, 일부 방향에서는 진전이 다른 방향보다 훨씬 느려 좁은 협곡과 닮은 경우) 훨씬 더 효과적이라는 추가적인 이점이 있습니다. 더욱이, 그들은 후속 기울기에 대해 평균화하여 더 안정적인 하강 방향을 얻을 수 있게 해 줍니다. 실제로 노이즈가 없는 볼록 문제에 대해서도 가속 측면은 모멘텀이 작동하는 이유이자 매우 잘 작동하는 핵심 이유 중 하나입니다.</p>
<p>예상대로 모멘텀은 그 효능 때문에 딥러닝 최적화 및 그 이상의 분야에서 잘 연구된 주제입니다. 심층 분석과 대화형 애니메이션은 :citet:<code>Goh.2017</code>의 아름다운 <a href="https://distill.pub/2017/momentum/">설명 기사</a>를 참조하십시오. 이는 :citet:<code>Polyak.1964</code>에 의해 제안되었습니다. :citet:<code>Nesterov.2018</code>은 볼록 최적화 맥락에서 상세한 이론적 논의를 담고 있습니다. 딥러닝에서의 모멘텀은 오랫동안 유익한 것으로 알려져 왔습니다. 자세한 내용은 :citet:<code>Sutskever.Martens.Dahl.ea.2013</code>의 토론을 참조하십시오.</p>
<h3 id="조건이-나쁜-문제-an-ill-conditioned-problem"><a class="header" href="#조건이-나쁜-문제-an-ill-conditioned-problem">조건이 나쁜 문제 (An Ill-conditioned Problem)</a></h3>
<p>모멘텀 방법의 기하학적 특성을 더 잘 이해하기 위해, 상당히 덜 유쾌한 목적 함수를 사용하여 경사 하강법을 다시 살펴봅니다. :numref:<code>sec_gd</code>에서 우리는 $f(\mathbf{x}) = x_1^2 + 2 x_2^2$, 즉 적당히 왜곡된 타원체 목적 함수를 사용했음을 상기하십시오. 우리는 이 함수를 $x_1$ 방향으로 늘려서 다음과 같이 더 왜곡합니다.</p>
<p>$$f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$</p>
<p>이전과 마찬가지로 $f$는 $(0, 0)$에서 최소값을 갖습니다. 이 함수는 $x_1$ 방향으로 <em>매우</em> 평평합니다. 이 새로운 함수에 대해 이전과 같이 경사 하강법을 수행하면 어떤 일이 일어나는지 봅시다. 학습률을 $0.4$로 선택합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()

eta = 0.4
def f_2d(x1, x2):
    return 0.1 * x1 ** 2 + 2 * x2 ** 2
def gd_2d(x1, x2, s1, s2):
    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)

d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

eta = 0.4
def f_2d(x1, x2):
    return 0.1 * x1 ** 2 + 2 * x2 ** 2
def gd_2d(x1, x2, s1, s2):
    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)

d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf

eta = 0.4
def f_2d(x1, x2):
    return 0.1 * x1 ** 2 + 2 * x2 ** 2
def gd_2d(x1, x2, s1, s2):
    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)

d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))
</code></pre>
<p>구조적으로 $x_2$ 방향의 기울기는 수평 $x_1$ 방향보다 <em>훨씬</em> 높고 훨씬 더 빠르게 변합니다. 따라서 우리는 두 가지 바람직하지 않은 선택 사이에 갇히게 됩니다: 작은 학습률을 선택하면 $x_2$ 방향에서 해가 발산하지 않도록 보장하지만 $x_1$ 방향에서는 느린 수렴을 겪게 됩니다. 반대로 큰 학습률을 사용하면 $x_1$ 방향에서는 빠르게 진행되지만 $x_2$에서는 발산합니다. 아래 예제는 학습률을 $0.4$에서 $0.6$으로 약간 올린 후에도 어떤 일이 일어나는지 보여줍니다. $x_1$ 방향의 수렴은 개선되지만 전체적인 솔루션 품질은 훨씬 나빠집니다.</p>
<pre><code class="language-{.python .input}">#@tab all
eta = 0.6
d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))
</code></pre>
<h3 id="모멘텀-방법-the-momentum-method"><a class="header" href="#모멘텀-방법-the-momentum-method">모멘텀 방법 (The Momentum Method)</a></h3>
<p>모멘텀 방법은 위에서 설명한 경사 하강법 문제를 해결할 수 있게 해 줍니다. 위의 최적화 궤적을 보면 과거의 기울기를 평균화하는 것이 잘 작동할 것이라고 직관적으로 느낄 수 있습니다. 결국 $x_1$ 방향에서는 이것이 잘 정렬된 기울기들을 집계하여 매 단계마다 우리가 이동하는 거리를 늘릴 것입니다. 반대로 기울기가 진동하는 $x_2$ 방향에서는 집계된 기울기가 서로 상쇄되는 진동으로 인해 단계 크기를 줄일 것입니다.</p>
<p>기울기 $\mathbf{g}_t$ 대신 $\mathbf{v}_t$를 사용하면 다음과 같은 업데이트 방정식이 생성됩니다.</p>
<p>$$
\begin{aligned}
\mathbf{v}<em>t &amp;\leftarrow \beta \mathbf{v}</em>{t-1} + \mathbf{g}_{t, t-1}, \
\mathbf{x}<em>t &amp;\leftarrow \mathbf{x}</em>{t-1} - \eta_t \mathbf{v}_t.
\end{aligned}
$$</p>
<p>$eta = 0$이면 일반적인 경사 하강법을 회복한다는 점에 유의하십시오. 수학적 특성을 깊이 파고들기 전에 알고리즘이 실전에서 어떻게 행동하는지 간단히 살펴봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
def momentum_2d(x1, x2, v1, v2):
    v1 = beta * v1 + 0.2 * x1
    v2 = beta * v2 + 4 * x2
    return x1 - eta * v1, x2 - eta * v2, v1, v2

eta, beta = 0.6, 0.5
d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))
</code></pre>
<p>보시다시피, 이전에 사용한 것과 동일한 학습률로도 모멘텀은 여전히 잘 수렴합니다. 모멘텀 파라미터를 줄이면 어떻게 되는지 봅시다. 이를 절반인 $eta = 0.25$로 줄이면 거의 수렴하지 않는 궤적이 생성됩니다. 그럼에도 불구하고 모멘텀이 없을 때(솔루션이 발산할 때)보다 훨씬 낫습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
eta, beta = 0.6, 0.25
d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))
</code></pre>
<p>모멘텀을 확률적 경사 하강법, 특히 미니배치 확률적 경사 하강법과 결합할 수 있다는 점에 유의하십시오. 유일한 변경 사항은 그 경우 기울기 $\mathbf{g}_{t, t-1}$을 $\mathbf{g}_t$로 대체하는 것입니다. 마지막으로 편의를 위해 시간 $t=0$에서 $\mathbf{v}_0 = 0$으로 초기화합니다. 누적 평균이 실제로 업데이트에 어떤 영향을 미치는지 살펴봅시다.</p>
<h3 id="유효-샘플-가중치-effective-sample-weight"><a class="header" href="#유효-샘플-가중치-effective-sample-weight">유효 샘플 가중치 (Effective Sample Weight)</a></h3>
<p>$\mathbf{v}<em>t = \sum</em>{\tau = 0}^{t-1} \beta^{\tau} \mathbf{g}<em>{t-\tau, t-\tau-1}$임을 상기하십시오. 극한에서 항들은 $\sum</em>{\tau=0}^\infty \beta^\tau = \frac{1}{1-\beta}$로 합산됩니다. 즉, 경사 하강법이나 확률적 경사 하강법에서 크기 $\eta$의 단계를 밟는 대신, 잠재적으로 훨씬 더 잘 행동하는 하강 방향을 다루면서 동시에 $\frac{\eta}{1-\beta}$ 크기의 단계를 밟는 것입니다. 이는 한 번에 두 가지 이점을 제공합니다. 서로 다른 $\beta$ 선택에 대해 가중치가 어떻게 행동하는지 설명하기 위해 아래 다이어그램을 고려해 보십시오.</p>
<pre><code class="language-{.python .input}">#@tab all
d2l.set_figsize()
betas = [0.95, 0.9, 0.6, 0]
for beta in betas:
    x = d2l.numpy(d2l.arange(40))
    d2l.plt.plot(x, beta ** x, label=f'beta = {beta:.2f}')
d2l.plt.xlabel('time')
d2l.plt.legend();
</code></pre>
<h2 id="실전-실험-practical-experiments"><a class="header" href="#실전-실험-practical-experiments">실전 실험 (Practical Experiments)</a></h2>
<p>모멘텀이 실전에서 어떻게 작동하는지, 즉 적절한 최적화 프로그램 내에서 사용될 때 어떻게 작동하는지 살펴봅시다. 이를 위해 좀 더 확장 가능한 구현이 필요합니다.</p>
<h3 id="밑바닥부터-구현하기-implementation-from-scratch-9"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-9">밑바닥부터 구현하기 (Implementation from Scratch)</a></h3>
<p>(미니배치) 확률적 경사 하강법과 비교하여 모멘텀 방법은 보조 변수 집합, 즉 속도를 유지해야 합니다. 이는 기울기(및 최적화 문제의 변수)와 동일한 모양을 갖습니다. 아래 구현에서 우리는 이러한 변수들을 <code>states</code>라고 부릅니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet,pytorch
def init_momentum_states(feature_dim):
    s_w = d2l.zeros((feature_dim, 1))
    s_b = d2l.zeros(1)
    return (s_w, s_b)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def init_momentum_states(features_dim):
    v_w = tf.Variable(d2l.zeros((features_dim, 1)))
    v_b = tf.Variable(d2l.zeros(1))
    return (v_w, v_b)
</code></pre>
<pre><code class="language-{.python .input}">#@tab mxnet
def sgd_momentum(params, states, hyperparams):
    for p, v in zip(params, states):
        v[:] = hyperparams['momentum'] * v + p.grad
        p[:] -= hyperparams['lr'] * v
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def sgd_momentum(params, states, hyperparams):
    for p, v in zip(params, states):
        with torch.no_grad():
            v[:] = hyperparams['momentum'] * v + p.grad
            p[:] -= hyperparams['lr'] * v
        p.grad.data.zero_()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def sgd_momentum(params, grads, states, hyperparams):
    for p, v, g in zip(params, states, grads):
            v[:].assign(hyperparams['momentum'] * v + g)
            p[:].assign(p - hyperparams['lr'] * v)
</code></pre>
<p>이것이 실전에서 어떻게 작동하는지 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
def train_momentum(lr, momentum, num_epochs=2):
    d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),
                   {'lr': lr, 'momentum': momentum}, data_iter,
                   feature_dim, num_epochs)

data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
train_momentum(0.02, 0.5)
</code></pre>
<p>모멘텀 하이퍼파라미터 <code>momentum</code>을 0.9로 늘리면 $\frac{1}{1 - 0.9} = 10$의 상당히 더 큰 유효 샘플 크기에 해당합니다. 상황을 제어하기 위해 학습률을 $0.01$로 약간 낮춥니다.</p>
<pre><code class="language-{.python .input}">#@tab all
train_momentum(0.01, 0.9)
</code></pre>
<p>학습률을 더 낮추면 매끄럽지 않은 최적화 문제의 모든 문제가 해결됩니다. $0.005$로 설정하면 좋은 수렴 특성을 얻을 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
train_momentum(0.005, 0.9)
</code></pre>
<h3 id="간결한-구현-concise-implementation-6"><a class="header" href="#간결한-구현-concise-implementation-6">간결한 구현 (Concise Implementation)</a></h3>
<p>표준 <code>sgd</code> 솔버에 이미 모멘텀이 내장되어 있으므로 Gluon에서 할 일은 거의 없습니다. 일치하는 파라미터를 설정하면 매우 유사한 궤적이 생성됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.train_concise_ch11('sgd', {'learning_rate': 0.005, 'momentum': 0.9},
                       data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
trainer = torch.optim.SGD
d2l.train_concise_ch11(trainer, {'lr': 0.005, 'momentum': 0.9}, data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
trainer = tf.keras.optimizers.SGD
d2l.train_concise_ch11(trainer, {'learning_rate': 0.005, 'momentum': 0.9},
                       data_iter)
</code></pre>
<h2 id="이론적-분석-theoretical-analysis"><a class="header" href="#이론적-분석-theoretical-analysis">이론적 분석 (Theoretical Analysis)</a></h2>
<p>지금까지 $f(x) = 0.1 x_1^2 + 2 x_2^2$의 2D 예제는 다소 인위적으로 보였을 수 있습니다. 우리는 이것이 적어도 볼록 이차 목적 함수를 최소화하는 경우 마주칠 수 있는 문제 유형을 상당히 잘 나타낸다는 것을 알게 될 것입니다.</p>
<h3 id="이차-볼록-함수-quadratic-convex-functions"><a class="header" href="#이차-볼록-함수-quadratic-convex-functions">이차 볼록 함수 (Quadratic Convex Functions)</a></h3>
<p>함수를 고려해 봅시다.</p>
<p>$$h(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{Q} \mathbf{x} + \mathbf{x}^\top \mathbf{c} + b.$$</p>
<p>이것은 일반적인 이차 함수입니다. 양의 고유값을 가진 행렬인 양의 정부호(positive definite) 행렬 $\mathbf{Q} \succ 0$에 대해, 이는 최소값 $b - \frac{1}{2} \mathbf{c}^\top \mathbf{Q}^{-1} \mathbf{c}$와 함께 $\mathbf{x}^* = -\mathbf{Q}^{-1} \mathbf{c}$에서 최소화자를 갖습니다. 따라서 우리는 $h$를 다음과 같이 다시 쓸 수 있습니다.</p>
<p>$$h(\mathbf{x}) = \frac{1}{2} (\mathbf{x} - \mathbf{Q}^{-1} \mathbf{c})^	op \mathbf{Q} (\mathbf{x} - \mathbf{Q}^{-1} \mathbf{c}) + b - \frac{1}{2} \mathbf{c}^\top \mathbf{Q}^{-1} \mathbf{c}.$$</p>
<p>기울기는 $\partial_{\mathbf{x}} h(\mathbf{x}) = \mathbf{Q} (\mathbf{x} - \mathbf{Q}^{-1} \mathbf{c})$로 주어집니다. 즉, $\mathbf{x}$와 최소화자 사이의 거리에 $\mathbf{Q}$를 곱한 값입니다. 결과적으로 속도 또한 $\mathbf{Q} (\mathbf{x}_t - \mathbf{Q}^{-1} \mathbf{c})$ 항들의 선형 결합입니다.</p>
<p>$\mathbf{Q}$는 양의 정부호이므로 직교(회전) 행렬 $\mathbf{O}$와 양의 고유값의 대각 행렬 $\boldsymbol{\Lambda}$를 통해 $\mathbf{Q} = \mathbf{O}^\top \boldsymbol{\Lambda} \mathbf{O}$로 고유계 분해가 가능합니다. 이를 통해 $\mathbf{x}$에서 $\mathbf{z} \stackrel{\textrm{def}}{=} \mathbf{O} (\mathbf{x} - \mathbf{Q}^{-1} \mathbf{c})$로 변수 변환을 수행하여 훨씬 단순화된 식을 얻을 수 있습니다.</p>
<p>$$h(\mathbf{z}) = \frac{1}{2} \mathbf{z}^\top \boldsymbol{\Lambda} \mathbf{z} + b'.$$</p>
<p>여기서 $b' = b - \frac{1}{2} \mathbf{c}^\top \mathbf{Q}^{-1} \mathbf{c}$입니다. $\mathbf{O}$는 직교 행렬일 뿐이므로 기울기를 의미 있게 섭동시키지 않습니다. $\mathbf{z}$의 관점에서 표현하면 경사 하강법은 다음과 같이 됩니다.</p>
<p>$$\mathbf{z}<em>t = \mathbf{z}</em>{t-1} - \boldsymbol{\Lambda} \mathbf{z}<em>{t-1} = (\mathbf{I} - \boldsymbol{\Lambda}) \mathbf{z}</em>{t-1}.$$</p>
<p>이 식에서 중요한 사실은 경사 하강법이 서로 다른 고유 공간 사이에서 <em>섞이지 않는다</em>는 것입니다. 즉, $\mathbf{Q}$의 고유계 관점에서 표현할 때 최적화 문제는 좌표별 방식으로 진행됩니다. 이는 다음에도 적용됩니다.</p>
<p>$$egin{aligned}
\mathbf{v}<em>t &amp; = \beta \mathbf{v}</em>{t-1} + \boldsymbol{\Lambda} \mathbf{z}<em>{t-1} \
\mathbf{z}<em>t &amp; = \mathbf{z}</em>{t-1} - \eta (\beta \mathbf{v}</em>{t-1} + \boldsymbol{\Lambda} \mathbf{z}<em>{t-1}) \
&amp; = (\mathbf{I} - \eta \boldsymbol{\Lambda}) \mathbf{z}</em>{t-1} - \eta \beta \mathbf{v}_{t-1}.
\end{aligned}$$</p>
<p>이를 통해 우리는 방금 다음 정리를 증명했습니다: 볼록 이차 함수에 대해 모멘텀이 있거나 없는 경사 하강법은 이차 행렬의 고유벡터 방향으로의 좌표별 최적화로 분해됩니다.</p>
<h3 id="스칼라-함수-scalar-functions"><a class="header" href="#스칼라-함수-scalar-functions">스칼라 함수 (Scalar Functions)</a></h3>
<p>위의 결과가 주어졌을 때 함수 $f(x) = \frac{\lambda}{2} x^2$를 최소화할 때 어떤 일이 일어나는지 봅시다. 경사 하강법의 경우</p>
<p>$$x_{t+1} = x_t - \eta \lambda x_t = (1 - \eta \lambda) x_t.$$</p>
<p>$|1 - \eta \lambda| &lt; 1$일 때마다 이 최적화는 지수 속도로 수렴합니다. $t$단계 후에 $x_t = (1 - \eta \lambda)^t x_0$를 갖기 때문입니다. 이는 $\eta \lambda = 1$이 될 때까지 학습률 $\eta$를 높임에 따라 초기에 수렴 속도가 어떻게 개선되는지 보여줍니다. 그 이상에서는 발산하기 시작하고 $\eta \lambda &gt; 2$에 대해 최적화 문제는 발산합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
 lambdas = [0.1, 1, 10, 19]
 eta = 0.1
 d2l.set_figsize((6, 4))
 for lam in lambdas:
    t = d2l.numpy(d2l.arange(20))
    d2l.plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')
 d2l.plt.xlabel('time')
 d2l.plt.legend();
</code></pre>
<p>모멘텀의 경우 수렴을 분석하기 위해 업데이트 방정식을 두 개의 스칼라(하나는 $x$용, 하나는 속도 $v$용)로 다시 쓰는 것으로 시작합니다. 이는 다음을 산출합니다.</p>
<p>$$
\begin{bmatrix} v_{t+1} \ x_{t+1} \end{bmatrix} =
\begin{bmatrix} \beta &amp; \lambda \ -\eta \beta &amp; (1 - \eta \lambda) \end{bmatrix}
\begin{bmatrix} v_{t} \ x_{t} \end{bmatrix} = \mathbf{R}(\beta, \eta, \lambda) \begin{bmatrix} v_{t} \ x_{t} \end{bmatrix}.
$$</p>
<p>우리는 수렴 동작을 지배하는 $2 \times 2$ 행렬을 나타내기 위해 $\mathbf{R}$을 사용했습니다. $t$단계 후에 초기 선택 $[v_0, x_0]$은 $\mathbf{R}(\beta, \eta, \lambda)^t [v_0, x_0]$이 됩니다. 따라서 수렴 속도를 결정하는 것은 $\mathbf{R}$의 고유값에 달려 있습니다. 멋진 애니메이션은 :citet:<code>Goh.2017</code>의 <a href="https://distill.pub/2017/momentum/">Distill 포스트</a>를, 상세한 분석은 :citet:<code>Flammarion.Bach.2015</code>를 참조하십시오. $0 &lt; \eta \lambda &lt; 2 + 2 \beta$에서 속도가 수렴함을 보일 수 있습니다. 이는 경사 하강법의 $0 &lt; \eta \lambda &lt; 2$와 비교할 때 가능한 파라미터의 더 넓은 범위입니다. 또한 일반적으로 큰 $\beta$ 값이 바람직함을 시사합니다. 추가 세부 사항은 상당한 양의 기술적 세부 사항을 필요로 하므로 관심 있는 독자는 원본 출판물을 참고하시기 바랍니다.</p>
<h2 id="요약-summary-56"><a class="header" href="#요약-summary-56">요약 (Summary)</a></h2>
<ul>
<li>모멘텀은 기울기를 과거 기울기에 대한 누적 평균으로 대체합니다. 이는 수렴을 크게 가속화합니다.</li>
<li>노이즈가 없는 경사 하강법과 (노이즈가 있는) 확률적 경사 하강법 모두에 바람직합니다.</li>
<li>모멘텀은 확률적 경사 하강법에서 훨씬 더 발생하기 쉬운 최적화 프로세스의 정체를 방지합니다.</li>
<li>과거 데이터의 지수적 가중치 감소로 인해 유효 기울기 수는 $\frac{1}{1-\beta}$로 주어집니다.</li>
<li>볼록 이차 문제의 경우 이를 상세히 명시적으로 분석할 수 있습니다.</li>
<li>구현은 꽤 간단하지만 추가적인 상태 벡터(속도 $\mathbf{v}$)를 저장해야 합니다.</li>
</ul>
<h2 id="연습-문제-exercises-71"><a class="header" href="#연습-문제-exercises-71">연습 문제 (Exercises)</a></h2>
<ol>
<li>다른 모멘텀 하이퍼파라미터와 학습률 조합을 사용하고 다양한 실험 결과를 관찰 및 분석해 보십시오.</li>
<li>여러 고유값이 있는 이차 문제, 즉 $f(x) = \frac{1}{2} \sum_i \lambda_i x_i^2$(예: $\lambda_i = 2^{-i}$)에 대해 경사 하강법과 모멘텀을 시도해 보십시오. 초기화 $x_i = 1$에 대해 $x$ 값이 어떻게 감소하는지 플롯하십시오.</li>
<li>$h(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{Q} \mathbf{x} + \mathbf{x}^\top \mathbf{c} + b$에 대한 최소값과 최소화자를 유도하십시오.</li>
<li>모멘텀과 함께 확률적 경사 하강법을 수행할 때 무엇이 변합니까? 모멘텀과 함께 미니배치 확률적 경사 하강법을 사용할 때 어떤 일이 일어납니까? 파라미터로 실험해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/354">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1070">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1071">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adagrad"><a class="header" href="#adagrad">Adagrad</a></h1>
<p>:label:<code>sec_adagrad</code></p>
<p>드물게 발생하는 특성(sparse features)이 있는 학습 문제를 고려하는 것으로 시작해 봅시다.</p>
<h2 id="희소-특성과-학습률-sparse-features-and-learning-rates"><a class="header" href="#희소-특성과-학습률-sparse-features-and-learning-rates">희소 특성과 학습률 (Sparse Features and Learning Rates)</a></h2>
<p>언어 모델을 훈련하고 있다고 상상해 보십시오. 좋은 정확도를 얻으려면 일반적으로 훈련을 계속하면서 학습률을 낮추고 싶어 하며, 보통 $\mathcal{O}(t^{-\frac{1}{2}})$ 또는 그보다 느린 속도로 낮춥니다. 이제 희소 특성, 즉 드물게만 발생하는 특성에서 모델을 훈련하는 것을 고려해 보십시오. 이는 자연어에서 흔히 볼 수 있는 현상입니다. 예를 들어 <em>preconditioning</em>이라는 단어는 <em>learning</em>이라는 단어보다 훨씬 덜 나타날 것입니다. 하지만 이는 계산 광고 및 개인화된 협업 필터링과 같은 다른 분야에서도 흔합니다. 결국 많은 것들이 소수의 사람들에게만 관심의 대상이기 때문입니다.</p>
<p>드문 특성과 관련된 파라미터는 해당 특성이 나타날 때만 의미 있는 업데이트를 받습니다. 감소하는 학습률이 주어지면, 흔한 특성에 대한 파라미터는 최적의 값으로 상당히 빨리 수렴하는 반면, 드문 특성에 대해서는 최적의 값을 결정하기에 충분히 자주 관찰하기 전에 이미 학습률이 너무 작아지는 상황에 처할 수 있습니다. 즉, 학습률이 흔한 특성에 대해서는 너무 느리게 감소하거나 드문 특성에 대해서는 너무 빨리 감소합니다.</p>
<p>이 문제를 해결하기 위한 가능한 꼼수는 특정 특성을 본 횟수를 세고 이를 학습률 조정을 위한 시계로 사용하는 것입니다. 즉, $\eta = \frac{\eta_0}{\sqrt{t + c}}$ 형태의 학습률을 선택하는 대신 $\eta_i = \frac{\eta_0}{\sqrt{s(i, t) + c}}$를 사용할 수 있습니다. 여기서 $s(i, t)$는 시간 $t$까지 관찰한 특성 $i$의 0이 아닌 값의 개수입니다. 이는 의미 있는 오버헤드 없이 구현하기가 상당히 쉽습니다. 그러나 이는 희소성이 있는 것이 아니라 기울기가 자주 매우 작고 드물게 큰 데이터의 경우에는 실패합니다. 결국 무엇을 관찰된 특성으로 간주할지 그 경계가 불분명하기 때문입니다.</p>
<p>:citet:<code>Duchi.Hazan.Singer.2011</code>에 의한 Adagrad는 다소 조잡한 카운터 $s(i, t)$를 이전에 관찰된 기울기의 제곱 합으로 대체함으로써 이를 해결합니다. 특히 학습률을 조정하는 수단으로 $s(i, t+1) = s(i, t) + \left(\partial_i f(\mathbf{x})\right)^2$를 사용합니다. 이는 두 가지 이점이 있습니다: 첫째, 기울기가 충분히 큰지 결정할 필요가 없습니다. 둘째, 기울기의 크기에 따라 자동으로 스케일이 조정됩니다. 일상적으로 큰 기울기에 해당하는 좌표는 상당히 축소되는 반면, 작은 기울기를 가진 다른 좌표는 훨씬 더 부드러운 대우를 받습니다. 실제로 이는 계산 광고 및 관련 문제에 대해 매우 효과적인 최적화 절차로 이어집니다. 하지만 이는 프리컨디셔닝(preconditioning)의 맥락에서 가장 잘 이해되는 Adagrad 고유의 몇 가지 추가적인 이점을 숨기고 있습니다.</p>
<h2 id="프리컨디셔닝-preconditioning-1"><a class="header" href="#프리컨디셔닝-preconditioning-1">프리컨디셔닝 (Preconditioning)</a></h2>
<p>볼록 최적화 문제는 알고리즘의 특성을 분석하는 데 좋습니다. 결국 대부분의 비볼록 문제에 대해 의미 있는 이론적 보장을 도출하기는 어렵지만, <em>직관</em>과 <em>통찰</em>은 종종 그대로 이어지기 때문입니다. $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{Q} \mathbf{x} + \mathbf{c}^\top \mathbf{x} + b$를 최소화하는 문제를 살펴봅시다.</p>
<p>:numref:<code>sec_momentum</code>에서 보았듯이, 이 문제를 고유값 분해 $\mathbf{Q} = \mathbf{U}^\top \boldsymbol{\Lambda} \mathbf{U}$를 통해 다시 써서 각 좌표를 개별적으로 풀 수 있는 훨씬 단순화된 문제로 도달할 수 있습니다.</p>
<p>$$f(\mathbf{x}) = \bar{f}(\bar{\mathbf{x}}) = \frac{1}{2} \bar{\mathbf{x}}^\top \boldsymbol{\Lambda} \bar{\mathbf{x}} + \bar{\mathbf{c}}^\top \bar{\mathbf{x}} + b.$$</p>
<p>여기서 $\bar{\mathbf{x}} = \mathbf{U} \mathbf{x}$를 사용했고 결과적으로 $\bar{\mathbf{c}} = \mathbf{U} \mathbf{c}$입니다. 수정된 문제의 최소화자는 $\bar{\mathbf{x}} = -\boldsymbol{\Lambda}^{-1} \bar{\mathbf{c}}$이고 최소값은 $-\frac{1}{2} \bar{\mathbf{c}}^\top \boldsymbol{\Lambda}^{-1} \bar{\mathbf{c}} + b$입니다. $\boldsymbol{\Lambda}$는 $\mathbf{Q}$의 고유값을 포함하는 대각 행렬이므로 이를 계산하는 것은 훨씬 쉽습니다.</p>
<p>우리가 $\mathbf{c}$를 약간 섭동시키면 $f$의 최소화자에서도 약간의 변화만 찾기를 바랄 것입니다. 불행히도 그렇지 않습니다. $\mathbf{c}$의 약간의 변화가 $\bar{\mathbf{c}}$에서도 똑같이 약간의 변화로 이어지지만, $f$(그리고 각각 $\bar{f}$)의 최소화자에 대해서는 그렇지 않습니다. 고유값 $\boldsymbol{\Lambda}_i$가 클 때마다 $\bar{x}_i$와 $\bar{f}$의 최소값에서 작은 변화만 보게 될 것입니다. 반대로 작은 $\boldsymbol{\Lambda}_i$에 대해서는 $\bar{x}_i$의 변화가 극적일 수 있습니다. 가장 큰 고유값과 가장 작은 고유값의 비율을 최적화 문제의 조건 수(condition number)라고 합니다.</p>
<p>$$\kappa = \frac{\boldsymbol{\Lambda}_1}{\boldsymbol{\Lambda}_d}.$$</p>
<p>조건 수 $\kappa$가 크면 최적화 문제를 정확하게 풀기가 어렵습니다. 넓은 동적 범위의 값들을 정확하게 맞추기 위해 주의를 기울여야 합니다. 우리의 분석은 명백하지만 다소 순진한 질문으로 이어집니다: 모든 고유값이 1이 되도록 공간을 왜곡하여 문제를 간단히 "고칠" 수 없을까요? 이론적으로 이는 상당히 쉽습니다: 문제를 $\mathbf{x}$에서 $\mathbf{z} \stackrel{\textrm{def}}{=} \boldsymbol{\Lambda}^{\frac{1}{2}} \mathbf{U} \mathbf{x}$의 문제로 다시 스케일링하기 위해 $\mathbf{Q}$의 고유값과 고유벡터만 필요합니다. 새 좌표계에서 $\mathbf{x}^\top \mathbf{Q} \mathbf{x}$는 $|\mathbf{z}|^2$로 단순화될 수 있습니다. 아쉽게도 이는 다소 비실용적인 제안입니다. 고유값과 고유벡터를 계산하는 것은 일반적으로 실제 문제를 푸는 것보다 <em>훨씬 더</em> 비쌉니다.</p>
<p>고유값을 정확하게 계산하는 것은 비쌀 수 있지만, 이를 추측하고 다소 근사적으로라도 계산하는 것은 아무것도 하지 않는 것보다 훨씬 나을 수 있습니다. 특히 $\mathbf{Q}$의 대각 항목을 사용하여 그에 따라 다시 스케일링할 수 있습니다. 이는 고유값을 계산하는 것보다 <em>훨씬</em> 저렴합니다.</p>
<p>$$\tilde{\mathbf{Q}} = \textrm{diag}^{-\frac{1}{2}}(\mathbf{Q}) \mathbf{Q} \textrm{diag}^{-\frac{1}{2}}(\mathbf{Q}).$$</p>
<p>이 경우 $\tilde{\mathbf{Q}}<em>{ij} = \mathbf{Q}</em>{ij} / \sqrt{\mathbf{Q}<em>{ii} \mathbf{Q}</em>{jj}}$이고 특히 모든 $i$에 대해 $\tilde{\mathbf{Q}}_{ii} = 1$입니다. 대부분의 경우 이는 조건 수를 상당히 단순화합니다. 예를 들어 우리가 이전에 논의한 사례들의 경우, 문제가 축에 정렬되어 있으므로 이 방법이 당면한 문제를 완전히 제거할 것입니다.</p>
<p>불행히도 우리는 또 다른 문제에 직면합니다: 딥러닝에서 우리는 일반적으로 목적 함수의 2계 도함수에 접근조차 할 수 없습니다. $\mathbf{x} \in \mathbb{R}^d$에 대해 미니배치에서도 2계 도함수는 계산하는 데 $\mathcal{O}(d^2)$ 공간과 작업이 필요할 수 있으므로 실제로는 불가능합니다. Adagrad의 기발한 아이디어는 계산하기 상대적으로 저렴하면서도 효과적인 헤시안(Hessian)의 포착하기 어려운 대각선에 대한 대리물로 기울기 자체의 크기를 사용하는 것입니다.</p>
<p>이것이 왜 작동하는지 확인하기 위해 $\bar{f}(\bar{\mathbf{x}})$를 살펴봅시다. 우리는 다음을 갖습니다.</p>
<p>$$\partial_{\bar{\mathbf{x}}} \bar{f}(\bar{\mathbf{x}}) = \boldsymbol{\Lambda} \bar{\mathbf{x}} + \bar{\mathbf{c}} = \boldsymbol{\Lambda} \left(\bar{\mathbf{x}} - \bar{\mathbf{x}}_0\right),$$</p>
<p>여기서 $\bar{\mathbf{x}}_0$는 $\bar{f}$의 최소화자입니다. 따라서 기울기의 크기는 $\boldsymbol{\Lambda}$와 최적성으로부터의 거리 모두에 의존합니다. $\bar{\mathbf{x}} - \bar{\mathbf{x}}<em>0$가 변하지 않는다면, 이것이 필요한 전부일 것입니다. 결국 이 경우 기울기 $\partial</em>{\bar{\mathbf{x}}} \bar{f}(\bar{\mathbf{x}})$의 크기로 충분하기 때문입니다. AdaGrad는 확률적 경사 하강법 알고리즘이므로, 최적점에서도 0이 아닌 분산을 가진 기울기를 보게 될 것입니다. 결과적으로 우리는 헤시안의 스케일에 대한 저렴한 대리물로 기울기의 분산을 안전하게 사용할 수 있습니다. 철저한 분석은 이 섹션의 범위를 벗어납니다(여러 페이지가 될 것입니다). 자세한 내용은 :cite:<code>Duchi.Hazan.Singer.2011</code>를 참조하십시오.</p>
<h2 id="알고리즘-the-algorithm"><a class="header" href="#알고리즘-the-algorithm">알고리즘 (The Algorithm)</a></h2>
<p>위의 논의를 공식화해 봅시다. 우리는 다음과 같이 과거 기울기 분산을 누적하기 위해 변수 $\mathbf{s}_t$를 사용합니다.</p>
<p>$$\begin{aligned}
\mathbf{g}<em>t &amp; = \partial</em>{\mathbf{w}} l(y_t, f(\mathbf{x}_t, \mathbf{w})), \
\mathbf{s}<em>t &amp; = \mathbf{s}</em>{t-1} + \mathbf{g}_t^2, \
\mathbf{w}<em>t &amp; = \mathbf{w}</em>{t-1} - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}} \cdot \mathbf{g}_t.
\end{aligned}$$</p>
<p>여기서 연산은 좌표별로 적용됩니다. 즉, $\mathbf{v}^2$은 항목 $v_i^2$을 갖습니다. 마찬가지로 $\frac{1}{\sqrt{v}}$은 항목 $\frac{1}{\sqrt{v_i}}$를 갖고 $\mathbf{u} \cdot \mathbf{v}$는 항목 $u_i v_i$를 갖습니다. 이전과 마찬가지로 $\eta$는 학습률이고 $\epsilon$은 $0$으로 나누지 않도록 보장하는 가산 상수입니다. 마지막으로 $\mathbf{s}_0 = \mathbf{0}$으로 초기화합니다.</p>
<p>모멘텀의 경우와 마찬가지로 우리는 보조 변수를 추적해야 하며, 이 경우 좌표당 개별 학습률을 허용합니다. 이는 SGD에 비해 Adagrad의 비용을 크게 증가시키지 않는데, 단순히 주요 비용이 일반적으로 $l(y_t, f(\mathbf{x}_t, \mathbf{w}))$와 그 도함수를 계산하는 것이기 때문입니다.</p>
<p>$\mathbf{s}_t$에 제곱된 기울기를 누적하면 $\mathbf{s}_t$가 본질적으로 선형 속도로 성장한다는 점에 유의하십시오(기울기가 처음에 줄어들기 때문에 실제로는 선형보다 약간 느립니다). 이는 좌표별로 조정되기는 하지만 $\mathcal{O}(t^{-\frac{1}{2}})$ 학습률로 이어집니다. 볼록 문제의 경우 이는 완벽하게 적절합니다. 하지만 딥러닝에서는 학습률을 다소 더 천천히 낮추고 싶을 수도 있습니다. 이로 인해 후속 장에서 논의할 여러 Adagrad 변형이 생겨났습니다. 지금은 이차 볼록 문제에서 이것이 어떻게 작동하는지 봅시다. 이전과 동일한 문제를 사용합니다.</p>
<p>$$f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$</p>
<p>이전에 사용한 것과 동일한 학습률 $\eta = 0.4$를 사용하여 Adagrad를 구현할 것입니다. 보시다시피 독립 변수의 반복 궤적이 더 매끄럽습니다. 그러나 $\boldsymbol{s}_t$의 누적 효과로 인해 학습률이 지속적으로 감소하므로 독립 변수는 반복의 후반 단계에서 많이 이동하지 않습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
def adagrad_2d(x1, x2, s1, s2):
    eps = 1e-6
    g1, g2 = 0.2 * x1, 4 * x2
    s1 += g1 ** 2
    s2 += g2 ** 2
    x1 -= eta / math.sqrt(s1 + eps) * g1
    x2 -= eta / math.sqrt(s2 + eps) * g2
    return x1, x2, s1, s2

def f_2d(x1, x2):
    return 0.1 * x1 ** 2 + 2 * x2 ** 2

eta = 0.4
d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))
</code></pre>
<p>학습률을 $2$로 높이면 훨씬 더 나은 동작을 볼 수 있습니다. 이는 노이즈가 없는 경우에도 학습률 감소가 다소 공격적일 수 있으며 파라미터가 적절하게 수렴하도록 보장해야 함을 이미 시사합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
eta = 2
d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))
</code></pre>
<h2 id="밑바닥부터-구현하기-implementation-from-scratch-10"><a class="header" href="#밑바닥부터-구현하기-implementation-from-scratch-10">밑바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>모멘텀 방법과 마찬가지로 Adagrad는 파라미터와 동일한 모양의 상태 변수를 유지해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def init_adagrad_states(feature_dim):
    s_w = d2l.zeros((feature_dim, 1))
    s_b = d2l.zeros(1)
    return (s_w, s_b)

def adagrad(params, states, hyperparams):
    eps = 1e-6
    for p, s in zip(params, states):
        s[:] += np.square(p.grad)
        p[:] -= hyperparams['lr'] * p.grad / np.sqrt(s + eps)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def init_adagrad_states(feature_dim):
    s_w = d2l.zeros((feature_dim, 1))
    s_b = d2l.zeros(1)
    return (s_w, s_b)

def adagrad(params, states, hyperparams):
    eps = 1e-6
    for p, s in zip(params, states):
        with torch.no_grad():
            s[:] += torch.square(p.grad)
            p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)
        p.grad.data.zero_()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def init_adagrad_states(feature_dim):
    s_w = tf.Variable(d2l.zeros((feature_dim, 1)))
    s_b = tf.Variable(d2l.zeros(1))
    return (s_w, s_b)

def adagrad(params, grads, states, hyperparams):
    eps = 1e-6
    for p, s, g in zip(params, states, grads):
        s[:].assign(s + tf.math.square(g))
        p[:].assign(p - hyperparams['lr'] * g / tf.math.sqrt(s + eps))
</code></pre>
<p>:numref:<code>sec_minibatch_sgd</code>의 실험과 비교하여 모델을 훈련하기 위해 더 큰 학습률을 사용합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(adagrad, init_adagrad_states(feature_dim),
               {'lr': 0.1}, data_iter, feature_dim);
</code></pre>
<h2 id="간결한-구현-concise-implementation-7"><a class="header" href="#간결한-구현-concise-implementation-7">간결한 구현 (Concise Implementation)</a></h2>
<p>알고리즘 <code>adagrad</code>의 <code>Trainer</code> 인스턴스를 사용하여 Gluon에서 Adagrad 알고리즘을 호출할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.train_concise_ch11('adagrad', {'learning_rate': 0.1}, data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
trainer = torch.optim.Adagrad
d2l.train_concise_ch11(trainer, {'lr': 0.1}, data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
trainer = tf.keras.optimizers.Adagrad
d2l.train_concise_ch11(trainer, {'learning_rate' : 0.1}, data_iter)
</code></pre>
<h2 id="요약-summary-57"><a class="header" href="#요약-summary-57">요약 (Summary)</a></h2>
<ul>
<li>Adagrad는 좌표별로 동적으로 학습률을 감소시킵니다.</li>
<li>진전이 얼마나 빨리 달성되는지 조정하는 수단으로 기울기의 크기를 사용합니다 - 큰 기울기를 가진 좌표는 더 작은 학습률로 보상받습니다.</li>
<li>딥러닝 문제에서는 메모리 및 계산 제약으로 인해 정확한 2계 도함수를 계산하는 것이 일반적으로 불가능합니다. 기울기는 유용한 대리물이 될 수 있습니다.</li>
<li>최적화 문제의 구조가 다소 고르지 않다면 Adagrad가 왜곡을 완화하는 데 도움이 될 수 있습니다.</li>
<li>Adagrad는 드물게 발생하는 항에 대해 학습률이 더 천천히 감소해야 하는 희소 특성에 특히 효과적입니다.</li>
<li>딥러닝 문제에서 Adagrad는 가끔 학습률을 줄이는 데 너무 공격적일 수 있습니다. :numref:<code>sec_adam</code>의 맥락에서 이를 완화하기 위한 전략을 논의할 것입니다.</li>
</ul>
<h2 id="연습-문제-exercises-72"><a class="header" href="#연습-문제-exercises-72">연습 문제 (Exercises)</a></h2>
<ol>
<li>직교 행렬 $\mathbf{U}$와 벡터 $\mathbf{c}$에 대해 다음이 성립함을 증명하십시오: $|\mathbf{c} - \mathbf{\delta}|_2 = |\mathbf{U} \mathbf{c} - \mathbf{U} \mathbf{\delta}|_2$. 이것이 왜 변수의 직교 변환 후에 섭동의 크기가 변하지 않음을 의미할까요?</li>
<li>$f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2$에 대해 Adagrad를 시도해 보고, 목적 함수가 45도 회전된 경우, 즉 $f(\mathbf{x}) = 0.1 (x_1 + x_2)^2 + 2 (x_1 - x_2)^2$에 대해서도 시도해 보십시오. 다르게 행동하나요?</li>
<li>행렬 $\mathbf{M}$의 고유값 $\lambda_i$가 적어도 하나의 $j$ 선택에 대해 $|λ_i - Μ_{jj}| ≤ ∑<em>{k
eq j} |Μ</em>{jk}|$를 만족한다는 <a href="https://ko.wikipedia.org/wiki/%EA%B1%B0%EC%8B%9C%EA%B3%A0%EB%A6%B0_%EC%9B%90%ED%8C%90_%EC%A0%95%EB%A6%AC">거시고린 원판 정리(Gerschgorin's circle theorem)</a>를 증명하십시오.</li>
<li>거시고린 정리는 대각 프리컨디셔닝된 행렬 $\textrm{diag}^{-\frac{1}{2}}(\mathbf{M}) \mathbf{M} \textrm{diag}^{-\frac{1}{2}}(\mathbf{M})$의 고유값에 대해 무엇을 알려주나요?</li>
<li>Fashion-MNIST에 적용된 :numref:<code>sec_lenet</code>과 같은 적절한 심층 네트워크에 대해 Adagrad를 시도해 보십시오.</li>
<li>학습률 감쇠를 덜 공격적으로 만들기 위해 Adagrad를 어떻게 수정해야 할까요?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/355">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1072">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1073">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rmsprop"><a class="header" href="#rmsprop">RMSProp</a></h1>
<p>:label:<code>sec_rmsprop</code></p>
<p>:numref:<code>sec_adagrad</code>에서의 주요 문제 중 하나는 학습률이 실질적으로 $\mathcal{O}(t^{-\frac{1}{2}})$의 미리 정의된 스케줄에 따라 감소한다는 것입니다.
While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.</p>
<p>:citet:<code>Tieleman.Hinton.2012</code>는 속도 스케줄링(rate scheduling)을 좌표별 적응형 학습률에서 분리하기 위한 간단한 수정안으로 RMSProp 알고리즘을 제안했습니다. 문제는 Adagrad가 기울기 $\mathbf{g}_t$의 제곱을 상태 벡터 $\mathbf{s}<em>t = \mathbf{s}</em>{t-1} + \mathbf{g}_t^2$에 누적한다는 것입니다. 그 결과 정규화가 부족하여 $\mathbf{s}_t$가 알고리즘이 수렴함에 따라 본질적으로 선형적으로 제한 없이 계속 커지게 됩니다.</p>
<p>이 문제를 해결하는 한 가지 방법은 $\mathbf{s}_t / t$를 사용하는 것입니다. $\mathbf{g}_t$의 합리적인 분포에 대해 이것은 수렴할 것입니다. 불행히도 절차가 값의 전체 궤적을 기억하기 때문에 극한 동작(limit behavior)이 중요해지기 시작할 때까지 매우 오랜 시간이 걸릴 수 있습니다. 대안은 모멘텀 방법에서 사용한 것과 동일한 방식으로 지수 이동 평균(leaky average)을 사용하는 것입니다. 즉, 어떤 파라미터 $\gamma &gt; 0$에 대해 $\mathbf{s}<em>t \leftarrow \gamma \mathbf{s}</em>{t-1} + (1-\gamma) \mathbf{g}_t^2$를 사용하는 것입니다. 다른 모든 부분을 변경하지 않은 채로 두면 RMSProp이 됩니다.</p>
<h2 id="알고리즘-the-algorithm-1"><a class="header" href="#알고리즘-the-algorithm-1">알고리즘 (The Algorithm)</a></h2>
<p>방정식을 자세히 작성해 봅시다.</p>
<p>$$\begin{aligned}
\mathbf{s}<em>t &amp; \leftarrow \gamma \mathbf{s}</em>{t-1} + (1 - \gamma) \mathbf{g}_t^2, \
\mathbf{x}<em>t &amp; \leftarrow \mathbf{x}</em>{t-1} - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}} \odot \mathbf{g}_t.
\end{aligned}$$</p>
<p>상수 $\epsilon &gt; 0$은 일반적으로 0으로 나누는 문제나 과도하게 큰 단계 크기로 고통받지 않도록 $10^{-6}$으로 설정됩니다. 이러한 확장이 주어지면 이제 좌표별로 적용되는 스케일링과는 독립적으로 학습률 $\eta$를 자유롭게 제어할 수 있습니다. 지수 이동 평균의 측면에서 우리는 이전에 모멘텀 방법의 경우에 적용했던 것과 동일한 추론을 적용할 수 있습니다. $\mathbf{s}_t$의 정의를 확장하면 다음과 같습니다.</p>
<p>$$
\begin{aligned}
\mathbf{s}<em>t &amp; = (1 - \gamma) \mathbf{g}<em>t^2 + \gamma \mathbf{s}</em>{t-1} \
&amp; = (1 - \gamma) \left(\mathbf{g}<em>t^2 + \gamma \mathbf{g}</em>{t-1}^2 + \gamma^2 \mathbf{g}</em>{t-2} + \ldots, \right).
\end{aligned}
$$</p>
<p>이전 :numref:<code>sec_momentum</code>에서와 같이 $1 + \gamma + \gamma^2 + \ldots, = \frac{1}{1-\gamma}$를 사용합니다. 따라서 가중치의 합은 1로 정규화되며 관찰의 반감기는 $\gamma^{-1}$입니다. 다양한 $\gamma$ 선택에 대해 지난 40개 타임스텝의 가중치를 시각화해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np, npx

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
import math
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
import math
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
d2l.set_figsize()
gammas = [0.95, 0.9, 0.8, 0.7]
for gamma in gammas:
    x = d2l.numpy(d2l.arange(40))
    d2l.plt.plot(x, (1-gamma) * gamma ** x, label=f'gamma = {gamma:.2f}')
d2l.plt.xlabel('time');
</code></pre>
<h2 id="바닥부터-구현하기-implementation-from-scratch"><a class="header" href="#바닥부터-구현하기-implementation-from-scratch">바닥부터 구현하기 (Implementation from Scratch)</a></h2>
<p>이전과 마찬가지로 이차 함수 $f(\mathbf{x})=0.1x_1^2+2x_2^2$를 사용하여 RMSProp의 궤적을 관찰합니다. :numref:<code>sec_adagrad</code>에서 학습률 0.4의 Adagrad를 사용했을 때, 학습률이 너무 빨리 감소하여 알고리즘의 후반 단계에서 변수가 매우 느리게만 움직였음을 상기하십시오. $\eta$가 별도로 제어되기 때문에 RMSProp에서는 이런 일이 발생하지 않습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def rmsprop_2d(x1, x2, s1, s2):
    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6
    s1 = gamma * s1 + (1 - gamma) * g1 ** 2
    s2 = gamma * s2 + (1 - gamma) * g2 ** 2
    x1 -= eta / math.sqrt(s1 + eps) * g1
    x2 -= eta / math.sqrt(s2 + eps) * g2
    return x1, x2, s1, s2

def f_2d(x1, x2):
    return 0.1 * x1 ** 2 + 2 * x2 ** 2

eta, gamma = 0.4, 0.9
d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))
</code></pre>
<p>Next, we implement RMSProp to be used in a deep network. This is equally straightforward.</p>
<pre><code class="language-{.python .input}">#@tab mxnet,pytorch
def init_rmsprop_states(feature_dim):
    s_w = d2l.zeros((feature_dim, 1))
    s_b = d2l.zeros(1)
    return (s_w, s_b)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def init_rmsprop_states(feature_dim):
    s_w = tf.Variable(d2l.zeros((feature_dim, 1)))
    s_b = tf.Variable(d2l.zeros(1))
    return (s_w, s_b)
</code></pre>
<pre><code class="language-{.python .input}">#@tab mxnet
def rmsprop(params, states, hyperparams):
    gamma, eps = hyperparams['gamma'], 1e-6
    for p, s in zip(params, states):
        s[:] = gamma * s + (1 - gamma) * np.square(p.grad)
        p[:] -= hyperparams['lr'] * p.grad / np.sqrt(s + eps)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def rmsprop(params, states, hyperparams):
    gamma, eps = hyperparams['gamma'], 1e-6
    for p, s in zip(params, states):
        with torch.no_grad():
            s[:] = gamma * s + (1 - gamma) * torch.square(p.grad)
            p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)
        p.grad.data.zero_()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def rmsprop(params, grads, states, hyperparams):
    gamma, eps = hyperparams['gamma'], 1e-6
    for p, s, g in zip(params, states, grads):
        s[:].assign(gamma * s + (1 - gamma) * tf.math.square(g))
        p[:].assign(p - hyperparams['lr'] * g / tf.math.sqrt(s + eps))
</code></pre>
<p>We set the initial learning rate to 0.01 and the weighting term $\gamma$ to 0.9. That is, $\mathbf{s}$ aggregates on average over the past $1/(1-\gamma) = 10$ observations of the square gradient.</p>
<pre><code class="language-{.python .input}">#@tab all
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(rmsprop, init_rmsprop_states(feature_dim),
               {'lr': 0.01, 'gamma': 0.9}, data_iter, feature_dim);
</code></pre>
<h2 id="concise-implementation"><a class="header" href="#concise-implementation">Concise Implementation</a></h2>
<p>Since RMSProp is a rather popular algorithm it is also available in the <code>Trainer</code> instance. All we need to do is instantiate it using an algorithm named <code>rmsprop</code>, assigning $\gamma$ to the parameter <code>gamma1</code>.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.train_concise_ch11('rmsprop', {'learning_rate': 0.01, 'gamma1': 0.9},
                       data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
trainer = torch.optim.RMSprop
d2l.train_concise_ch11(trainer, {'lr': 0.01, 'alpha': 0.9},
                       data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
trainer = tf.keras.optimizers.RMSprop
d2l.train_concise_ch11(trainer, {'learning_rate': 0.01, 'rho': 0.9},
                       data_iter)
</code></pre>
<h2 id="요약-summary-58"><a class="header" href="#요약-summary-58">요약 (Summary)</a></h2>
<ul>
<li>RMSProp은 계수를 스케일링하기 위해 기울기의 제곱을 사용한다는 점에서 Adagrad와 매우 유사합니다.</li>
<li>RMSProp은 모멘텀과 지수 이동 평균을 공유합니다. 그러나 RMSProp은 좌표별 프리컨디셔너를 조정하기 위해 이 기술을 사용합니다.</li>
<li>학습률은 실제로 실험자에 의해 스케줄링되어야 합니다.</li>
<li>계수 $\gamma$는 좌표별 스케일을 조정할 때 이력이 얼마나 긴지 결정합니다.</li>
</ul>
<h2 id="연습-문제-exercises-73"><a class="header" href="#연습-문제-exercises-73">연습 문제 (Exercises)</a></h2>
<ol>
<li>$\gamma = 1$로 설정하면 실험적으로 어떤 일이 발생합니까? 왜 그렇습니까?</li>
<li>$f(\mathbf{x}) = 0.1 (x_1 + x_2)^2 + 2 (x_1 - x_2)^2$를 최소화하도록 최적화 문제를 회전시키십시오. 수렴에 어떤 일이 발생합니까?</li>
<li>Fashion-MNIST에서 훈련하는 것과 같은 실제 머신러닝 문제에서 RMSProp에 어떤 일이 일어나는지 시도해 보십시오. 학습률 조정을 위한 다양한 선택으로 실험해 보십시오.</li>
<li>최적화가 진행됨에 따라 $\gamma$를 조정하고 싶습니까? RMSProp은 이에 얼마나 민감합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/356">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1074">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1075">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adadelta"><a class="header" href="#adadelta">Adadelta</a></h1>
<p>:label:<code>sec_adadelta</code></p>
<p>Adadelta는 AdaGrad(:numref:<code>sec_adagrad</code>)의 또 다른 변형입니다. 주요 차이점은 학습률이 좌표에 적응하는 정도를 줄인다는 점입니다. 더욱이, 전통적으로 Adadelta는 미래의 변화를 위한 보정으로 변화량 자체를 사용하기 때문에 학습률이 없는 것으로 지칭되기도 했습니다. 이 알고리즘은 :citet:<code>Zeiler.2012</code>에서 제안되었습니다. 지금까지의 이전 알고리즘들에 대한 논의를 고려할 때 상당히 간단합니다.</p>
<h2 id="알고리즘-the-algorithm-2"><a class="header" href="#알고리즘-the-algorithm-2">알고리즘 (The Algorithm)</a></h2>
<p>요컨대, Adadelta는 두 개의 상태 변수를 사용합니다. $\mathbf{s}_t$는 기울기 2차 모멘트의 누적 평균(leaky average)을 저장하고, $\Delta\mathbf{x}_t$는 모델 자체의 파라미터 변화량의 2차 모멘트의 누적 평균을 저장합니다. 다른 논문 및 구현과의 호환성을 위해 저자들의 원래 표기법과 명명법을 사용한다는 점에 유의하십시오(모멘텀, Adagrad, RMSProp 및 Adadelta에서 동일한 목적을 수행하는 파라미터를 나타내기 위해 서로 다른 그리스 변수를 사용해야 할 다른 실제 이유는 없습니다).</p>
<p>Adadelta의 기술적 세부 사항은 다음과 같습니다. 파라미터가 $\rho$일 때, :numref:<code>sec_rmsprop</code>와 유사하게 다음과 같은 누적 업데이트를 얻습니다.</p>
<p>$$\begin{aligned}
\mathbf{s}<em>t &amp; = \rho \mathbf{s}</em>{t-1} + (1 - \rho) \mathbf{g}_t^2.
\end{aligned}$$</p>
<p>:numref:<code>sec_rmsprop</code>와의 차이점은 재조정된 기울기 $\mathbf{g}_t'$를 사용하여 업데이트를 수행한다는 것입니다. 즉,</p>
<p>$$\begin{aligned}
\mathbf{x}<em>t  &amp; = \mathbf{x}</em>{t-1} - \mathbf{g}_t'. <br />
\end{aligned}$$</p>
<p>그렇다면 재조정된 기울기 $\mathbf{g}_t'$는 무엇일까요? 다음과 같이 계산할 수 있습니다.</p>
<p>$$\begin{aligned}
\mathbf{g}<em>t' &amp; = \frac{\sqrt{\Delta\mathbf{x}</em>{t-1} + \epsilon}}{\sqrt{{\mathbf{s}_t + \epsilon}}} \odot \mathbf{g}_t, <br />
\end{aligned}$$</p>
<p>여기서 $\Delta \mathbf{x}_{t-1}$은 제곱된 재조정 기울기 $\mathbf{g}<em>t'$의 누적 평균입니다. $\Delta \mathbf{x}</em>{0}$를 $0$으로 초기화하고 각 단계에서 $\mathbf{g}_t'$를 사용하여 업데이트합니다. 즉,</p>
<p>$$\begin{aligned}
\Delta \mathbf{x}<em>t &amp; = \rho \Delta\mathbf{x}</em>{t-1} + (1 - \rho) {\mathbf{g}_t'}^2,
\end{aligned}$$</p>
<p>그리고 수치적 안정성을 유지하기 위해 $\epsilon$($10^{-5}$와 같은 작은 값)이 추가됩니다.</p>
<h2 id="구현-implementation-1"><a class="header" href="#구현-implementation-1">구현 (Implementation)</a></h2>
<p>Adadelta는 각 변수에 대해 $\mathbf{s}_t$와 $\Delta\mathbf{x}_t$라는 두 개의 상태 변수를 유지해야 합니다. 이는 다음과 같은 구현으로 이어집니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()

def init_adadelta_states(feature_dim):
    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)
    delta_w, delta_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)
    return ((s_w, delta_w), (s_b, delta_b))

def adadelta(params, states, hyperparams):
    rho, eps = hyperparams['rho'], 1e-5
    for p, (s, delta) in zip(params, states):
        # [:]를 통한 제자리(in-place) 업데이트
        s[:] = rho * s + (1 - rho) * np.square(p.grad)
        g = (np.sqrt(delta + eps) / np.sqrt(s + eps)) * p.grad
        p[:] -= g
        delta[:] = rho * delta + (1 - rho) * g * g
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

def init_adadelta_states(feature_dim):
    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)
    delta_w, delta_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)
    return ((s_w, delta_w), (s_b, delta_b))

def adadelta(params, states, hyperparams):
    rho, eps = hyperparams['rho'], 1e-5
    for p, (s, delta) in zip(params, states):
        with torch.no_grad():
            # [:]를 통한 제자리 업데이트
            s[:] = rho * s + (1 - rho) * torch.square(p.grad)
            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad
            p[:] -= g
            delta[:] = rho * delta + (1 - rho) * g * g
        p.grad.data.zero_()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf

def init_adadelta_states(feature_dim):
    s_w = tf.Variable(d2l.zeros((feature_dim, 1)))
    s_b = tf.Variable(d2l.zeros(1))
    delta_w = tf.Variable(d2l.zeros((feature_dim, 1)))
    delta_b = tf.Variable(d2l.zeros(1))
    return ((s_w, delta_w), (s_b, delta_b))

def adadelta(params, grads, states, hyperparams):
    rho, eps = hyperparams['rho'], 1e-5
    for p, (s, delta), grad in zip(params, states, grads):
        s[:].assign(rho * s + (1 - rho) * tf.math.square(grad))
        g = (tf.math.sqrt(delta + eps) / tf.math.sqrt(s + eps)) * grad
        p[:].assign(p - g)
        delta[:].assign(rho * delta + (1 - rho) * g * g)
</code></pre>
<p>$\rho = 0.9$를 선택하는 것은 각 파라미터 업데이트에 대해 반감기(half-life time) 10에 해당합니다. 이는 상당히 잘 작동하는 경향이 있습니다. 다음과 같은 동작을 얻습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(adadelta, init_adadelta_states(feature_dim),
               {'rho': 0.9}, data_iter, feature_dim);
</code></pre>
<p>간결한 구현을 위해 고수준 API의 Adadelta 알고리즘을 사용합니다. 이는 훨씬 더 압축된 호출을 위한 다음 한 줄의 코드를 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.train_concise_ch11('adadelta', {'rho': 0.9}, data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
trainer = torch.optim.Adadelta
d2l.train_concise_ch11(trainer, {'rho': 0.9}, data_iter)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# adadelta는 기본 학습률에서 수렴하지 않지만
# lr = 5.0에서는 수렴합니다
trainer = tf.keras.optimizers.Adadelta
d2l.train_concise_ch11(trainer, {'learning_rate':5.0, 'rho': 0.9}, data_iter)
</code></pre>
<h2 id="요약-summary-59"><a class="header" href="#요약-summary-59">요약 (Summary)</a></h2>
<ul>
<li>Adadelta에는 학습률 파라미터가 없습니다. 대신 파라미터 자체의 변화율을 사용하여 학습률을 조정합니다.</li>
<li>Adadelta는 기울기 2차 모멘트와 파라미터 변화량을 저장하기 위해 두 개의 상태 변수가 필요합니다.</li>
<li>Adadelta는 적절한 통계의 실행 추정치를 유지하기 위해 누적 평균(leaky averages)을 사용합니다.</li>
</ul>
<h2 id="연습-문제-exercises-74"><a class="header" href="#연습-문제-exercises-74">연습 문제 (Exercises)</a></h2>
<ol>
<li>$\rho$ 값을 조정해 보십시오. 어떻게 됩니까?</li>
<li>$\mathbf{g}_t'$를 사용하지 않고 알고리즘을 구현하는 방법을 보여주십시오. 이것이 왜 좋은 아이디어일까요?</li>
<li>Adadelta는 정말로 학습률이 없나요? Adadelta를 무너뜨리는 최적화 문제를 찾을 수 있을까요?</li>
<li>Adadelta를 Adagrad 및 RMS prop과 비교하여 수렴 동작을 논의해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/357">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1076">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1077">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="아담-adam"><a class="header" href="#아담-adam">아담 (Adam)</a></h1>
<p>:label:<code>sec_adam</code></p>
<p>이 섹션에 앞서 논의한 내용에서 우리는 효율적인 최적화를 위한 여러 기술을 접했습니다. 여기서 그것들을 자세히 요약해 봅시다:</p>
<ul>
<li>우리는 :numref:<code>sec_sgd</code>가 최적화 문제를 해결할 때 경사 하강법보다 더 효과적임을 보았습니다. 예를 들어 중복 데이터에 대한 고유한 복원력 때문입니다.</li>
<li>우리는 :numref:<code>sec_minibatch_sgd</code>가 벡터화를 통해 얻는 상당한 추가 효율성을 제공하며, 한 미니배치에서 더 큰 관찰 집합을 사용함을 보았습니다. 이것이 효율적인 다중 머신, 다중 GPU 및 전반적인 병렬 처리의 핵심입니다.</li>
<li>:numref:<code>sec_momentum</code>은 수렴을 가속화하기 위해 과거 기울기의 이력을 집계하는 메커니즘을 추가했습니다.</li>
<li>:numref:<code>sec_adagrad</code>는 계산적으로 효율적인 프리컨디셔너(preconditioner)를 허용하기 위해 좌표별 스케일링을 사용했습니다.</li>
<li>:numref:<code>sec_rmsprop</code>은 학습률 조정에서 좌표별 스케일링을 분리했습니다.</li>
</ul>
<p>Adam :cite:<code>Kingma.Ba.2014</code>은 이 모든 기술을 하나의 효율적인 학습 알고리즘으로 결합합니다. 예상대로, 이것은 딥러닝에서 사용하기에 더 견고하고 효과적인 최적화 알고리즘 중 하나로 상당히 인기를 얻은 알고리즘입니다. 하지만 문제가 없는 것은 아닙니다. 특히 :cite:<code>Reddi.Kale.Kumar.2019</code>는 Adam이 열악한 분산 제어로 인해 발산할 수 있는 상황이 있음을 보여줍니다. 후속 연구에서 :citet:<code>Zaheer.Reddi.Sachan.ea.2018</code>은 이러한 문제를 해결하는 Yogi라는 Adam의 핫픽스를 제안했습니다. 이에 대해서는 나중에 더 자세히 설명하겠습니다. 지금은 Adam 알고리즘을 검토해 봅시다.</p>
<h2 id="알고리즘-the-algorithm-3"><a class="header" href="#알고리즘-the-algorithm-3">알고리즘 (The Algorithm)</a></h2>
<p>Adam의 핵심 구성 요소 중 하나는 지수 가중 이동 평균(leaky averaging이라고도 함)을 사용하여 기울기의 모멘텀과 2차 모멘트(second moment)를 모두 추정한다는 것입니다. 즉, 다음과 같은 상태 변수를 사용합니다.</p>
<p>$$\begin{aligned}
\mathbf{v}<em>t &amp; \leftarrow \beta_1 \mathbf{v}</em>{t-1} + (1 - \beta_1) \mathbf{g}_t, \
\mathbf{s}<em>t &amp; \leftarrow \beta_2 \mathbf{s}</em>{t-1} + (1 - \beta_2) \mathbf{g}_t^2.
\end{aligned}$$</p>
<p>여기서 $\beta_1$과 $\beta_2$는 비음수 가중치 파라미터입니다. 일반적인 선택은 $\beta_1 = 0.9$ 및 $\beta_2 = 0.999$입니다. 즉, 분산 추정치는 모멘텀 항보다 <em>훨씬 더 천천히</em> 움직입니다. $\mathbf{v}_0 = \mathbf{s}<em>0 = 0$으로 초기화하면 초기에 더 작은 값으로 치우친 상당한 양의 편향이 발생한다는 점에 유의하십시오. 이는 $\sum</em>{i=0}^{t-1} \beta^i = \frac{1 - \beta^t}{1 - \beta}$라는 사실을 사용하여 항을 재정규화함으로써 해결할 수 있습니다. 그에 대응하여 정규화된 상태 변수는 다음과 같이 주어집니다.</p>
<p>$$\hat{\mathbf{v}}_t = \frac{\mathbf{v}_t}{1 - \beta_1^t} \textrm{ 및 } \hat{\mathbf{s}}_t = \frac{\mathbf{s}_t}{1 - \beta_2^t}.$$</p>
<p>적절한 추정치가 준비되면 이제 업데이트 방정식을 작성할 수 있습니다. 먼저 RMSProp과 매우 유사한 방식으로 기울기를 재조정하여 다음을 얻습니다.</p>
<p>$$\mathbf{g}_t' = \frac{\eta \hat{\mathbf{v}}_t}{\sqrt{\hat{\mathbf{s}}_t} + \epsilon}.$$</p>
<p>RMSProp과 달리 우리의 업데이트는 기울기 자체가 아니라 모멘텀 $\hat{\mathbf{v}}_t$를 사용합니다. 더욱이, 재조정이 $\frac{1}{\sqrt{\hat{\mathbf{s}}_t + \epsilon}}$ 대신 $\frac{1}{\sqrt{\hat{\mathbf{s}}_t} + \epsilon}$를 사용하여 발생한다는 약간의 외관상 차이가 있습니다. 전자가 실제로 약간 더 잘 작동하므로 RMSProp에서 벗어났습니다. 일반적으로 수치적 안정성과 충실도 사이의 좋은 절충안으로 $\epsilon = 10^{-6}$을 선택합니다.</p>
<p>이제 업데이트를 계산하기 위한 모든 조각이 준비되었습니다. 이는 다소 실망스러울 정도로 간단하며 다음과 같은 형태의 간단한 업데이트를 가집니다.</p>
<p>$$\mathbf{x}<em>t \leftarrow \mathbf{x}</em>{t-1} - \mathbf{g}_t'.$$</p>
<p>Adam의 설계를 검토하면 그 영감이 명확해집니다. 모멘텀과 스케일은 상태 변수에서 명확하게 보입니다. 그들의 다소 독특한 정의는 우리가 항의 편향을 제거하도록 강제합니다(이는 약간 다른 초기화 및 업데이트 조건으로 수정될 수 있습니다). 둘째, 두 항의 조합은 RMSProp이 주어지면 꽤 간단합니다. 마지막으로, 명시적인 학습률 $\eta$를 통해 수렴 문제를 해결하기 위해 단계 길이를 제어할 수 있습니다.</p>
<h2 id="구현-implementation-2"><a class="header" href="#구현-implementation-2">구현 (Implementation)</a></h2>
<p>Adam을 처음부터 구현하는 것은 그리 어렵지 않습니다. 편의를 위해 타임스텝 카운터 $t$를 <code>hyperparams</code> 딕셔너리에 저장합니다. 그 외에는 모두 간단합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()

def init_adam_states(feature_dim):
    v_w, v_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)
    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)
    return ((v_w, s_w), (v_b, s_b))

def adam(params, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-6
    for p, (v, s) in zip(params, states):
        v[:] = beta1 * v + (1 - beta1) * p.grad
        s[:] = beta2 * s + (1 - beta2) * np.square(p.grad)
        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
        p[:] -= hyperparams['lr'] * v_bias_corr / (np.sqrt(s_bias_corr) + eps)
    hyperparams['t'] += 1

</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

def init_adam_states(feature_dim):
    v_w, v_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)
    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)
    return ((v_w, s_w), (v_b, s_b))

def adam(params, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-6
    for p, (v, s) in zip(params, states):
        with torch.no_grad():
            v[:] = beta1 * v + (1 - beta1) * p.grad
            s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad)
            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)
                                                       + eps)
        p.grad.data.zero_()
    hyperparams['t'] += 1

</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf

def init_adam_states(feature_dim):
    v_w = tf.Variable(d2l.zeros((feature_dim, 1)))
    v_b = tf.Variable(d2l.zeros(1))
    s_w = tf.Variable(d2l.zeros((feature_dim, 1)))
    s_b = tf.Variable(d2l.zeros(1))
    return ((v_w, s_w), (v_b, s_b))

def adam(params, grads, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-6
    for p, (v, s), grad in zip(params, states, grads):
        v[:].assign(beta1 * v  + (1 - beta1) * grad)
        s[:].assign(beta2 * s + (1 - beta2) * tf.math.square(grad))
        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
        p[:].assign(p - hyperparams['lr'] * v_bias_corr  
                    / tf.math.sqrt(s_bias_corr) + eps)

</code></pre>
<p>이제 모델을 훈련하기 위해 Adam을 사용할 준비가 되었습니다. $\eta = 0.01$의 학습률을 사용합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(adam, init_adam_states(feature_dim),
               {'lr': 0.01, 't': 1}, data_iter, feature_dim);

</code></pre>
<p><code>adam</code>은 Gluon <code>trainer</code> 최적화 라이브러리의 일부로 제공되는 알고리즘 중 하나이므로 더 간결한 구현이 가능합니다. 따라서 Gluon에서의 구현을 위해 구성 파라미터만 전달하면 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.train_concise_ch11('adam', {'learning_rate': 0.01}, data_iter)

</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
trainer = torch.optim.Adam
d2l.train_concise_ch11(trainer, {'lr': 0.01}, data_iter)

</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
trainer = tf.keras.optimizers.Adam
d2l.train_concise_ch11(trainer, {'learning_rate': 0.01}, data_iter)

</code></pre>
<h2 id="yogi"><a class="header" href="#yogi">Yogi</a></h2>
<p>Adam의 문제 중 하나는 $\mathbf{s}_t$의 2차 모멘트 추정치가 급증할 때 볼록(convex) 설정에서도 수렴하지 못할 수 있다는 것입니다. 해결책으로 :citet:<code>Zaheer.Reddi.Sachan.ea.2018</code>은 $\mathbf{s}_t$에 대해 정제된 업데이트(및 초기화)를 제안했습니다. 무슨 일이 일어나고 있는지 이해하기 위해 Adam 업데이트를 다음과 같이 다시 써 봅시다.</p>
<p>$$\mathbf{s}<em>t \leftarrow \mathbf{s}</em>{t-1} + (1 - \beta_2) \left(\mathbf{g}<em>t^2 - \mathbf{s}</em>{t-1}\right).$$</p>
<p>$\mathbf{g}_t^2$의 분산이 크거나 업데이트가 희소할 때마다, $\mathbf{s}_t$는 과거 값을 너무 빨리 잊어버릴 수 있습니다. 이에 대한 가능한 해결책은 $\mathbf{g}<em>t^2 - \mathbf{s}</em>{t-1}$을 $\mathbf{g}_t^2 \odot \mathop{\textrm{sgn}}(\mathbf{g}<em>t^2 - \mathbf{s}</em>{t-1})}$으로 바꾸는 것입니다. 이제 업데이트의 크기는 더 이상 편차의 양에 의존하지 않습니다. 이것은 Yogi 업데이트를 산출합니다.</p>
<p>$$\mathbf{s}<em>t \leftarrow \mathbf{s}</em>{t-1} + (1 - \beta_2) \mathbf{g}_t^2 \odot \mathop{\textrm{sgn}}(\mathbf{g}<em>t^2 - \mathbf{s}</em>{t-1}).$$</p>
<p>저자들은 또한 단순히 초기 포인트별 추정치가 아니라 더 큰 초기 배치에서 모멘텀을 초기화할 것을 권장합니다. 논의에 필수적이지 않고 이것이 없어도 수렴이 꽤 잘 유지되므로 자세한 내용은 생략합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def yogi(params, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-3
    for p, (v, s) in zip(params, states):
        v[:] = beta1 * v + (1 - beta1) * p.grad
        s[:] = s + (1 - beta2) * np.sign(
            np.square(p.grad) - s) * np.square(p.grad)
        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
        p[:] -= hyperparams['lr'] * v_bias_corr / (np.sqrt(s_bias_corr) + eps)
    hyperparams['t'] += 1

data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(yogi, init_adam_states(feature_dim),
               {'lr': 0.01, 't': 1}, data_iter, feature_dim);

</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def yogi(params, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-3
    for p, (v, s) in zip(params, states):
        with torch.no_grad():
            v[:] = beta1 * v + (1 - beta1) * p.grad
            s[:] = s + (1 - beta2) * torch.sign(
                torch.square(p.grad) - s) * torch.square(p.grad)
            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)
                                                       + eps)
        p.grad.data.zero_()
    hyperparams['t'] += 1

data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(yogi, init_adam_states(feature_dim),
               {'lr': 0.01, 't': 1}, data_iter, feature_dim);

</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def yogi(params, grads, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-6
    for p, (v, s), grad in zip(params, states, grads):
        v[:].assign(beta1 * v  + (1 - beta1) * grad)
        s[:].assign(s + (1 - beta2) * tf.math.sign(
                   tf.math.square(grad) - s) * tf.math.square(grad))
        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
        p[:].assign(p - hyperparams['lr'] * v_bias_corr  
                    / tf.math.sqrt(s_bias_corr) + eps)
    hyperparams['t'] += 1

data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(yogi, init_adam_states(feature_dim),
               {'lr': 0.01, 't': 1}, data_iter, feature_dim);

</code></pre>
<h2 id="요약-summary-60"><a class="header" href="#요약-summary-60">요약 (Summary)</a></h2>
<ul>
<li>Adam은 많은 최적화 알고리즘의 기능을 상당히 견고한 업데이트 규칙으로 결합합니다.</li>
<li>RMSProp을 기반으로 만들어진 Adam은 미니배치 확률적 기울기에서 EWMA를 사용합니다.</li>
<li>Adam은 모멘텀과 2차 모멘트를 추정할 때 느린 시작을 조정하기 위해 편향 수정을 사용합니다.</li>
<li>분산이 큰 기울기의 경우 수렴에 문제가 발생할 수 있습니다. 이는 더 큰 미니배치를 사용하거나 $\mathbf{s}_t$에 대해 개선된 추정치를 사용하도록 전환함으로써 수정될 수 있습니다. Yogi는 그러한 대안을 제공합니다.</li>
</ul>
<h2 id="연습-문제-exercises-75"><a class="header" href="#연습-문제-exercises-75">연습 문제 (Exercises)</a></h2>
<ol>
<li>학습률을 조정하고 실험 결과를 관찰하고 분석하십시오.</li>
<li>편향 수정이 필요하지 않도록 모멘텀 및 2차 모멘트 업데이트를 다시 작성할 수 있습니까?</li>
<li>수렴할 때 학습률 $\eta$를 줄여야 하는 이유는 무엇입니까?</li>
<li>Adam은 발산하고 Yogi는 수렴하는 사례를 구성해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/358">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1078">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1079">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="학습률-스케줄링-learning-rate-scheduling"><a class="header" href="#학습률-스케줄링-learning-rate-scheduling">학습률 스케줄링 (Learning Rate Scheduling)</a></h1>
<p>:label:<code>sec_scheduler</code></p>
<p>지금까지 우리는 가중치 벡터를 업데이트하는 <em>속도</em>보다는 어떻게 업데이트할지에 대한 최적화 <em>알고리즘</em>에 주로 초점을 맞추었습니다. 그럼에도 불구하고, 학습률을 조정하는 것은 종종 실제 알고리즘만큼이나 중요합니다. 고려해야 할 몇 가지 측면이 있습니다.</p>
<ul>
<li>가장 명백하게 학습률의 <em>크기</em>가 중요합니다. 너무 크면 최적화가 발산하고, 너무 작으면 훈련하는 데 너무 오래 걸리거나 차선책인 결과에 도달하게 됩니다. 우리는 이전에 문제의 조건 수(condition number)가 중요하다는 것을 보았습니다(자세한 내용은 예: :numref:<code>sec_momentum</code> 참조). 직관적으로 이는 가장 민감하지 않은 방향 대 가장 민감한 방향의 변화량 비율입니다.</li>
<li>둘째, 감쇠 속도도 똑같이 중요합니다. 학습률이 계속 크면 단순히 최소값 주변을 맴돌게 되어 최적점에 도달하지 못할 수 있습니다. :numref:<code>sec_minibatch_sgd</code>에서 이를 어느 정도 자세히 논의했으며 :numref:<code>sec_sgd</code>에서 성능 보장을 분석했습니다. 요컨대, 속도가 감쇠하기를 원하지만 볼록 문제에 좋은 선택인 $\mathcal{O}(t^{-\frac{1}{2}})$보다는 아마도 더 천천히 감쇠하기를 원할 것입니다.</li>
<li>똑같이 중요한 또 다른 측면은 <em>초기화</em>입니다. 이는 파라미터가 처음에 어떻게 설정되는지(자세한 내용은 :numref:<code>sec_numerical_stability</code> 검토)와 처음에 어떻게 진화하는지 모두와 관련이 있습니다. 이것은 *워밍업(warmup)*이라는 이름으로 불리는데, 즉 처음에 얼마나 빨리 솔루션을 향해 이동하기 시작하는지를 나타냅니다. 특히 초기 파라미터 세트가 무작위이기 때문에 초기에 큰 단계를 밟는 것은 유익하지 않을 수 있습니다. 초기 업데이트 방향도 꽤 무의미할 수 있습니다.</li>
<li>마지막으로, 주기적인 학습률 조정을 수행하는 여러 최적화 변형이 있습니다. 이는 현재 장의 범위를 벗어납니다. 독자들에게 :citet:<code>Izmailov.Podoprikhin.Garipov.ea.2018</code>의 세부 사항, 예를 들어 전체 파라미터 <em>경로</em>에 대해 평균을 내어 더 나은 솔루션을 얻는 방법을 검토할 것을 권장합니다.</li>
</ul>
<p>학습률을 관리하는 데 많은 세부 사항이 필요하다는 사실을 고려하여, 대부분의 딥러닝 프레임워크에는 이를 자동으로 처리하는 도구가 있습니다. 현재 장에서는 서로 다른 스케줄이 정확도에 미치는 영향을 검토하고 *학습률 스케줄러(learning rate scheduler)*를 통해 이를 효율적으로 관리하는 방법을 보여줍니다.</p>
<h2 id="장난감-문제-toy-problem"><a class="header" href="#장난감-문제-toy-problem">장난감 문제 (Toy Problem)</a></h2>
<p>쉽게 계산할 수 있을 만큼 저렴하면서도 몇 가지 핵심 측면을 설명하기에 충분히 비자명한 장난감 문제로 시작합니다. 이를 위해 Fashion-MNIST에 적용된 LeNet의 약간 현대화된 버전(<code>sigmoid</code> 대신 <code>relu</code> 활성화, AveragePooling 대신 MaxPooling)을 선택합니다. 또한 성능을 위해 네트워크를 하이브리드화합니다. 대부분의 코드가 표준이므로 더 자세한 설명 없이 기본 사항만 소개합니다. 필요한 경우 :numref:<code>chap_cnn</code>을 다시 참조하십시오.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, lr_scheduler, np, npx
from mxnet.gluon import nn
npx.set_np()

net = nn.HybridSequential()
net.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='relu'),
        nn.MaxPool2D(pool_size=2, strides=2),
        nn.Conv2D(channels=16, kernel_size=5, activation='relu'),
        nn.MaxPool2D(pool_size=2, strides=2),
        nn.Dense(120, activation='relu'),
        nn.Dense(84, activation='relu'),
        nn.Dense(10))
net.hybridize()
loss = gluon.loss.SoftmaxCrossEntropyLoss()
device = d2l.try_gpu()

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

# 코드는 합성곱 신경망 장의 lenet 섹션에 정의된 `d2l.train_ch6`와 거의 동일합니다
def train(net, train_iter, test_iter, num_epochs, loss, trainer, device):
    net.initialize(force_reinit=True, ctx=device, init=init.Xavier())
    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples
        for i, (X, y) in enumerate(train_iter):
            X, y = X.as_in_ctx(device), y.as_in_ctx(device)
            with autograd.record():
                y_hat = net(X)
                l = loss(y_hat, y)
            l.backward()
            trainer.step(X.shape[0])
            metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])
            train_loss = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % 50 == 0:
                animator.add(epoch + i / len(train_iter),
                             (train_loss, train_acc, None))
        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, ')
          f'test acc {test_acc:.3f}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
from torch import nn
from torch.optim import lr_scheduler

def net_fn():
    model = nn.Sequential(
        nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Flatten(),
        nn.Linear(16 * 5 * 5, 120), nn.ReLU(),
        nn.Linear(120, 84), nn.ReLU(),
        nn.Linear(84, 10))

    return model

loss = nn.CrossEntropyLoss()
device = d2l.try_gpu()

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

# 코드는 합성곱 신경망 장의 lenet 섹션에 정의된 `d2l.train_ch6`와 거의 동일합니다
def train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
          scheduler=None):
    net.to(device)
    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])

    for epoch in range(num_epochs):
        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples
        for i, (X, y) in enumerate(train_iter):
            net.train()
            trainer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            trainer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            train_loss = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % 50 == 0:
                animator.add(epoch + i / len(train_iter),
                             (train_loss, train_acc, None))

        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch+1, (None, None, test_acc))

        if scheduler:
            if scheduler.__module__ == lr_scheduler.__name__:
                # PyTorch 내장 스케줄러 사용
                scheduler.step()
            else:
                # 사용자 정의 스케줄러 사용
                for param_group in trainer.param_groups:
                    param_group['lr'] = scheduler(epoch)

    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, ')
          f'test acc {test_acc:.3f}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
import math
from tensorflow.keras.callbacks import LearningRateScheduler

def net():
    return tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu',
                               padding='same'),
        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),
        tf.keras.layers.Conv2D(filters=16, kernel_size=5,
                               activation='relu'),
        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(120, activation='relu'),
        tf.keras.layers.Dense(84, activation='sigmoid'),
        tf.keras.layers.Dense(10)])


batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

# 코드는 합성곱 신경망 장의 lenet 섹션에 정의된 `d2l.train_ch6`와 거의 동일합니다
def train(net_fn, train_iter, test_iter, num_epochs, lr,
              device=d2l.try_gpu(), custom_callback = False):
    device_name = device._device_name
    strategy = tf.distribute.OneDeviceStrategy(device_name)
    with strategy.scope():
        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)
        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
        net = net_fn()
        net.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
    callback = d2l.TrainCallback(net, train_iter, test_iter, num_epochs,
                             device_name)
    if custom_callback is False:
        net.fit(train_iter, epochs=num_epochs, verbose=0,
                callbacks=[callback])
    else:
         net.fit(train_iter, epochs=num_epochs, verbose=0,
                 callbacks=[callback, custom_callback])
    return net
</code></pre>
<p>이 알고리즘을 학습률 $0.3$과 같은 기본 설정으로 호출하고 $30$회 반복하여 훈련하면 어떤 일이 일어나는지 살펴봅시다. 훈련 정확도는 계속 증가하는 반면 테스트 정확도 측면에서의 진행은 어느 시점 이후 정체되는 방식에 주목하십시오. 두 곡선 사이의 간격은 과대적합을 나타냅니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
lr, num_epochs = 0.3, 30
net.initialize(force_reinit=True, ctx=device, init=init.Xavier())
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
lr, num_epochs = 0.3, 30
net = net_fn()
trainer = torch.optim.SGD(net.parameters(), lr=lr)
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
lr, num_epochs = 0.3, 30
train(net, train_iter, test_iter, num_epochs, lr)
</code></pre>
<h2 id="스케줄러-schedulers"><a class="header" href="#스케줄러-schedulers">스케줄러 (Schedulers)</a></h2>
<p>학습률을 조정하는 한 가지 방법은 각 단계에서 명시적으로 설정하는 것입니다. 이는 <code>set_learning_rate</code> 메서드를 통해 편리하게 달성됩니다. 우리는 최적화가 어떻게 진행되는지에 대응하여 매 에폭 후(또는 매 미니배치 후에도) 동적인 방식으로 하향 조정할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
trainer.set_learning_rate(0.1)
print(f'learning rate is now {trainer.learning_rate:.2f}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
lr = 0.1
trainer.param_groups[0]["lr"] = lr
print(f'learning rate is now {trainer.param_groups[0]["lr"]:.2f}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
lr = 0.1
dummy_model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
dummy_model.compile(tf.keras.optimizers.SGD(learning_rate=lr), loss='mse')
print(f'learning rate is now ,', dummy_model.optimizer.lr.numpy())
</code></pre>
<p>보다 일반적으로 우리는 스케줄러를 정의하고 싶어 합니다. 업데이트 횟수와 함께 호출되면 학습률의 적절한 값을 반환합니다. 학습률을 $\eta = \eta_0 (t + 1)^{-\frac{1}{2}}$로 설정하는 간단한 스케줄러를 정의해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
class SquareRootScheduler:
    def __init__(self, lr=0.1):
        self.lr = lr

    def __call__(self, num_update):
        return self.lr * pow(num_update + 1.0, -0.5)
</code></pre>
<p>다양한 값 범위에서 그 동작을 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
scheduler = SquareRootScheduler(lr=0.1)
d2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
</code></pre>
<p>이제 이것이 Fashion-MNIST 훈련에 어떻게 적용되는지 봅시다. 단순히 스케줄러를 훈련 알고리즘의 추가 인수로 제공합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
trainer = gluon.Trainer(net.collect_params(), 'sgd',
                        {'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = net_fn()
trainer = torch.optim.SGD(net.parameters(), lr)
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
      scheduler)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
train(net, train_iter, test_iter, num_epochs, lr,
      custom_callback=LearningRateScheduler(scheduler))
</code></pre>
<p>이것은 이전보다 상당히 잘 작동했습니다. 두 가지가 눈에 띕니다: 곡선이 이전보다 다소 더 매끄러웠습니다. 둘째, 과대적합이 적었습니다. 불행히도 왜 특정 전략이 <em>이론적</em>으로 과대적합을 덜 일으키는지에 대해서는 잘 해결되지 않은 문제입니다. 더 작은 단계 크기가 제로에 더 가깝고 따라서 더 단순한 파라미터로 이어진다는 주장이 있습니다. 그러나 우리가 정말로 조기 종료를 하는 것이 아니라 단순히 학습률을 부드럽게 낮추는 것이기 때문에 이것이 현상을 완전히 설명하지는 못합니다.</p>
<h2 id="정책-policies"><a class="header" href="#정책-policies">정책 (Policies)</a></h2>
<p>학습률 스케줄러의 전체 다양성을 다 다룰 수는 없지만, 아래에서 인기 있는 정책들에 대한 간략한 개요를 제공하고자 합니다. 일반적인 선택은 다항식 감쇠(polynomial decay) 및 구간별 상수(piecewise constant) 스케줄입니다. 그 외에도 코사인 학습률 스케줄이 일부 문제에서 경험적으로 잘 작동하는 것으로 밝혀졌습니다. 마지막으로, 일부 문제에서는 큰 학습률을 사용하기 전에 최적화 프로그램을 워밍업(warmup)하는 것이 유익합니다.</p>
<h3 id="팩터-스케줄러-factor-scheduler"><a class="header" href="#팩터-스케줄러-factor-scheduler">팩터 스케줄러 (Factor Scheduler)</a></h3>
<p>다항식 감쇠의 한 가지 대안은 승법 감쇠(multiplicative decay)일 것입니다. 즉, $\alpha \in (0, 1)$에 대해 $\eta_{t+1} \leftarrow \eta_t \cdot \alpha$입니다. 학습률이 합리적인 하한선 너머로 감쇠하는 것을 방지하기 위해 업데이트 방정식은 종종 $\eta_{t+1} \leftarrow \mathop{\mathrm{max}}(\eta_{\mathrm{min}}, \eta_t \cdot \alpha)$로 수정됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
class FactorScheduler:
    def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.1):
        self.factor = factor
        self.stop_factor_lr = stop_factor_lr
        self.base_lr = base_lr

    def __call__(self, num_update):
        self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)
        return self.base_lr

scheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0)
d2l.plot(d2l.arange(50), [scheduler(t) for t in range(50)])
</code></pre>
<p>이는 MXNet에서 <code>lr_scheduler.FactorScheduler</code> 객체를 통해 내장된 스케줄러로도 달성할 수 있습니다. 워밍업 기간, 워밍업 모드(선형 또는 상수), 원하는 최대 업데이트 횟수 등과 같은 몇 가지 파라미터를 더 취합니다. 앞으로 우리는 적절하게 내장된 스케줄러를 사용하고 여기서 그 기능만 설명할 것입니다. 보시다시피 필요한 경우 자신만의 스케줄러를 구축하는 것은 상당히 간단합니다.</p>
<h3 id="멀티-팩터-스케줄러-multi-factor-scheduler"><a class="header" href="#멀티-팩터-스케줄러-multi-factor-scheduler">멀티 팩터 스케줄러 (Multi Factor Scheduler)</a></h3>
<p>심층 네트워크를 훈련하기 위한 일반적인 전략은 학습률을 구간별 상수로 유지하고 매번 특정 양만큼 감소시키는 것입니다. 즉, 감쇠할 시간 세트(예: $s = {5, 10, 20}$)가 주어지면 $t \in s$일 때마다 $\eta_{t+1} \leftarrow \eta_t \cdot \alpha$로 감소시킵니다. 각 단계에서 값이 절반으로 줄어든다고 가정하면 다음과 같이 이를 구현할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
scheduler = lr_scheduler.MultiFactorScheduler(step=[15, 30], factor=0.5,
                                              base_lr=0.5)
d2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = net_fn()
trainer = torch.optim.SGD(net.parameters(), lr=0.5)
scheduler = lr_scheduler.MultiStepLR(trainer, milestones=[15, 30], gamma=0.5)

def get_lr(trainer, scheduler):
    lr = scheduler.get_last_lr()[0]
    trainer.step()
    scheduler.step()
    return lr

d2l.plot(d2l.arange(num_epochs), [get_lr(trainer, scheduler)
                                  for t in range(num_epochs)])
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
class MultiFactorScheduler:
    def __init__(self, step, factor, base_lr):
        self.step = step
        self.factor = factor
        self.base_lr = base_lr

    def __call__(self, epoch):
        if epoch in self.step:
            self.base_lr = self.base_lr * self.factor
            return self.base_lr
        else:
            return self.base_lr

scheduler = MultiFactorScheduler(step=[15, 30], factor=0.5, base_lr=0.5)
d2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
</code></pre>
<p>이 구간별 상수 학습률 스케줄 뒤에 숨겨진 직관은 가중치 벡터의 분포 측면에서 정상 상태(stationary point)에 도달할 때까지 최적화를 진행하게 하는 것입니다. 그런 다음(그리고 나서야) 좋은 국소 최소값에 대한 더 높은 품질의 대리물을 얻기 위해 속도를 줄입니다. 아래 예제는 이것이 어떻게 점점 더 약간 더 나은 솔루션을 생성할 수 있는지 보여줍니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
trainer = gluon.Trainer(net.collect_params(), 'sgd',
                        {'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
      scheduler)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
train(net, train_iter, test_iter, num_epochs, lr,
      custom_callback=LearningRateScheduler(scheduler))
</code></pre>
<h3 id="코사인-스케줄러-cosine-scheduler"><a class="header" href="#코사인-스케줄러-cosine-scheduler">코사인 스케줄러 (Cosine Scheduler)</a></h3>
<p>다소 당혹스러운 휴리스틱이 :citet:<code>Loshchilov.Hutter.2016</code>에 의해 제안되었습니다. 이는 우리가 처음에 학습률을 너무 급격하게 낮추고 싶지 않을 수 있으며, 더욱이 마지막에 아주 작은 학습률을 사용하여 솔루션을 "정제"하고 싶을 수 있다는 관찰에 기반합니다. 이는 $t \in [0, T]$ 범위의 학습률에 대해 다음과 같은 기능적 형태를 가진 코사인 스타일의 스케줄을 생성합니다.</p>
<p>$$\eta_t = \eta_T + \frac{\eta_0 - \eta_T}{2} \left(1 + \cos(\pi t/T)\right)$$</p>
<p>여기서 $\eta_0$는 초기 학습률이고, $\eta_T$는 시간 $T$에서의 목표 학습률입니다. 더욱이 $t &gt; T$에 대해 우리는 값을 다시 늘리지 않고 단순히 $\eta_T$로 고정합니다. 다음 예제에서는 최대 업데이트 단계 $T = 20$을 설정했습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
scheduler = lr_scheduler.CosineScheduler(max_update=20, base_lr=0.3,
                                         final_lr=0.01)
d2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch, tensorflow
class CosineScheduler:
    def __init__(self, max_update, base_lr=0.01, final_lr=0,
               warmup_steps=0, warmup_begin_lr=0):
        self.base_lr_orig = base_lr
        self.max_update = max_update
        self.final_lr = final_lr
        self.warmup_steps = warmup_steps
        self.warmup_begin_lr = warmup_begin_lr
        self.max_steps = self.max_update - self.warmup_steps

    def get_warmup_lr(self, epoch):
        increase = (self.base_lr_orig - self.warmup_begin_lr) \
                       * float(epoch) / float(self.warmup_steps)
        return self.warmup_begin_lr + increase

    def __call__(self, epoch):
        if epoch &lt; self.warmup_steps:
            return self.get_warmup_lr(epoch)
        if epoch &lt;= self.max_update:
            self.base_lr = self.final_lr + (
                self.base_lr_orig - self.final_lr) * (1 + math.cos(
                math.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2
        return self.base_lr

scheduler = CosineScheduler(max_update=20, base_lr=0.3, final_lr=0.01)
d2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
</code></pre>
<p>컴퓨터 비전의 맥락에서 이 스케줄은 개선된 결과로 이어질 <em>수</em> 있습니다. 하지만 그러한 개선이 보장되지는 않는다는 점에 유의하십시오(아래에서 볼 수 있듯이).</p>
<pre><code class="language-{.python .input}">#@tab mxnet
trainer = gluon.Trainer(net.collect_params(), 'sgd',
                        {'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = net_fn()
trainer = torch.optim.SGD(net.parameters(), lr=0.3)
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
      scheduler)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
train(net, train_iter, test_iter, num_epochs, lr,
      custom_callback=LearningRateScheduler(scheduler))
</code></pre>
<h3 id="워밍업-warmup"><a class="header" href="#워밍업-warmup">워밍업 (Warmup)</a></h3>
<p>어떤 경우에는 파라미터를 초기화하는 것만으로는 좋은 솔루션을 보장하기에 충분하지 않습니다. 이는 특히 불안정한 최적화 문제로 이어질 수 있는 일부 고급 네트워크 디자인에서 문제가 됩니다. 우리는 처음에 발산을 방지하기 위해 충분히 작은 학습률을 선택함으로써 이를 해결할 수 있습니다. 불행히도 이는 진전이 느리다는 것을 의미합니다. 반대로 처음에 큰 학습률은 발산으로 이어집니다.</p>
<p>이 딜레마에 대한 상당히 간단한 수정안은 학습률이 초기 최대값까지 <em>증가</em>하는 워밍업 기간을 사용하고 최적화 프로세스가 끝날 때까지 속도를 낮추는 것입니다. 단순함을 위해 일반적으로 이를 위해 선형 증가를 사용합니다. 이는 아래에 표시된 형태의 스케줄을 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
scheduler = lr_scheduler.CosineScheduler(20, warmup_steps=5, base_lr=0.3,
                                         final_lr=0.01)
d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch, tensorflow
scheduler = CosineScheduler(20, warmup_steps=5, base_lr=0.3, final_lr=0.01)
d2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
</code></pre>
<p>네트워크가 처음에 더 잘 수렴함에 주목하십시오(특히 처음 5 에폭 동안의 성능을 관찰하십시오).</p>
<pre><code class="language-{.python .input}">#@tab mxnet
trainer = gluon.Trainer(net.collect_params(), 'sgd',
                        {'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = net_fn()
trainer = torch.optim.SGD(net.parameters(), lr=0.3)
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
      scheduler)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
train(net, train_iter, test_iter, num_epochs, lr,
      custom_callback=LearningRateScheduler(scheduler))
</code></pre>
<p>워밍업은 모든 스케줄러에 적용될 수 있습니다(코사인뿐만 아니라). 학습률 스케줄에 대한 더 자세한 토론과 더 많은 실험에 대해서는 :cite:<code>Gotmare.Keskar.Xiong.ea.2018</code>도 참조하십시오. 특히 그들은 워밍업 단계가 매우 깊은 네트워크에서 파라미터의 발산 정도를 제한한다는 것을 발견했습니다. 이는 처음에 진전을 이루는 데 가장 많은 시간이 걸리는 네트워크 부분에서 무작위 초기화로 인해 상당한 발산을 예상할 수 있기 때문에 직관적으로 말이 됩니다.</p>
<h2 id="요약-summary-61"><a class="header" href="#요약-summary-61">요약 (Summary)</a></h2>
<ul>
<li>훈련 중에 학습률을 낮추면 정확도가 향상되고 (가장 당혹스럽게도) 모델의 과대적합이 줄어들 수 있습니다.</li>
<li>진행이 정체될 때마다 학습률을 구간별로 감소시키는 것이 실전에서 효과적입니다. 본질적으로 이는 우리가 적절한 솔루션으로 효율적으로 수렴하도록 보장하고, 그런 다음 학습률을 줄임으로써 파라미터의 고유한 분산을 줄이게 합니다.</li>
<li>코사인 스케줄러는 일부 컴퓨터 비전 문제에서 인기가 있습니다. 그러한 스케줄러의 세부 사항은 예: <a href="http://gluon-cv.mxnet.io">GluonCV</a>를 참조하십시오.</li>
<li>최적화 전의 워밍업 기간은 발산을 방지할 수 있습니다.</li>
<li>최적화는 딥러닝에서 여러 목적으로 쓰입니다. 훈련 목적 함수를 최소화하는 것 외에도, 최적화 알고리즘과 학습률 스케줄링의 서로 다른 선택은 (동일한 양의 훈련 오차에 대해) 테스트 세트에서의 일반화 및 과대적합 양을 상당히 다르게 만들 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-76"><a class="header" href="#연습-문제-exercises-76">연습 문제 (Exercises)</a></h2>
<ol>
<li>주어진 고정 학습률에 대한 최적화 동작을 실험해 보십시오. 이 방법으로 얻을 수 있는 최고의 모델은 무엇입니까?</li>
<li>학습률 감소의 지수를 변경하면 수렴이 어떻게 변합니까? 실험의 편의를 위해 <code>PolyScheduler</code>를 사용하십시오.</li>
<li>코사인 스케줄러를 대규모 컴퓨터 비전 문제, 예를 들어 ImageNet 훈련에 적용해 보십시오. 다른 스케줄러와 비교하여 성능에 어떤 영향을 미칩니까?</li>
<li>워밍업은 얼마나 오래 지속되어야 합니까?</li>
<li>최적화와 샘플링을 연결할 수 있습니까? 확률적 경사 랑주뱅 역학(Stochastic Gradient Langevin Dynamics)에 대한 :citet:<code>Welling.Teh.2011</code>의 결과를 사용하여 시작해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/359">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1080">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1081">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="계산-성능-computational-performance"><a class="header" href="#계산-성능-computational-performance">계산 성능 (Computational Performance)</a></h1>
<p>:label:<code>chap_performance</code></p>
<p>딥러닝에서,
데이터셋과 모델은 일반적으로 크며,
이는 많은 계산을 수반합니다.
따라서 계산 성능이 매우 중요합니다.
이 장에서는 계산 성능에 영향을 미치는 주요 요인인
명령형 프로그래밍(imperative programming), 기호 프로그래밍(symbolic programming), 비동기 컴퓨팅(asynchronous computing), 자동 병렬 처리(automatic parallelism), 다중 GPU 계산(multi-GPU computation)에 초점을 맞출 것입니다.
이 장을 공부함으로써, 여러분은 이전 장에서 구현된 모델들의 계산 성능을 더욱 향상시킬 수 있습니다.
예를 들어, 정확도에 영향을 주지 않으면서 훈련 시간을 단축할 수 있습니다.</p>
<pre><code class="language-toc">:maxdepth: 2

hybridize
async-computation
auto-parallelism
hardware
multiple-gpus
multiple-gpus-concise
parameterserver
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="컴파일러와-인터프리터-compilers-and-interpreters"><a class="header" href="#컴파일러와-인터프리터-compilers-and-interpreters">컴파일러와 인터프리터 (Compilers and Interpreters)</a></h1>
<p>:label:<code>sec_hybridize</code></p>
<p>지금까지 이 책은 <code>print</code>, <code>+</code>, <code>if</code>와 같은 문을 사용하여 프로그램의 상태를 변경하는 명령형 프로그래밍(imperative programming)에 집중해 왔습니다. 간단한 명령형 프로그램의 다음 예제를 고려해 보십시오.</p>
<pre><code class="language-{.python .input}">#@tab all
def add(a, b):
    return a + b

def fancy_func(a, b, c, d):
    e = add(a, b)
    f = add(c, d)
    g = add(e, f)
    return g

print(fancy_func(1, 2, 3, 4))
</code></pre>
<p>Python은 <em>인터프리터 언어</em>입니다. 위의 <code>fancy_func</code> 함수를 평가할 때 함수 본문을 구성하는 연산을 <em>순차적으로</em> 수행합니다. 즉, <code>e = add(a, b)</code>를 평가하고 결과를 변수 <code>e</code>로 저장하여 프로그램의 상태를 변경합니다. 다음 두 문 <code>f = add(c, d)</code>와 <code>g = add(e, f)</code>도 유사하게 실행되어 덧셈을 수행하고 결과를 변수로 저장합니다. :numref:<code>fig_compute_graph</code>는 데이터의 흐름을 보여줍니다.</p>
<p><img src="chapter_computational-performance/../img/computegraph.svg" alt="명령형 프로그램의 데이터 흐름." />
:label:<code>fig_compute_graph</code></p>
<p>명령형 프로그래밍은 편리하지만 비효율적일 수 있습니다. 한편으로 <code>fancy_func</code> 전체에서 <code>add</code> 함수가 반복적으로 호출되더라도 Python은 세 번의 함수 호출을 개별적으로 실행합니다. 이들이 가령 GPU(또는 여러 GPU)에서 실행된다면 Python 인터프리터에서 발생하는 오버헤드가 엄청날 수 있습니다. 더욱이 <code>fancy_func</code>의 모든 문이 실행될 때까지 <code>e</code>와 <code>f</code>의 변수 값을 저장해야 합니다. 이는 <code>e = add(a, b)</code>와 <code>f = add(c, d)</code> 문이 실행된 후 프로그램의 다른 부분에서 변수 <code>e</code>와 <code>f</code>가 사용될지 모르기 때문입니다.</p>
<h2 id="기호-프로그래밍-symbolic-programming"><a class="header" href="#기호-프로그래밍-symbolic-programming">기호 프로그래밍 (Symbolic Programming)</a></h2>
<p>대안으로 프로세스가 완전히 정의된 후에만 계산이 수행되는 *기호 프로그래밍(symbolic programming)*을 고려해 보십시오. 이 전략은 Theano와 TensorFlow(후자는 명령형 확장을 획득함)를 포함한 여러 딥러닝 프레임워크에서 사용됩니다. 일반적으로 다음 단계가 포함됩니다:</p>
<ol>
<li>실행할 연산을 정의합니다.</li>
<li>연산을 실행 가능한 프로그램으로 컴파일합니다.</li>
<li>필요한 입력을 제공하고 실행을 위해 컴파일된 프로그램을 호출합니다.</li>
</ol>
<p>이를 통해 상당한 양의 최적화가 가능합니다. 첫째, 많은 경우 Python 인터프리터를 건너뛸 수 있으므로 CPU의 단일 Python 스레드와 결합된 여러 고속 GPU에서 유의미해질 수 있는 성능 병목 현상을 제거합니다.
둘째, 컴파일러는 위의 코드를 <code>print((1 + 2) + (3 + 4))</code> 또는 심지어 <code>print(10)</code>으로 최적화하고 다시 쓸 수 있습니다. 이는 컴파일러가 코드를 기계 명령어로 바꾸기 전에 전체 코드를 볼 수 있기 때문에 가능합니다. 예를 들어 변수가 더 이상 필요하지 않을 때마다 메모리를 해제(또는 아예 할당하지 않음)할 수 있습니다. 또는 코드를 완전히 동등한 조각으로 변환할 수 있습니다.
더 나은 아이디어를 얻으려면 아래의 명령형 프로그래밍 시뮬레이션(결국 Python입니다)을 고려해 보십시오.</p>
<pre><code class="language-{.python .input}">#@tab all
def add_():
    return '''
def add(a, b):
    return a + b
'''

def fancy_func_():
    return '''
def fancy_func(a, b, c, d):
    e = add(a, b)
    f = add(c, d)
    g = add(e, f)
    return g
'''

def evoke_():
    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'

prog = evoke_()
print(prog)
y = compile(prog, '', 'exec')
exec(y)
</code></pre>
<p>명령형(인터프리터) 프로그래밍과 기호 프로그래밍의 차이점은 다음과 같습니다:</p>
<ul>
<li>명령형 프로그래밍이 더 쉽습니다. Python에서 명령형 프로그래밍을 사용하면 코드의 대부분이 직관적이고 작성하기 쉽습니다. 명령형 프로그래밍 코드를 디버깅하는 것도 더 쉽습니다. 모든 관련 중간 변수 값을 얻고 인쇄하거나 Python의 내장 디버깅 도구를 사용하기가 더 쉽기 때문입니다.</li>
<li>기호 프로그래밍은 더 효율적이고 이식하기 쉽습니다. 기호 프로그래밍은 컴파일 중에 코드를 최적화하기 더 쉬우며, 프로그램을 Python에 독립적인 형식으로 이식할 수 있는 능력도 있습니다. 이를 통해 프로그램을 Python이 아닌 환경에서 실행할 수 있으므로 Python 인터프리터와 관련된 잠재적인 성능 문제를 피할 수 있습니다.</li>
</ul>
<h2 id="하이브리드-프로그래밍-hybrid-programming"><a class="header" href="#하이브리드-프로그래밍-hybrid-programming">하이브리드 프로그래밍 (Hybrid Programming)</a></h2>
<p>역사적으로 대부분의 딥러닝 프레임워크는 명령형 또는 기호적 접근 방식 중 하나를 선택합니다. 예를 들어 Theano, TensorFlow(전자의 영감을 받음), Keras, CNTK는 모델을 기호적으로 공식화합니다. 반대로 Chainer와 PyTorch는 명령형 접근 방식을 취합니다. 나중에 버전에서 TensorFlow 2.0과 Keras에 명령형 모드가 추가되었습니다.</p>
<p>:begin_tab:<code>mxnet</code>
Gluon을 설계할 때 개발자들은 두 프로그래밍 패러다임의 장점을 결합하는 것이 가능할지 고려했습니다. 이로 인해 사용자가 순수 명령형 프로그래밍으로 개발하고 디버깅하면서도, 제품 수준의 컴퓨팅 성능과 배포가 필요할 때 대부분의 프로그램을 기호 프로그램으로 변환하여 실행할 수 있게 해주는 하이브리드 모델이 탄생했습니다.</p>
<p>실전에서 이는 <code>HybridBlock</code> 또는 <code>HybridSequential</code> 클래스를 사용하여 모델을 구축함을 의미합니다. 기본적으로 이들 중 어느 것도 명령형 프로그래밍에서 <code>Block</code> 또는 <code>Sequential</code> 클래스가 실행되는 것과 동일한 방식으로 실행됩니다.
<code>HybridSequential</code> 클래스는 <code>HybridBlock</code>의 서브클래스입니다(<code>Sequential</code>이 <code>Block</code>을 상속하는 것과 같습니다). <code>hybridize</code> 함수가 호출되면 Gluon은 모델을 기호 프로그래밍에서 사용되는 형식으로 컴파일합니다. 이를 통해 모델 구현 방식을 희생하지 않고도 계산 집약적인 구성 요소를 최적화할 수 있습니다. 아래에서 순차 모델과 블록에 초점을 맞추어 그 이점을 설명하겠습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
위에서 언급했듯이 PyTorch는 명령형 프로그래밍을 기반으로 하며 동적 계산 그래프를 사용합니다. 기호 프로그래밍의 이식성과 효율성을 활용하기 위해 개발자들은 두 프로그래밍 패러다임의 장점을 결합하는 것이 가능할지 고려했습니다. 이로 인해 사용자가 순수 명령형 프로그래밍을 사용하여 개발하고 디버깅하면서도, 제품 수준의 컴퓨팅 성능과 배포가 필요할 때 대부분의 프로그램을 기호 프로그램으로 변환하여 실행할 수 있게 해주는 torchscript가 탄생했습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
명령형 프로그래밍 패러다임은 이제 TensorFlow 2에서 기본값이 되었으며, 이는 이 언어를 처음 접하는 사람들에게 환영할 만한 변화입니다. 그러나 동일한 기호 프로그래밍 기술과 그에 따른 계산 그래프는 여전히 TensorFlow에 존재하며, 사용하기 쉬운 <code>tf.function</code> 데코레이터를 통해 접근할 수 있습니다. 이는 명령형 프로그래밍 패러다임을 TensorFlow에 가져왔고, 사용자가 더 직관적인 함수를 정의한 다음 TensorFlow 팀이 <a href="https://www.tensorflow.org/api_docs/python/tf/autograph">autograph</a>라고 부르는 기능을 사용하여 자동으로 이를 래핑하고 계산 그래프로 컴파일할 수 있게 해 주었습니다.
:end_tab:</p>
<h2 id="sequential-클래스-하이브리드화하기-hybridizing-the-sequential-class"><a class="header" href="#sequential-클래스-하이브리드화하기-hybridizing-the-sequential-class"><code>Sequential</code> 클래스 하이브리드화하기 (Hybridizing the <code>Sequential</code> Class)</a></h2>
<p>하이브리드화가 어떻게 작동하는지 느끼는 가장 쉬운 방법은 여러 레이어가 있는 심층 네트워크를 고려하는 것입니다. 관례적으로 Python 인터프리터는 CPU 또는 GPU로 전달될 수 있는 명령을 생성하기 위해 모든 레이어에 대한 코드를 실행해야 합니다. 단일(고속) 컴퓨팅 장치의 경우 이는 큰 문제를 일으키지 않습니다. 반면에 AWS P3dn.24xlarge 인스턴스와 같은 고급 8-GPU 서버를 사용한다면 Python은 모든 GPU를 바쁘게 유지하는 데 어려움을 겪을 것입니다. 싱글 스레드 Python 인터프리터가 여기서 병목 현상이 됩니다. <code>Sequential</code>을 <code>HybridSequential</code>로 교체하여 코드의 상당 부분에 대해 이 문제를 어떻게 해결할 수 있는지 봅시다. 간단한 MLP를 정의하는 것으로 시작합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()

# 네트워크 팩토리
def get_net():
    net = nn.HybridSequential()  
    net.add(nn.Dense(256, activation='relu'),
            nn.Dense(128, activation='relu'),
            nn.Dense(2))
    net.initialize()
    return net

x = np.random.normal(size=(1, 512))
net = get_net()
net(x)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn

# 네트워크 팩토리
def get_net():
    net = nn.Sequential(nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 2))
    return net

x = torch.randn(size=(1, 512))
net = get_net()
net(x)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
from tensorflow.keras.layers import Dense

# 네트워크 팩토리
def get_net():
    net = tf.keras.Sequential()
    net.add(Dense(256, input_shape = (512,), activation = "relu"))
    net.add(Dense(128, activation = "relu"))
    net.add(Dense(2, activation = "linear"))
    return net

x = tf.random.normal([1,512])
net = get_net()
net(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
<code>hybridize</code> 함수를 호출함으로써 MLP에서의 계산을 컴파일하고 최적화할 수 있습니다. 모델의 계산 결과는 변경되지 않습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<code>torch.jit.script</code> 함수를 사용하여 모델을 변환함으로써 MLP에서의 계산을 컴파일하고 최적화할 수 있습니다. 모델의 계산 결과는 변경되지 않습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
이전에 TensorFlow에서 구축된 모든 함수는 계산 그래프로 구축되었으므로 기본적으로 JIT 컴파일되었습니다. 그러나 TensorFlow 2.X 및 EagerTensor의 릴리스와 함께 이것은 더 이상 기본 동작이 아닙니다.
우리는 <code>tf.function</code>으로 이 기능을 다시 활성화할 수 있습니다. <code>tf.function</code>은 함수 데코레이터로 더 흔히 사용되지만 아래와 같이 일반 Python 함수처럼 직접 호출할 수도 있습니다. 모델의 계산 결과는 변경되지 않습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net.hybridize()
net(x)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = torch.jit.script(net)
net(x)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
net = tf.function(net)
net(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
이것은 정말 좋아 보입니다: 단순히 블록을 <code>HybridSequential</code>로 지정하고 이전과 동일한 코드를 작성한 후 <code>hybridize</code>를 호출하면 됩니다. 일단 이 일이 발생하면 네트워크는 최적화됩니다(아래에서 성능을 벤치마킹할 것입니다). 불행히도 이것이 모든 레이어에 대해 마법처럼 작동하지는 않습니다. 즉, 레이어가 <code>HybridBlock</code> 클래스 대신 <code>Block</code> 클래스를 상속한다면 최적화되지 않을 것입니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
이것은 정말 좋아 보입니다: 이전과 동일한 코드를 작성하고 단순히 <code>torch.jit.script</code>를 사용하여 모델을 변환하면 됩니다. 일단 이 일이 발생하면 네트워크는 최적화됩니다(아래에서 성능을 벤치마킹할 것입니다).
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
이것은 정말 좋아 보입니다: 이전과 동일한 코드를 작성하고 단순히 <code>tf.function</code>을 사용하여 모델을 변환하면 됩니다. 일단 이 일이 발생하면 네트워크는 TensorFlow의 MLIR 중간 표현에서 계산 그래프로 구축되며 빠른 실행을 위해 컴파일러 수준에서 고도로 최적화됩니다(아래에서 성능을 벤치마킹할 것입니다).
<code>tf.function()</code> 호출에 <code>jit_compile = True</code> 플래그를 명시적으로 추가하면 TensorFlow에서 XLA(Accelerated Linear Algebra) 기능이 활성화됩니다. XLA는 특정 경우에 JIT 컴파일된 코드를 더욱 최적화할 수 있습니다. 그래프 모드 실행은 이 명시적인 정의 없이도 활성화되지만, XLA는 특히 GPU 환경에서 특정 대규모 선형 대수 연산(딥러닝 응용 프로그램에서 보는 것과 같은 맥락)을 훨씬 더 빠르게 만들 수 있습니다.
:end_tab:</p>
<h3 id="하이브리드화에-의한-가속-acceleration-by-hybridization"><a class="header" href="#하이브리드화에-의한-가속-acceleration-by-hybridization">하이브리드화에 의한 가속 (Acceleration by Hybridization)</a></h3>
<p>컴파일을 통해 얻은 성능 향상을 입증하기 위해 하이브리드화 전후의 <code>net(x)</code> 평가에 필요한 시간을 비교합니다. 먼저 이 시간을 측정하기 위한 클래스를 정의합시다. 성능을 측정(및 개선)하기 위해 이 장 전체에서 유용하게 쓰일 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
class Benchmark:
    """실행 시간 측정용."""
    def __init__(self, description='Done'):
        self.description = description

    def __enter__(self):
        self.timer = d2l.Timer()
        return self

    def __exit__(self, *args):
        print(f'{self.description}: {self.timer.stop():.4f} sec')
</code></pre>
<p>:begin_tab:<code>mxnet</code>
이제 네트워크를 두 번 호출할 수 있습니다. 한 번은 하이브리드화 없이, 다른 한 번은 하이브리드화와 함께입니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
이제 네트워크를 두 번 호출할 수 있습니다. 한 번은 torchscript 없이, 다른 한 번은 torchscript와 함께입니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
이제 네트워크를 세 번 호출할 수 있습니다. 한 번은 eager 모드로 실행되고, 한 번은 그래프 모드 실행으로, 그리고 다시 JIT 컴파일된 XLA를 사용하여 실행됩니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = get_net()
with Benchmark('Without hybridization'):
    for i in range(1000): net(x)
    npx.waitall()

net.hybridize()
with Benchmark('With hybridization'):
    for i in range(1000): net(x)
    npx.waitall()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = get_net()
with Benchmark('Without torchscript'):
    for i in range(1000): net(x)

net = torch.jit.script(net)
with Benchmark('With torchscript'):
    for i in range(1000): net(x)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
net = get_net()
with Benchmark('Eager Mode'):
    for i in range(1000): net(x)

net = tf.function(net)
with Benchmark('Graph Mode'):
    for i in range(1000): net(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
위의 결과에서 관찰된 바와 같이, <code>HybridSequential</code> 인스턴스가 <code>hybridize</code> 함수를 호출한 후 기호 프로그래밍의 사용을 통해 컴퓨팅 성능이 향상되었습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
위의 결과에서 관찰된 바와 같이, <code>nn.Sequential</code> 인스턴스가 <code>torch.jit.script</code> 함수를 사용하여 스크립팅된 후 기호 프로그래밍의 사용을 통해 컴퓨팅 성능이 향상되었습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
위의 결과에서 관찰된 바와 같이, <code>tf.keras.Sequential</code> 인스턴스가 <code>tf.function</code> 함수를 사용하여 스크립팅된 후 TensorFlow의 그래프 모드 실행을 통해 컴퓨팅 성능이 향상되었습니다.
:end_tab:</p>
<h3 id="직렬화-serialization"><a class="header" href="#직렬화-serialization">직렬화 (Serialization)</a></h3>
<p>:begin_tab:<code>mxnet</code>
모델을 컴파일하는 것의 이점 중 하나는 모델과 그 파라미터를 디스크에 직렬화(저장)할 수 있다는 것입니다. 이를 통해 선택한 프론트엔드 언어와 독립적인 방식으로 모델을 저장할 수 있습니다. 이를 통해 훈련된 모델을 다른 장치에 배포하고 다른 프론트엔드 프로그래밍 언어를 쉽게 사용할 수 있습니다. 동시에 코드는 명령형 프로그래밍에서 달성할 수 있는 것보다 종종 더 빠릅니다. <code>export</code> 함수가 작동하는 것을 봅시다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
모델을 컴파일하는 것의 이점 중 하나는 모델과 그 파라미터를 디스크에 직렬화(저장)할 수 있다는 것입니다. 이를 통해 선택한 프론트엔드 언어와 독립적인 방식으로 모델을 저장할 수 있습니다. 이를 통해 훈련된 모델을 다른 장치에 배포하고 다른 프론트엔드 프로그래밍 언어를 쉽게 사용할 수 있습니다. 동시에 코드는 명령형 프로그래밍에서 달성할 수 있는 것보다 종종 더 빠릅니다. <code>save</code> 함수가 작동하는 것을 봅시다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
모델을 컴파일하는 것의 이점 중 하나는 모델과 그 파라미터를 디스크에 직렬화(저장)할 수 있다는 것입니다. 이를 통해 선택한 프론트엔드 언어와 독립적인 방식으로 모델을 저장할 수 있습니다. 이를 통해 훈련된 모델을 다른 장치에 배포하고 다른 프론트엔드 프로그래밍 언어를 쉽게 사용하거나 서버에서 훈련된 모델을 실행할 수 있습니다. 동시에 코드는 명령형 프로그래밍에서 달성할 수 있는 것보다 종종 더 빠릅니다.
TensorFlow에서 저장을 가능하게 하는 저수준 API는 <code>tf.saved_model</code>입니다.
<code>saved_model</code> 인스턴스가 작동하는 것을 봅시다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net.export('my_mlp')
!ls -lh my_mlp*
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net.save('my_mlp')
!ls -lh my_mlp*
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
net = get_net()
tf.saved_model.save(net, 'my_mlp')
!ls -lh my_mlp*
</code></pre>
<p>:begin_tab:<code>mxnet</code>
모델은 (큰 이진) 파라미터 파일과 모델 계산을 실행하는 데 필요한 프로그램의 JSON 설명으로 분해됩니다. 파일은 C++, R, Scala, Perl과 같이 Python이나 MXNet에서 지원하는 다른 프론트엔드 언어에서 읽을 수 있습니다. 모델 설명의 처음 몇 줄을 살펴봅시다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
!head my_mlp-symbol.json
</code></pre>
<p>:begin_tab:<code>mxnet</code>
앞서 우리는 <code>hybridize</code> 함수를 호출한 후 모델이 우수한 컴퓨팅 성능과 이식성을 달성할 수 있음을 입증했습니다. 하지만 하이브리드화가 모델의 유연성, 특히 제어 흐름 측면에서 영향을 줄 수 있다는 점에 유의하십시오.</p>
<p>게다가 <code>forward</code> 함수를 사용해야 하는 <code>Block</code> 인스턴스와 달리, <code>HybridBlock</code> 인스턴스의 경우 <code>hybrid_forward</code> 함수를 사용해야 합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class HybridNet(nn.HybridBlock):
    def __init__(self, **kwargs):
        super(HybridNet, self).__init__(**kwargs)
        self.hidden = nn.Dense(4)
        self.output = nn.Dense(2)

    def hybrid_forward(self, F, x):
        print('module F: ', F)
        print('value  x: ', x)
        x = F.npx.relu(self.hidden(x))
        print('result  : ', x)
        return self.output(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
위의 코드는 4개의 은닉 유닛과 2개의 출력을 가진 간단한 네트워크를 구현합니다. <code>hybrid_forward</code> 함수는 추가 인수 <code>F</code>를 취합니다. 코드가 하이브리드화되었는지 여부에 따라 처리를 위해 약간 다른 라이브러리(<code>ndarray</code> 또는 <code>symbol</code>)를 사용하기 때문에 이것이 필요합니다. 두 클래스는 매우 유사한 기능을 수행하며 MXNet이 자동으로 인수를 결정합니다. 무슨 일이 일어나고 있는지 이해하기 위해 함수 호출의 일부로 인수를 인쇄합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = HybridNet()
net.initialize()
x = np.random.normal(size=(1, 3))
net(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
순방향 계산을 반복하면 동일한 출력이 생성됩니다(세부 사항은 생략합니다). 이제 <code>hybridize</code> 함수를 호출하면 어떻게 되는지 봅시다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net.hybridize()
net(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
<code>ndarray</code> 대신 이제 <code>F</code>에 대해 <code>symbol</code> 모듈을 사용합니다. 더욱이 입력이 <code>ndarray</code> 유형이더라도 네트워크를 흐르는 데이터는 이제 컴파일 프로세스의 일부로 <code>symbol</code> 유형으로 변환됩니다. 함수 호출을 반복하면 놀라운 결과가 나옵니다:
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
이것은 우리가 이전에 보았던 것과 매우 다릅니다. <code>hybrid_forward</code>에 정의된 모든 print 문이 생략되었습니다. 실제로 하이브리드화 후 <code>net(x)</code>의 실행은 더 이상 Python 인터프리터를 포함하지 않습니다. 이는 훨씬 더 간소화된 실행과 더 나은 성능을 위해 불필요한 Python 코드(예: print 문)가 생략됨을 의미합니다. 대신 MXNet은 직접 C++ 백엔드를 호출합니다. 또한 일부 함수는 <code>symbol</code> 모듈에서 지원되지 않으며(예: <code>asnumpy</code>), <code>a += b</code> 및 <code>a[:] = a + b</code>와 같은 제자리 연산은 <code>a = a + b</code>로 다시 써야 한다는 점에 유의하십시오. 그럼에도 불구하고 모델 컴파일은 속도가 중요할 때마다 노력할 가치가 있습니다. 그 이점은 모델의 복잡성, CPU 속도, GPU의 속도와 수에 따라 작은 퍼센트 포인트에서 두 배 이상의 속도까지 다양할 수 있습니다.
:end_tab:</p>
<h2 id="요약-summary-62"><a class="header" href="#요약-summary-62">요약 (Summary)</a></h2>
<ul>
<li>명령형 프로그래밍은 제어 흐름이 있는 코드를 작성할 수 있고 Python 소프트웨어 생태계의 상당 부분을 사용할 수 있는 능력이 있기 때문에 새로운 모델을 설계하기 쉽게 만듭니다.</li>
<li>기호 프로그래밍은 프로그램을 지정하고 실행하기 전에 컴파일해야 합니다. 이점은 향상된 성능입니다.</li>
</ul>
<p>:begin_tab:<code>mxnet</code></p>
<ul>
<li>MXNet은 필요에 따라 두 접근 방식의 장점을 결합할 수 있습니다.</li>
<li><code>HybridSequential</code> 및 <code>HybridBlock</code> 클래스에 의해 구축된 모델은 <code>hybridize</code> 함수를 호출하여 명령형 프로그램을 기호 프로그램으로 변환할 수 있습니다.
:end_tab:</li>
</ul>
<h2 id="연습-문제-exercises-77"><a class="header" href="#연습-문제-exercises-77">연습 문제 (Exercises)</a></h2>
<p>:begin_tab:<code>mxnet</code></p>
<ol>
<li>이 섹션의 <code>HybridNet</code> 클래스의 <code>hybrid_forward</code> 함수 첫 번째 줄에 <code>x.asnumpy()</code>를 추가하십시오. 코드를 실행하고 마주치는 오류를 관찰하십시오. 왜 발생합니까?</li>
<li><code>hybrid_forward</code> 함수에 제어 흐름, 즉 Python 문 <code>if</code>와 <code>for</code>를 추가하면 어떻게 됩니까?</li>
<li>이전 장에서 관심 있는 모델들을 검토하십시오. 이들을 재구현하여 계산 성능을 향상시킬 수 있습니까?
:end_tab:</li>
</ol>
<p>:begin_tab:<code>pytorch,tensorflow</code></p>
<ol>
<li>이전 장에서 관심 있는 모델들을 검토하십시오. 이들을 재구현하여 계산 성능을 향상시킬 수 있습니까?
:end_tab:</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/360">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/2490">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/2492">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="비동기-계산-asynchronous-computation"><a class="header" href="#비동기-계산-asynchronous-computation">비동기 계산 (Asynchronous Computation)</a></h1>
<p>:label:<code>sec_async</code></p>
<p>오늘날의 컴퓨터는 고도로 병렬화된 시스템으로, 여러 CPU 코어(종종 코어당 여러 스레드), GPU당 여러 처리 요소, 그리고 종종 장치당 여러 GPU로 구성됩니다. 요컨대, 우리는 종종 다른 장치에서 동시에 많은 다른 것들을 처리할 수 있습니다. 불행히도 Python은 추가적인 도움 없이는 병렬 및 비동기 코드를 작성하기에 좋은 방법이 아닙니다. 결국 Python은 싱글 스레드이며 이는 미래에도 바뀔 가능성이 낮습니다. MXNet 및 TensorFlow와 같은 딥러닝 프레임워크는 성능을 향상시키기 위해 <em>비동기 프로그래밍(asynchronous programming)</em> 모델을 채택하는 반면, PyTorch는 Python 자체 스케줄러를 사용하여 다른 성능 절충안을 제시합니다.
PyTorch의 경우 기본적으로 GPU 연산은 비동기적입니다. GPU를 사용하는 함수를 호출하면 연산이 특정 장치에 대기열로 들어가지만 나중에까지 반드시 실행되지는 않습니다. 이를 통해 CPU 또는 다른 GPU에서의 연산을 포함하여 더 많은 계산을 병렬로 실행할 수 있습니다.</p>
<p>따라서 비동기 프로그래밍이 어떻게 작동하는지 이해하면 계산 요구 사항과 상호 의존성을 사전에 줄여 더 효율적인 프로그램을 개발하는 데 도움이 됩니다. 이를 통해 메모리 오버헤드를 줄이고 프로세서 활용도를 높일 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
import numpy, os, subprocess
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import numpy, os, subprocess
import torch
from torch import nn
</code></pre>
<h2 id="백엔드를-통한-비동기성-asynchrony-via-backend"><a class="header" href="#백엔드를-통한-비동기성-asynchrony-via-backend">백엔드를 통한 비동기성 (Asynchrony via Backend)</a></h2>
<p>:begin_tab:<code>mxnet</code>
워밍업으로 다음 장난감 문제를 고려해 봅시다: 무작위 행렬을 생성하고 곱하고 싶습니다. 차이를 확인하기 위해 NumPy와 <code>mxnet.np</code> 모두에서 수행해 봅시다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
워밍업으로 다음 장난감 문제를 고려해 봅시다: 무작위 행렬을 생성하고 곱하고 싶습니다. 차이를 확인하기 위해 NumPy와 PyTorch 텐서 모두에서 수행해 봅시다.
PyTorch <code>tensor</code>는 GPU에서 정의된다는 점에 유의하십시오.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
with d2l.Benchmark('numpy'):
    for _ in range(10):
        a = numpy.random.normal(size=(1000, 1000))
        b = numpy.dot(a, a)

with d2l.Benchmark('mxnet.np'):
    for _ in range(10):
        a = np.random.normal(size=(1000, 1000))
        b = np.dot(a, a)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# GPU 계산을 위한 워밍업
device = d2l.try_gpu()
a = torch.randn(size=(1000, 1000), device=device)
b = torch.mm(a, a)

with d2l.Benchmark('numpy'):
    for _ in range(10):
        a = numpy.random.normal(size=(1000, 1000))
        b = numpy.dot(a, a)

with d2l.Benchmark('torch'):
    for _ in range(10):
        a = torch.randn(size=(1000, 1000), device=device)
        b = torch.mm(a, a)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
MXNet을 통한 벤치마크 출력은 몇 배나 더 빠릅니다. 둘 다 동일한 프로세서에서 실행되므로 다른 무언가가 진행되고 있음에 틀림없습니다.
반환하기 전에 MXNet이 모든 백엔드 계산을 완료하도록 강제하면 이전에 발생한 일을 알 수 있습니다: 프론트엔드가 Python에 제어권을 반환하는 동안 백엔드에서 계산이 실행됩니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
PyTorch를 통한 벤치마크 출력은 몇 배나 더 빠릅니다.
NumPy 내적은 CPU 프로세서에서 실행되는 반면 PyTorch 행렬 곱셈은 GPU에서 실행되므로 후자가 훨씬 더 빠를 것으로 예상됩니다. 그러나 거대한 시간 차이는 다른 무언가가 진행되고 있음을 시사합니다.
기본적으로 PyTorch에서 GPU 연산은 비동기적입니다.
반환하기 전에 PyTorch가 모든 계산을 완료하도록 강제하면 이전에 발생한 일을 알 수 있습니다: 프론트엔드가 Python에 제어권을 반환하는 동안 백엔드에서 계산이 실행되고 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
with d2l.Benchmark():
    for _ in range(10):
        a = np.random.normal(size=(1000, 1000))
        b = np.dot(a, a)
    npx.waitall()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
with d2l.Benchmark():
    for _ in range(10):
        a = torch.randn(size=(1000, 1000), device=device)
        b = torch.mm(a, a)
    torch.cuda.synchronize(device)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
넓게 말해서, MXNet은 Python 등을 통해 사용자와 직접 상호 작용하기 위한 프론트엔드와 시스템이 계산을 수행하는 데 사용하는 백엔드를 가지고 있습니다.
:numref:<code>fig_frontends</code>에 표시된 것처럼, 사용자는 Python, R, Scala, C++와 같은 다양한 프론트엔드 언어로 MXNet 프로그램을 작성할 수 있습니다. 사용된 프론트엔드 프로그래밍 언어에 관계없이, MXNet 프로그램의 실행은 주로 C++ 구현의 백엔드에서 발생합니다. 프론트엔드 언어에서 발행된 연산은 실행을 위해 백엔드로 전달됩니다.
백엔드는 대기 중인 작업을 지속적으로 수집하고 실행하는 자체 스레드를 관리합니다. 이것이 작동하려면 백엔드가 계산 그래프의 다양한 단계 간의 의존성을 추적할 수 있어야 한다는 점에 유의하십시오. 따라서 서로 의존하는 연산을 병렬화하는 것은 불가능합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
넓게 말해서, PyTorch는 Python 등을 통해 사용자와 직접 상호 작용하기 위한 프론트엔드와 시스템이 계산을 수행하는 데 사용하는 백엔드를 가지고 있습니다.
:numref:<code>fig_frontends</code>에 표시된 것처럼, 사용자는 Python 및 C++와 같은 다양한 프론트엔드 언어로 PyTorch 프로그램을 작성할 수 있습니다. 사용된 프론트엔드 프로그래밍 언어에 관계없이, PyTorch 프로그램의 실행은 주로 C++ 구현의 백엔드에서 발생합니다. 프론트엔드 언어에서 발행된 연산은 실행을 위해 백엔드로 전달됩니다.
백엔드는 대기 중인 작업을 지속적으로 수집하고 실행하는 자체 스레드를 관리합니다.
이것이 작동하려면 백엔드가 계산 그래프의 다양한 단계 간의 의존성을 추적할 수 있어야 한다는 점에 유의하십시오.
따라서 서로 의존하는 연산을 병렬화하는 것은 불가능합니다.
:end_tab:</p>
<p><img src="chapter_computational-performance/../img/frontends.png" alt="프로그래밍 언어 프론트엔드 및 딥러닝 프레임워크 백엔드." />
:width:<code>300px</code>
:label:<code>fig_frontends</code></p>
<p>의존성 그래프를 좀 더 잘 이해하기 위해 다른 장난감 예제를 살펴봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.ones((1, 2))
y = np.ones((1, 2))
z = x * y + 2
z
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.ones((1, 2), device=device)
y = torch.ones((1, 2), device=device)
z = x * y + 2
z
</code></pre>
<p><img src="chapter_computational-performance/../img/asyncgraph.svg" alt="백엔드는 계산 그래프의 다양한 단계 간의 의존성을 추적합니다." />
:label:<code>fig_asyncgraph</code></p>
<p>위의 코드 스니펫은 :numref:<code>fig_asyncgraph</code>에서도 설명되어 있습니다.
Python 프론트엔드 스레드가 처음 세 명령문 중 하나를 실행할 때마다, 단순히 작업을 백엔드 대기열로 반환합니다. 마지막 명령문의 결과를 <em>인쇄</em>해야 할 때, Python 프론트엔드 스레드는 C++ 백엔드 스레드가 변수 <code>z</code>의 결과 계산을 완료할 때까지 기다립니다. 이 디자인의 한 가지 이점은 Python 프론트엔드 스레드가 실제 계산을 수행할 필요가 없다는 것입니다. 따라서 Python의 성능에 관계없이 프로그램의 전체 성능에 거의 영향을 미치지 않습니다. :numref:<code>fig_threading</code>은 프론트엔드와 백엔드가 상호 작용하는 방식을 보여줍니다.</p>
<p><img src="chapter_computational-performance/../img/threading.svg" alt="프론트엔드와 백엔드의 상호 작용." />
:label:<code>fig_threading</code></p>
<h2 id="장벽과-차단기-barriers-and-blockers"><a class="header" href="#장벽과-차단기-barriers-and-blockers">장벽과 차단기 (Barriers and Blockers)</a></h2>
<p>:begin_tab:<code>mxnet</code>
Python이 완료를 기다리도록 강제하는 여러 연산이 있습니다:</p>
<ul>
<li>가장 명백한 것은 <code>npx.waitall()</code>로, 계산 명령이 언제 발행되었는지에 관계없이 모든 계산이 완료될 때까지 기다립니다. 실제로 이 연산자를 사용하면 성능이 저하될 수 있으므로 절대적으로 필요한 경우가 아니면 사용하지 않는 것이 좋습니다.</li>
<li>특정 변수가 사용 가능해질 때까지만 기다리고 싶다면 <code>z.wait_to_read()</code>를 호출할 수 있습니다. 이 경우 MXNet은 변수 <code>z</code>가 계산될 때까지 Python으로의 반환을 차단합니다. 다른 계산은 그 후에 계속될 수 있습니다.</li>
</ul>
<p>이것이 실제로 어떻게 작동하는지 봅시다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
with d2l.Benchmark('waitall'):
    b = np.dot(a, a)
    npx.waitall()

with d2l.Benchmark('wait_to_read'):
    b = np.dot(a, a)
    b.wait_to_read()
</code></pre>
<p>:begin_tab:<code>mxnet</code>
두 연산 모두 완료하는 데 거의 같은 시간이 걸립니다. 명백한 차단 연산 외에도 <em>암시적</em> 차단기를 인식하는 것이 좋습니다. 변수를 인쇄하려면 분명히 변수가 사용 가능해야 하므로 차단기입니다. 마지막으로, <code>z.asnumpy()</code>를 통한 NumPy로의 변환과 <code>z.item()</code>을 통한 스칼라로의 변환은 차단적입니다. NumPy에는 비동기성 개념이 없기 때문입니다. <code>print</code> 함수와 마찬가지로 값에 대한 액세스가 필요합니다.</p>
<p>MXNet의 범위에서 NumPy로 그리고 다시 소량의 데이터를 빈번하게 복사하면 효율적인 코드의 성능을 파괴할 수 있습니다. 각 연산은 다른 어떤 작업을 수행하기 <em>전에</em> 관련 항을 얻는 데 필요한 모든 중간 결과를 계산 그래프가 평가해야 하기 때문입니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
with d2l.Benchmark('numpy conversion'):
    b = np.dot(a, a)
    b.asnumpy()

with d2l.Benchmark('scalar conversion'):
    b = np.dot(a, a)
    b.sum().item()
</code></pre>
<h2 id="계산-개선-improving-computation"><a class="header" href="#계산-개선-improving-computation">계산 개선 (Improving Computation)</a></h2>
<p>:begin_tab:<code>mxnet</code>
멀티 스레드가 많은 시스템(일반적인 노트북조차도 4개 이상의 스레드를 가지고 있으며 멀티 소켓 서버에서는 이 숫자가 256을 초과할 수 있음)에서 스케줄링 연산의 오버헤드는 상당할 수 있습니다. 이것이 계산과 스케줄링이 비동기적으로 병렬로 발생하는 것이 매우 바람직한 이유입니다. 그렇게 함으로써 얻는 이점을 설명하기 위해, 변수를 1씩 여러 번 증가시키는 작업을 순차적으로 또는 비동기적으로 수행하면 어떻게 되는지 봅시다. 각 덧셈 사이에 <code>wait_to_read</code> 장벽을 삽입하여 동기식 실행을 시뮬레이션합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
with d2l.Benchmark('synchronous'):
    for _ in range(10000):
        y = x + 1
        y.wait_to_read()

with d2l.Benchmark('asynchronous'):
    for _ in range(10000):
        y = x + 1
    npx.waitall()
</code></pre>
<p>:begin_tab:<code>mxnet</code>
Python 프론트엔드 스레드와 C++ 백엔드 스레드 간의 약간 단순화된 상호 작용은 다음과 같이 요약될 수 있습니다:</p>
<ol>
<li>프론트엔드가 백엔드에 계산 작업 <code>y = x + 1</code>을 대기열에 삽입하도록 명령합니다.</li>
<li>백엔드는 대기열에서 계산 작업을 수신하고 실제 계산을 수행합니다.</li>
<li>백엔드는 계산 결과를 프론트엔드에 반환합니다.
이 세 단계의 기간을 각각 $t_1, t_2, t_3$라고 가정합시다. 비동기 프로그래밍을 사용하지 않으면 10,000번의 계산을 수행하는 데 걸리는 총 시간은 약 $10000 (t_1+ t_2 + t_3)$입니다. 비동기 프로그래밍을 사용하면 프론트엔드가 각 루프에 대해 백엔드가 계산 결과를 반환할 때까지 기다릴 필요가 없으므로 10,000번의 계산을 수행하는 데 걸리는 총 시간을 $t_1 + 10000 t_2 + t_3$으로 줄일 수 있습니다($10000 t_2 &gt; 9999t_1$ 가정).
:end_tab:</li>
</ol>
<h2 id="요약-summary-63"><a class="header" href="#요약-summary-63">요약 (Summary)</a></h2>
<ul>
<li>딥러닝 프레임워크는 Python 프론트엔드를 실행 백엔드에서 분리할 수 있습니다. 이를 통해 명령을 백엔드에 빠르게 비동기적으로 삽입하고 관련 병렬 처리를 가능하게 합니다.</li>
<li>비동기성은 상당히 반응이 빠른 프론트엔드로 이어집니다. 그러나 작업 대기열을 너무 많이 채우면 과도한 메모리 소비로 이어질 수 있으므로 주의하십시오. 프론트엔드와 백엔드를 대략적으로 동기화된 상태로 유지하기 위해 각 미니배치마다 동기화하는 것이 좋습니다.</li>
<li>칩 공급업체는 딥러닝의 효율성에 대해 훨씬 더 세밀한 통찰력을 얻기 위해 정교한 성능 분석 도구를 제공합니다.</li>
</ul>
<p>:begin_tab:<code>mxnet</code></p>
<ul>
<li>MXNet의 메모리 관리에서 Python으로의 변환은 백엔드가 특정 변수가 준비될 때까지 기다리도록 강제한다는 사실을 인지하십시오. <code>print</code>, <code>asnumpy</code>, <code>item</code>과 같은 함수는 모두 이 효과를 가집니다. 이는 바람직할 수 있지만 부주의한 동기화 사용은 성능을 망칠 수 있습니다.
:end_tab:</li>
</ul>
<h2 id="연습-문제-exercises-78"><a class="header" href="#연습-문제-exercises-78">연습 문제 (Exercises)</a></h2>
<p>:begin_tab:<code>mxnet</code></p>
<ol>
<li>위에서 비동기 계산을 사용하면 10,000번의 계산을 수행하는 데 필요한 총 시간을 $t_1 + 10000 t_2 + t_3$으로 줄일 수 있다고 언급했습니다. 왜 여기서 $10000 t_2 &gt; 9999 t_1$이라고 가정해야 합니까?</li>
<li><code>waitall</code>과 <code>wait_to_read</code> 사이의 차이를 측정하십시오. 힌트: 여러 명령을 수행하고 중간 결과에 대해 동기화하십시오.
:end_tab:</li>
</ol>
<p>:begin_tab:<code>pytorch</code></p>
<ol>
<li>CPU에서 이 섹션의 동일한 행렬 곱셈 연산을 벤치마킹하십시오. 여전히 백엔드를 통한 비동기성을 관찰할 수 있습니까?
:end_tab:</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/361">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/2564">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="자동-병렬-처리-automatic-parallelism"><a class="header" href="#자동-병렬-처리-automatic-parallelism">자동 병렬 처리 (Automatic Parallelism)</a></h1>
<p>:label:<code>sec_auto_para</code></p>
<p>MXNet 및 PyTorch와 같은 딥러닝 프레임워크는 백엔드에서 자동으로 계산 그래프를 구축합니다.
계산 그래프를 사용하면 시스템은 모든 의존성을 인식하고, 속도를 높이기 위해 상호 의존적이지 않은 여러 작업을 선택적으로 병렬로 실행할 수 있습니다.
예를 들어, :numref:<code>sec_async</code>의 :numref:<code>fig_asyncgraph</code>는 두 변수를 독립적으로 초기화합니다. 결과적으로 시스템은 이들을 병렬로 실행하도록 선택할 수 있습니다.</p>
<p>일반적으로 단일 연산자는 모든 CPU의 모든 계산 리소스 또는 단일 GPU를 사용합니다.
예를 들어, <code>dot</code> 연산자는 단일 기계에 여러 CPU 프로세서가 있더라도 모든 CPU의 모든 코어(및 스레드)를 사용합니다.
단일 GPU에도 동일하게 적용됩니다.
따라서 병렬 처리는 단일 장치 컴퓨터에서는 그렇게 유용하지 않습니다.
장치가 여러 개인 경우 상황이 더 중요해집니다.
병렬 처리는 일반적으로 여러 GPU 간에 가장 관련이 있지만, 로컬 CPU를 추가하면 성능이 약간 향상됩니다.
예를 들어, GPU와 CPU를 결합하여 컴퓨터 비전 모델을 훈련하는 데 중점을 둔 :citet:<code>Hadjis.Zhang.Mitliagkas.ea.2016</code>을 참조하십시오.
자동으로 병렬화되는 프레임워크의 편리함을 통해 우리는 몇 줄의 Python 코드로 동일한 목표를 달성할 수 있습니다.
보다 광범위하게, 자동 병렬 계산에 대한 우리의 논의는 CPU와 GPU를 모두 사용하는 병렬 계산뿐만 아니라 계산과 통신의 병렬화에 중점을 둡니다.</p>
<p>이 섹션의 실험을 실행하려면 적어도 두 개의 GPU가 필요합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
</code></pre>
<h2 id="gpu에서의-병렬-계산-parallel-computation-on-gpus"><a class="header" href="#gpu에서의-병렬-계산-parallel-computation-on-gpus">GPU에서의 병렬 계산 (Parallel Computation on GPUs)</a></h2>
<p>테스트할 기준 작업 부하를 정의하는 것부터 시작하겠습니다: 아래의 <code>run</code> 함수는 <code>x_gpu1</code>과 <code>x_gpu2</code> 두 변수에 할당된 데이터를 사용하여 선택한 장치에서 10번의 행렬-행렬 곱셈을 수행합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
devices = d2l.try_all_gpus()
def run(x):
    return [x.dot(x) for _ in range(50)]

x_gpu1 = np.random.uniform(size=(4000, 4000), ctx=devices[0])
x_gpu2 = np.random.uniform(size=(4000, 4000), ctx=devices[1])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
devices = d2l.try_all_gpus()
def run(x):
    return [x.mm(x) for _ in range(50)]

x_gpu1 = torch.rand(size=(4000, 4000), device=devices[0])
x_gpu2 = torch.rand(size=(4000, 4000), device=devices[1])
</code></pre>
<p>:begin_tab:<code>mxnet</code>
이제 데이터에 함수를 적용합니다. 캐싱이 결과에 영향을 미치지 않도록 측정 전에 두 장치 중 하나에서 단일 패스를 수행하여 장치를 워밍업합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
이제 데이터에 함수를 적용합니다. 캐싱이 결과에 영향을 미치지 않도록 측정 전에 두 장치 중 하나에서 단일 패스를 수행하여 장치를 워밍업합니다.
<code>torch.cuda.synchronize()</code>는 CUDA 장치의 모든 스트림에 있는 모든 커널이 완료될 때까지 기다립니다.
동기화가 필요한 장치인 <code>device</code> 인수를 받습니다.
장치 인수가 <code>None</code>(기본값)인 경우 <code>current_device()</code>에 의해 제공되는 현재 장치를 사용합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
run(x_gpu1)  # 두 장치 모두 워밍업
run(x_gpu2)
npx.waitall()

with d2l.Benchmark('GPU1 time'):
    run(x_gpu1)
    npx.waitall()

with d2l.Benchmark('GPU2 time'):
    run(x_gpu2)
    npx.waitall()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
run(x_gpu1)
run(x_gpu2)  # 모든 장치 워밍업
torch.cuda.synchronize(devices[0])
torch.cuda.synchronize(devices[1])

with d2l.Benchmark('GPU1 time'):
    run(x_gpu1)
    torch.cuda.synchronize(devices[0])

with d2l.Benchmark('GPU2 time'):
    run(x_gpu2)
    torch.cuda.synchronize(devices[1])
</code></pre>
<p>:begin_tab:<code>mxnet</code>
두 작업 사이의 <code>waitall</code> 문을 제거하면 시스템은 두 장치 모두에서 자동으로 계산을 병렬화할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
두 작업 사이의 <code>synchronize</code> 문을 제거하면 시스템은 두 장치 모두에서 자동으로 계산을 병렬화할 수 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
with d2l.Benchmark('GPU1 &amp; GPU2'):
    run(x_gpu1)
    run(x_gpu2)
    npx.waitall()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
with d2l.Benchmark('GPU1 &amp; GPU2'):
    run(x_gpu1)
    run(x_gpu2)
    torch.cuda.synchronize()
</code></pre>
<p>위의 경우 총 실행 시간은 각 부분의 합보다 작습니다. 딥러닝 프레임워크가 사용자를 대신하여 정교한 코드 없이도 두 GPU 장치에서 자동으로 계산을 스케줄링하기 때문입니다.</p>
<h2 id="병렬-계산과-통신-parallel-computation-and-communication"><a class="header" href="#병렬-계산과-통신-parallel-computation-and-communication">병렬 계산과 통신 (Parallel Computation and Communication)</a></h2>
<p>많은 경우 CPU와 GPU 사이 또는 서로 다른 GPU 사이와 같이 서로 다른 장치 간에 데이터를 이동해야 합니다.
예를 들어,
여러 가속기 카드에서 기울기를 집계해야 하는 분산 최적화를 수행하려는 경우에 발생합니다.
GPU에서 계산한 다음 결과를 다시 CPU로 복사하여 이를 시뮬레이션해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def copy_to_cpu(x):
    return [y.copyto(npx.cpu()) for y in x]

with d2l.Benchmark('Run on GPU1'):
    y = run(x_gpu1)
    npx.waitall()

with d2l.Benchmark('Copy to CPU'):
    y_cpu = copy_to_cpu(y)
    npx.waitall()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def copy_to_cpu(x, non_blocking=False):
    return [y.to('cpu', non_blocking=non_blocking) for y in x]

with d2l.Benchmark('Run on GPU1'):
    y = run(x_gpu1)
    torch.cuda.synchronize()

with d2l.Benchmark('Copy to CPU'):
    y_cpu = copy_to_cpu(y)
    torch.cuda.synchronize()
</code></pre>
<p>:begin_tab:<code>mxnet</code>
이것은 다소 비효율적입니다. 리스트의 나머지가 여전히 계산되는 동안 <code>y</code>의 일부를 이미 CPU로 복사하기 시작할 수 있다는 점에 유의하십시오.
이 상황은 예를 들어 미니배치에서 기울기를 계산할 때 발생합니다. 일부 파라미터의 기울기는 다른 것보다 일찍 사용할 수 있습니다.
따라서 GPU가 여전히 실행 중인 동안 PCI-Express 버스 대역폭을 사용하기 시작하는 것이 우리에게 유리합니다.
두 부분 사이의 <code>waitall</code>을 제거하면 이 시나리오를 시뮬레이션할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
이것은 다소 비효율적입니다. 리스트의 나머지가 여전히 계산되는 동안 <code>y</code>의 일부를 이미 CPU로 복사하기 시작할 수 있다는 점에 유의하십시오.
이 상황은 예를 들어 미니배치에서 (역전파) 기울기를 계산할 때 발생합니다. 일부 파라미터의 기울기는 다른 것보다 일찍 사용할 수 있습니다.
따라서 GPU가 여전히 실행 중인 동안 PCI-Express 버스 대역폭을 사용하기 시작하는 것이 우리에게 유리합니다.
PyTorch에서는 <code>to()</code> 및 <code>copy_()</code>와 같은 여러 함수가 명시적인 <code>non_blocking</code> 인수를 허용하며, 이를 통해 호출자는 불필요한 경우 동기화를 우회할 수 있습니다.
<code>non_blocking=True</code>로 설정하면 이 시나리오를 시뮬레이션할 수 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
with d2l.Benchmark('Run on GPU1 and copy to CPU'):
    y = run(x_gpu1)
    y_cpu = copy_to_cpu(y)
    npx.waitall()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
with d2l.Benchmark('Run on GPU1 and copy to CPU'):
    y = run(x_gpu1)
    y_cpu = copy_to_cpu(y, True)
    torch.cuda.synchronize()
</code></pre>
<p>두 연산에 필요한 총 시간은 (예상대로) 각 부분의 합보다 작습니다.
이 작업은 다른 리소스인 CPU와 GPU 사이의 버스를 사용하므로 병렬 계산과는 다릅니다.
사실 우리는 두 장치 모두에서 계산하고 통신하는 것을 동시에 할 수 있습니다.
위에서 언급했듯이 계산과 통신 사이에는 의존성이 있습니다: <code>y[i]</code>가 CPU로 복사되기 전에 먼저 계산되어야 합니다.
다행히 시스템은 총 실행 시간을 줄이기 위해 <code>y[i]</code>를 계산하는 동안 <code>y[i-1]</code>을 복사할 수 있습니다.</p>
<p>CPU와 두 개의 GPU에서 훈련할 때 간단한 2개 레이어 MLP의 계산 그래프와 그 의존성을 보여주는 그림 :numref:<code>fig_twogpu</code>로 마무리합니다.
이로 인해 발생하는 병렬 프로그램을 수동으로 스케줄링하는 것은 꽤 고통스러울 것입니다.
이것이 최적화를 위해 그래프 기반 컴퓨팅 백엔드를 갖는 것이 유리한 부분입니다.</p>
<p><img src="chapter_computational-performance/../img/twogpu.svg" alt="CPU와 두 개의 GPU에 있는 2개 레이어 MLP의 계산 그래프와 그 의존성." />
:label:<code>fig_twogpu</code></p>
<h2 id="요약-summary-64"><a class="header" href="#요약-summary-64">요약 (Summary)</a></h2>
<ul>
<li>현대 시스템에는 여러 개의 GPU와 CPU와 같은 다양한 장치가 있습니다. 이들은 비동기적으로 병렬로 사용될 수 있습니다.</li>
<li>현대 시스템에는 또한 PCI Express, 저장 장치(일반적으로 솔리드 스테이트 드라이브 또는 네트워크를 통해), 네트워크 대역폭과 같은 통신을 위한 다양한 리소스가 있습니다. 이들은 최대 효율을 위해 병렬로 사용될 수 있습니다.</li>
<li>백엔드는 자동 병렬 계산 및 통신을 통해 성능을 향상시킬 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-79"><a class="header" href="#연습-문제-exercises-79">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 섹션에 정의된 <code>run</code> 함수에서 8개의 연산이 수행되었습니다. 그들 사이에는 의존성이 없습니다. 딥러닝 프레임워크가 자동으로 병렬로 실행하는지 확인하기 위한 실험을 설계하십시오.</li>
<li>개별 연산자의 작업 부하가 충분히 작을 때, 병렬 처리는 단일 CPU나 GPU에서도 도움이 될 수 있습니다. 이를 확인하기 위한 실험을 설계하십시오.</li>
<li>CPU, GPU에서의 병렬 계산과 두 장치 간의 통신을 사용하는 실험을 설계하십시오.</li>
<li>NVIDIA의 <a href="https://developer.nvidia.com/nsight-compute-2019_5">Nsight</a>와 같은 디버거를 사용하여 코드가 효율적인지 확인하십시오.</li>
<li>더 복잡한 데이터 의존성을 포함하는 계산 작업을 설계하고, 성능을 향상시키면서 올바른 결과를 얻을 수 있는지 확인하기 위해 실험을 실행하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/362">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1681">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="하드웨어-hardware"><a class="header" href="#하드웨어-hardware">하드웨어 (Hardware)</a></h1>
<p>:label:<code>sec_hardware</code></p>
<p>뛰어난 성능을 갖춘 시스템을 구축하려면 문제의 통계적 측면을 포착하기 위한 알고리즘과 모델에 대한 좋은 이해가 필요합니다. 동시에 기저 하드웨어에 대한 최소한의 지식을 갖는 것도 필수적입니다. 이 섹션은 하드웨어 및 시스템 설계에 대한 정식 과정을 대체할 수는 없습니다. 대신 일부 알고리즘이 다른 알고리즘보다 왜 더 효율적인지, 그리고 어떻게 좋은 처리량(throughput)을 달성할 수 있는지 이해하기 위한 출발점 역할을 할 수 있습니다. 좋은 설계는 쉽게 한 자릿수 이상의 차이를 만들 수 있으며, 결과적으로 이는 네트워크를 훈련할 수 있느냐(예: 일주일 내에) 없느냐(3개월이 걸려 마감 기한을 놓치는 경우)의 차이를 만들 수 있습니다.
먼저 컴퓨터를 살펴보는 것으로 시작하겠습니다. 그런 다음 CPU와 GPU를 더 자세히 살펴보기 위해 확대해 보겠습니다. 마지막으로 서버 센터나 클라우드에서 여러 대의 컴퓨터가 어떻게 연결되는지 검토하기 위해 축소해 보겠습니다.</p>
<p><img src="chapter_computational-performance/../img/latencynumbers.png" alt="모든 프로그래머가 알아야 할 지연 시간 수치." />
:label:<code>fig_latencynumbers</code></p>
<p>성급한 독자들은 :numref:<code>fig_latencynumbers</code>만으로도 충분할 수 있습니다. 이는 지난 10년 동안의 진전에 대한 좋은 개요를 제공하는 Colin Scott의 <a href="https://people.eecs.berkeley.edu/%7Ercs/research/interactive_latency.html">대화형 포스트</a>에서 가져온 것입니다. 원래 수치는 제프 딘(Jeff Dean)의 <a href="https://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf">2010년 스탠포드 강연</a>에서 기인합니다.
아래 논의는 이러한 수치에 대한 몇 가지 근거와 이것이 알고리즘 설계에 어떻게 가이드를 줄 수 있는지 설명합니다. 아래 논의는 매우 높은 수준이며 요약적입니다. 이는 정식 과정을 <em>대체하는 것</em>이 아니라 통계 모델러가 적절한 설계 결정을 내리는 데 충분한 정보를 제공하기 위한 것입니다. 컴퓨터 아키텍처에 대한 심층적인 개요는 :cite:<code>Hennessy.Patterson.2011</code>이나 <a href="http://inst.eecs.berkeley.edu/%7Ecs152/sp19/">Arste Asanovic</a>의 최근 강의와 같은 관련 과정을 참조하십시오.</p>
<h2 id="컴퓨터-computers"><a class="header" href="#컴퓨터-computers">컴퓨터 (Computers)</a></h2>
<p>대부분의 딥러닝 연구자와 실무자는 상당한 양의 메모리, 계산 능력, GPU와 같은 형태의 가속기 또는 그 배수개를 갖춘 컴퓨터에 액세스할 수 있습니다. 컴퓨터는 다음과 같은 핵심 구성 요소로 이루어져 있습니다:</p>
<ul>
<li>우리가 제공하는 프로그램을 실행할 수 있는 프로세서(CPU라고도 함)(운영 체제 및 기타 많은 것들을 실행하는 것 외에도), 일반적으로 8개 이상의 코어로 구성됨.</li>
<li>가중치 벡터, 활성화 값, 훈련 데이터와 같은 계산 결과를 저장하고 검색하기 위한 메모리(RAM).</li>
<li>1GB/s에서 100GB/s 범위의 속도를 가진 이더넷 네트워크 연결(때로는 여러 개). 하이엔드 서버에서는 더 발전된 상호 연결을 찾을 수 있습니다.</li>
<li>시스템을 하나 이상의 GPU에 연결하기 위한 고속 확장 버스(PCIe). 서버에는 종종 고급 토폴로지로 연결된 최대 8개의 가속기가 있는 반면, 데스크탑 시스템은 사용자의 예산과 전원 공급 장치의 크기에 따라 1개 또는 2개를 갖습니다.</li>
<li>마그네틱 하드 디스크 드라이브(HDD), 솔리드 스테이트 드라이브(SSD)와 같은 지속성 저장 장치로, 많은 경우 PCIe 버스를 사용하여 연결됩니다. 이는 시스템으로의 훈련 데이터의 효율적인 전송과 필요에 따른 중간 체크포인트 저장을 제공합니다.</li>
</ul>
<p><img src="chapter_computational-performance/../img/mobo-symbol.svg" alt="컴퓨터 구성 요소의 연결성." />
:label:<code>fig_mobo-symbol</code></p>
<p>:numref:<code>fig_mobo-symbol</code>이 나타내듯이, 대부분의 구성 요소(네트워크, GPU, 저장 장치)는 PCIe 버스를 통해 CPU에 연결됩니다. 이는 CPU에 직접 부착된 여러 레인으로 구성됩니다. 예를 들어 AMD의 Threadripper 3는 64개의 PCIe 4.0 레인을 가지고 있으며, 각 레인은 양방향으로 16 Gbit/s 데이터 전송이 가능합니다. 메모리는 최대 100 GB/s의 총 대역폭으로 CPU에 직접 부착됩니다.</p>
<p>컴퓨터에서 코드를 실행할 때 우리는 프로세서(CPU 또는 GPU)로 데이터를 섞어 넣고, 계산을 수행한 다음, 결과를 프로세서에서 RAM 및 지속성 저장 장치로 다시 이동해야 합니다. 따라서 좋은 성능을 얻으려면 이러한 시스템 중 어느 하나도 주요 병목 현상이 되지 않고 원활하게 작동하도록 해야 합니다. 예를 들어 이미지를 충분히 빨리 로드할 수 없다면 프로세서는 할 일이 없을 것입니다. 마찬가지로 행렬을 CPU(또는 GPU)로 충분히 빨리 이동할 수 없다면 그 처리 요소들은 굶주리게 될 것입니다. 마지막으로 네트워크를 통해 여러 대의 컴퓨터를 동기화하려는 경우, 네트워크가 계산을 늦추어서는 안 됩니다. 한 가지 옵션은 통신과 계산을 엇갈리게 배치하는 것입니다. 다양한 구성 요소를 더 자세히 살펴봅시다.</p>
<h2 id="메모리-memory"><a class="header" href="#메모리-memory">메모리 (Memory)</a></h2>
<p>가장 기본적으로 메모리는 즉시 액세스해야 하는 데이터를 저장하는 데 사용됩니다. 현재 CPU RAM은 일반적으로 <a href="https://en.wikipedia.org/wiki/DDR4_SDRAM">DDR4</a> 종류로, 모듈당 20~25 GB/s 대역폭을 제공합니다. 각 모듈은 64비트 너비의 버스를 갖습니다. 일반적으로 메모리 모듈은 다중 채널을 허용하기 위해 쌍으로 사용됩니다. CPU는 2개에서 4개의 메모리 채널을 가지며, 즉 40 GB/s에서 100 GB/s 사이의 피크 메모리 대역폭을 갖습니다. 종종 채널당 두 개의 뱅크가 있습니다. 예를 들어 AMD의 Zen 3 Threadripper는 8개의 슬롯을 갖습니다.</p>
<p>이러한 수치들은 정말 인상적이지만, 이야기의 일부만 말해줄 뿐입니다. 메모리에서 일부를 읽고 싶을 때 먼저 메모리 모듈에 정보가 어디에 있는지 알려줘야 합니다. 즉, 먼저 RAM에 <em>주소</em>를 보내야 합니다. 이것이 완료되면 단일 64비트 레코드 또는 긴 레코드 시퀀스를 읽도록 선택할 수 있습니다. 후자를 *버스트 읽기(burst read)*라고 합니다. 요컨대, 메모리에 주소를 보내고 전송을 설정하는 데는 약 100ns가 걸리며(세부 사항은 사용된 메모리 칩의 특정 타이밍 계수 에 따라 다름), 그 이후의 모든 전송에는 0.2ns만 걸립니다. 간단히 말해서 첫 번째 읽기는 후속 읽기보다 500배나 더 비쌉니다! 우리는 초당 최대 10,000,000번의 랜덤 읽기를 수행할 수 있음에 유의하십시오. 이는 가능한 한 랜덤 메모리 액세스를 피하고 대신 버스트 읽기(및 쓰기)를 사용해야 함을 시사합니다.</p>
<p>여러 개의 *뱅크(banks)*가 있다는 점을 고려하면 상황은 좀 더 복잡해집니다. 각 뱅크는 메모리를 거의 독립적으로 읽을 수 있습니다. 이는 두 가지를 의미합니다.
한편으로, 유효 랜덤 읽기 횟수는 메모리 전체에 고르게 분산되어 있다면 최대 4배 더 높습니다. 또한 버스트 읽기도 4배 더 빠르기 때문에 여전히 랜덤 읽기를 수행하는 것은 좋지 않은 생각임을 의미합니다. 다른 한편으로, 64비트 경계에 대한 메모리 정렬 때문에 동일한 경계로 모든 데이터 구조를 정렬하는 것이 좋습니다. 컴파일러는 적절한 플래그가 설정되어 있을 때 이를 거의 <a href="https://en.wikipedia.org/wiki/Data_structure_alignment">자동으로</a> 수행합니다. 호기심 많은 독자들은 <a href="http://web.cecs.pdx.edu/%7Ezeshan/ece585_lec5.pdf">Zeshan Chishti</a>의 강의와 같은 DRAM에 대한 강의를 검토해 보시기 바랍니다.</p>
<p>GPU 메모리는 CPU보다 처리 요소가 훨씬 더 많기 때문에 훨씬 더 높은 대역폭 요구 사항을 따릅니다. 대체로 이를 해결하기 위한 두 가지 옵션이 있습니다. 첫 번째는 메모리 버스를 훨씬 더 넓게 만드는 것입니다. 예를 들어 NVIDIA의 RTX 2080 Ti는 352비트 너비의 버스를 갖습니다. 이를 통해 훨씬 더 많은 정보를 동시에 전송할 수 있습니다. 둘째, GPU는 특정 고성능 메모리를 사용합니다. NVIDIA의 RTX 및 Titan 시리즈와 같은 소비자용 장치는 일반적으로 500 GB/s 이상의 총 대역폭을 가진 <a href="https://en.wikipedia.org/wiki/GDDR6_SDRAM">GDDR6</a> 칩을 사용합니다. 대안은 HBM(high bandwidth memory) 모듈을 사용하는 것입니다. 이들은 매우 다른 인터페이스를 사용하며 전용 실리콘 웨이퍼에서 GPU와 직접 연결됩니다. 이로 인해 매우 비싸며 그 사용은 일반적으로 NVIDIA Volta V100 가속기 시리즈와 같은 하이엔드 서버 칩으로 제한됩니다. 전혀 놀랍지 않게도, GPU 메모리는 일반적으로 전자의 높은 비용 때문에 CPU 메모리보다 <em>훨씬</em> 작습니다. 우리의 목적을 위해, 대체로 그들의 성능 특성은 유사하며 단지 훨씬 더 빠를 뿐입니다. 이 책의 목적을 위해 세부 사항은 무시해도 안전합니다. 고처리량을 위해 GPU 커널을 튜닝할 때만 중요합니다.</p>
<h2 id="저장-장치-storage"><a class="header" href="#저장-장치-storage">저장 장치 (Storage)</a></h2>
<p>우리는 RAM의 주요 특징 중 일부가 <em>대역폭</em>과 <em>지연 시간</em>임을 보았습니다. 저장 장치도 마찬가지이며, 차이점이 훨씬 더 극단적일 수 있습니다.</p>
<h3 id="하드-디스크-드라이브-hard-disk-drives"><a class="header" href="#하드-디스크-드라이브-hard-disk-drives">하드 디스크 드라이브 (Hard Disk Drives)</a></h3>
<p><em>하드 디스크 드라이브</em>(HDD)는 반세기 넘게 사용되어 왔습니다. 요컨대 이들은 주어진 트랙에서 읽거나 쓰기 위해 위치를 잡을 수 있는 헤드가 있는 여러 개의 회전하는 플래터(platters)를 포함합니다. 하이엔드 디스크는 9개의 플래터에 최대 16TB를 담습니다. HDD의 주요 장점 중 하나는 상대적으로 저렴하다는 것입니다. 많은 단점 중 하나는 일반적으로 치명적인 실패 모드와 상대적으로 높은 읽기 지연 시간입니다.</p>
<p>후자를 이해하기 위해 HDD가 약 7,200 RPM(분당 회전수)으로 회전한다는 사실을 고려하십시오. 만약 그보다 훨씬 빠르다면 플래터에 가해지는 원심력 때문에 산산조각이 날 것입니다. 이는 디스크의 특정 섹터에 액세스할 때 큰 단점이 됩니다: 플래터가 제 위치로 회전할 때까지 기다려야 합니다(헤드는 움직일 수 있지만 실제 디스크를 가속할 수는 없습니다). 따라서 요청된 데이터를 사용할 수 있을 때까지 8ms 이상 걸릴 수 있습니다. 이를 표현하는 일반적인 방법은 HDD가 약 100 IOPs(초당 입출력 연산 수)로 작동할 수 있다고 말하는 것입니다. 이 수치는 지난 20년 동안 본질적으로 변하지 않았습니다. 더 나쁜 것은 대역폭을 늘리는 것도 똑같이 어렵다는 것입니다(100~200 MB/s 정도입니다). 결국 각 헤드는 비트 트랙을 읽으므로 비트 전송률은 정보 밀도의 제곱근에 비례하여 확장될 뿐입니다. 결과적으로 HDD는 빠르게 아카이브 저장소 및 매우 큰 데이터셋을 위한 저급 저장소로 밀려나고 있습니다.</p>
<h3 id="솔리드-스테이트-드라이브-solid-state-drives"><a class="header" href="#솔리드-스테이트-드라이브-solid-state-drives">솔리드 스테이트 드라이브 (Solid State Drives)</a></h3>
<p>솔리드 스테이트 드라이브(SSD)는 플래시 메모리를 사용하여 정보를 영구적으로 저장합니다. 이를 통해 저장된 레코드에 <em>훨씬 더 빠른</em> 액세스가 가능합니다. 현대의 SSD는 100,000에서 500,000 IOPs로 작동할 수 있으며, 즉 HDD보다 최대 3배나 더 빠릅니다. 게다가 대역폭은 1~3GB/s에 달할 수 있으며, 이는 HDD보다 한 자릿수 더 빠릅니다. 이러한 개선 사항은 너무 좋아서 믿기지 않을 정도입니다. 실제로 SSD가 설계된 방식 때문에 다음과 같은 주의 사항이 따릅니다.</p>
<ul>
<li>SSD는 정보를 블록(256KB 이상) 단위로 저장합니다. 이들은 전체로만 쓰여질 수 있으며, 이는 상당한 시간이 걸립니다. 결과적으로 SSD에서의 비트 단위 랜덤 쓰기는 성능이 매우 좋지 않습니다. 마찬가지로, 블록을 읽고 지운 다음 새로운 정보로 다시 써야 하기 때문에 일반적으로 데이터를 쓰는 데는 상당한 시간이 걸립니다. 현재 SSD 컨트롤러와 펌웨어는 이를 완화하기 위한 알고리즘을 개발했습니다. 그럼에도 불구하고 쓰기는 훨씬 더 느릴 수 있으며, 특히 QLC(quad level cell) SSD의 경우 더욱 그렇습니다. 성능 향상을 위한 핵심은 작업 <em>대기열</em>을 유지하고, 읽기를 선호하며, 가능하다면 큰 블록 단위로 쓰는 것입니다.</li>
<li>SSD의 메모리 셀은 상대적으로 빨리 마모됩니다(종종 수천 번의 쓰기 후에 이미 발생합니다). 마모 평준화(wear-leveling) 보호 알고리즘은 많은 셀에 걸쳐 열화를 분산시킬 수 있습니다. 그렇긴 하지만 스와핑 파일이나 대량의 로그 파일 집계에 SSD를 사용하는 것은 권장되지 않습니다.</li>
<li>마지막으로, 대역폭의 대폭적인 증가로 인해 컴퓨터 설계자들은 SSD를 PCIe 버스에 직접 부착하게 되었습니다. 이를 처리할 수 있는 드라이브를 NVMe(Non Volatile Memory enhanced)라고 하며, 최대 4개의 PCIe 레인을 사용할 수 있습니다. 이는 PCIe 4.0에서 최대 8GB/s에 해당합니다.</li>
</ul>
<h3 id="클라우드-저장소-cloud-storage"><a class="header" href="#클라우드-저장소-cloud-storage">클라우드 저장소 (Cloud Storage)</a></h3>
<p>클라우드 저장소는 구성 가능한 범위의 성능을 제공합니다. 즉, 가상 머신에 대한 저장소 할당은 사용자가 선택한 대로 수량과 속도 측면에서 동적입니다. 지연 시간이 너무 높을 때마다(예: 많은 작은 레코드로 훈련하는 동안) 사용자가 할당된 IOPs 수를 늘릴 것을 권장합니다.</p>
<h2 id="cpu"><a class="header" href="#cpu">CPU</a></h2>
<p>중앙 처리 장치(CPU)는 모든 컴퓨터의 중심입니다. 이들은 여러 핵심 구성 요소로 이루어져 있습니다: 기계 코드를 실행할 수 있는 <em>프로세서 코어</em>, 이들을 연결하는 <em>버스</em>(특정 토폴로지는 프로세서 모델, 세대 및 벤더에 따라 크게 다름), 그리고 메인 메모리에서 읽는 것보다 더 높은 대역폭과 더 낮은 지연 시간의 메모리 액세스를 가능하게 하는 <em>캐시</em>입니다. 마지막으로, 거의 모든 현대 CPU에는 미디어 처리 및 머신러닝에서 흔히 볼 수 있는 고성능 선형 대수 및 합성곱을 돕기 위한 <em>벡터 처리 유닛</em>이 포함되어 있습니다.</p>
<p><img src="chapter_computational-performance/../img/skylake.svg" alt="Intel Skylake 소비자용 쿼드 코어 CPU." />
:label:<code>fig_skylake</code></p>
<p>:numref:<code>fig_skylake</code>는 Intel Skylake 소비자급 쿼드 코어 CPU를 묘사합니다. 통합 GPU, 캐시, 그리고 네 개의 코어를 연결하는 링 버스(ringbus)를 가지고 있습니다. 이더넷, WiFi, Bluetooth, SSD 컨트롤러, USB와 같은 주변 장치는 칩셋의 일부이거나 CPU에 직접 부착(PCIe)되어 있습니다.</p>
<h3 id="마이크로아키텍처-microarchitecture"><a class="header" href="#마이크로아키텍처-microarchitecture">마이크로아키텍처 (Microarchitecture)</a></h3>
<p>각 프로세서 코어는 상당히 정교한 구성 요소 집합으로 이루어져 있습니다. 세부 사항은 세대와 벤더마다 다르지만 기본 기능은 거의 표준입니다. 프론트엔드는 명령어를 로드하고 어떤 경로를 택할지 예측하려고 시도합니다(예: 제어 흐름의 경우). 명령어는 어셈블리 코드에서 마이크로 명령어로 디코딩됩니다. 어셈블리 코드는 종종 프로세서가 실행하는 가장 낮은 수준의 코드가 아닙니다. 대신 복잡한 명령어는 일련의 더 낮은 수준의 연산으로 디코딩될 수 있습니다. 이들은 실제 실행 코어에 의해 처리됩니다. 종종 후자는 많은 연산을 동시에 수행할 수 있습니다. 예를 들어 :numref:<code>fig_cortexa77</code>의 ARM Cortex A77 코어는 최대 8개의 연산을 동시에 수행할 수 있습니다.</p>
<p><img src="chapter_computational-performance/../img/a77.svg" alt="ARM Cortex A77 마이크로아키텍처." />
:label:<code>fig_cortexa77</code></p>
<p>이는 효율적인 프로그램이 독립적으로 수행될 수 있다면 클록 사이클당 하나 이상의 명령어를 수행할 수 있음을 의미합니다. 모든 유닛이 동일하게 생성되는 것은 아닙니다. 일부는 정수 명령어에 특화되어 있는 반면 다른 유닛은 부동 소수점 성능에 최적화되어 있습니다. 처리량을 늘리기 위해 프로세서는 분기 명령어에서 여러 코드 경로를 동시에 따라가고 택하지 않은 분기의 결과는 버릴 수도 있습니다. 이것이 분기 예측 유닛(프론트엔드에 있음)이 중요한 이유이며, 가장 유망한 경로만 추구되도록 합니다.</p>
<h3 id="벡터화-vectorization-1"><a class="header" href="#벡터화-vectorization-1">벡터화 (Vectorization)</a></h3>
<p>딥러닝은 계산 집약적입니다. 따라서 CPU를 머신러닝에 적합하게 만들려면 한 클록 사이클에 많은 연산을 수행해야 합니다. 이는 벡터 유닛을 통해 달성됩니다. 이들은 이름이 다릅니다: ARM에서는 NEON이라고 불리고, x86에서는 (최근 세대) <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX2</a> 유닛이라고 지칭됩니다. 공통적인 측면은 이들이 SIMD(single instruction multiple data) 연산을 수행할 수 있다는 점입니다. :numref:<code>fig_neon128</code>은 ARM에서 한 클록 사이클에 8개의 단정수(short integers)를 더하는 방법을 보여줍니다.</p>
<p><img src="chapter_computational-performance/../img/neon128.svg" alt="128비트 NEON 벡터화." />
:label:<code>fig_neon128</code></p>
<p>아키텍처 선택에 따라 이러한 레지스터는 최대 512비트 길이로, 최대 64쌍의 숫자를 조합할 수 있습니다. 예를 들어, 우리는 두 숫자를 곱하고 이를 세 번째 숫자에 더할 수도 있는데, 이는 FMA(fused multiply-add)로도 알려져 있습니다. Intel의 <a href="https://01.org/openvinotoolkit">OpenVino</a>는 서버급 CPU에서 딥러닝을 위한 상당한 처리량을 달성하기 위해 이들을 사용합니다. 하지만 이 수치는 GPU가 달성할 수 있는 것에 비해 완전히 왜소해진다는 점에 유의하십시오. 예를 들어 NVIDIA의 RTX 2080 Ti는 4,352개의 CUDA 코어를 가지고 있으며, 각 코어는 언제든지 이러한 연산을 처리할 수 있습니다.</p>
<h3 id="캐시-cache"><a class="header" href="#캐시-cache">캐시 (Cache)</a></h3>
<p>다음 상황을 고려해 보십시오: 위 :numref:<code>fig_skylake</code>에 묘사된 것과 같이 2 GHz 주파수에서 실행되는 4개의 코어를 가진 적당한 CPU 코어가 있습니다.
또한 IPC(클록당 명령어 수) 카운트가 1이고 유닛에 256비트 너비의 AVX2가 활성화되어 있다고 가정합시다. 또한 AVX2 연산에 사용되는 레지스터 중 적어도 하나를 메모리에서 가져와야 한다고 가정합시다. 이는 CPU가 클록 사이클당 $4 \times 256 \textrm{ bit} = 128 \textrm{ bytes}$의 데이터를 소비함을 의미합니다. 초당 $2 \times 10^9 \times 128 = 256 \times 10^9$ 바이트를 프로세서로 전송할 수 없다면 처리 요소들은 굶주리게 될 것입니다. 불행히도 그러한 칩의 메모리 인터페이스는 20~40 GB/s의 데이터 전송만 지원하며, 즉 한 자릿수 이상 적습니다. 해결책은 가능한 한 메모리에서 <em>새로운</em> 데이터를 로드하는 것을 피하고 오히려 CPU에 로컬로 캐싱하는 것입니다. 여기서 캐시가 유용합니다. 일반적으로 다음과 같은 이름이나 개념이 사용됩니다:</p>
<ul>
<li>**레지스터(Registers)**는 엄밀히 말하면 캐시의 일부가 아닙니다. 이들은 명령어 단계를 돕습니다. 그렇긴 하지만, CPU 레지스터는 CPU가 지연 페널티 없이 클록 속도로 액세스할 수 있는 메모리 위치입니다. CPU는 수십 개의 레지스터를 갖습니다. 레지스터를 효율적으로 사용하는 것은 컴파일러(또는 프로그래머)의 몫입니다. 예를 들어 C 프로그래밍 언어에는 <code>register</code> 키워드가 있습니다.</li>
<li><strong>L1 캐시</strong>는 높은 메모리 대역폭 요구 사항에 대한 첫 번째 방어선입니다. L1 캐시는 아주 작으며(일반적인 크기는 32~64 KB일 수 있음) 종종 데이터 캐시와 명령어 캐시로 나뉩니다. L1 캐시에서 데이터를 찾으면 액세스가 매우 빠릅니다. 거기서 찾을 수 없으면 검색은 캐시 계층 구조 아래로 진행됩니다.</li>
<li><strong>L2 캐시</strong>는 다음 정거장입니다. 아키텍처 설계와 프로세서 크기에 따라 전용(exclusive)일 수 있습니다. 특정 코어만 액세스할 수 있거나 여러 코어 간에 공유될 수 있습니다. L2 캐시는 L1보다 크고(일반적으로 코어당 256~512 KB) 느립니다. 더욱이 L2에 있는 무언가에 액세스하려면 먼저 데이터가 L1에 없다는 것을 확인해야 하므로 약간의 추가 지연 시간이 추가됩니다.</li>
<li><strong>L3 캐시</strong>는 여러 코어 간에 공유되며 매우 클 수 있습니다. AMD의 Epyc 3 서버 CPU는 여러 칩렛에 걸쳐 분산된 무려 256MB의 캐시를 갖습니다. 더 일반적인 수치는 4~8MB 범위입니다.</li>
</ul>
<p>어떤 메모리 요소가 다음에 필요할지 예측하는 것은 칩 설계에서 핵심적인 최적화 파라미터 중 하나입니다. 예를 들어, 대부분의 캐싱 알고리즘은 거꾸로보다는 <em>앞으로</em> 읽으려고(read ahead) 시도하기 때문에 메모리를 순방향으로 횡단하는 것이 권장됩니다. 마찬가지로 메모리 액세스 패턴을 국소적으로 유지하는 것은 성능을 향상시키는 좋은 방법입니다.</p>
<p>캐시를 추가하는 것은 양날의 검입니다. 한편으로 그들은 프로세서 코어가 데이터 부족으로 굶주리지 않도록 보장합니다. 동시에 그들은 칩 크기를 늘려 처리 능력을 높이는 데 사용될 수 있었던 면적을 차지합니다. 더욱이, *캐시 미스(cache misses)*는 비쌀 수 있습니다. :numref:<code>fig_falsesharing</code>에 묘사된 최악의 시나리오인 *거짓 공유(false sharing)*를 고려하십시오. 프로세서 1의 스레드가 데이터를 요청할 때 메모리 위치가 프로세서 0에 캐싱되어 있는 경우입니다. 이를 얻기 위해 프로세서 0은 하던 일을 멈추고 정보를 메인 메모리에 다시 써야 하며, 그런 다음 프로세서 1이 메모리에서 이를 읽도록 해야 합니다. 이 작업 동안 두 프로세서 모두 기다립니다. 잠재적으로 그러한 코드는 효율적인 단일 프로세서 구현과 비교할 때 여러 프로세서에서 <em>더 느리게</em> 실행됩니다. 이것이 캐시 크기에 실질적인 제한이 있는 또 다른 이유입니다(물리적 크기 외에도).</p>
<p><img src="chapter_computational-performance/../img/falsesharing.svg" alt="거짓 공유 (이미지 제공: Intel)." />
:label:<code>fig_falsesharing</code></p>
<h2 id="gpu-및-기타-가속기"><a class="header" href="#gpu-및-기타-가속기">GPU 및 기타 가속기</a></h2>
<p>GPU 없이는 딥러닝이 성공하지 못했을 것이라고 주장해도 과언이 아닙니다. 같은 맥락에서, GPU 제조업체의 재산이 딥러닝 덕분에 크게 늘어났다고 주장하는 것도 매우 합리적입니다. 하드웨어와 알고리즘의 이러한 공동 진화는 좋든 싫든 딥러닝이 선호되는 통계 모델링 패러다임이 된 상황으로 이어졌습니다. 따라서 GPU와 TPU :cite:<code>Jouppi.Young.Patil.ea.2017</code>와 같은 관련 가속기가 제공하는 구체적인 이점을 이해하는 것이 보람이 있습니다.</p>
<p>실전에서 종종 구분되는 점이 있습니다: 가속기는 훈련 또는 추론 중 하나에 최적화됩니다. 후자의 경우 네트워크의 순전파만 계산하면 됩니다. 역전파를 위한 중간 데이터의 저장은 필요하지 않습니다. 더욱이 매우 정밀한 계산이 필요하지 않을 수도 있습니다(FP16 또는 INT8이면 일반적으로 충분함). 반면 훈련 중에는 기울기를 계산하기 위해 모든 중간 결과의 저장이 필요합니다. 더욱이 기울기를 누적하려면 수치적 언더플로(또는 오버플로)를 피하기 위해 더 높은 정밀도가 필요합니다. 이는 FP16(또는 FP32와의 혼합 정밀도)이 최소 요구 사항임을 의미합니다. 이 모든 것은 더 빠르고 큰 메모리(HBM2 vs. GDDR6)와 더 많은 처리 능력을 필요로 합니다. 예를 들어 NVIDIA의 <a href="https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/">Turing</a> T4 GPU는 추론에 최적화된 반면 V100 GPU는 훈련에 적합합니다.</p>
<p>:numref:<code>fig_neon128</code>에서 설명한 벡터화를 떠올려 보십시오. 프로세서 코어에 벡터 유닛을 추가함으로써 처리량을 크게 늘릴 수 있었습니다. 예를 들어 :numref:<code>fig_neon128</code>의 예에서는 16개의 연산을 동시에 수행할 수 있었습니다.
첫째, 벡터 간의 연산뿐만 아니라 행렬 간의 연산까지 최적화하는 연산을 추가하면 어떨까요? 이 전략은 텐서 코어(곧 다룸)로 이어졌습니다.
둘째, 훨씬 더 많은 코어를 추가하면 어떨까요? 요컨대, 이 두 가지 전략이 GPU의 설계 결정을 요약합니다. :numref:<code>fig_turing_processing_block</code>은 기본 처리 블록의 개요를 제공합니다. 16개의 정수 유닛과 16개의 부동 소수점 유닛을 포함합니다. 그에 더해, 두 개의 텐서 코어가 딥러닝과 관련된 추가 연산의 좁은 하위 집합을 가속화합니다. 각 스트리밍 멀티프로세서는 이러한 네 개의 블록으로 구성됩니다.</p>
<p><img src="chapter_computational-performance/../img/turing-processing-block.png" alt="NVIDIA Turing 처리 블록 (이미지 제공: NVIDIA)." />
:width:<code>150px</code>
:label:<code>fig_turing_processing_block</code></p>
<p>다음으로, 12개의 스트리밍 멀티프로세서가 그래픽 처리 클러스터로 그룹화되어 하이엔드 TU102 프로세서를 구성합니다. 충분한 메모리 채널과 L2 캐시가 설정을 보완합니다. :numref:<code>fig_turing</code>에 관련 세부 사항이 있습니다. 이러한 장치를 설계하는 이유 중 하나는 개별 블록을 필요에 따라 추가하거나 제거할 수 있게 하여 더 소형인 칩을 만들고 수율 문제(결함이 있는 모듈은 활성화되지 않을 수 있음)를 처리하기 위함입니다. 다행히도 이러한 장치를 프로그래밍하는 것은 CUDA와 프레임워크 코드의 레이어 아래에 일반 딥러닝 연구자로부터 잘 숨겨져 있습니다. 특히 사용 가능한 리소스가 있다면 GPU에서 동시에 둘 이상의 프로그램이 실행될 수도 있습니다. 그럼에도 불구하고 장치 메모리에 맞지 않는 모델을 선택하는 것을 피하기 위해 장치의 한계를 인지하는 것이 보람이 있습니다.</p>
<p><img src="chapter_computational-performance/../img/turing.png" alt="NVIDIA Turing 아키텍처 (이미지 제공: NVIDIA)" />
:width:<code>350px</code>
:label:<code>fig_turing</code></p>
<p>A 마지막으로 더 자세히 언급할 가치가 있는 측면은 *텐서 코어(tensor cores)*입니다. 이들은 딥러닝에 특히 효과적인 더 최적화된 회로를 추가하는 최근 트렌드의 한 예입니다. 예를 들어, TPU는 빠른 행렬 곱셈을 위해 시스톨릭 어레이(systolic array) :cite:<code>Kung.1988</code>를 추가했습니다. 거기서 설계는 매우 적은 수의(TPU 1세대에서는 하나) 큰 연산을 지원하는 것이었습니다. 텐서 코어는 그 반대편에 있습니다. 이들은 수치 정밀도에 따라 $4 \times 4$에서 $16 \times 16$ 사이의 행렬을 포함하는 작은 연산에 최적화되어 있습니다. :numref:<code>fig_tensorcore</code>는 최적화의 개요를 제공합니다.</p>
<p><img src="chapter_computational-performance/../img/tensorcore.jpg" alt="NVIDIA 텐서 코어 in Turing (이미지 제공: NVIDIA)." />
:width:<code>400px</code>
:label:<code>fig_tensorcore</code></p>
<p>분명히 계산을 위해 최적화할 때 우리는 특정 타협을 하게 됩니다. 그중 하나는 GPU가 인터럽트와 희소 데이터(sparse data)를 처리하는 데 그리 능숙하지 않다는 점입니다. <a href="https://github.com/gunrock/gunrock">Gunrock</a> :cite:<code>Wang.Davidson.Pan.ea.2016</code>과 같은 주목할 만한 예외가 있지만, 희소 행렬과 벡터의 액세스 패턴은 GPU가 뛰어난 고대역폭 버스트 읽기 연산과 잘 어울리지 않습니다. 두 목표를 일치시키는 것은 활발한 연구 분야입니다. 그래프 상의 딥러닝을 위해 튜닝된 라이브러리인 <a href="http://dgl.ai">DGL</a>을 참조하십시오.</p>
<h2 id="네트워크-및-버스-networks-and-buses"><a class="header" href="#네트워크-및-버스-networks-and-buses">네트워크 및 버스 (Networks and Buses)</a></h2>
<p>단일 장치가 최적화에 불충분할 때마다 처리를 동기화하기 위해 장치로 데이터를 주고받아야 합니다. 여기서 네트워크와 버스가 유용합니다. 우리에게는 대역폭, 비용, 거리, 유연성이라는 몇 가지 설계 파라미터가 있습니다.
한쪽 끝에는 범위가 꽤 좋고 사용하기 매우 쉽지만(결국 전선이 없음), 가격이 저렴한 반면 비교적 평범한 대역폭과 지연 시간을 제공하는 WiFi가 있습니다. 제정신인 머신러닝 연구자라면 서버 클러스터를 구축하는 데 이를 사용하지 않을 것입니다. 다음에서는 딥러닝에 적합한 상호 연결에 초점을 맞춥니다.</p>
<ul>
<li><strong>PCIe</strong>는 16레인 슬롯에서 PCIe 4.0 기준 최대 32 GB/s의 매우 높은 대역폭의 점대점(point-to-point) 연결을 위한 전용 버스입니다. 지연 시간은 한 자릿수 마이크로초(5 μs) 수준입니다. PCIe 링크는 소중합니다. 프로세서는 제한된 수의 링크만 가집니다: AMD의 EPYC 3는 128레인, Intel의 Xeon은 칩당 최대 48레인을 가집니다; 데스크탑급 CPU에서는 각각 20개(Ryzen 9) 및 16개(Core i9)입니다. GPU는 일반적으로 16개 레인을 갖기 때문에 풀 대역폭으로 CPU에 연결할 수 있는 GPU의 수가 제한됩니다. 결국 저장 장치 및 이더넷과 같은 다른 고대역폭 주변 장치와 링크를 공유해야 하기 때문입니다. RAM 액세스와 마찬가지로 줄어든 패킷 오버헤드 때문에 대량 전송이 선호됩니다.</li>
<li>**이더넷(Ethernet)**은 컴퓨터를 연결하는 가장 일반적으로 사용되는 방법입니다. PCIe보다 훨씬 느리지만 설치가 매우 저렴하고 탄력적이며 훨씬 더 긴 거리를 커버합니다. 저급 서버의 일반적인 대역폭은 1 GBit/s입니다. 고성능 장치(예: 클라우드의 <a href="https://aws.amazon.com/ec2/instance-types/c5/">C5 인스턴스</a>)는 10에서 100 GBit/s 사이의 대역폭을 제공합니다. 이전의 모든 사례와 마찬가지로 데이터 전송에는 상당한 오버헤드가 있습니다. 우리는 원시 이더넷을 직접 사용하는 경우가 거의 없으며 오히려 물리적 상호 연결 위에서 실행되는 프로토콜(예: UDP 또는 TCP/IP)을 사용합니다. 이는 추가적인 오버헤드를 더합니다. PCIe와 마찬가지로 이더넷은 두 장치(예: 컴퓨터와 스위치)를 연결하도록 설계되었습니다.</li>
<li>**스위치(Switches)**는 임의의 한 쌍의 장치가 동시에 (일반적으로 풀 대역폭의) 점대점 연결을 수행할 수 있는 방식으로 여러 장치를 연결할 수 있게 해 줍니다. 예를 들어 이더넷 스위치는 높은 단면 대역폭(cross-sectional bandwidth)으로 40대의 서버를 연결할 수 있습니다. 스위치가 전통적인 컴퓨터 네트워크에만 국한된 것은 아니라는 점에 유의하십시오. PCIe 레인조차도 <a href="https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches">스위칭</a>될 수 있습니다. 이는 <a href="https://aws.amazon.com/ec2/instance-types/p2/">P2 인스턴스</a>의 경우처럼 많은 수의 GPU를 호스트 프로세서에 연결하기 위해 발생합니다.</li>
<li><strong>NVLink</strong>는 매우 높은 대역폭의 상호 연결에 있어 PCIe의 대안입니다. 링크당 최대 300 Gbit/s의 데이터 전송률을 제공합니다. 서버 GPU(Volta V100)는 6개의 링크를 갖는 반면 소비자용 GPU(RTX 2080 Ti)는 단 하나의 링크만 가지며 100 Gbit/s의 줄어든 속도로 작동합니다. GPU 간의 높은 데이터 전송을 달성하기 위해 <a href="https://github.com/NVIDIA/nccl">NCCL</a>을 사용할 것을 권장합니다.</li>
</ul>
<h2 id="더-많은-지연-시간-수치-more-latency-numbers"><a class="header" href="#더-많은-지연-시간-수치-more-latency-numbers">더 많은 지연 시간 수치 (More Latency Numbers)</a></h2>
<p>:numref:<code>table_latency_numbers</code> 및 :numref:<code>table_latency_numbers_tesla</code>의 요약은 수치들의 업데이트된 버전을 <a href="https://gist.github.com/eshelman/343a1c46cb3fba142c1afdcdeec17646">GitHub gist</a>로 관리하는 <a href="https://gist.github.com/eshelman">Eliot Eshelman</a>으로부터 가져온 것입니다.</p>
<p>:일반적인 지연 시간 수치.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">동작</th><th style="text-align: right">시간</th><th style="text-align: left">비고</th></tr></thead><tbody>
<tr><td style="text-align: left">L1 캐시 참조/히트</td><td style="text-align: right">1.5 ns</td><td style="text-align: left">4 사이클</td></tr>
<tr><td style="text-align: left">부동 소수점 덧셈/곱셈/FMA</td><td style="text-align: right">1.5 ns</td><td style="text-align: left">4 사이클</td></tr>
<tr><td style="text-align: left">L2 캐시 참조/히트</td><td style="text-align: right">5 ns</td><td style="text-align: left">12 ~ 17 사이클</td></tr>
<tr><td style="text-align: left">분기 예측 미스</td><td style="text-align: right">6 ns</td><td style="text-align: left">15 ~ 20 사이클</td></tr>
<tr><td style="text-align: left">L3 캐시 히트 (공유되지 않은 캐시)</td><td style="text-align: right">16 ns</td><td style="text-align: left">42 사이클</td></tr>
<tr><td style="text-align: left">L3 캐시 히트 (다른 코어에서 공유)</td><td style="text-align: right">25 ns</td><td style="text-align: left">65 사이클</td></tr>
<tr><td style="text-align: left">뮤텍스 락/언락</td><td style="text-align: right">25 ns</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">L3 캐시 히트 (다른 코어에서 수정됨)</td><td style="text-align: right">29 ns</td><td style="text-align: left">75 사이클</td></tr>
<tr><td style="text-align: left">L3 캐시 히트 (원격 CPU 소켓에 있음)</td><td style="text-align: right">40 ns</td><td style="text-align: left">100 ~ 300 사이클 (40 ~ 116 ns)</td></tr>
<tr><td style="text-align: left">다른 CPU로의 QPI 홉 (홉당)</td><td style="text-align: right">40 ns</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">64MB 메모리 참조 (로컬 CPU)</td><td style="text-align: right">46 ns</td><td style="text-align: left">Broadwell E5-2690v4의 TinyMemBench</td></tr>
<tr><td style="text-align: left">64MB 메모리 참조 (원격 CPU)</td><td style="text-align: right">70 ns</td><td style="text-align: left">Broadwell E5-2690v4의 TinyMemBench</td></tr>
<tr><td style="text-align: left">256MB 메모리 참조 (로컬 CPU)</td><td style="text-align: right">75 ns</td><td style="text-align: left">Broadwell E5-2690v4의 TinyMemBench</td></tr>
<tr><td style="text-align: left">Intel Optane 랜덤 쓰기</td><td style="text-align: right">94 ns</td><td style="text-align: left">UCSD Non-Volatile Systems Lab</td></tr>
<tr><td style="text-align: left">256MB 메모리 참조 (원격 CPU)</td><td style="text-align: right">120 ns</td><td style="text-align: left">Broadwell E5-2690v4의 TinyMemBench</td></tr>
<tr><td style="text-align: left">Intel Optane 랜덤 읽기</td><td style="text-align: right">305 ns</td><td style="text-align: left">UCSD Non-Volatile Systems Lab</td></tr>
<tr><td style="text-align: left">100 Gbps HPC 패브릭을 통한 4KB 전송</td><td style="text-align: right">1 μs</td><td style="text-align: left">Intel Omni-Path 상의 MVAPICH2</td></tr>
<tr><td style="text-align: left">Google Snappy로 1KB 압축</td><td style="text-align: right">3 μs</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">10 Gbps 이더넷을 통한 4KB 전송</td><td style="text-align: right">10 μs</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">NVMe SSD에 4KB 무작위 쓰기</td><td style="text-align: right">30 μs</td><td style="text-align: left">DC P3608 NVMe SSD (QOS 99%는 500μs)</td></tr>
<tr><td style="text-align: left">NVLink GPU로/로부터 1MB 전송</td><td style="text-align: right">30 μs</td><td style="text-align: left">NVIDIA 40GB NVLink에서 ~33GB/s</td></tr>
<tr><td style="text-align: left">PCI-E GPU로/로부터 1MB 전송</td><td style="text-align: right">80 μs</td><td style="text-align: left">PCIe 3.0 x16 링크에서 ~12GB/s</td></tr>
<tr><td style="text-align: left">NVMe SSD에서 4KB 무작위 읽기</td><td style="text-align: right">120 μs</td><td style="text-align: left">DC P3608 NVMe SSD (QOS 99%)</td></tr>
<tr><td style="text-align: left">NVMe SSD에서 1MB 순차 읽기</td><td style="text-align: right">208 μs</td><td style="text-align: left">DC P3608 NVMe SSD에서 ~4.8GB/s</td></tr>
<tr><td style="text-align: left">SATA SSD에 4KB 무작위 쓰기</td><td style="text-align: right">500 μs</td><td style="text-align: left">DC S3510 SATA SSD (QOS 99.9%)</td></tr>
<tr><td style="text-align: left">SATA SSD에서 4KB 무작위 읽기</td><td style="text-align: right">500 μs</td><td style="text-align: left">DC S3510 SATA SSD (QOS 99.9%)</td></tr>
<tr><td style="text-align: left">동일한 데이터 센터 내 라운드 트립</td><td style="text-align: right">500 μs</td><td style="text-align: left">단방향 핑은 ~250μs</td></tr>
<tr><td style="text-align: left">SATA SSD에서 1MB 순차 읽기</td><td style="text-align: right">2 ms</td><td style="text-align: left">DC S3510 SATA SSD에서 ~550MB/s</td></tr>
<tr><td style="text-align: left">디스크에서 1MB 순차 읽기</td><td style="text-align: right">5 ms</td><td style="text-align: left">서버 HDD에서 ~200MB/s</td></tr>
<tr><td style="text-align: left">랜덤 디스크 액세스 (탐색+회전)</td><td style="text-align: right">10 ms</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">캘리포니아-&gt;네덜란드-&gt;캘리포니아 패킷 전송</td><td style="text-align: right">150 ms</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">:label:<code>table_latency_numbers</code></td><td style="text-align: right"></td><td style="text-align: left"></td></tr>
</tbody></table>
</div>
<p>:NVIDIA Tesla GPU 지연 시간 수치.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">동작</th><th style="text-align: right">시간</th><th style="text-align: left">비고</th></tr></thead><tbody>
<tr><td style="text-align: left">GPU 공유 메모리(Shared Memory) 액세스</td><td style="text-align: right">30 ns</td><td style="text-align: left">30~90 사이클 (뱅크 충돌이 지연 추가)</td></tr>
<tr><td style="text-align: left">GPU 글로벌 메모리(Global Memory) 액세스</td><td style="text-align: right">200 ns</td><td style="text-align: left">200~800 사이클</td></tr>
<tr><td style="text-align: left">GPU에서 CUDA 커널 런칭</td><td style="text-align: right">10 μs</td><td style="text-align: left">호스트 CPU가 GPU에 커널 시작 지시</td></tr>
<tr><td style="text-align: left">NVLink GPU로/로부터 1MB 전송</td><td style="text-align: right">30 μs</td><td style="text-align: left">NVIDIA 40GB NVLink에서 ~33GB/s</td></tr>
<tr><td style="text-align: left">PCI-E GPU로/로부터 1MB 전송</td><td style="text-align: right">80 μs</td><td style="text-align: left">PCI-Express x16 링크에서 ~12GB/s</td></tr>
<tr><td style="text-align: left">:label:<code>table_latency_numbers_tesla</code></td><td style="text-align: right"></td><td style="text-align: left"></td></tr>
</tbody></table>
</div>
<h2 id="요약-summary-65"><a class="header" href="#요약-summary-65">요약 (Summary)</a></h2>
<ul>
<li>장치는 연산에 대한 오버헤드가 있습니다. 따라서 많은 작은 전송보다는 적은 수의 큰 전송을 목표로 하는 것이 중요합니다. 이는 RAM, SSD, 네트워크 및 GPU에 모두 적용됩니다.</li>
<li>벡터화는 성능의 핵심입니다. 가속기의 구체적인 능력을 숙지하십시오. 예: 일부 Intel Xeon CPU는 INT8 연산에 특히 뛰어나고, NVIDIA Volta GPU는 FP16 행렬-행렬 연산에 탁월하며, NVIDIA Turing은 FP16, INT8, INT4 연산에서 빛을 발합니다.</li>
<li>작은 데이터 타입으로 인한 수치적 오버플로는 훈련 중에 (추론 중에는 덜함) 문제가 될 수 있습니다.</li>
<li>앨리어싱(aliasing)은 성능을 크게 저하시킬 수 있습니다. 예를 들어, 64비트 CPU에서의 메모리 정렬은 64비트 경계에 맞춰 이루어져야 합니다. GPU에서는 합성곱 크기를 텐서 코어 등에 맞춰 정렬하는 것이 좋습니다.</li>
<li>알고리즘을 하드웨어(예: 메모리 사용량 및 대역폭)에 맞추십시오. 파라미터를 캐시에 맞출 때 엄청난 속도 향상(수 자릿수)을 얻을 수 있습니다.</li>
<li>실험 결과를 확인하기 전에 종이에 새로운 알고리즘의 성능을 스케치해 볼 것을 권장합니다. 한 자릿수 이상의 불일치는 우려할 만한 이유가 됩니다.</li>
<li>성능 병목 현상을 디버깅하기 위해 프로파일러를 사용하십시오.</li>
<li>훈련 및 추론 하드웨어는 가격과 성능 측면에서 서로 다른 최적의 지점(sweet spots)을 갖습니다.</li>
</ul>
<h2 id="연습-문제-exercises-80"><a class="header" href="#연습-문제-exercises-80">연습 문제 (Exercises)</a></h2>
<ol>
<li>외부 메모리 인터페이스에 대해 정렬된 메모리 액세스와 잘못 정렬된 메모리 액세스 사이의 속도 차이가 있는지 테스트하기 위해 C 코드를 작성하십시오. 힌트: 캐싱 효과에 주의하십시오.</li>
<li>메모리를 순차적으로 액세스할 때와 특정 스트라이드(stride)로 액세스할 때의 속도 차이를 테스트하십시오.</li>
<li>CPU의 캐시 크기를 어떻게 측정할 수 있을까요?</li>
<li>최대 대역폭을 얻기 위해 여러 메모리 채널에 데이터를 어떻게 배치하시겠습니까? 많은 작은 스레드가 있다면 어떻게 배치하시겠습니까?</li>
<li>엔터프라이즈급 HDD가 10,000 rpm으로 회전하고 있습니다. HDD가 데이터를 읽기 전에 최악의 경우 소비해야 하는 절대적인 최소 시간은 얼마입니까(헤드가 거의 즉시 움직인다고 가정할 수 있음)? 왜 2.5인치 HDD가 상업용 서버에서 인기를 얻고 있을까요(3.5인치 및 5.25인치 드라이브에 비해)?</li>
<li>HDD 제조업체가 저장 밀도를 제곱인치당 1Tbit에서 5Tbit로 늘린다고 가정합시다. 2.5인치 HDD의 링 하나에 얼마나 많은 정보를 저장할 수 있습니까? 내부 트랙과 외부 트랙 사이에 차이가 있습니까?</li>
<li>8비트에서 16비트 데이터 타입으로 이동하면 실리콘 양이 약 4배 증가합니다. 왜 그럴까요? 왜 NVIDIA가 Turing GPU에 INT4 연산을 추가했을까요?</li>
<li>메모리를 순방향으로 읽는 것이 역방향으로 읽는 것보다 얼마나 더 빠릅니까? 이 수치는 컴퓨터와 CPU 벤더마다 다릅니까? 왜 그럴까요? C 코드를 작성하여 실험해 보십시오.</li>
<li>디스크의 캐시 크기를 측정할 수 있습니까? 일반적인 HDD의 경우 얼마입니까? SSD에도 캐시가 필요할까요?</li>
<li>이더넷을 통해 메시지를 보낼 때 패킷 오버헤드를 측정하십시오. UDP와 TCP/IP 연결의 차이점을 찾아보십시오.</li>
<li>직접 메모리 액세스(DMA)는 CPU 이외의 장치가 메모리에 직접 쓰고 읽을 수 있게 해 줍니다. 이것이 왜 좋은 아이디어일까요?</li>
<li>Turing T4 GPU의 성능 수치를 보십시오. FP16에서 INT8 및 INT4로 이동할 때 왜 성능이 "단지" 두 배만 증가할까요?</li>
<li>샌프란시스코와 암스테르담 사이를 왕복하는 패킷이 걸리는 최단 시간은 얼마입니까? 힌트: 거리는 10,000km라고 가정할 수 있습니다.</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/363">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="다중-gpu-훈련-training-on-multiple-gpus"><a class="header" href="#다중-gpu-훈련-training-on-multiple-gpus">다중 GPU 훈련 (Training on Multiple GPUs)</a></h1>
<p>:label:<code>sec_multi_gpu</code></p>
<p>지금까지 우리는 CPU와 GPU에서 모델을 효율적으로 훈련하는 방법에 대해 논의했습니다. :numref:<code>sec_auto_para</code>에서는 딥러닝 프레임워크가 이들 간의 계산 및 통신을 자동으로 병렬화하는 방법까지 보여주었습니다. 또한 :numref:<code>sec_use_gpu</code>에서는 <code>nvidia-smi</code> 명령을 사용하여 컴퓨터에서 사용 가능한 모든 GPU를 나열하는 방법을 보여주었습니다.
우리가 논의하지 <em>않은</em> 것은 딥러닝 훈련을 실제로 병렬화하는 방법입니다.
대신 데이터를 여러 장치에 어떻게든 분할하여 작동하게 할 것이라고 암시했습니다. 이 섹션에서는 세부 사항을 채우고 처음부터 시작할 때 병렬로 네트워크를 훈련하는 방법을 보여줍니다. 고수준 API의 기능을 활용하는 방법에 대한 자세한 내용은 :numref:<code>sec_multi_gpu_concise</code>로 미룹니다.
:numref:<code>sec_minibatch_sgd</code>에 설명된 것과 같은 미니배치 확률적 경사 하강법 알고리즘에 익숙하다고 가정합니다.</p>
<h2 id="문제-분할-splitting-the-problem"><a class="header" href="#문제-분할-splitting-the-problem">문제 분할 (Splitting the Problem)</a></h2>
<p>간단한 컴퓨터 비전 문제와 약간 구식인 네트워크, 예를 들어 여러 층의 합성곱, 풀링, 그리고 마지막에 몇 개의 완전 연결 층이 있는 네트워크로 시작해 봅시다.
즉, LeNet :cite:<code>LeCun.Bottou.Bengio.ea.1998</code>이나 AlexNet :cite:<code>Krizhevsky.Sutskever.Hinton.2012</code>과 매우 유사해 보이는 네트워크로 시작해 봅시다.
여러 GPU(데스크탑 서버의 경우 2개, AWS g4dn.12xlarge 인스턴스의 경우 4개, p3.16xlarge의 경우 8개, p2.16xlarge의 경우 16개)가 주어졌을 때, 우리는 간단하고 재현 가능한 설계 선택의 이점을 동시에 누리면서 좋은 속도 향상을 달성하는 방식으로 훈련을 분할하고 싶습니다. 결국 여러 GPU는 <em>메모리</em>와 <em>계산</em> 능력을 모두 증가시킵니다. 요컨대, 분류하고자 하는 훈련 데이터의 미니배치가 주어졌을 때 다음과 같은 선택지가 있습니다.</p>
<p>첫째, 네트워크를 여러 GPU에 걸쳐 분할할 수 있습니다. 즉, 각 GPU는 특정 층으로 들어오는 데이터를 입력으로 받아 여러 후속 층에 걸쳐 데이터를 처리한 다음 데이터를 다음 GPU로 보냅니다.
이를 통해 단일 GPU가 처리할 수 있는 것보다 더 큰 네트워크로 데이터를 처리할 수 있습니다.
게다가
GPU당 메모리 사용량을 잘 제어할 수 있습니다(전체 네트워크 사용량의 일부임).</p>
<p>그러나 층(따라서 GPU) 간의 인터페이스에는 긴밀한 동기화가 필요합니다. 특히 층 간의 계산 작업 부하가 적절하게 일치하지 않는 경우 까다로울 수 있습니다. 이 문제는 GPU 수가 많을수록 악화됩니다.
층 간의 인터페이스는 또한 활성화 및 기울기와 같은 대량의 데이터 전송을 필요로 합니다.
이는 GPU 버스의 대역폭을 압도할 수 있습니다.
또한 계산 집약적이지만 순차적인 연산은 분할하기가 쉽지 않습니다. 이와 관련하여 최선의 노력에 대해서는 예: :citet:<code>Mirhoseini.Pham.Le.ea.2017</code>를 참조하십시오. 이는 여전히 어려운 문제이며 비자명한 문제에서 좋은(선형) 확장을 달성할 수 있는지 불분명합니다. 여러 GPU를 함께 연결하는 것에 대한 훌륭한 프레임워크나 운영 체제 지원이 없다면 권장하지 않습니다.</p>
<p>둘째, 작업을 층별로 분할할 수 있습니다. 예를 들어 단일 GPU에서 64개 채널을 계산하는 대신 4개의 GPU에 걸쳐 문제를 분할하여 각각 16개 채널에 대한 데이터를 생성하게 할 수 있습니다.
마찬가지로 완전 연결 층의 경우 출력 유닛 수를 분할할 수 있습니다. :numref:<code>fig_alexnet_original</code>(:citet:<code>Krizhevsky.Sutskever.Hinton.2012</code>에서 가져옴)은 이 설계를 보여주는데, 이 전략은 메모리 사용량이 매우 작은(당시 2GB) GPU를 처리하는 데 사용되었습니다.
채널(또는 유닛) 수가 너무 작지 않다면 계산 측면에서 좋은 확장을 허용합니다.
게다가
사용 가능한 메모리가 선형적으로 확장되므로 여러 GPU가 점점 더 큰 네트워크를 처리할 수 있습니다.</p>
<p><img src="chapter_computational-performance/../img/alexnet-original.svg" alt="제한된 GPU 메모리로 인한 원래 AlexNet 설계의 모델 병렬화." />
:label:<code>fig_alexnet_original</code></p>
<p>그러나
각 층이 다른 모든 층의 결과에 의존하기 때문에 <em>매우 많은</em> 수의 동기화 또는 장벽(barrier) 연산이 필요합니다.
게다가 전송해야 하는 데이터 양은 층을 GPU에 걸쳐 분산할 때보다 훨씬 더 클 수 있습니다. 따라서 대역폭 비용과 복잡성 때문에 이 접근 방식을 권장하지 않습니다.</p>
<p>마지막으로, 데이터를 여러 GPU에 걸쳐 분할할 수 있습니다. 이렇게 하면 모든 GPU가 서로 다른 관찰에 대해서지만 동일한 유형의 작업을 수행합니다. 기울기는 훈련 데이터의 각 미니배치 후에 GPU 간에 집계됩니다.
이것은 가장 간단한 접근 방식이며 모든 상황에 적용될 수 있습니다.
각 미니배치 후에만 동기화하면 됩니다. 그렇긴 하지만 다른 파라미터가 아직 계산되는 동안 이미 계산된 파라미터의 기울기 교환을 시작하는 것이 매우 바람직합니다.
또한 GPU 수가 많을수록 미니배치 크기가 커져 훈련 효율성이 높아집니다.
그러나 GPU를 더 추가한다고 해서 더 큰 모델을 훈련할 수 있는 것은 아닙니다.</p>
<p><img src="chapter_computational-performance/../img/splitting.svg" alt="여러 GPU에서의 병렬화. 왼쪽에서 오른쪽으로: 원래 문제, 네트워크 분할, 층별 분할, 데이터 병렬화." />
:label:<code>fig_splitting</code></p>
<p>여러 GPU에서의 다양한 병렬화 방법 비교는 :numref:<code>fig_splitting</code>에 묘사되어 있습니다.
대체로 충분히 큰 메모리를 가진 GPU에 액세스할 수 있다면 데이터 병렬화가 진행하기에 가장 편리한 방법입니다. 분산 훈련을 위한 분할에 대한 자세한 설명은 :cite:<code>Li.Andersen.Park.ea.2014</code>를 참조하십시오. GPU 메모리는 딥러닝 초기에 문제였지만 지금은 가장 특이한 경우를 제외하고는 이 문제가 해결되었습니다. 다음에서는 데이터 병렬화에 초점을 맞춥니다.</p>
<h2 id="데이터-병렬화-data-parallelism"><a class="header" href="#데이터-병렬화-data-parallelism">데이터 병렬화 (Data Parallelism)</a></h2>
<p>기계에 $k$개의 GPU가 있다고 가정합시다. 훈련할 모델이 주어지면 각 GPU는 파라미터 값이 GPU 간에 동일하고 동기화되지만 독립적으로 전체 모델 파라미터 세트를 유지 관리합니다.
예를 들어,
:numref:<code>fig_data_parallel</code>은 $k=2$일 때 데이터 병렬화로 훈련하는 것을 보여줍니다.</p>
<p><img src="chapter_computational-performance/../img/data-parallel.svg" alt="두 개의 GPU에서 데이터 병렬화를 사용한 미니배치 확률적 경사 하강법 계산." />
:label:<code>fig_data_parallel</code></p>
<p>일반적으로 훈련은 다음과 같이 진행됩니다:</p>
<ul>
<li>훈련의 임의의 반복에서 무작위 미니배치가 주어지면, 배치의 예제들을 $k$개 부분으로 나누고 GPU 전체에 균등하게 분배합니다.</li>
<li>각 GPU는 할당된 미니배치 부분 집합을 기반으로 손실과 모델 파라미터의 기울기를 계산합니다.</li>
<li>$k$개 GPU 각각의 로컬 기울기를 집계하여 현재 미니배치 확률적 기울기를 얻습니다.</li>
<li>집계된 기울기를 각 GPU에 재분배합니다.</li>
<li>각 GPU는 이 미니배치 확률적 기울기를 사용하여 유지 관리하는 전체 모델 파라미터 세트를 업데이트합니다.</li>
</ul>
<p>실제로 $k$개의 GPU에서 훈련할 때 미니배치 크기를 $k$배로 <em>늘려서</em> 각 GPU가 마치 단일 GPU에서 훈련하는 것처럼 동일한 양의 작업을 수행하도록 합니다. 16-GPU 서버에서 이는 미니배치 크기를 상당히 증가시킬 수 있으며 이에 따라 학습률을 높여야 할 수도 있습니다.
또한 :numref:<code>sec_batch_norm</code>의 배치 정규화는 조정이 필요합니다. 예를 들어 GPU당 별도의 배치 정규화 계수를 유지하는 것입니다.
다음에서는 장난감 네트워크를 사용하여 다중 GPU 훈련을 설명하겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<h2 id="장난감-네트워크-a-toy-network"><a class="header" href="#장난감-네트워크-a-toy-network">[<strong>장난감 네트워크 (A Toy Network)</strong>]</a></h2>
<p>:numref:<code>sec_lenet</code>에서 소개한 LeNet을 사용합니다(약간 수정됨). 파라미터 교환 및 동기화를 자세히 설명하기 위해 처음부터 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 모델 파라미터 초기화
scale = 0.01
W1 = np.random.normal(scale=scale, size=(20, 1, 3, 3))
b1 = np.zeros(20)
W2 = np.random.normal(scale=scale, size=(50, 20, 5, 5))
b2 = np.zeros(50)
W3 = np.random.normal(scale=scale, size=(800, 128))
b3 = np.zeros(128)
W4 = np.random.normal(scale=scale, size=(128, 10))
b4 = np.zeros(10)
params = [W1, b1, W2, b2, W3, b3, W4, b4]

# 모델 정의
def lenet(X, params):
    h1_conv = npx.convolution(data=X, weight=params[0], bias=params[1],
                              kernel=(3, 3), num_filter=20)
    h1_activation = npx.relu(h1_conv)
    h1 = npx.pooling(data=h1_activation, pool_type='avg', kernel=(2, 2),
                     stride=(2, 2))
    h2_conv = npx.convolution(data=h1, weight=params[2], bias=params[3],
                              kernel=(5, 5), num_filter=50)
    h2_activation = npx.relu(h2_conv)
    h2 = npx.pooling(data=h2_activation, pool_type='avg', kernel=(2, 2),
                     stride=(2, 2))
    h2 = h2.reshape(h2.shape[0], -1)
    h3_linear = np.dot(h2, params[4]) + params[5]
    h3 = npx.relu(h3_linear)
    y_hat = np.dot(h3, params[6]) + params[7]
    return y_hat

# 교차 엔트로피 손실 함수
loss = gluon.loss.SoftmaxCrossEntropyLoss()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 모델 파라미터 초기화
scale = 0.01
W1 = torch.randn(size=(20, 1, 3, 3)) * scale
b1 = torch.zeros(20)
W2 = torch.randn(size=(50, 20, 5, 5)) * scale
b2 = torch.zeros(50)
W3 = torch.randn(size=(800, 128)) * scale
b3 = torch.zeros(128)
W4 = torch.randn(size=(128, 10)) * scale
b4 = torch.zeros(10)
params = [W1, b1, W2, b2, W3, b3, W4, b4]

# 모델 정의
def lenet(X, params):
    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])
    h1_activation = F.relu(h1_conv)
    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))
    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])
    h2_activation = F.relu(h2_conv)
    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))
    h2 = h2.reshape(h2.shape[0], -1)
    h3_linear = torch.mm(h2, params[4]) + params[5]
    h3 = F.relu(h3_linear)
    y_hat = torch.mm(h3, params[6]) + params[7]
    return y_hat

# 교차 엔트로피 손실 함수
loss = nn.CrossEntropyLoss(reduction='none')
</code></pre>
<h2 id="데이터-동기화-data-synchronization"><a class="header" href="#데이터-동기화-data-synchronization">데이터 동기화 (Data Synchronization)</a></h2>
<p>효율적인 다중 GPU 훈련을 위해서는 두 가지 기본 연산이 필요합니다.
첫째, [<strong>파라미터 리스트를 여러 장치에 분배</strong>]하고 기울기를 첨부할 수 있는 기능(<code>get_params</code>)이 필요합니다. 파라미터 없이는 GPU에서 네트워크를 평가할 수 없습니다.
둘째, 여러 장치에 걸쳐 파라미터를 합산하는 기능, 즉 <code>allreduce</code> 함수가 필요합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def get_params(params, device):
    new_params = [p.copyto(device) for p in params]
    for p in new_params:
        p.attach_grad()
    return new_params
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def get_params(params, device):
    new_params = [p.to(device) for p in params]
    for p in new_params:
        p.requires_grad_()
    return new_params
</code></pre>
<p>모델 파라미터를 하나의 GPU에 복사하여 시도해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
new_params = get_params(params, d2l.try_gpu(0))
print('b1 weight:', new_params[1])
print('b1 grad:', new_params[1].grad)
</code></pre>
<p>아직 계산을 수행하지 않았으므로 편향 파라미터에 대한 기울기는 여전히 0입니다.
이제 벡터가 여러 GPU에 분산되어 있다고 가정해 봅시다. 다음 [<strong><code>allreduce</code> 함수는 모든 벡터를 더하고 결과를 모든 GPU에 다시 브로드캐스트합니다</strong>]. 이것이 작동하려면 결과를 누적하는 장치에 데이터를 복사해야 한다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def allreduce(data):
    for i in range(1, len(data)):
        data[0][:] += data[i].copyto(data[0].ctx)
    for i in range(1, len(data)):
        data[0].copyto(data[i])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def allreduce(data):
    for i in range(1, len(data)):
        data[0][:] += data[i].to(data[0].device)
    for i in range(1, len(data)):
        data[i][:] = data[0].to(data[i].device)
</code></pre>
<p>서로 다른 장치에 다른 값을 가진 벡터를 만들고 집계하여 이를 테스트해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
data = [np.ones((1, 2), ctx=d2l.try_gpu(i)) * (i + 1) for i in range(2)]
print('before allreduce:\n', data[0], '\n', data[1])
allreduce(data)
print('after allreduce:\n', data[0], '\n', data[1])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]
print('before allreduce:\n', data[0], '\n', data[1])
allreduce(data)
print('after allreduce:\n', data[0], '\n', data[1])
</code></pre>
<h2 id="데이터-분배-distributing-data"><a class="header" href="#데이터-분배-distributing-data">데이터 분배 (Distributing Data)</a></h2>
<p>우리는 [<strong>미니배치를 여러 GPU에 균등하게 분배</strong>]하는 간단한 유틸리티 함수가 필요합니다. 예를 들어 두 개의 GPU에서 데이터의 절반이 각 GPU에 복사되기를 원합니다.
더 편리하고 간결하므로 딥러닝 프레임워크의 내장 함수를 사용하여 $4 	imes 5$ 행렬에서 시도해 봅니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
data = np.arange(20).reshape(4, 5)
devices = [npx.gpu(0), npx.gpu(1)]
split = gluon.utils.split_and_load(data, devices)
print('input :', data)
print('load into', devices)
print('output:', split)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
data = torch.arange(20).reshape(4, 5)
devices = [torch.device('cuda:0'), torch.device('cuda:1')]
split = nn.parallel.scatter(data, devices)
print('input :', data)
print('load into', devices)
print('output:', split)
</code></pre>
<p>나중에 재사용하기 위해 데이터와 레이블을 모두 분할하는 <code>split_batch</code> 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def split_batch(X, y, devices):
    ""`X`와 `y`를 여러 장치로 분할합니다."""
    assert X.shape[0] == y.shape[0]
    return (gluon.utils.split_and_load(X, devices),
            gluon.utils.split_and_load(y, devices))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def split_batch(X, y, devices):
    ""`X`와 `y`를 여러 장치로 분할합니다."""
    assert X.shape[0] == y.shape[0]
    return (nn.parallel.scatter(X, devices),
            nn.parallel.scatter(y, devices))
</code></pre>
<h2 id="훈련-training-23"><a class="header" href="#훈련-training-23">훈련 (Training)</a></h2>
<p>이제 [<strong>단일 미니배치에 대한 다중 GPU 훈련</strong>]을 구현할 수 있습니다. 그 구현은 주로 이 섹션에서 설명한 데이터 병렬화 접근 방식을 기반으로 합니다. 방금 논의한 보조 함수 <code>allreduce</code>와 <code>split_and_load</code>를 사용하여 여러 GPU 간에 데이터를 동기화할 것입니다. 병렬화를 달성하기 위해 특정 코드를 작성할 필요가 없다는 점에 유의하십시오. 계산 그래프에는 미니배치 내에서 장치 간의 의존성이 없으므로 <em>자동으로</em> 병렬로 실행됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train_batch(X, y, device_params, devices, lr):
    X_shards, y_shards = split_batch(X, y, devices)
    with autograd.record():  # 손실은 각 GPU에서 별도로 계산됩니다
        ls = [loss(lenet(X_shard, device_W), y_shard)
              for X_shard, y_shard, device_W in zip(
                  X_shards, y_shards, device_params)]
    for l in ls:  # 역전파는 각 GPU에서 별도로 수행됩니다
        l.backward()
    # 각 GPU의 모든 기울기를 합산하여 모든 GPU에 브로드캐스트합니다
    for i in range(len(device_params[0])):
        allreduce([device_params[c][i].grad for c in range(len(devices))])
    # 모델 파라미터는 각 GPU에서 별도로 업데이트됩니다
    for param in device_params:
        d2l.sgd(param, lr, X.shape[0])  # 여기서 전체 크기 배치를 사용합니다
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train_batch(X, y, device_params, devices, lr):
    X_shards, y_shards = split_batch(X, y, devices)
    # 손실은 각 GPU에서 별도로 계산됩니다
    ls = [loss(lenet(X_shard, device_W), y_shard).sum()
          for X_shard, y_shard, device_W in zip(
              X_shards, y_shards, device_params)]
    for l in ls:  # 역전파는 각 GPU에서 별도로 수행됩니다
        l.backward()
    # 각 GPU의 모든 기울기를 합산하여 모든 GPU에 브로드캐스트합니다
    with torch.no_grad():
        for i in range(len(device_params[0])):
            allreduce([device_params[c][i].grad for c in range(len(devices))])
    # 모델 파라미터는 각 GPU에서 별도로 업데이트됩니다
    for param in device_params:
        d2l.sgd(param, lr, X.shape[0]) # 여기서 전체 크기 배치를 사용합니다
</code></pre>
<p>이제 [<strong>훈련 함수</strong>]를 정의할 수 있습니다. 이전 장에서 사용한 것과 약간 다릅니다: GPU를 할당하고 모든 모델 파라미터를 모든 장치에 복사해야 합니다.
분명히 각 배치는 다중 GPU를 처리하기 위해 <code>train_batch</code> 함수를 사용하여 처리됩니다. 편의상(그리고 코드의 간결성을 위해) 단일 GPU에서 정확도를 계산하지만, 다른 GPU가 유휴 상태이므로 <em>비효율적</em>입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train(num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]
    # 모델 파라미터를 `num_gpus`개의 GPU에 복사
    device_params = [get_params(params, d) for d in devices]
    num_epochs = 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
    timer = d2l.Timer()
    for epoch in range(num_epochs):
        timer.start()
        for X, y in train_iter:
            # 단일 미니배치에 대해 다중 GPU 훈련 수행
            train_batch(X, y, device_params, devices, lr)
            npx.waitall()
        timer.stop()
        # GPU 0에서 모델 평가
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(
            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))
    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '
          f'on {str(devices)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train(num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]
    # 모델 파라미터를 `num_gpus`개의 GPU에 복사
    device_params = [get_params(params, d) for d in devices]
    num_epochs = 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
    timer = d2l.Timer()
    for epoch in range(num_epochs):
        timer.start()
        for X, y in train_iter:
            # 단일 미니배치에 대해 다중 GPU 훈련 수행
            train_batch(X, y, device_params, devices, lr)
            torch.cuda.synchronize()
        timer.stop()
        # GPU 0에서 모델 평가
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(
            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))
    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '
          f'on {str(devices)}')
</code></pre>
<p>이것이 [<strong>단일 GPU에서</strong>] 얼마나 잘 작동하는지 봅시다.
먼저 배치 크기 256과 학습률 0.2를 사용합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
train(num_gpus=1, batch_size=256, lr=0.2)
</code></pre>
<p>배치 크기와 학습률을 변경하지 않고 [<strong>GPU 수를 2로 늘리면</strong>], 테스트 정확도가 이전 실험과 대략 동일하게 유지됨을 알 수 있습니다.
최적화 알고리즘 측면에서 그들은 동일합니다. 불행히도 여기서는 얻을 수 있는 의미 있는 속도 향상이 없습니다. 모델이 너무 작기 때문입니다. 게다가 데이터셋도 작아서 다중 GPU 훈련을 구현하는 우리의 약간 정교하지 못한 접근 방식이 상당한 Python 오버헤드로 인해 어려움을 겪었습니다. 앞으로 더 복잡한 모델과 더 정교한 병렬화 방법을 만나게 될 것입니다.
그럼에도 불구하고 Fashion-MNIST에서 어떤 일이 일어나는지 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
train(num_gpus=2, batch_size=256, lr=0.2)
</code></pre>
<h2 id="요약-summary-66"><a class="header" href="#요약-summary-66">요약 (Summary)</a></h2>
<ul>
<li>다중 GPU에 걸쳐 딥 네트워크 훈련을 분할하는 여러 가지 방법이 있습니다. 층 사이, 층 전체, 또는 데이터 전체에 걸쳐 분할할 수 있습니다. 앞의 두 가지는 데이터 전송을 엄격하게 조율해야 합니다. 데이터 병렬화는 가장 간단한 전략입니다.</li>
<li>데이터 병렬 훈련은 간단합니다. 그러나 효율성을 위해서는 유효 미니배치 크기를 늘려야 합니다.</li>
<li>데이터 병렬화에서 데이터는 여러 GPU에 걸쳐 분할되며, 각 GPU는 자체 순전파 및 역전파 연산을 실행하고 이후에 기울기가 집계되고 결과가 다시 GPU로 브로드캐스트됩니다.</li>
<li>더 큰 미니배치에 대해 약간 증가된 학습률을 사용할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-81"><a class="header" href="#연습-문제-exercises-81">연습 문제 (Exercises)</a></h2>
<ol>
<li>$k$개의 GPU에서 훈련할 때 미니배치 크기를 $b$에서 $k \cdot b$로 변경하십시오. 즉, GPU 수만큼 확장하십시오.</li>
<li>다양한 학습률에 대한 정확도를 비교하십시오. GPU 수에 따라 어떻게 확장됩니까?</li>
<li>서로 다른 GPU에서 다른 파라미터를 집계하는 더 효율적인 <code>allreduce</code> 함수를 구현하십시오. 왜 더 효율적입니까?</li>
<li>다중 GPU 테스트 정확도 계산을 구현하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/364">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1669">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="다중-gpu를-위한-간결한-구현-concise-implementation-for-multiple-gpus"><a class="header" href="#다중-gpu를-위한-간결한-구현-concise-implementation-for-multiple-gpus">다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></h1>
<p>:label:<code>sec_multi_gpu_concise</code></p>
<p>새로운 모델마다 병렬 처리를 처음부터 구현하는 것은 재미없는 일입니다. 게다가 고성능을 위해 동기화 도구를 최적화하는 것에는 상당한 이점이 있습니다. 다음에서는 딥러닝 프레임워크의 고수준 API를 사용하여 이를 수행하는 방법을 보여줍니다.
수학과 알고리즘은 :numref:<code>sec_multi_gpu</code>와 동일합니다.
이 섹션의 코드를 실행하려면 적어도 두 개의 GPU가 필요하다는 것은 놀라운 일이 아닙니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<h2 id="장난감-네트워크-a-toy-network-1"><a class="header" href="#장난감-네트워크-a-toy-network-1">[<strong>장난감 네트워크 (A Toy Network)</strong>]</a></h2>
<p>:numref:<code>sec_multi_gpu</code>의 LeNet보다 약간 더 의미 있으면서도 훈련하기 쉽고 빠른 네트워크를 사용해 봅시다.
우리는 ResNet-18 변형 :cite:<code>He.Zhang.Ren.ea.2016</code>을 선택합니다. 입력 이미지가 작기 때문에 약간 수정합니다. 특히 :numref:<code>sec_resnet</code>과의 차이점은 처음에 더 작은 합성곱 커널, 스트라이드, 패딩을 사용한다는 것입니다.
또한 최대 풀링 층을 제거합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def resnet18(num_classes):
    """약간 수정된 ResNet-18 모델."""
    def resnet_block(num_channels, num_residuals, first_block=False):
        blk = nn.Sequential()
        for i in range(num_residuals):
            if i == 0 and not first_block:
                blk.add(d2l.Residual(
                    num_channels, use_1x1conv=True, strides=2))
            else:
                blk.add(d2l.Residual(num_channels))
        return blk

    net = nn.Sequential()
    # 이 모델은 더 작은 합성곱 커널, 스트라이드, 패딩을 사용하고
    # 최대 풀링 층을 제거합니다
    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),
            nn.BatchNorm(), nn.Activation('relu'))
    net.add(resnet_block(64, 2, first_block=True),
            resnet_block(128, 2),
            resnet_block(256, 2),
            resnet_block(512, 2))
    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))
    return net
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def resnet18(num_classes, in_channels=1):
    """약간 수정된 ResNet-18 모델."""
    def resnet_block(in_channels, out_channels, num_residuals,
                     first_block=False):
        blk = []
        for i in range(num_residuals):
            if i == 0 and not first_block:
                blk.append(d2l.Residual(out_channels, use_1x1conv=True, 
                                        strides=2))
            else:
                blk.append(d2l.Residual(out_channels))
        return nn.Sequential(*blk)

    # 이 모델은 더 작은 합성곱 커널, 스트라이드, 패딩을 사용하고
    # 최대 풀링 층을 제거합니다
    net = nn.Sequential(
        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(64),
        nn.ReLU())
    net.add_module("resnet_block1", resnet_block(64, 64, 2, first_block=True))
    net.add_module("resnet_block2", resnet_block(64, 128, 2))
    net.add_module("resnet_block3", resnet_block(128, 256, 2))
    net.add_module("resnet_block4", resnet_block(256, 512, 2))
    net.add_module("global_avg_pool", nn.AdaptiveAvgPool2d((1,1)))
    net.add_module("fc", nn.Sequential(nn.Flatten(),
                                       nn.Linear(512, num_classes)))
    return net
</code></pre>
<h2 id="네트워크-초기화-network-initialization"><a class="header" href="#네트워크-초기화-network-initialization">네트워크 초기화 (Network Initialization)</a></h2>
<p>:begin_tab:<code>mxnet</code>
<code>initialize</code> 함수를 사용하면 선택한 장치에서 파라미터를 초기화할 수 있습니다.
초기화 방법에 대한 복습은 :numref:<code>sec_numerical_stability</code>를 참조하십시오. 특히 편리한 점은 <em>여러</em> 장치에서 동시에 네트워크를 초기화할 수도 있다는 것입니다. 이것이 실제로 어떻게 작동하는지 시도해 봅시다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
우리는 훈련 루프 내에서 네트워크를 초기화할 것입니다.
초기화 방법에 대한 복습은 :numref:<code>sec_numerical_stability</code>를 참조하십시오.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = resnet18(10)
# GPU 목록 가져오기
devices = d2l.try_all_gpus()
# 네트워크의 모든 파라미터 초기화
net.initialize(init=init.Normal(sigma=0.01), ctx=devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = resnet18(10)
# GPU 목록 가져오기
devices = d2l.try_all_gpus()
# 우리는 훈련 루프 내에서 네트워크를 초기화할 것입니다
</code></pre>
<p>:begin_tab:<code>mxnet</code>
:numref:<code>sec_multi_gpu</code>에서 소개한 <code>split_and_load</code> 함수를 사용하여 데이터 미니배치를 나누고 <code>devices</code> 변수가 제공하는 장치 목록으로 복사할 수 있습니다. 네트워크 인스턴스는 순전파 값을 계산하기 위해 <em>자동으로</em> 적절한 GPU를 사용합니다. 여기서는 4개의 관찰을 생성하고 GPU에 분할합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.random.uniform(size=(4, 1, 28, 28))
x_shards = gluon.utils.split_and_load(x, devices)
net(x_shards[0]), net(x_shards[1])
</code></pre>
<p>:begin_tab:<code>mxnet</code>
데이터가 네트워크를 통과하면 해당 파라미터는 <em>데이터가 통과한 장치에서</em> 초기화됩니다.
이는 초기화가 장치별로 발생함을 의미합니다. 초기화를 위해 GPU 0과 GPU 1을 선택했으므로 네트워크는 거기에서만 초기화되고 CPU에서는 초기화되지 않습니다. 사실 파라미터는 CPU에 존재하지도 않습니다. 파라미터를 출력하고 발생할 수 있는 오류를 관찰하여 이를 확인할 수 있습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
weight = net[0].params.get('weight')

try:
    weight.data()
except RuntimeError:
    print('not initialized on cpu')
weight.data(devices[0])[0], weight.data(devices[1])[0]
</code></pre>
<p>:begin_tab:<code>mxnet</code>
다음으로, [<strong>정확도를 평가하는</strong>] 코드를 (<strong>여러 장치에서 병렬로</strong>) 작동하는 코드로 대체해 보겠습니다. 이는 :numref:<code>sec_lenet</code>의 <code>evaluate_accuracy_gpu</code> 함수를 대체하는 역할을 합니다. 주요 차이점은 네트워크를 호출하기 전에 미니배치를 분할한다는 것입니다. 그 외에는 본질적으로 동일합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def evaluate_accuracy_gpus(net, data_iter, split_f=d2l.split_batch):
    """여러 GPU를 사용하여 데이터셋에서 모델의 정확도를 계산합니다."""
    # 장치 목록 쿼리
    devices = list(net.collect_params().values())[0].list_ctx()
    # 올바른 예측 수, 예측 수
    metric = d2l.Accumulator(2)
    for features, labels in data_iter:
        X_shards, y_shards = split_f(features, labels, devices)
        # 병렬 실행
        pred_shards = [net(X_shard) for X_shard in X_shards]
        metric.add(sum(float(d2l.accuracy(pred_shard, y_shard)) for
                       pred_shard, y_shard in zip(
                           pred_shards, y_shards)), labels.size)
    return metric[0] / metric[1]
</code></pre>
<h2 id="훈련-training-24"><a class="header" href="#훈련-training-24">[<strong>훈련 (Training)</strong>]</a></h2>
<p>이전과 마찬가지로, 훈련 코드는 효율적인 병렬화를 위해 몇 가지 기본 기능을 수행해야 합니다:</p>
<ul>
<li>네트워크 파라미터는 모든 장치에서 초기화되어야 합니다.</li>
<li>데이터셋을 반복하는 동안 미니배치는 모든 장치에 나누어져야 합니다.</li>
<li>우리는 장치 전체에서 손실과 그 기울기를 병렬로 계산합니다.</li>
<li>기울기가 집계되고 파라미터가 그에 따라 업데이트됩니다.</li>
</ul>
<p>마지막으로 우리는 네트워크의 최종 성능을 보고하기 위해 정확도를 (다시 병렬로) 계산합니다. 훈련 루틴은 데이터를 분할하고 집계해야 한다는 점을 제외하고는 이전 장의 구현과 매우 유사합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train(num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    ctx = [d2l.try_gpu(i) for i in range(num_gpus)]
    net.initialize(init=init.Normal(sigma=0.01), ctx=ctx, force_reinit=True)
    trainer = gluon.Trainer(net.collect_params(), 'sgd',
                            {'learning_rate': lr})
    loss = gluon.loss.SoftmaxCrossEntropyLoss()
    timer, num_epochs = d2l.Timer(), 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
    for epoch in range(num_epochs):
        timer.start()
        for features, labels in train_iter:
            X_shards, y_shards = d2l.split_batch(features, labels, ctx)
            with autograd.record():
                ls = [loss(net(X_shard), y_shard) for X_shard, y_shard
                      in zip(X_shards, y_shards)]
            for l in ls:
                l.backward()
            trainer.step(batch_size)
        npx.waitall()
        timer.stop()
        animator.add(epoch + 1, (evaluate_accuracy_gpus(net, test_iter),))
    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '
          f'on {str(ctx)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train(net, num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]
    def init_weights(module):
        if type(module) in [nn.Linear, nn.Conv2d]:
            nn.init.normal_(module.weight, std=0.01)
    net.apply(init_weights)
    # 모델을 여러 GPU에 설정
    net = nn.DataParallel(net, device_ids=devices)
    trainer = torch.optim.SGD(net.parameters(), lr)
    loss = nn.CrossEntropyLoss()
    timer, num_epochs = d2l.Timer(), 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
    for epoch in range(num_epochs):
        net.train()
        timer.start()
        for X, y in train_iter:
            trainer.zero_grad()
            X, y = X.to(devices[0]), y.to(devices[0])
            l = loss(net(X), y)
            l.backward()
            trainer.step()
        timer.stop()
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))
    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '
          f'on {str(devices)}')
</code></pre>
<p>실제로 어떻게 작동하는지 봅시다. 워밍업으로 [<strong>단일 GPU에서 네트워크를 훈련</strong>]시킵니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train(num_gpus=1, batch_size=256, lr=0.1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train(net, num_gpus=1, batch_size=256, lr=0.1)
</code></pre>
<p>다음으로 [<strong>훈련에 2개의 GPU를 사용</strong>]합니다. :numref:<code>sec_multi_gpu</code>에서 평가된 LeNet과 비교할 때, ResNet-18 모델은 상당히 더 복잡합니다. 이것이 병렬화가 이점을 보여주는 곳입니다. 계산 시간이 파라미터 동기화 시간보다 의미 있게 더 큽니다. 병렬화에 대한 오버헤드가 덜 관련성이 있으므로 확장성이 향상됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train(num_gpus=2, batch_size=512, lr=0.2)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train(net, num_gpus=2, batch_size=512, lr=0.2)
</code></pre>
<h2 id="요약-summary-67"><a class="header" href="#요약-summary-67">요약 (Summary)</a></h2>
<p>:begin_tab:<code>mxnet</code></p>
<ul>
<li>Gluon은 컨텍스트 목록을 제공하여 여러 장치에서 모델을 초기화하기 위한 프리미티브를 제공합니다.
:end_tab:</li>
<li>데이터는 데이터가 있는 장치에서 자동으로 평가됩니다.</li>
<li>해당 장치의 파라미터에 액세스하기 전에 각 장치에서 네트워크를 초기화하도록 주의하십시오. 그렇지 않으면 오류가 발생합니다.</li>
<li>최적화 알고리즘은 여러 GPU에 걸쳐 자동으로 집계됩니다.</li>
</ul>
<h2 id="연습-문제-exercises-82"><a class="header" href="#연습-문제-exercises-82">연습 문제 (Exercises)</a></h2>
<p>:begin_tab:<code>mxnet</code></p>
<ol>
<li>이 섹션에서는 ResNet-18을 사용합니다. 다양한 에폭, 배치 크기, 학습률을 시도해 보십시오. 계산에 더 많은 GPU를 사용해 보십시오. 16개의 GPU(예: AWS p2.16xlarge 인스턴스)로 시도하면 어떻게 됩니까?</li>
<li>때때로 다른 장치는 다른 계산 능력을 제공합니다. GPU와 CPU를 동시에 사용할 수 있습니다. 작업을 어떻게 나누어야 합니까? 노력할 가치가 있습니까? 왜 그렇습니까? 왜 그렇지 않습니까?</li>
<li><code>npx.waitall()</code>을 생략하면 어떻게 됩니까? 병렬화를 위해 최대 2단계의 중첩을 갖도록 훈련을 어떻게 수정하겠습니까?
:end_tab:</li>
</ol>
<p>:begin_tab:<code>pytorch</code></p>
<ol>
<li>이 섹션에서는 ResNet-18을 사용합니다. 다양한 에폭, 배치 크기, 학습률을 시도해 보십시오. 계산에 더 많은 GPU를 사용해 보십시오. 16개의 GPU(예: AWS p2.16xlarge 인스턴스)로 시도하면 어떻게 됩니까?</li>
<li>때때로 다른 장치는 다른 계산 능력을 제공합니다. GPU와 CPU를 동시에 사용할 수 있습니다. 작업을 어떻게 나누어야 합니까? 노력할 가치가 있습니까? 왜 그렇습니까? 왜 그렇지 않습니까?
:end_tab:</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/365">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1403">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="파라미터-서버-parameter-servers"><a class="header" href="#파라미터-서버-parameter-servers">파라미터 서버 (Parameter Servers)</a></h1>
<p>:label:<code>sec_parameterserver</code></p>
<p>단일 GPU에서 다중 GPU로, 그리고 다중 GPU를 포함하는 다중 서버로 이동함에 따라(아마도 모두 여러 랙과 네트워크 스위치에 걸쳐 분산되어 있을 것입니다), 분산 및 병렬 훈련을 위한 우리의 알고리즘은 훨씬 더 정교해져야 합니다. 서로 다른 상호 연결은 매우 다른 대역폭을 갖기 때문에 세부 사항이 중요합니다(예: NVLink는 적절한 설정에서 6개 링크를 가로질러 최대 100 GB/s를 제공할 수 있고, PCIe 4.0(16레인)은 32 GB/s를 제공하는 반면, 고속 100GbE 이더넷조차도 10 GB/s에 불과합니다). 동시에 통계 모델러가 네트워킹 및 시스템 전문가가 되기를 기대하는 것은 비합리적입니다.</p>
<p>파라미터 서버의 핵심 아이디어는 분산 잠재 변수 모델의 맥락에서 :citet:<code>Smola.Narayanamurthy.2010</code>에서 도입되었습니다. push 및 pull 시맨틱에 대한 설명은 :citet:<code>Ahmed.Aly.Gonzalez.ea.2012</code>에서 이어졌고, 시스템 및 오픈 소스 라이브러리에 대한 설명은 :citet:<code>Li.Andersen.Park.ea.2014</code>에서 이어졌습니다. 다음에서는 효율성을 위해 필요한 구성 요소들에 대해 동기를 부여할 것입니다.</p>
<h2 id="데이터-병렬-훈련-data-parallel-training"><a class="header" href="#데이터-병렬-훈련-data-parallel-training">데이터 병렬 훈련 (Data-Parallel Training)</a></h2>
<p>분산 훈련에 대한 데이터 병렬 훈련 접근 방식을 검토해 봅시다. 실전에서 구현하기가 현저히 간단하기 때문에 이 섹션에서는 다른 모든 것들을 제외하고 이것만 사용할 것입니다. 요즘 GPU는 메모리가 풍부하기 때문에 다른 병렬 처리 전략이 선호되는 사용 사례(그래프 상의 딥러닝 제외)는 거의 없습니다. :numref:<code>fig_parameterserver</code>는 우리가 :numref:<code>sec_multi_gpu</code>에서 구현한 데이터 병렬화의 변형을 설명합니다. 여기서 핵심 측면은 업데이트된 파라미터가 모든 GPU에 재브로드캐스트되기 전에 단일 GPU(GPU 0)에서 기울기의 집계가 발생한다는 것입니다.</p>
<p><img src="chapter_computational-performance/../img/ps.svg" alt="왼쪽: 단일 GPU 훈련. 오른쪽: 다중 GPU 훈련의 변형: (1) 손실과 기울기를 계산함, (2) 모든 기울기가 하나의 GPU에서 집계됨, (3) 파라미터 업데이트가 발생하고 파라미터가 모든 GPU에 재분배됨." />
:label:<code>fig_parameterserver</code></p>
<p>지나고 나서 보면, GPU 0에서 집계하기로 한 결정은 다소 임의적으로 보입니다. 결국 CPU에서 집계할 수도 있습니다. 사실, 일부 파라미터는 한 GPU에서 집계하고 다른 일부는 다른 GPU에서 집계하기로 결정할 수도 있습니다. 최적화 알고리즘이 이를 지원한다면 그렇게 하지 못할 실제 이유는 없습니다. 예를 들어 연관된 기울기 $\mathbf{g}_1, \ldots, \mathbf{g}_4$를 가진 네 개의 파라미터 벡터가 있다면, 각 $\mathbf{g}_i$ ($i = 1, \ldots, 4$)에 대해 하나의 GPU에서 기울기를 집계할 수 있습니다.</p>
<p>이 추론은 자의적이고 경박해 보일 수 있습니다. 결국 수학은 전반적으로 동일하기 때문입니다. 하지만 우리는 :numref:<code>sec_hardware</code>에서 논의한 대로 서로 다른 버스가 서로 다른 대역폭을 갖는 실제 물리적 하드웨어를 다루고 있습니다.
실제 4-way GPU 서버를 고려해 보십시오 :numref:<code>fig_bw_hierarchy</code>. 특히 연결이 잘 되어 있다면 100 GbE 네트워크 카드를 가질 수도 있습니다. 더 일반적인 수치는 100 MB/s에서 1 GB/s의 유효 대역폭을 가진 1~10 GbE 범위입니다.
CPU는 모든 GPU를 직접 연결하기에 PCIe 레인이 너무 적기 때문에(예: 소비자급 Intel CPU는 24개 레인을 가짐), 우리는 <a href="https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches">멀티플렉서</a>가 필요합니다. 16x Gen3 링크에서 CPU로부터의 대역폭은 16 GB/s입니다. 이는 또한 <em>각</em> GPU가 스위치에 연결되는 속도이기도 합니다. 이는 장치들끼리 통신하는 것이 더 효과적임을 의미합니다.</p>
<p><img src="chapter_computational-performance/../img/bw-hierarchy.svg" alt="4-way GPU 서버." />
:label:<code>fig_bw_hierarchy</code></p>
<p>논의를 위해 기울기가 160 MB라고 가정해 봅시다. 이 경우 나머지 3개의 GPU에서 네 번째 GPU로 기울기를 보내는 데 30ms가 걸립니다(각 전송에는 10ms = 160 MB / 16 GB/s가 소요됨). 가중치 벡터를 다시 전송하기 위해 또 다른 30ms를 더하면 총 60ms에 도달합니다.
모든 데이터를 CPU로 보낸다면 네 개의 GPU <em>각각</em>이 데이터를 CPU로 보내야 하므로 40ms의 페널티가 발생하여 총 80ms가 됩니다. 마지막으로 기울기를 각각 40 MB씩 4개 부분으로 나눌 수 있다고 가정해 봅시다. 이제 PCIe 스위치가 모든 링크 간에 풀 대역폭 연산을 제공하므로, 각 부분을 서로 다른 GPU에서 <em>동시에</em> 집계할 수 있습니다. 30ms 대신 7.5ms가 소요되어 동기화 작업에 총 15ms가 걸립니다. 요컨대, 파라미터를 어떻게 동기화하느냐에 따라 동일한 작업이 15ms에서 80ms까지 걸릴 수 있습니다. :numref:<code>fig_ps_distributed</code>는 파라미터 교환을 위한 서로 다른 전략을 묘사합니다.</p>
<p><img src="chapter_computational-performance/../img/ps-distributed.svg" alt="파라미터 동기화 전략." />
:label:<code>fig_ps_distributed</code></p>
<p>성능을 향상시키는 데 있어 우리가 처분할 수 있는 또 다른 도구가 있음에 유의하십시오: 심층 네트워크에서 상단부터 하단까지 모든 기울기를 계산하는 데는 시간이 좀 걸립니다. 다른 파라미터 그룹에 대해 여전히 계산하느라 바쁜 동안에도 이미 계산된 일부 파라미터 그룹에 대해 기울기 동기화를 시작할 수 있습니다. <a href="https://github.com/horovod/horovod">Horovod</a>에서 이를 수행하는 방법에 대한 자세한 내용은 예: :citet:<code>Sergeev.Del-Balso.2018</code>를 참조하십시오.</p>
<h2 id="링-동기화-ring-synchronization"><a class="header" href="#링-동기화-ring-synchronization">링 동기화 (Ring Synchronization)</a></h2>
<p>현대 딥러닝 하드웨어에서의 동기화와 관련하여, 우리는 종종 상당히 맞춤화된 네트워크 연결성을 마주하게 됩니다. 예를 들어 AWS p3.16xlarge 및 NVIDIA DGX-2 인스턴스는 :numref:<code>fig_nvlink</code>의 연결 구조를 공유합니다. 각 GPU는 최상으로 16 GB/s로 작동하는 PCIe 링크를 통해 호스트 CPU에 연결됩니다. 추가적으로 각 GPU는 또한 6개의 NVLink 연결을 가지며, 각 연결은 양방향으로 300 Gbit/s를 전송할 수 있습니다. 이는 방향당 링크당 약 18 GB/s에 해당합니다. 요컨대, 집계된 NVLink 대역폭은 PCIe 대역폭보다 현저히 높습니다. 질문은 이를 어떻게 가장 효율적으로 사용하느냐입니다.</p>
<p><img src="chapter_computational-performance/../img/nvlink.svg" alt="8개의 V100 GPU 서버에서의 NVLink 연결성 (이미지 제공: NVIDIA)." />
:label:<code>fig_nvlink</code></p>
<p>최적의 동기화 전략은 네트워크를 두 개의 링으로 분해하고 이를 사용하여 데이터를 직접 동기화하는 것으로 밝혀졌습니다 :cite:<code>Wang.Li.Liberty.ea.2018</code>. :numref:<code>fig_nvlink_twoloop</code>은 네트워크가 이중 NVLink 대역폭을 가진 하나의 링(1-2-3-4-5-6-7-8-1)과 일반 대역폭을 가진 다른 하나(1-4-6-3-5-8-2-7-1)로 분해될 수 있음을 보여줍니다. 이 경우 효율적인 동기화 프로토콜을 설계하는 것은 간단하지 않습니다.</p>
<p><img src="chapter_computational-performance/../img/nvlink-twoloop.svg" alt="NVLink 네트워크를 두 개의 링으로 분해." />
:label:<code>fig_nvlink_twoloop</code></p>
<p>다음 사고 실험을 고려해 보십시오: $n$개의 컴퓨팅 노드(또는 GPU)로 구성된 링이 주어졌을 때, 우리는 첫 번째 노드에서 두 번째 노드로 기울기를 보낼 수 있습니다. 거기서 로컬 기울기에 더해져 세 번째 노드로 전송되고, 이런 식으로 계속됩니다. $n-1$단계 후에 집계된 기울기는 마지막으로 방문한 노드에서 찾을 수 있습니다. 즉, 기울기를 집계하는 시간은 노드 수에 따라 선형적으로 증가합니다. 하지만 이렇게 하면 알고리즘이 상당히 비효율적입니다. 결국 어느 시점에서도 통신하는 노드는 하나뿐이기 때문입니다. 기울기를 $n$개의 덩어리로 나누고 노드 $i$에서 시작하여 덩어리 $i$를 동기화하기 시작하면 어떨까요?
각 덩어리의 크기가 $1/n$이므로 이제 총 시간은 $(n-1)/n \approx 1$입니다. 즉, 기울기를 집계하는 데 소요되는 시간은 링의 크기가 커짐에 따라 <em>증가하지 않습니다</em>. 이는 꽤 놀라운 결과입니다. :numref:<code>fig_ringsync</code>는 $n=4$개 노드에서의 단계 시퀀스를 보여줍니다.</p>
<p><img src="chapter_computational-performance/../img/ringsync.svg" alt="4개 노드에서의 링 동기화. 각 노드는 조립된 기울기가 오른쪽 이웃에서 발견될 때까지 기울기의 일부를 왼쪽 이웃으로 전송하기 시작합니다." />
:label:<code>fig_ringsync</code></p>
<p>8개의 V100 GPU 전체에서 160 MB를 동기화하는 동일한 예제를 사용하면 약 $2 \cdot 160 \textrm{MB} / (3 \cdot 18 \textrm{GB/s}) \approx 6 \textrm{ms}$에 도달합니다. 이는 이제 8개의 GPU를 사용하고 있음에도 불구하고 PCIe 버스를 사용하는 것보다 낫습니다. 실전에서 이러한 수치들은 조금 더 나쁜데, 딥러닝 프레임워크가 종종 통신을 큰 버스트 전송으로 조립하는 데 실패하기 때문입니다.</p>
<p>링 동기화가 다른 동기화 알고리즘과 근본적으로 다르다는 일반적인 오해가 있음에 유의하십시오. 유일한 차이점은 단순한 트리 구조와 비교할 때 동기화 경로가 다소 더 정교하다는 점뿐입니다.</p>
<h2 id="다중-기기-훈련-multi-machine-training"><a class="header" href="#다중-기기-훈련-multi-machine-training">다중 기기 훈련 (Multi-Machine Training)</a></h2>
<p>여러 대의 기기에서 분산 훈련을 하는 것은 추가적인 과제를 더합니다: 우리는 어떤 경우에는 한 자릿수 이상 더 느릴 수 있는 상대적으로 낮은 대역폭의 패브릭을 통해서만 연결된 서버들과 통신해야 합니다.
장치들 간의 동기화는 까다롭습니다. 결국 훈련 코드를 실행하는 서로 다른 기기들은 미묘하게 다른 속도를 가질 것이기 때문입니다. 따라서 동기식 분산 최적화를 사용하려면 이들을 <em>동기화</em>해야 합니다. :numref:<code>fig_ps_multimachine</code>은 분산 병렬 훈련이 어떻게 발생하는지 설명합니다.</p>
<ol>
<li>각 기기에서 (서로 다른) 데이터 배치를 읽고, 여러 GPU에 분할하여 GPU 메모리로 전송합니다. 거기서 각 GPU 배치에 대해 별도로 예측과 기울기를 계산합니다.</li>
<li>모든 로컬 GPU로부터의 기울기가 하나의 GPU에서 집계됩니다(또는 그 일부가 서로 다른 GPU들에서 집계됩니다).</li>
<li>기울기가 CPU로 전송됩니다.</li>
<li>CPU는 모든 기울기를 집계하는 중앙 파라미터 서버로 기울기를 보냅니다.</li>
<li>그런 다음 집계된 기울기를 사용하여 파라미터를 업데이트하고, 업데이트된 파라미터를 개별 CPU로 다시 브로드캐스트합니다.</li>
<li>정보가 하나(또는 여러 개)의 GPU로 전송됩니다.</li>
<li>업데이트된 파라미터가 모든 GPU에 퍼집니다.</li>
</ol>
<p><img src="chapter_computational-performance/../img/ps-multimachine.svg" alt="다중 기기 다중 GPU 분산 병렬 훈련." />
:label:<code>fig_ps_multimachine</code></p>
<p>이러한 각 연산은 상당히 직관적으로 보입니다. 그리고 실제로 단일 기기 <em>내에서</em> 효율적으로 수행될 수 있습니다. 하지만 여러 기기를 살펴보면 중앙 파라미터 서버가 병목 현상이 됨을 알 수 있습니다. 결국 서버당 대역폭이 제한되어 있으므로, $m$명의 작업자(workers)에 대해 모든 기울기를 서버로 보내는 데 걸리는 시간은 $\mathcal{O}(m)$입니다. 서버 수를 $n$으로 늘림으로써 이 장벽을 뚫을 수 있습니다. 이 시점에서 각 서버는 파라미터의 $\mathcal{O}(1/n)$만 저장하면 되므로, 업데이트 및 최적화를 위한 총 시간은 $\mathcal{O}(m/n)$이 됩니다.
두 수치를 일치시키면 우리가 얼마나 많은 작업자를 다루든 관계없이 일정한 확장을 얻을 수 있습니다. 실전에서 우리는 작업자와 서버로 <em>동일한</em> 기기를 사용합니다. :numref:<code>fig_ps_multips</code>는 이 설계를 보여줍니다(자세한 내용은 :cite:<code>Li.Andersen.Park.ea.2014</code> 참조).
특히 여러 기기가 비합리적인 지연 없이 작동하도록 보장하는 것은 간단하지 않습니다.</p>
<p><img src="chapter_computational-performance/../img/ps-multips.svg" alt="상단: 단일 파라미터 서버는 대역폭이 한정되어 있어 병목 현상이 됩니다. 하단: 여러 파라미터 서버가 집계된 대역폭으로 파라미터의 일부를 저장합니다." />
:label:<code>fig_ps_multips</code></p>
<h2 id="키-값-저장소-key--value-stores"><a class="header" href="#키-값-저장소-key--value-stores">키-값 저장소 (Key--Value Stores)</a></h2>
<p>분산 다중 GPU 훈련에 필요한 단계들을 실전에서 구현하는 것은 비자명합니다.
이것이 공통적인 추상화, 즉 재정의된 업데이트 시맨틱을 가진 *키-값 저장소(key--value store)*의 추상화를 사용하는 것이 유익한 이유입니다.</p>
<p>많은 작업자와 많은 GPU에 걸쳐 기울기 $i$에 대한 계산은 다음과 같이 정의될 수 있습니다.</p>
<p>$$\mathbf{g}<em>{i} = \sum</em>{k \in \textrm{workers}} \sum_{j \in \textrm{GPUs}} \mathbf{g}_{ijk},$$</p>
<p>여기서 $\mathbf{g}_{ijk}$는 작업자 $k$의 GPU $j$에서 분할된 기울기 $i$의 일부입니다.
이 연산의 핵심 측면은 그것이 *교환 법칙이 성립하는 축소(commutative reduction)*라는 점입니다. 즉, 많은 벡터를 하나로 바꾸고 연산이 적용되는 순서가 중요하지 않다는 것입니다. 이는 우리가 어떤 기울기가 언제 수신되는지에 대해 세밀하게 제어할 필요가 없기 때문에 우리의 목적에 아주 좋습니다. 게다가 이 연산은 서로 다른 $i$ 간에 독립적임에 유의하십시오.</p>
<p>이를 통해 다음과 같은 두 가지 연산을 정의할 수 있습니다: 기울기를 누적하는 <em>push</em>, 그리고 집계된 기울기를 검색하는 <em>pull</em>입니다. 우리는 많은 서로 다른 기울기 세트(결국 많은 레이어가 있으므로)를 가지고 있기 때문에, 키 $i$로 기울기를 인덱싱해야 합니다. Dynamo :cite:<code>DeCandia.Hastorun.Jampani.ea.2007</code> 등에서 도입된 키-값 저장소와의 이러한 유사성은 우연이 아닙니다. 이들도 특히 여러 서버에 걸쳐 파라미터를 분산시키는 것과 관련하여 많은 유사한 특성을 만족하기 때문입니다.</p>
<p>키-값 저장소를 위한 push 및 pull 연산은 다음과 같이 설명됩니다:</p>
<ul>
<li>**push(key, value)**는 특정 기울기(값)를 작업자로부터 공통 저장소로 보냅니다. 거기서 값은 예를 들어 합산됨으로써 집계됩니다.</li>
<li>**pull(key, value)**은 모든 작업자로부터의 기울기를 결합한 후와 같이 공통 저장소로부터 집계된 값을 검색합니다.</li>
</ul>
<p>동기화에 대한 모든 복잡성을 간단한 push 및 pull 연산 뒤에 숨김으로써, 우리는 최적화를 간단한 용어로 표현하고 싶어 하는 통계 모델러의 관심사와 분산 동기화에 내재된 복잡성을 다뤄야 하는 시스템 엔지니어의 관심사를 분리할 수 있습니다.</p>
<h2 id="요약-summary-68"><a class="header" href="#요약-summary-68">요약 (Summary)</a></h2>
<ul>
<li>동기화는 서버 내의 특정 네트워크 인프라와 연결성에 고도로 적응해야 합니다. 이는 동기화에 걸리는 시간에 큰 차이를 만들 수 있습니다.</li>
<li>링 동기화는 p3 및 DGX-2 서버에 최적일 수 있습니다. 다른 서버들의 경우에는 아마 그렇지 않을 수도 있습니다.</li>
<li>대역폭 증가를 위해 여러 파라미터 서버를 추가할 때 계층적 동기화 전략이 잘 작동합니다.</li>
</ul>
<h2 id="연습-문제-exercises-83"><a class="header" href="#연습-문제-exercises-83">연습 문제 (Exercises)</a></h2>
<ol>
<li>링 동기화를 더욱 강화할 수 있습니까? 힌트: 양방향으로 메시지를 보낼 수 있습니다.</li>
<li>(계산이 여전히 진행 중인 동안) 비동기 통신을 허용하는 것이 가능합니까? 성능에 어떤 영향을 미칩니까?</li>
<li>오래 실행되는 계산 중에 서버를 잃으면 어떻게 될까요? 계산을 완전히 다시 시작하는 것을 피하기 위해 <em>결함 허용(fault tolerance)</em> 메커니즘을 어떻게 설계할 수 있을까요?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/366">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="컴퓨터-비전-computer-vision"><a class="header" href="#컴퓨터-비전-computer-vision">컴퓨터 비전 (Computer Vision)</a></h1>
<p>:label:<code>chap_cv</code></p>
<p>의료 진단, 자율 주행 차량, 카메라 모니터링, 스마트 필터 등 컴퓨터 비전 분야의 많은 응용 프로그램은 현재와 미래의 우리 삶과 밀접한 관련이 있습니다.
최근 몇 년 동안 딥러닝은 컴퓨터 비전 시스템의 성능을 발전시키는 혁신적인 힘이었습니다.
가장 진보된 컴퓨터 비전 애플리케이션은 딥러닝과 거의 분리할 수 없다고 말할 수 있습니다.
이러한 관점에서 이 장에서는 컴퓨터 비전 분야에 초점을 맞추고 최근 학계와 산업계에서 영향력 있는 방법과 응용 프로그램을 조사할 것입니다.</p>
<p>:numref:<code>chap_cnn</code> 및 :numref:<code>chap_modern_cnn</code>에서 우리는 컴퓨터 비전에서 일반적으로 사용되는 다양한 합성곱 신경망을 연구하고 이를 간단한 이미지 분류 작업에 적용했습니다.
이 장의 시작 부분에서는 모델 일반화를 개선할 수 있는 두 가지 방법, 즉 *이미지 증강(image augmentation)*과 *미세 조정(fine-tuning)*을 설명하고 이를 이미지 분류에 적용할 것입니다.
심층 신경망은 이미지를 여러 수준에서 효과적으로 표현할 수 있기 때문에, 이러한 계층별 표현은 <em>객체 감지(object detection)</em>, <em>시맨틱 분할(semantic segmentation)</em>, *스타일 전송(style transfer)*과 같은 다양한 컴퓨터 비전 작업에서 성공적으로 사용되었습니다.
컴퓨터 비전에서 계층별 표현을 활용하는 핵심 아이디어에 따라, 우리는 객체 감지를 위한 주요 구성 요소와 기술부터 시작할 것입니다.
다음으로 이미지의 시맨틱 분할을 위해 *완전 합성곱 네트워크(fully convolutional networks)*를 사용하는 방법을 보여줄 것입니다.
그런 다음 스타일 전송 기술을 사용하여 이 책의 표지와 같은 이미지를 생성하는 방법을 설명할 것입니다.
마지막으로, 널리 사용되는 두 가지 컴퓨터 비전 벤치마크 데이터셋에 이 장과 이전 몇 장의 자료를 적용하여 이 장을 마무리합니다.</p>
<pre><code class="language-toc">:maxdepth: 2

image-augmentation
fine-tuning
bounding-box
anchor
multiscale-object-detection
object-detection-dataset
ssd
rcnn
semantic-segmentation-and-dataset
transposed-conv
fcn
neural-style
kaggle-cifar10
kaggle-dog
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="이미지-증강-image-augmentation"><a class="header" href="#이미지-증강-image-augmentation">이미지 증강 (Image Augmentation)</a></h1>
<p>:label:<code>sec_image_augmentation</code></p>
<p>:numref:<code>sec_alexnet</code>에서,
우리는 대규모 데이터셋이
다양한 응용 분야에서 심층 신경망의 성공을 위한
전제 조건이라고 언급했습니다.
*이미지 증강(Image augmentation)*은
훈련 이미지에 일련의 무작위 변경을 가한 후
유사하지만 구별되는 훈련 예제를 생성하여 훈련 세트의 크기를 확장합니다.
대안으로,
이미지 증강은
훈련 예제의 무작위 조정을 통해
모델이 특정 속성에 덜 의존하게 함으로써
일반화 능력을 향상시킬 수 있다는 사실에 의해 동기 부여될 수 있습니다.
예를 들어, 우리는 관심 있는 객체가 다른 위치에 나타나도록 이미지를 다양한 방식으로 자를 수 있으며,
이를 통해 객체의 위치에 대한 모델의 의존성을 줄일 수 있습니다.
또한 밝기 및 색상과 같은 요소를 조정하여 색상에 대한 모델의 민감도를 줄일 수도 있습니다.
그 당시 AlexNet의 성공을 위해
이미지 증강이 필수적이었다는 것은 아마도 사실일 것입니다.
이 섹션에서는 컴퓨터 비전에서 널리 사용되는 이 기술에 대해 논의할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, image, init, np, npx
from mxnet.gluon import nn

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
import torchvision
from torch import nn
</code></pre>
<h2 id="일반적인-이미지-증강-방법-common-image-augmentation-methods"><a class="header" href="#일반적인-이미지-증강-방법-common-image-augmentation-methods">일반적인 이미지 증강 방법 (Common Image Augmentation Methods)</a></h2>
<p>일반적인 이미지 증강 방법에 대한 조사에서, 우리는 다음 $400\times 500$ 이미지를 예로 사용할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.set_figsize()
img = image.imread('../img/cat1.jpg')
d2l.plt.imshow(img.asnumpy());
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
d2l.set_figsize()
img = d2l.Image.open('../img/cat1.jpg')
d2l.plt.imshow(img);
</code></pre>
<p>대부분의 이미지 증강 방법은 어느 정도의 무작위성을 가지고 있습니다. 이미지 증강의 효과를 더 쉽게 관찰하기 위해, 다음으로 보조 함수 <code>apply</code>를 정의합니다. 이 함수는 입력 이미지 <code>img</code>에 대해 이미지 증강 방법 <code>aug</code>를 여러 번 실행하고 모든 결과를 보여줍니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    Y = [aug(img) for _ in range(num_rows * num_cols)]
    d2l.show_images(Y, num_rows, num_cols, scale=scale)
</code></pre>
<h3 id="뒤집기-및-자르기-flipping-and-cropping"><a class="header" href="#뒤집기-및-자르기-flipping-and-cropping">뒤집기 및 자르기 (Flipping and Cropping)</a></h3>
<p>:begin_tab:<code>mxnet</code>
[<strong>이미지를 좌우로 뒤집는 것</strong>]은 일반적으로 객체의 범주를 변경하지 않습니다.
이것은 가장 초기에 널리 사용된 이미지 증강 방법 중 하나입니다.
다음으로, <code>transforms</code> 모듈을 사용하여 <code>RandomFlipLeftRight</code> 인스턴스를 생성합니다.
이는 50%의 확률로 이미지를 좌우로 뒤집습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
[<strong>이미지를 좌우로 뒤집는 것</strong>]은 일반적으로 객체의 범주를 변경하지 않습니다.
이것은 가장 초기에 널리 사용된 이미지 증강 방법 중 하나입니다.
다음으로, <code>transforms</code> 모듈을 사용하여 <code>RandomHorizontalFlip</code> 인스턴스를 생성합니다.
이는 50%의 확률로 이미지를 좌우로 뒤집습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
apply(img, gluon.data.vision.transforms.RandomFlipLeftRight())
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
apply(img, torchvision.transforms.RandomHorizontalFlip())
</code></pre>
<p>:begin_tab:<code>mxnet</code>
[<strong>상하 뒤집기</strong>]는 좌우 뒤집기만큼 일반적이지 않습니다. 하지만 적어도 이 예제 이미지의 경우, 상하 뒤집기는 인식을 방해하지 않습니다.
다음으로, <code>RandomFlipTopBottom</code> 인스턴스를 생성하여
50%의 확률로 이미지를 상하로 뒤집습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
[<strong>상하 뒤집기</strong>]는 좌우 뒤집기만큼 일반적이지 않습니다. 하지만 적어도 이 예제 이미지의 경우, 상하 뒤집기는 인식을 방해하지 않습니다.
다음으로, <code>RandomVerticalFlip</code> 인스턴스를 생성하여
50%의 확률로 이미지를 상하로 뒤집습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
apply(img, gluon.data.vision.transforms.RandomFlipTopBottom())
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
apply(img, torchvision.transforms.RandomVerticalFlip())
</code></pre>
<p>우리가 사용한 예제 이미지에서 고양이는 이미지 중앙에 있지만, 일반적으로 그렇지 않을 수 있습니다.
:numref:<code>sec_pooling</code>에서, 우리는 풀링 레이어가 타겟 위치에 대한 합성곱 레이어의 민감도를 줄일 수 있다고 설명했습니다.
또한 이미지를 무작위로 잘라 객체가 이미지의 다른 위치에 다른 크기로 나타나게 할 수 있으며, 이는 타겟 위치에 대한 모델의 민감도를 줄일 수도 있습니다.</p>
<p>아래 코드에서, 우리는 매번 원래 면적의 $10% \sim 100%$인 영역을 [<strong>무작위로 자르고</strong>], 이 영역의 너비 대 높이 비율은 $0.5 \sim 2$에서 무작위로 선택됩니다. 그런 다음 영역의 너비와 높이는 모두 200 픽셀로 조정됩니다.
달리 명시되지 않는 한, 이 섹션에서 $a$와 $b$ 사이의 난수는 구간 $[a, b]$에서 무작위 및 균일 샘플링으로 얻은 연속 값을 나타냅니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
shape_aug = gluon.data.vision.transforms.RandomResizedCrop(
    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))
apply(img, shape_aug)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
shape_aug = torchvision.transforms.RandomResizedCrop(
    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))
apply(img, shape_aug)
</code></pre>
<h3 id="색상-변경-changing-colors"><a class="header" href="#색상-변경-changing-colors">색상 변경 (Changing Colors)</a></h3>
<p>또 다른 증강 방법은 색상을 변경하는 것입니다. 우리는 이미지 색상의 네 가지 측면인 밝기, 대비, 채도 및 색조를 변경할 수 있습니다. 아래 예제에서는 이미지의 [<strong>밝기를 무작위로 변경</strong>]하여 원래 이미지의 50% ($1-0.5$)와 150% ($1+0.5$) 사이의 값으로 만듭니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
apply(img, gluon.data.vision.transforms.RandomBrightness(0.5))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
apply(img, torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0, saturation=0, hue=0))
</code></pre>
<p>마찬가지로, 이미지의 [<strong>색조를 무작위로 변경</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
apply(img, gluon.data.vision.transforms.RandomHue(0.5))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
apply(img, torchvision.transforms.ColorJitter(
    brightness=0, contrast=0, saturation=0, hue=0.5))
</code></pre>
<p>우리는 또한 <code>RandomColorJitter</code> 인스턴스를 생성하고 이미지의 [<strong><code>brightness</code>(밝기), <code>contrast</code>(대비), <code>saturation</code>(채도), <code>hue</code>(색조)를 동시에 무작위로 변경</strong>]하는 방법을 설정할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
color_aug = gluon.data.vision.transforms.RandomColorJitter(
    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)
apply(img, color_aug)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
color_aug = torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)
apply(img, color_aug)
</code></pre>
<h3 id="여러-이미지-증강-방법-결합-combining-multiple-image-augmentation-methods"><a class="header" href="#여러-이미지-증강-방법-결합-combining-multiple-image-augmentation-methods">여러 이미지 증강 방법 결합 (Combining Multiple Image Augmentation Methods)</a></h3>
<p>실제로, 우리는 [<strong>여러 이미지 증강 방법을 결합</strong>]할 것입니다.
예를 들어,
우리는 위에서 정의한 다양한 이미지 증강 방법을 결합하고 <code>Compose</code> 인스턴스를 통해 각 이미지에 적용할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
augs = gluon.data.vision.transforms.Compose([
    gluon.data.vision.transforms.RandomFlipLeftRight(), color_aug, shape_aug])
apply(img, augs)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])
apply(img, augs)
</code></pre>
<h2 id="이미지-증강을-사용한-훈련"><a class="header" href="#이미지-증강을-사용한-훈련">[<strong>이미지 증강을 사용한 훈련</strong>]</a></h2>
<p>이미지 증강을 사용하여 모델을 훈련해 봅시다.
여기서는 이전에 사용했던 Fashion-MNIST 데이터셋 대신 CIFAR-10 데이터셋을 사용합니다.
Fashion-MNIST 데이터셋의 객체 위치와 크기는 정규화된 반면, CIFAR-10 데이터셋의 객체 색상과 크기는 더 큰 차이를 보이기 때문입니다.
CIFAR-10 데이터셋의 처음 32개 훈련 이미지는 다음과 같습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.show_images(gluon.data.vision.CIFAR10(
    train=True)[:32][0], 4, 8, scale=0.8);
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
all_images = torchvision.datasets.CIFAR10(train=True, root="../data",
                                          download=True)
d2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8);
</code></pre>
<p>예측 중에 확실한 결과를 얻기 위해, 우리는 일반적으로 훈련 예제에만 이미지 증강을 적용하고, 예측 중에는 무작위 작업이 포함된 이미지 증강을 사용하지 않습니다.
[<strong>여기서는 가장 간단한 무작위 좌우 뒤집기 방법만 사용합니다</strong>]. 또한 <code>ToTensor</code> 인스턴스를 사용하여 이미지 미니배치를 딥러닝 프레임워크에 필요한 형식, 즉 (배치 크기, 채널 수, 높이, 너비) 모양의 0과 1 사이의 32비트 부동 소수점 숫자로 변환합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train_augs = gluon.data.vision.transforms.Compose([
    gluon.data.vision.transforms.RandomFlipLeftRight(),
    gluon.data.vision.transforms.ToTensor()])

test_augs = gluon.data.vision.transforms.Compose([
    gluon.data.vision.transforms.ToTensor()])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train_augs = torchvision.transforms.Compose([
     torchvision.transforms.RandomHorizontalFlip(),
     torchvision.transforms.ToTensor()])

test_augs = torchvision.transforms.Compose([
     torchvision.transforms.ToTensor()])
</code></pre>
<p>:begin_tab:<code>mxnet</code>
다음으로, 이미지 읽기 및 이미지 증강 적용을 용이하게 하기 위해 보조 함수를 정의합니다.
Gluon의 데이터셋이 제공하는 <code>transform_first</code> 함수는 각 훈련 예제(이미지와 레이블)의 첫 번째 요소, 즉 이미지에 이미지 증강을 적용합니다.
<code>DataLoader</code>에 대한 자세한 소개는 :numref:<code>sec_fashion_mnist</code>를 참조하십시오.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
다음으로, 우리는 [<strong>이미지 읽기 및 이미지 증강 적용을 용이하게 하기 위해 보조 함수를 정의합니다</strong>].
PyTorch의 데이터셋이 제공하는 <code>transform</code> 인수는 증강을 적용하여 이미지를 변환합니다.
<code>DataLoader</code>에 대한 자세한 소개는 :numref:<code>sec_fashion_mnist</code>를 참조하십시오.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def load_cifar10(is_train, augs, batch_size):
    return gluon.data.DataLoader(
        gluon.data.vision.CIFAR10(train=is_train).transform_first(augs),
        batch_size=batch_size, shuffle=is_train,
        num_workers=d2l.get_dataloader_workers())
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def load_cifar10(is_train, augs, batch_size):
    dataset = torchvision.datasets.CIFAR10(root="../data", train=is_train,
                                           transform=augs, download=True)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                    shuffle=is_train, num_workers=d2l.get_dataloader_workers())
    return dataloader
</code></pre>
<h3 id="다중-gpu-훈련-multi-gpu-training"><a class="header" href="#다중-gpu-훈련-multi-gpu-training">다중 GPU 훈련 (Multi-GPU Training)</a></h3>
<p>우리는 CIFAR-10 데이터셋에서 :numref:<code>sec_resnet</code>의 ResNet-18 모델을 훈련합니다.
:numref:<code>sec_multi_gpu_concise</code>의 다중 GPU 훈련에 대한 소개를 상기하십시오.
다음에서,
[<strong>우리는 다중 GPU를 사용하여 모델을 훈련하고 평가하는 함수를 정의합니다</strong>].</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def train_batch_ch13(net, features, labels, loss, trainer, devices,
                     split_f=d2l.split_batch):
    """다중 GPU를 사용하여 미니배치에 대해 훈련합니다(13장에서 정의됨)."""
    X_shards, y_shards = split_f(features, labels, devices)
    with autograd.record():
        pred_shards = [net(X_shard) for X_shard in X_shards]
        ls = [loss(pred_shard, y_shard) for pred_shard, y_shard
              in zip(pred_shards, y_shards)]
    for l in ls:
        l.backward()
    # `True` 플래그는 오래된 기울기가 있는 파라미터를 허용하며, 이는 나중에 유용합니다
    # (예: BERT 미세 조정)
    trainer.step(labels.shape[0], ignore_stale_grad=True)
    train_loss_sum = sum([float(l.sum()) for l in ls])
    train_acc_sum = sum(d2l.accuracy(pred_shard, y_shard)
                        for pred_shard, y_shard in zip(pred_shards, y_shards))
    return train_loss_sum, train_acc_sum
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def train_batch_ch13(net, X, y, loss, trainer, devices):
    """다중 GPU를 사용하여 미니배치에 대해 훈련합니다(13장에서 정의됨)."""
    if isinstance(X, list):
        # BERT 미세 조정에 필요합니다(나중에 다룸)
        X = [x.to(devices[0]) for x in X]
    else:
        X = X.to(devices[0])
    y = y.to(devices[0])
    net.train()
    trainer.zero_grad()
    pred = net(X)
    l = loss(pred, y)
    l.sum().backward()
    trainer.step()
    train_loss_sum = l.sum()
    train_acc_sum = d2l.accuracy(pred, y)
    return train_loss_sum, train_acc_sum
</code></pre>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
               devices=d2l.try_all_gpus(), split_f=d2l.split_batch):
    """다중 GPU를 사용하여 모델을 훈련합니다(13장에서 정의됨)."""
    timer, num_batches = d2l.Timer(), len(train_iter)
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
                            legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        # 훈련 손실의 합, 훈련 정확도의 합, 예제 수,
        # 예측 수
        metric = d2l.Accumulator(4)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = train_batch_ch13(
                net, features, labels, loss, trainer, devices, split_f)
            metric.add(l, acc, labels.shape[0], labels.size)
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[3],
                              None))
        test_acc = d2l.evaluate_accuracy_gpus(net, test_iter, split_f)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {metric[0] / metric[2]:.3f}, train acc '
          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
          f'{str(devices)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
               devices=d2l.try_all_gpus()):
    """다중 GPU를 사용하여 모델을 훈련합니다(13장에서 정의됨)."""
    timer, num_batches = d2l.Timer(), len(train_iter)
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
                            legend=['train loss', 'train acc', 'test acc'])
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    for epoch in range(num_epochs):
        # 훈련 손실의 합, 훈련 정확도의 합, 예제 수,
        # 예측 수
        metric = d2l.Accumulator(4)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = train_batch_ch13(
                net, features, labels, loss, trainer, devices)
            metric.add(l, acc, labels.shape[0], labels.numel())
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[3],
                              None))
        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {metric[0] / metric[2]:.3f}, train acc '
          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
          f'{str(devices)}')
</code></pre>
<p>이제 우리는 [<strong>이미지 증강을 사용하여 모델을 훈련하기 위해 <code>train_with_data_aug</code> 함수를 정의</strong>]할 수 있습니다.
이 함수는 사용 가능한 모든 GPU를 가져오고,
Adam을 최적화 알고리즘으로 사용하며,
훈련 데이터셋에 이미지 증강을 적용하고,
마지막으로 방금 정의한 <code>train_ch13</code> 함수를 호출하여 모델을 훈련하고 평가합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10)
net.initialize(init=init.Xavier(), ctx=devices)

def train_with_data_aug(train_augs, test_augs, net, lr=0.001):
    train_iter = load_cifar10(True, train_augs, batch_size)
    test_iter = load_cifar10(False, test_augs, batch_size)
    loss = gluon.loss.SoftmaxCrossEntropyLoss()
    trainer = gluon.Trainer(net.collect_params(), 'adam',
                            {'learning_rate': lr})
    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)
net.apply(d2l.init_cnn)

def train_with_data_aug(train_augs, test_augs, net, lr=0.001):
    train_iter = load_cifar10(True, train_augs, batch_size)
    test_iter = load_cifar10(False, test_augs, batch_size)
    loss = nn.CrossEntropyLoss(reduction="none")
    trainer = torch.optim.Adam(net.parameters(), lr=lr)
    net(next(iter(train_iter))[0])
    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)
</code></pre>
<p>무작위 좌우 뒤집기를 기반으로 한 이미지 증강을 사용하여 [<strong>모델을 훈련해 봅시다</strong>].</p>
<pre><code class="language-{.python .input}">#@tab all
train_with_data_aug(train_augs, test_augs, net)
</code></pre>
<h2 id="요약-summary-69"><a class="header" href="#요약-summary-69">요약 (Summary)</a></h2>
<ul>
<li>이미지 증강은 기존 훈련 데이터를 기반으로 무작위 이미지를 생성하여 모델의 일반화 능력을 향상시킵니다.</li>
<li>예측 중에 확실한 결과를 얻기 위해, 우리는 일반적으로 훈련 예제에만 이미지 증강을 적용하고, 예측 중에는 무작위 작업이 포함된 이미지 증강을 사용하지 않습니다.</li>
<li>딥러닝 프레임워크는 동시에 적용할 수 있는 다양한 이미지 증강 방법을 제공합니다.</li>
</ul>
<h2 id="연습-문제-exercises-84"><a class="header" href="#연습-문제-exercises-84">연습 문제 (Exercises)</a></h2>
<ol>
<li>이미지 증강을 사용하지 않고 모델을 훈련하십시오: <code>train_with_data_aug(test_augs, test_augs)</code>. 이미지 증강을 사용할 때와 사용하지 않을 때의 훈련 및 테스트 정확도를 비교하십시오. 이 비교 실험이 이미지 증강이 과대적합을 완화할 수 있다는 주장을 뒷받침할 수 있습니까? 그 이유는 무엇입니까?</li>
<li>CIFAR-10 데이터셋에 대한 모델 훈련에서 여러 다른 이미지 증강 방법을 결합하십시오. 테스트 정확도가 향상됩니까?</li>
<li>딥러닝 프레임워크의 온라인 문서를 참조하십시오. 다른 어떤 이미지 증강 방법도 제공합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/367">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1404">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="미세-조정-fine-tuning"><a class="header" href="#미세-조정-fine-tuning">미세 조정 (Fine-Tuning)</a></h1>
<p>:label:<code>sec_fine_tuning</code></p>
<p>이전 장들에서, 우리는 60,000개의 이미지만 있는 Fashion-MNIST 훈련 데이터셋에서 모델을 훈련하는 방법에 대해 논의했습니다. 우리는 또한 학계에서 가장 널리 사용되는 대규모 이미지 데이터셋인 ImageNet에 대해서도 설명했는데, 이는 1,000만 개 이상의 이미지와 1,000개의 객체를 가지고 있습니다. 그러나 우리가 평소에 마주치는 데이터셋의 크기는 대개 이 두 데이터셋 사이의 어딘가에 있습니다.</p>
<p>이미지에서 다양한 유형의 의자를 인식한 다음 사용자에게 구매 링크를 추천하고 싶다고 가정해 봅시다. 한 가지 가능한 방법은 먼저 100개의 일반적인 의자를 식별하고, 각 의자에 대해 서로 다른 각도에서 1,000개의 이미지를 찍은 다음, 수집된 이미지 데이터셋에서 분류 모델을 훈련하는 것입니다. 이 의자 데이터셋이 Fashion-MNIST 데이터셋보다 클 수는 있지만, 예제 수는 여전히 ImageNet의 10분의 1도 되지 않습니다. 이로 인해 ImageNet에 적합한 복잡한 모델이 이 의자 데이터셋에서 과대적합될 수 있습니다. 게다가 훈련 예제 수가 제한되어 있기 때문에 훈련된 모델의 정확도가 실제 요구 사항을 충족하지 못할 수도 있습니다.</p>
<p>위의 문제를 해결하기 위한 명백한 솔루션은 더 많은 데이터를 수집하는 것입니다. 그러나 데이터를 수집하고 라벨링하는 데는 많은 시간과 비용이 들 수 있습니다. 예를 들어, ImageNet 데이터셋을 수집하기 위해 연구자들은 연구 자금에서 수백만 달러를 지출했습니다. 현재 데이터 수집 비용이 크게 줄어들었지만, 이 비용은 여전히 무시할 수 없습니다.</p>
<p>또 다른 솔루션은 *전이 학습(transfer learning)*을 적용하여 *소스 데이터셋(source dataset)*에서 학습한 지식을 *타겟 데이터셋(target dataset)*으로 전이하는 것입니다. 예를 들어, ImageNet 데이터셋의 대부분 이미지가 의자와 무관하더라도, 이 데이터셋에서 훈련된 모델은 모서리, 질감, 모양 및 객체 구성을 식별하는 데 도움이 될 수 있는 더 일반적인 이미지 특성을 추출할 수 있습니다. 이러한 유사한 특성은 의자를 인식하는 데에도 효과적일 수 있습니다.</p>
<h2 id="단계-steps"><a class="header" href="#단계-steps">단계 (Steps)</a></h2>
<p>이 섹션에서는 전이 학습의 일반적인 기술인 *미세 조정(fine-tuning)*을 소개합니다. :numref:<code>fig_finetune</code>에 표시된 것처럼, 미세 조정은 다음 네 단계로 구성됩니다:</p>
<ol>
<li>소스 데이터셋(예: ImageNet 데이터셋)에서 신경망 모델인 <em>소스 모델</em>을 사전 훈련합니다.</li>
<li>새로운 신경망 모델인 <em>타겟 모델</em>을 생성합니다. 이는 출력 레이어를 제외한 소스 모델의 모든 모델 설계와 파라미터를 복사합니다. 우리는 이러한 모델 파라미터가 소스 데이터셋에서 학습한 지식을 포함하고 있으며 이 지식이 타겟 데이터셋에도 적용 가능할 것이라고 가정합니다. 또한 소스 모델의 출력 레이어는 소스 데이터셋의 레이블과 밀접하게 관련되어 있다고 가정하므로 타겟 모델에서는 사용되지 않습니다.</li>
<li>타겟 모델에 출력 레이어를 추가합니다. 출력 수는 타겟 데이터셋의 카테고리 수입니다. 그런 다음 이 레이어의 모델 파라미터를 무작위로 초기화합니다.</li>
<li>의자 데이터셋과 같은 타겟 데이터셋에서 타겟 모델을 훈련합니다. 출력 레이어는 처음부터 훈련되는 반면, 다른 모든 레이어의 파라미터는 소스 모델의 파라미터를 기반으로 미세 조정됩니다.</li>
</ol>
<p><img src="chapter_computer-vision/../img/finetune.svg" alt="미세 조정." />
:label:<code>fig_finetune</code></p>
<p>타겟 데이터셋이 소스 데이터셋보다 훨씬 작을 때, 미세 조정은 모델의 일반화 능력을 향상시키는 데 도움이 됩니다.</p>
<h2 id="핫도그-인식-hot-dog-recognition"><a class="header" href="#핫도그-인식-hot-dog-recognition">핫도그 인식 (Hot Dog Recognition)</a></h2>
<p>구체적인 사례인 핫도그 인식을 통해 미세 조정을 시연해 봅시다. 우리는 ImageNet 데이터셋에서 사전 훈련된 ResNet 모델을 작은 데이터셋에서 미세 조정할 것입니다. 이 작은 데이터셋은 핫도그가 포함된 이미지와 포함되지 않은 이미지 수천 개로 구성됩니다. 우리는 미세 조정된 모델을 사용하여 이미지에서 핫도그를 인식할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn
import os

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from torch import nn
import torch
import torchvision
import os
</code></pre>
<h3 id="데이터셋-읽기-reading-the-dataset-4"><a class="header" href="#데이터셋-읽기-reading-the-dataset-4">데이터셋 읽기 (Reading the Dataset)</a></h3>
<p>[<strong>우리가 사용하는 핫도그 데이터셋은 온라인 이미지에서 가져온 것입니다</strong>]. 이 데이터셋은 핫도그를 포함하는 1,400개의 양성 클래스 이미지와 다른 음식을 포함하는 동일한 수의 음성 클래스 이미지로 구성됩니다. 각 클래스의 1,000개 이미지는 훈련에 사용되고 나머지는 테스트에 사용됩니다.</p>
<p>다운로드한 데이터셋의 압축을 풀면 <code>hotdog/train</code> 및 <code>hotdog/test</code> 두 개의 폴더를 얻습니다. 두 폴더 모두 <code>hotdog</code> 및 <code>not-hotdog</code> 하위 폴더를 가지고 있으며, 각 하위 폴더에는 해당 클래스의 이미지가 포함되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip', 
                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')

data_dir = d2l.download_extract('hotdog')
</code></pre>
<p>우리는 각각 훈련 및 테스트 데이터셋의 모든 이미지 파일을 읽기 위해 두 개의 인스턴스를 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train_imgs = gluon.data.vision.ImageFolderDataset(
    os.path.join(data_dir, 'train'))
test_imgs = gluon.data.vision.ImageFolderDataset(
    os.path.join(data_dir, 'test'))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))
test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))
</code></pre>
<p>처음 8개의 양성 예제와 마지막 8개의 음성 이미지가 아래에 표시됩니다. 보시다시피, [<strong>이미지들의 크기와 가로세로 비율이 다양합니다</strong>].</p>
<pre><code class="language-{.python .input}">#@tab all
hotdogs = [train_imgs[i][0] for i in range(8)]
not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]
d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);
</code></pre>
<p>훈련 중에, 우리는 먼저 이미지에서 무작위 크기와 무작위 가로세로 비율의 무작위 영역을 자른 다음, 이 영역을 $224 \times 224$ 입력 이미지로 스케일링합니다. 테스트 중에, 우리는 이미지의 높이와 너비를 모두 256 픽셀로 스케일링한 다음, 중앙의 $224 \times 224$ 영역을 입력으로 자릅니다. 또한, 세 가지 RGB(빨강, 초록, 파랑) 색상 채널에 대해 채널별로 값을 <em>표준화</em>합니다. 구체적으로, 채널의 평균 값을 각 값에서 빼고 결과를 해당 채널의 표준 편차로 나눕니다.</p>
<p>[<del>데이터 증강</del>]</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 각 채널을 표준화하기 위해 세 RGB 채널의 평균과 표준 편차를 지정합니다
normalize = gluon.data.vision.transforms.Normalize(
    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

train_augs = gluon.data.vision.transforms.Compose([
    gluon.data.vision.transforms.RandomResizedCrop(224),
    gluon.data.vision.transforms.RandomFlipLeftRight(),
    gluon.data.vision.transforms.ToTensor(),
    normalize])

test_augs = gluon.data.vision.transforms.Compose([
    gluon.data.vision.transforms.Resize(256),
    gluon.data.vision.transforms.CenterCrop(224),
    gluon.data.vision.transforms.ToTensor(),
    normalize])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 각 채널을 표준화하기 위해 세 RGB 채널의 평균과 표준 편차를 지정합니다
normalize = torchvision.transforms.Normalize(
    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

train_augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(224),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    normalize])

test_augs = torchvision.transforms.Compose([
    torchvision.transforms.Resize([256, 256]),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    normalize])
</code></pre>
<h3 id="모델-정의-및-초기화-defining-and-initializing-the-model"><a class="header" href="#모델-정의-및-초기화-defining-and-initializing-the-model">모델 정의 및 초기화 (Defining and Initializing the Model)</a></h3>
<p>우리는 ImageNet 데이터셋에서 사전 훈련된 ResNet-18을 소스 모델로 사용합니다. 여기서는 <code>pretrained=True</code>를 지정하여 사전 훈련된 모델 파라미터를 자동으로 다운로드합니다. 이 모델을 처음 사용하는 경우 다운로드를 위해 인터넷 연결이 필요합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
pretrained_net = torchvision.models.resnet18(pretrained=True)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
사전 훈련된 소스 모델 인스턴스에는 <code>features</code>와 <code>output</code>이라는 두 개의 멤버 변수가 포함되어 있습니다. 전자는 출력 레이어를 제외한 모델의 모든 레이어를 포함하고, 후자는 모델의 출력 레이어입니다. 이 분할의 주된 목적은 출력 레이어를 제외한 모든 레이어의 모델 파라미터 미세 조정을 용이하게 하기 위함입니다. 소스 모델의 멤버 변수 <code>output</code>은 아래에 표시됩니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
사전 훈련된 소스 모델 인스턴스에는 여러 특징 레이어와 출력 레이어 <code>fc</code>가 포함되어 있습니다. 이 분할의 주된 목적은 출력 레이어를 제외한 모든 레이어의 모델 파라미터 미세 조정을 용이하게 하기 위함입니다. 소스 모델의 멤버 변수 <code>fc</code>는 아래와 같습니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
pretrained_net.output
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
pretrained_net.fc
</code></pre>
<p>완전 연결 레이어로서, 이는 ResNet의 최종 글로벌 평균 풀링 출력을 ImageNet 데이터셋의 1,000개 클래스 출력으로 변환합니다. 그런 다음 우리는 타겟 모델로 새로운 신경망을 구축합니다. 이는 최종 레이어의 출력 수가 타겟 데이터셋의 카테고리 수(1,000개가 아닌 2개)로 설정된다는 점을 제외하면 사전 훈련된 소스 모델과 동일하게 정의됩니다.</p>
<p>아래 코드에서, 타겟 모델 인스턴스 <code>finetune_net</code>의 출력 레이어 이전 모델 파라미터는 소스 모델의 해당 레이어 모델 파라미터로 초기화됩니다. 이러한 모델 파라미터는 ImageNet에서의 사전 훈련을 통해 얻은 것이므로 효과적입니다. 따라서 우리는 이러한 사전 훈련된 파라미터를 <em>미세 조정</em>하기 위해 작은 학습률만 사용할 수 있습니다. 대조적으로, 출력 레이어의 모델 파라미터는 무작위로 초기화되며 일반적으로 처음부터 학습하기 위해 더 큰 학습률이 필요합니다. 기본 학습률을 $\eta$라고 하면, 출력 레이어의 모델 파라미터를 반복하는 데 $10\eta$의 학습률이 사용될 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
finetune_net = gluon.model_zoo.vision.resnet18_v2(classes=2)
finetune_net.features = pretrained_net.features
finetune_net.output.initialize(init.Xavier())
# 출력 레이어의 모델 파라미터는 10배 더 큰 학습률을 사용하여 반복됩니다
finetune_net.output.collect_params().setattr('lr_mult', 10)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
finetune_net = torchvision.models.resnet18(pretrained=True)
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)
nn.init.xavier_uniform_(finetune_net.fc.weight);
</code></pre>
<h3 id="모델-미세-조정-fine-tuning-the-model"><a class="header" href="#모델-미세-조정-fine-tuning-the-model">모델 미세 조정 (Fine-Tuning the Model)</a></h3>
<p>먼저, 미세 조정을 사용하는 훈련 함수 <code>train_fine_tuning</code>을 정의하여 여러 번 호출할 수 있도록 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5):
    train_iter = gluon.data.DataLoader(
        train_imgs.transform_first(train_augs), batch_size, shuffle=True)
    test_iter = gluon.data.DataLoader(
        test_imgs.transform_first(test_augs), batch_size)
    devices = d2l.try_all_gpus() 
    net.collect_params().reset_ctx(devices)
    net.hybridize()
    loss = gluon.loss.SoftmaxCrossEntropyLoss()
    trainer = gluon.Trainer(net.collect_params(), 'sgd', {
        'learning_rate': learning_rate, 'wd': 0.001})
    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
                   devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# `param_group=True`이면 출력 레이어의 모델 파라미터가 10배 더 큰 학습률을 사용하여 업데이트됩니다
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,
                      param_group=True):
    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train'), transform=train_augs),
        batch_size=batch_size, shuffle=True)
    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'test'), transform=test_augs),
        batch_size=batch_size)
    devices = d2l.try_all_gpus()
    loss = nn.CrossEntropyLoss(reduction="none")
    if param_group:
        params_1x = [param for name, param in net.named_parameters()
             if name not in ["fc.weight", "fc.bias"]]
        trainer = torch.optim.SGD([{'params': params_1x},
                                   {'params': net.fc.parameters(),
                                    'lr': learning_rate * 10}],
                                lr=learning_rate, weight_decay=0.001)
    else:
        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,
                                  weight_decay=0.001)    
    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
                   devices)
</code></pre>
<p>우리는 사전 훈련을 통해 얻은 모델 파라미터를 <em>미세 조정</em>하기 위해 [<strong>기본 학습률을 작은 값으로 설정</strong>]합니다. 이전 설정에 따라, 타겟 모델의 출력 레이어 파라미터는 10배 더 큰 학습률을 사용하여 처음부터 훈련될 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train_fine_tuning(finetune_net, 0.01)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train_fine_tuning(finetune_net, 5e-5)
</code></pre>
<p>[<strong>비교를 위해,</strong>] 우리는 동일한 모델을 정의하지만 (<strong>모든 모델 파라미터를 무작위 값으로 초기화</strong>)합니다. 전체 모델을 처음부터 훈련해야 하므로 더 큰 학습률을 사용할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
scratch_net = gluon.model_zoo.vision.resnet18_v2(classes=2)
scratch_net.initialize(init=init.Xavier())
train_fine_tuning(scratch_net, 0.1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
scratch_net = torchvision.models.resnet18()
scratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)
train_fine_tuning(scratch_net, 5e-4, param_group=False)
</code></pre>
<p>보시다시피, 미세 조정된 모델은 초기 파라미터 값이 더 효과적이기 때문에 동일한 에포크에서 더 나은 성능을 보이는 경향이 있습니다.</p>
<h2 id="요약-summary-70"><a class="header" href="#요약-summary-70">요약 (Summary)</a></h2>
<ul>
<li>전이 학습은 소스 데이터셋에서 학습한 지식을 타겟 데이터셋으로 전이합니다. 미세 조정은 전이 학습의 일반적인 기술입니다.</li>
<li>타겟 모델은 출력 레이어를 제외한 소스 모델의 모든 모델 설계와 파라미터를 복사하고, 타겟 데이터셋을 기반으로 이러한 파라미터를 미세 조정합니다. 반면, 타겟 모델의 출력 레이어는 처음부터 훈련되어야 합니다.</li>
<li>일반적으로 파라미터 미세 조정에는 작은 학습률을 사용하고, 출력 레이어를 처음부터 훈련하는 데는 큰 학습률을 사용할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-85"><a class="header" href="#연습-문제-exercises-85">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>finetune_net</code>의 학습률을 계속 높여 보십시오. 모델의 정확도는 어떻게 변합니까?</li>
<li>비교 실험에서 <code>finetune_net</code>과 <code>scratch_net</code>의 하이퍼파라미터를 추가로 조정해 보십시오. 정확도 차이가 여전합니까?</li>
<li><code>finetune_net</code>의 출력 레이어 이전 파라미터를 소스 모델의 파라미터로 설정하고 훈련 중에 업데이트하지 마십시오. 모델의 정확도는 어떻게 변합니까? 다음 코드를 사용할 수 있습니다.</li>
</ol>
<pre><code class="language-{.python .input}">#@tab mxnet
finetune_net.features.collect_params().setattr('grad_req', 'null')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
for param in finetune_net.parameters():
    param.requires_grad = False
</code></pre>
<ol start="4">
<li>사실, <code>ImageNet</code> 데이터셋에는 "핫도그" 클래스가 있습니다. 출력 레이어에서 해당 가중치 파라미터는 다음 코드를 통해 얻을 수 있습니다. 이 가중치 파라미터를 어떻게 활용할 수 있을까요?</li>
</ol>
<pre><code class="language-{.python .input}">#@tab mxnet
weight = pretrained_net.output.weight
hotdog_w = np.split(weight.data(), 1000, axis=0)[713]
hotdog_w.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
weight = pretrained_net.fc.weight
hotdog_w = torch.split(weight.data, 1, dim=0)[934]
hotdog_w.shape
</code></pre>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/368">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1439">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="객체-감지-및-바운딩-박스-object-detection-and-bounding-boxes"><a class="header" href="#객체-감지-및-바운딩-박스-object-detection-and-bounding-boxes">객체 감지 및 바운딩 박스 (Object Detection and Bounding Boxes)</a></h1>
<p>:label:<code>sec_bbox</code></p>
<p>이전 섹션(예: :numref:<code>sec_alexnet</code>--:numref:<code>sec_googlenet</code>)에서,
우리는 이미지 분류를 위한 다양한 모델을 소개했습니다.
이미지 분류 작업에서,
우리는 이미지에 <em>하나의</em> 주요 객체만 있다고 가정하고
그 범주를 인식하는 방법에만 초점을 맞춥니다.
그러나 관심 있는 이미지에는 종종 <em>여러</em> 객체가 있습니다.
우리는 범주뿐만 아니라 이미지 내의 구체적인 위치도 알고 싶어 합니다.
컴퓨터 비전에서는 이러한 작업을 <em>객체 감지(object detection)</em> (또는 <em>객체 인식(object recognition)</em>)라고 합니다.</p>
<p>객체 감지는 많은 분야에서 널리 적용되었습니다.
예를 들어, 자율 주행은 캡처된 비디오 이미지에서 차량, 보행자, 도로 및 장애물의 위치를 감지하여 주행 경로를 계획해야 합니다.
또한 로봇은 환경을 탐색하는 동안 관심 객체를 감지하고 위치를 파악하기 위해 이 기술을 사용할 수 있습니다.
게다가 보안 시스템은 침입자나 폭탄과 같은 비정상적인 물체를 감지해야 할 수도 있습니다.</p>
<p>다음 몇 섹션에서는 객체 감지를 위한 몇 가지 딥러닝 방법을 소개합니다.
객체의 <em>위치(positions)</em> (또는 <em>장소(locations)</em>)에 대한 소개로 시작하겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import image, npx, np

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<p>이 섹션에서 사용할 샘플 이미지를 로드합니다. 이미지 왼쪽에 개가 있고 오른쪽에 고양이가 있는 것을 볼 수 있습니다.
이들은 이 이미지의 두 가지 주요 객체입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.set_figsize()
img = image.imread('../img/catdog.jpg').asnumpy()
d2l.plt.imshow(img);
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch, tensorflow
d2l.set_figsize()
img = d2l.plt.imread('../img/catdog.jpg')
d2l.plt.imshow(img);
</code></pre>
<h2 id="바운딩-박스-bounding-boxes"><a class="header" href="#바운딩-박스-bounding-boxes">바운딩 박스 (Bounding Boxes)</a></h2>
<p>객체 감지에서,
우리는 일반적으로 객체의 공간적 위치를 설명하기 위해 *바운딩 박스(bounding box)*를 사용합니다.
바운딩 박스는 직사각형이며, 직사각형의 왼쪽 상단 모서리의 $x$ 및 $y$ 좌표와 오른쪽 하단 모서리의 좌표에 의해 결정됩니다.
일반적으로 사용되는 또 다른 바운딩 박스 표현은 바운딩 박스 중심의 $(x, y)$축 좌표와 박스의 너비 및 높이입니다.</p>
<p>[<strong>여기서 우리는 이 두 가지 표현 사이를 변환하는 함수를 정의합니다</strong>]:
<code>box_corner_to_center</code>는 두 모서리 표현에서 중심-너비-높이 표현으로 변환하고,
<code>box_center_to_corner</code>는 그 반대로 변환합니다.
입력 인수 <code>boxes</code>는 ($n$, 4) 모양의 2차원 텐서여야 합니다. 여기서 $n$은 바운딩 박스의 수입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def box_corner_to_center(boxes):
    """(왼쪽 상단, 오른쪽 하단)에서 (중심, 너비, 높이)로 변환합니다."""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    boxes = d2l.stack((cx, cy, w, h), axis=-1)
    return boxes

#@save
def box_center_to_corner(boxes):
    """(중심, 너비, 높이)에서 (왼쪽 상단, 오른쪽 하단)으로 변환합니다."""
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = d2l.stack((x1, y1, x2, y2), axis=-1)
    return boxes
</code></pre>
<p>좌표 정보를 기반으로 [<strong>이미지에 있는 개와 고양이의 바운딩 박스를 정의</strong>]합니다.
이미지 좌표의 원점은 이미지의 왼쪽 상단 모서리이며, 오른쪽과 아래쪽이 각각 $x$축과 $y$축의 양의 방향입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
# 여기서 `bbox`는 bounding box의 약어입니다
dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]
</code></pre>
<p>두 번 변환하여 두 바운딩 박스 변환 함수의 정확성을 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
boxes = d2l.tensor((dog_bbox, cat_bbox))
box_center_to_corner(box_corner_to_center(boxes)) == boxes
</code></pre>
<p>정확한지 확인하기 위해 [<strong>이미지에 바운딩 박스를 그려봅시다</strong>].
그리기 전에, <code>matplotlib</code> 패키지의 바운딩 박스 형식으로 바운딩 박스를 나타내는 도우미 함수 <code>bbox_to_rect</code>를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def bbox_to_rect(bbox, color):
    """바운딩 박스를 matplotlib 형식으로 변환합니다."""
    # 바운딩 박스 (왼쪽 상단 x, 왼쪽 상단 y, 오른쪽 하단 x, 오른쪽 하단 y) 형식을
    # matplotlib 형식 ((왼쪽 상단 x, 왼쪽 상단 y), 너비, 높이)로 변환합니다
    return d2l.plt.Rectangle(
        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
        fill=False, edgecolor=color, linewidth=2)
</code></pre>
<p>이미지에 바운딩 박스를 추가한 후,
두 객체의 주요 윤곽이 기본적으로 두 박스 안에 있는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
fig = d2l.plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));
</code></pre>
<h2 id="요약-summary-71"><a class="header" href="#요약-summary-71">요약 (Summary)</a></h2>
<ul>
<li>객체 감지는 이미지에서 관심 있는 모든 객체뿐만 아니라 그 위치도 인식합니다. 위치는 일반적으로 직사각형 바운딩 박스로 표현됩니다.</li>
<li>우리는 일반적으로 사용되는 두 가지 바운딩 박스 표현 사이를 변환할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-86"><a class="header" href="#연습-문제-exercises-86">연습 문제 (Exercises)</a></h2>
<ol>
<li>다른 이미지를 찾아 객체를 포함하는 바운딩 박스에 레이블을 지정해 보십시오. 바운딩 박스 레이블링과 범주 레이블링을 비교해 보십시오: 어느 것이 일반적으로 더 오래 걸립니까?</li>
<li><code>box_corner_to_center</code>와 <code>box_center_to_corner</code>의 입력 인수 <code>boxes</code>의 가장 안쪽 차원이 항상 4인 이유는 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/369">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1527">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="앵커-박스-anchor-boxes"><a class="header" href="#앵커-박스-anchor-boxes">앵커 박스 (Anchor Boxes)</a></h1>
<p>:label:<code>sec_anchor</code></p>
<p>객체 감지 알고리즘은 일반적으로
입력 이미지에서 수많은 영역을 샘플링하고, 이 영역에 관심 있는 객체가 포함되어 있는지 확인하며,
객체의 *실제 바운딩 박스(ground-truth bounding boxes)*를
더 정확하게 예측하도록 영역의 경계를 조정합니다.
모델마다 다른 영역 샘플링 방식을 채택할 수 있습니다.
여기서는 그러한 방법 중 하나를 소개합니다.
각 픽셀을 중심으로 다양한 스케일과 가로세로 비율(aspect ratio)을 가진 여러 바운딩 박스를 생성하는 것입니다.
이러한 바운딩 박스를 *앵커 박스(anchor boxes)*라고 합니다.
:numref:<code>sec_ssd</code>에서 앵커 박스를 기반으로 한 객체 감지 모델을 설계할 것입니다.</p>
<p>먼저 더 간결한 출력을 위해 인쇄 정확도를 수정해 보겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, np, npx

np.set_printoptions(2)  # 인쇄 정확도 단순화
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

torch.set_printoptions(2)  # 인쇄 정확도 단순화
</code></pre>
<h2 id="여러-앵커-박스-생성-generating-multiple-anchor-boxes"><a class="header" href="#여러-앵커-박스-생성-generating-multiple-anchor-boxes">여러 앵커 박스 생성 (Generating Multiple Anchor Boxes)</a></h2>
<p>입력 이미지의 높이가 $h$, 너비가 $w$라고 가정해 보겠습니다.
우리는 이미지의 각 픽셀을 중심으로 다양한 모양의 앵커 박스를 생성합니다.
*스케일(scale)*을 $s\in (0, 1]$로,
<em>가로세로 비율(aspect ratio)</em> (높이 대비 너비의 비율)을 $r &gt; 0$이라고 합시다.
그러면 [<strong>앵커 박스의 너비와 높이는 각각 $ws\sqrt{r}$와 $hs/\sqrt{r}$입니다.</strong>]
중심 위치가 주어지면 너비와 높이가 알려진 앵커 박스가 결정됩니다.</p>
<p>다양한 모양의 여러 앵커 박스를 생성하기 위해,
일련의 스케일 $s_1,\ldots, s_n$과
일련의 가로세로 비율 $r_1,\ldots, r_m$을 설정해 봅시다.
각 픽셀을 중심으로 이러한 스케일과 가로세로 비율의 모든 조합을 사용할 때,
입력 이미지에는 총 $whnm$개의 앵커 박스가 생깁니다. 이러한 앵커 박스가 모든 실제 바운딩 박스를 커버할 수 있지만, 계산 복잡도가 너무 높아질 수 있습니다.
실제로,
우리는 (<strong>$s_1$ 또는 $r_1$을 포함하는 조합만 고려</strong>)할 수 있습니다:</p>
<p>(<strong>$$(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1).$$</strong>)</p>
<p>즉, 동일한 픽셀을 중심으로 하는 앵커 박스의 수는 $n+m-1$입니다. 전체 입력 이미지에 대해 총 $wh(n+m-1)$개의 앵커 박스를 생성하게 됩니다.</p>
<p>위의 앵커 박스 생성 방법은 다음 <code>multibox_prior</code> 함수에 구현되어 있습니다. 입력 이미지, 스케일 목록, 가로세로 비율 목록을 지정하면 이 함수가 모든 앵커 박스를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def multibox_prior(data, sizes, ratios):
    """서로 다른 모양의 앵커 박스를 픽셀 단위로 생성합니다."""
    in_height, in_width = data.shape[-2:]
    device, num_sizes, num_ratios = data.ctx, len(sizes), len(ratios)
    boxes_per_pixel = (num_sizes + num_ratios - 1)
    size_tensor = d2l.tensor(sizes, ctx=device)
    ratio_tensor = d2l.tensor(ratios, ctx=device)
    # 앵커를 픽셀 중심으로 이동하려면 오프셋이 필요합니다. 
    # 픽셀의 높이가 1이고 너비가 1이므로 중심을 0.5만큼 오프셋하기로 선택합니다.
    offset_h, offset_w = 0.5, 0.5
    steps_h = 1.0 / in_height  # y축의 스케일링된 단계
    steps_w = 1.0 / in_width  # x축의 스케일링된 단계

    # 앵커 박스의 모든 중심점 생성
    center_h = (d2l.arange(in_height, ctx=device) + offset_h) * steps_h
    center_w = (d2l.arange(in_width, ctx=device) + offset_w) * steps_w
    shift_x, shift_y = d2l.meshgrid(center_w, center_h)
    shift_x, shift_y = shift_x.reshape(-1), shift_y.reshape(-1)

    # 나중에 앵커 박스 모서리 좌표(xmin, xmax, ymin, ymax)를 만드는 데 사용되는
    # `boxes_per_pixel` 수의 높이와 너비를 생성합니다.
    w = np.concatenate((size_tensor * np.sqrt(ratio_tensor[0]),
                        sizes[0] * np.sqrt(ratio_tensor[1:]))) \
                        * in_height / in_width  # 직사각형 입력 처리
    h = np.concatenate((size_tensor / np.sqrt(ratio_tensor[0]),
                        sizes[0] / np.sqrt(ratio_tensor[1:])))
    # 반 높이와 반 너비를 얻으려면 2로 나눕니다.
    anchor_manipulations = np.tile(np.stack((-w, -h, w, h)).T,
                                   (in_height * in_width, 1)) / 2

    # 각 중심점에는 `boxes_per_pixel` 수의 앵커 박스가 있으므로
    # `boxes_per_pixel` 반복으로 모든 앵커 박스 중심의 그리드를 생성합니다.
    out_grid = d2l.stack([shift_x, shift_y, shift_x, shift_y],
                         axis=1).repeat(boxes_per_pixel, axis=0)
    output = out_grid + anchor_manipulations
    return np.expand_dims(output, axis=0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def multibox_prior(data, sizes, ratios):
    """서로 다른 모양의 앵커 박스를 픽셀 단위로 생성합니다."""
    in_height, in_width = data.shape[-2:]
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)
    boxes_per_pixel = (num_sizes + num_ratios - 1)
    size_tensor = d2l.tensor(sizes, device=device)
    ratio_tensor = d2l.tensor(ratios, device=device)
    # 앵커를 픽셀 중심으로 이동하려면 오프셋이 필요합니다. 
    # 픽셀의 높이가 1이고 너비가 1이므로 중심을 0.5만큼 오프셋하기로 선택합니다.
    offset_h, offset_w = 0.5, 0.5
    steps_h = 1.0 / in_height  # y축의 스케일링된 단계
    steps_w = 1.0 / in_width  # x축의 스케일링된 단계

    # 앵커 박스의 모든 중심점 생성
    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')
    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)

    # 나중에 앵커 박스 모서리 좌표(xmin, xmax, ymin, ymax)를 만드는 데 사용되는
    # `boxes_per_pixel` 수의 높이와 너비를 생성합니다.
    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),
                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\
                   * in_height / in_width  # 직사각형 입력 처리
    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
                   sizes[0] / torch.sqrt(ratio_tensor[1:])))
    # 반 높이와 반 너비를 얻으려면 2로 나눕니다.
    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(
                                        in_height * in_width, 1) / 2

    # 각 중심점에는 `boxes_per_pixel` 수의 앵커 박스가 있으므로
    # `boxes_per_pixel` 반복으로 모든 앵커 박스 중심의 그리드를 생성합니다.
    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
                dim=1).repeat_interleave(boxes_per_pixel, dim=0)
    output = out_grid + anchor_manipulations
    return output.unsqueeze(0)
</code></pre>
<p>[<strong>반환된 앵커 박스 변수 <code>Y</code>의 모양</strong>]은 (배치 크기, 앵커 박스 수, 4)임을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
img = image.imread('../img/catdog.jpg').asnumpy()
h, w = img.shape[:2]

print(h, w)
X = np.random.uniform(size=(1, 3, h, w))  # 입력 데이터 구성
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
Y.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
img = d2l.plt.imread('../img/catdog.jpg')
h, w = img.shape[:2]

print(h, w)
X = torch.rand(size=(1, 3, h, w))  # 입력 데이터 구성
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
Y.shape
</code></pre>
<p>앵커 박스 변수 <code>Y</code>의 모양을 (이미지 높이, 이미지 너비, 동일한 픽셀을 중심으로 하는 앵커 박스 수, 4)로 변경하면,
지정된 픽셀 위치를 중심으로 하는 모든 앵커 박스를 얻을 수 있습니다.
다음에서,
우리는 [<strong>(250, 250)을 중심으로 하는 첫 번째 앵커 박스에 액세스합니다</strong>]. 여기에는 앵커 박스의 왼쪽 상단 모서리의 $(x, y)$축 좌표와 오른쪽 하단 모서리의 $(x, y)$축 좌표인 네 가지 요소가 있습니다.
두 축의 좌표 값은 각각 이미지의 너비와 높이로 나누어집니다.</p>
<pre><code class="language-{.python .input}">#@tab all
boxes = Y.reshape(h, w, 5, 4)
boxes[250, 250, 0, :]
</code></pre>
<p>[<strong>이미지의 한 픽셀을 중심으로 하는 모든 앵커 박스를 표시</strong>]하기 위해,
이미지에 여러 바운딩 박스를 그리는 <code>show_bboxes</code> 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def show_bboxes(axes, bboxes, labels=None, colors=None):
    """바운딩 박스를 표시합니다."""

    def make_list(obj, default_values=None):
        if obj is None:
            obj = default_values
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj

    labels = make_list(labels)
    colors = make_list(colors, ['b', 'g', 'r', 'm', 'c'])
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        rect = d2l.bbox_to_rect(d2l.numpy(bbox), color)
        axes.add_patch(rect)
        if labels and len(labels) &gt; i:
            text_color = 'k' if color == 'w' else 'w'
            axes.text(rect.xy[0], rect.xy[1], labels[i],
                      va='center', ha='center', fontsize=9, color=text_color,
                      bbox=dict(facecolor=color, lw=0))
</code></pre>
<p>방금 본 것처럼 변수 <code>boxes</code>의 $x$ 및 $y$ 축 좌표 값은 각각 이미지의 너비와 높이로 나누어졌습니다.
앵커 박스를 그릴 때,
원래 좌표 값을 복원해야 하므로,
아래에서 <code>bbox_scale</code> 변수를 정의합니다.
이제 이미지의 (250, 250)을 중심으로 하는 모든 앵커 박스를 그릴 수 있습니다.
보시다시피 스케일이 0.75이고 가로세로 비율이 1인 파란색 앵커 박스가 이미지의 개를 잘 둘러싸고 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
d2l.set_figsize()
bbox_scale = d2l.tensor((w, h, w, h))
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',
             's=0.75, r=0.5'])
</code></pre>
<h2 id="iou-intersection-over-union"><a class="header" href="#iou-intersection-over-union">[<strong>IoU (Intersection over Union)</strong>]</a></h2>
<p>방금 앵커 박스가 이미지의 개를 "잘" 둘러싸고 있다고 언급했습니다.
객체의 실제 바운딩 박스가 알려진 경우, 여기서 "잘"을 어떻게 정량화할 수 있습니까?
직관적으로, 우리는 앵커 박스와 실제 바운딩 박스 사이의 유사성을 측정할 수 있습니다.
우리는 *자카드 지수(Jaccard index)*가 두 집합 간의 유사성을 측정할 수 있음을 알고 있습니다. 집합 $\mathcal{A}$와 $\mathcal{B}$가 주어지면, 자카드 지수는 교집합의 크기를 합집합의 크기로 나눈 것입니다:</p>
<p>$$J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}.$$</p>
<p>사실, 우리는 모든 바운딩 박스의 픽셀 영역을 픽셀 집합으로 간주할 수 있습니다.
이런 식으로 픽셀 집합의 자카드 지수를 통해 두 바운딩 박스의 유사성을 측정할 수 있습니다. 두 바운딩 박스의 경우, 우리는 일반적으로 이 자카드 지수를 *IoU(Intersection over Union)*라고 부르며, 이는 :numref:<code>fig_iou</code>와 같이 교차 영역 대 합집합 영역의 비율입니다.
IoU의 범위는 0에서 1 사이입니다:
0은 두 바운딩 박스가 전혀 겹치지 않음을 의미하고,
1은 두 바운딩 박스가 동일함을 나타냅니다.</p>
<p><img src="chapter_computer-vision/../img/iou.svg" alt="IoU는 두 바운딩 박스의 교차 영역 대 합집합 영역의 비율입니다." />
:label:<code>fig_iou</code></p>
<p>이 섹션의 나머지 부분에서는 IoU를 사용하여 앵커 박스와 실제 바운딩 박스 간의 유사성, 그리고 서로 다른 앵커 박스 간의 유사성을 측정합니다.
두 개의 앵커 또는 바운딩 박스 목록이 주어지면,
다음 <code>box_iou</code>는 이 두 목록에 걸쳐 쌍별 IoU를 계산합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def box_iou(boxes1, boxes2):
    """두 앵커 또는 바운딩 박스 목록에 걸쳐 쌍별 IoU를 계산합니다."""
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    # `boxes1`, `boxes2`, `areas1`, `areas2`의 모양: (boxes1 수, 4),
    # (boxes2 수, 4), (boxes1 수,), (boxes2 수,)
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    # `inter_upperlefts`, `inter_lowerrights`, `inters`의 모양: (boxes1 수,
    # boxes2 수, 2)
    inter_upperlefts = np.maximum(boxes1[:, None, :2], boxes2[:, :2])
    inter_lowerrights = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])
    inters = (inter_lowerrights - inter_upperlefts).clip(min=0)
    # `inter_areas` 및 `union_areas`의 모양: (boxes1 수, boxes2 수)
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    union_areas = areas1[:, None] + areas2 - inter_areas
    return inter_areas / union_areas
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def box_iou(boxes1, boxes2):
    """두 앵커 또는 바운딩 박스 목록에 걸쳐 쌍별 IoU를 계산합니다."""
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    # `boxes1`, `boxes2`, `areas1`, `areas2`의 모양: (boxes1 수, 4),
    # (boxes2 수, 4), (boxes1 수,), (boxes2 수,)
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    # `inter_upperlefts`, `inter_lowerrights`, `inters`의 모양: (boxes1 수,
    # boxes2 수, 2)
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)
    # `inter_areas` 및 `union_areas`의 모양: (boxes1 수, boxes2 수)
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    union_areas = areas1[:, None] + areas2 - inter_areas
    return inter_areas / union_areas
</code></pre>
<h2 id="훈련-데이터에서-앵커-박스-라벨링-labeling-anchor-boxes-in-training-data"><a class="header" href="#훈련-데이터에서-앵커-박스-라벨링-labeling-anchor-boxes-in-training-data">훈련 데이터에서 앵커 박스 라벨링 (Labeling Anchor Boxes in Training Data)</a></h2>
<p>:label:<code>subsec_labeling-anchor-boxes</code></p>
<p>훈련 데이터셋에서,
우리는 각 앵커 박스를 훈련 예제로 간주합니다.
객체 감지 모델을 훈련하려면,
각 앵커 박스에 대한 <em>클래스(class)</em> 및 <em>오프셋(offset)</em> 레이블이 필요합니다.
전자는 앵커 박스와 관련된 객체의 클래스이고,
후자는 앵커 박스에 대한 실제 바운딩 박스의 오프셋입니다.
예측 중에,
각 이미지에 대해
여러 앵커 박스를 생성하고,
모든 앵커 박스에 대한 클래스와 오프셋을 예측하고,
예측된 오프셋에 따라 위치를 조정하여 예측된 바운딩 박스를 얻고,
마지막으로 특정 기준을 충족하는 예측된 바운딩 박스만 출력합니다.</p>
<p>알다시피, 객체 감지 훈련 세트에는
<em>실제 바운딩 박스</em>의 위치와
둘러싸인 객체의 클래스에 대한 레이블이 함께 제공됩니다.
생성된 <em>앵커 박스</em>에 레이블을 지정하기 위해,
앵커 박스에 가장 가까운 <em>할당된</em> 실제 바운딩 박스의 레이블 위치와 클래스를 참조합니다.
다음에서,
가장 가까운 실제 바운딩 박스를 앵커 박스에 할당하는 알고리즘을 설명합니다.</p>
<h3 id="실제-바운딩-박스를-앵커-박스에-할당하기"><a class="header" href="#실제-바운딩-박스를-앵커-박스에-할당하기">[<strong>실제 바운딩 박스를 앵커 박스에 할당하기</strong>]</a></h3>
<p>이미지가 주어졌을 때,
앵커 박스가 $A_1, A_2, \ldots, A_{n_a}$이고 실제 바운딩 박스가 $B_1, B_2, \ldots, B_{n_b}$라고 가정합니다. 여기서 $n_a \geq n_b$입니다.
행렬 $\mathbf{X} \in \mathbb{R}^{n_a \times n_b}$를 정의합시다. 여기서 $i$번째 행과 $j$번째 열의 요소 $x_{ij}$는 앵커 박스 $A_i$와 실제 바운딩 박스 $B_j$의 IoU입니다. 알고리즘은 다음 단계로 구성됩니다:</p>
<ol>
<li>행렬 $\mathbf{X}$에서 가장 큰 요소를 찾아 행과 열 인덱스를 각각 $i_1$과 $j_1$로 표시합니다. 그러면 실제 바운딩 박스 $B_{j_1}$이 앵커 박스 $A_{i_1}$에 할당됩니다. 이것은 꽤 직관적입니다. $A_{i_1}$과 $B_{j_1}$이 모든 앵커 박스와 실제 바운딩 박스 쌍 중에서 가장 가깝기 때문입니다. 첫 번째 할당 후, 행렬 $\mathbf{X}$의 ${i_1}$번째 행과 ${j_1}$번째 열의 모든 요소를 버립니다.</li>
<li>행렬 $\mathbf{X}$의 나머지 요소 중에서 가장 큰 요소를 찾아 행과 열 인덱스를 각각 $i_2$와 $j_2$로 표시합니다. 실제 바운딩 박스 $B_{j_2}$를 앵커 박스 $A_{i_2}$에 할당하고 행렬 $\mathbf{X}$의 ${i_2}$번째 행과 ${j_2}$번째 열의 모든 요소를 버립니다.</li>
<li>이 시점에서 행렬 $\mathbf{X}$의 두 행과 두 열의 요소가 버려졌습니다. 행렬 $\mathbf{X}$의 $n_b$ 열에 있는 모든 요소가 버려질 때까지 진행합니다. 이때, 우리는 $n_b$개의 앵커 박스 각각에 실제 바운딩 박스를 할당했습니다.</li>
<li>나머지 $n_a - n_b$ 앵커 박스만 순회합니다. 예를 들어, 앵커 박스 $A_i$가 주어지면 행렬 $\mathbf{X}$의 $i$번째 행 전체에서 $A_i$와 가장 큰 IoU를 가진 실제 바운딩 박스 $B_j$를 찾고, 이 IoU가 미리 정의된 임계값보다 큰 경우에만 $B_j$를 $A_i$에 할당합니다.</li>
</ol>
<p>구체적인 예를 사용하여 위 알고리즘을 설명해 보겠습니다.
:numref:<code>fig_anchor_label</code> (왼쪽)과 같이, 행렬 $\mathbf{X}$의 최대값이 $x_{23}$이라고 가정하면 실제 바운딩 박스 $B_3$을 앵커 박스 $A_2$에 할당합니다.
그런 다음 행렬의 2행 3열의 모든 요소를 버리고, 나머지 요소(음영 처리된 영역)에서 가장 큰 $x_{71}$을 찾아 실제 바운딩 박스 $B_1$을 앵커 박스 $A_7$에 할당합니다.
다음으로, :numref:<code>fig_anchor_label</code> (가운데)와 같이 행렬의 7행 1열의 모든 요소를 버리고, 나머지 요소(음영 처리된 영역)에서 가장 큰 $x_{54}$를 찾아 실제 바운딩 박스 $B_4$를 앵커 박스 $A_5$에 할당합니다.
마지막으로, :numref:<code>fig_anchor_label</code> (오른쪽)과 같이 행렬의 5행 4열의 모든 요소를 버리고, 나머지 요소(음영 처리된 영역)에서 가장 큰 $x_{92}$를 찾아 실제 바운딩 박스 $B_2$를 앵커 박스 $A_9$에 할당합니다.
그 후에는 나머지 앵커 박스 $A_1, A_3, A_4, A_6, A_8$을 순회하고 임계값에 따라 실제 바운딩 박스를 할당할지 여부를 결정하기만 하면 됩니다.</p>
<p><img src="chapter_computer-vision/../img/anchor-label.svg" alt="실제 바운딩 박스를 앵커 박스에 할당하기." />
:label:<code>fig_anchor_label</code></p>
<p>이 알고리즘은 다음 <code>assign_anchor_to_bbox</code> 함수에 구현되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """가장 가까운 실제 바운딩 박스를 앵커 박스에 할당합니다."""
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
    # i번째 행과 j번째 열의 요소 x_ij는 앵커 박스 i와 실제 바운딩 박스 j의 IoU입니다.
    jaccard = box_iou(anchors, ground_truth)
    # 각 앵커에 대해 할당된 실제 바운딩 박스를 유지할 텐서를 초기화합니다.
    anchors_bbox_map = np.full((num_anchors,), -1, dtype=np.int32, ctx=device)
    # 임계값에 따라 실제 바운딩 박스를 할당합니다.
    max_ious, indices = np.max(jaccard, axis=1), np.argmax(jaccard, axis=1)
    anc_i = np.nonzero(max_ious &gt;= iou_threshold)[0]
    box_j = indices[max_ious &gt;= iou_threshold]
    anchors_bbox_map[anc_i] = box_j
    col_discard = np.full((num_anchors,), -1)
    row_discard = np.full((num_gt_boxes,), -1)
    for _ in range(num_gt_boxes):
        max_idx = np.argmax(jaccard)  # 가장 큰 IoU 찾기
        box_idx = (max_idx % num_gt_boxes).astype('int32')
        anc_idx = (max_idx / num_gt_boxes).astype('int32')
        anchors_bbox_map[anc_idx] = box_idx
        jaccard[:, box_idx] = col_discard
        jaccard[anc_idx, :] = row_discard
    return anchors_bbox_map
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """가장 가까운 실제 바운딩 박스를 앵커 박스에 할당합니다."""
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
    # i번째 행과 j번째 열의 요소 x_ij는 앵커 박스 i와 실제 바운딩 박스 j의 IoU입니다.
    jaccard = box_iou(anchors, ground_truth)
    # 각 앵커에 대해 할당된 실제 바운딩 박스를 유지할 텐서를 초기화합니다.
    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
                                  device=device)
    # 임계값에 따라 실제 바운딩 박스를 할당합니다.
    max_ious, indices = torch.max(jaccard, dim=1)
    anc_i = torch.nonzero(max_ious &gt;= iou_threshold).reshape(-1)
    box_j = indices[max_ious &gt;= iou_threshold]
    anchors_bbox_map[anc_i] = box_j
    col_discard = torch.full((num_anchors,), -1)
    row_discard = torch.full((num_gt_boxes,), -1)
    for _ in range(num_gt_boxes):
        max_idx = torch.argmax(jaccard)  # 가장 큰 IoU 찾기
        box_idx = (max_idx % num_gt_boxes).long()
        anc_idx = (max_idx / num_gt_boxes).long()
        anchors_bbox_map[anc_idx] = box_idx
        jaccard[:, box_idx] = col_discard
        jaccard[anc_idx, :] = row_discard
    return anchors_bbox_map
</code></pre>
<h3 id="클래스-및-오프셋-라벨링-labeling-classes-and-offsets"><a class="header" href="#클래스-및-오프셋-라벨링-labeling-classes-and-offsets">클래스 및 오프셋 라벨링 (Labeling Classes and Offsets)</a></h3>
<p>이제 각 앵커 박스에 대한 클래스와 오프셋을 라벨링할 수 있습니다. 앵커 박스 $A$가 실제 바운딩 박스 $B$에 할당되었다고 가정합니다.
한편으로,
앵커 박스 $A$의 클래스는 $B$의 클래스로 라벨링됩니다.
다른 한편으로, 앵커 박스 $A$의 오프셋은 $B$와 $A$의 중심 좌표 사이의 상대적 위치와
이 두 박스 사이의 상대적 크기에 따라 라벨링됩니다.
데이터셋에 있는 다양한 상자의 위치와 크기가 주어지면,
우리는 더 균일하게 분포된 오프셋으로 이어질 수 있는 변환을
해당 상대적 위치와 크기에 적용할 수 있습니다.
이러한 오프셋은 맞추기(fit) 더 쉽습니다.
여기서는 일반적인 변환을 설명합니다.
[**$A$와 $B$의 중심 좌표가 각각 $(x_a, y_a)$와 $(x_b, y_b)$이고,
너비가 $w_a$와 $w_b$,
높이가 $h_a$와 $h_b$라고 주어졌을 때. 우리는 $A$의 오프셋을 다음과 같이 라벨링할 수 있습니다.</p>
<p>$$\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},
\frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},
\frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},
\frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right),
**]
여기서 상수의 기본값은 $\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1$, $\sigma_w=\sigma_h=0.2$입니다.
이 변환은 아래 <code>offset_boxes</code> 함수에 구현되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """앵커 박스 오프셋을 위한 변환."""
    c_anc = d2l.box_corner_to_center(anchors)
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
    offset_wh = 5 * d2l.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    offset = d2l.concat([offset_xy, offset_wh], axis=1)
    return offset
</code></pre>
<p>앵커 박스에 할당된 실제 바운딩 박스가 없는 경우, 앵커 박스의 클래스를 "배경(background)"으로 라벨링합니다.
클래스가 배경인 앵커 박스를 종종 <em>음성(negative)</em> 앵커 박스라고 하고,
나머지를 <em>양성(positive)</em> 앵커 박스라고 합니다.
우리는 다음 <code>multibox_target</code> 함수를 구현하여
실제 바운딩 박스(<code>labels</code> 인수)를 사용하여 [<strong>앵커 박스(<code>anchors</code> 인수)에 대한 클래스와 오프셋을 라벨링</strong>]합니다.
이 함수는 배경 클래스를 0으로 설정하고 새 클래스의 정수 인덱스를 1씩 증가시킵니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def multibox_target(anchors, labels):
    """실제 바운딩 박스를 사용하여 앵커 박스에 라벨을 지정합니다."""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.ctx, anchors.shape[0]
    for i in range(batch_size):
        label = labels[i, :, :]
        anchors_bbox_map = assign_anchor_to_bbox(
            label[:, 1:], anchors, device)
        bbox_mask = np.tile((np.expand_dims((anchors_bbox_map &gt;= 0), 
                                            axis=-1)), (1, 4)).astype('int32')
        # 클래스 레이블 및 할당된 바운딩 박스 좌표를 0으로 초기화
        class_labels = d2l.zeros(num_anchors, dtype=np.int32, ctx=device)
        assigned_bb = d2l.zeros((num_anchors, 4), dtype=np.float32,
                                ctx=device)
        # 할당된 실제 바운딩 박스를 사용하여 앵커 박스의 클래스에 라벨을 지정합니다.
        # 앵커 박스에 할당된 것이 없으면 클래스를 배경으로 라벨링합니다(값은 0으로 유지됨).
        indices_true = np.nonzero(anchors_bbox_map &gt;= 0)[0]
        bb_idx = anchors_bbox_map[indices_true]
        class_labels[indices_true] = label[bb_idx, 0].astype('int32') + 1
        assigned_bb[indices_true] = label[bb_idx, 1:]
        # 오프셋 변환
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)
    bbox_offset = d2l.stack(batch_offset)
    bbox_mask = d2l.stack(batch_mask)
    class_labels = d2l.stack(batch_class_labels)
    return (bbox_offset, bbox_mask, class_labels)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def multibox_target(anchors, labels):
    """실제 바운딩 박스를 사용하여 앵커 박스에 라벨을 지정합니다."""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    batch_offset, batch_mask, batch_class_labels = [], [], []
    device, num_anchors = anchors.device, anchors.shape[0]
    for i in range(batch_size):
        label = labels[i, :, :]
        anchors_bbox_map = assign_anchor_to_bbox(
            label[:, 1:], anchors, device)
        bbox_mask = ((anchors_bbox_map &gt;= 0).float().unsqueeze(-1)).repeat(
            1, 4)
        # 클래스 레이블 및 할당된 바운딩 박스 좌표를 0으로 초기화
        class_labels = torch.zeros(num_anchors, dtype=torch.long,
                                   device=device)
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,
                                  device=device)
        # 할당된 실제 바운딩 박스를 사용하여 앵커 박스의 클래스에 라벨을 지정합니다.
        # 앵커 박스에 할당된 것이 없으면 클래스를 배경으로 라벨링합니다(값은 0으로 유지됨).
        indices_true = torch.nonzero(anchors_bbox_map &gt;= 0)
        bb_idx = anchors_bbox_map[indices_true]
        class_labels[indices_true] = label[bb_idx, 0].long() + 1
        assigned_bb[indices_true] = label[bb_idx, 1:]
        # 오프셋 변환
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
        batch_offset.append(offset.reshape(-1))
        batch_mask.append(bbox_mask.reshape(-1))
        batch_class_labels.append(class_labels)
    bbox_offset = torch.stack(batch_offset)
    bbox_mask = torch.stack(batch_mask)
    class_labels = torch.stack(batch_class_labels)
    return (bbox_offset, bbox_mask, class_labels)
</code></pre>
<h3 id="예제-an-example-1"><a class="header" href="#예제-an-example-1">예제 (An Example)</a></h3>
<p>구체적인 예를 통해 앵커 박스 라벨링을 설명해 보겠습니다.
로드된 이미지의 개와 고양이에 대한 실제 바운딩 박스를 정의합니다.
첫 번째 요소는 클래스(개는 0, 고양이는 1)이고 나머지 4개 요소는
왼쪽 상단 모서리와 오른쪽 하단 모서리의 $(x, y)$축 좌표입니다(범위는 0과 1 사이).
또한 왼쪽 상단 모서리와 오른쪽 하단 모서리의 좌표를 사용하여
라벨링할 5개의 앵커 박스 $A_0, \ldots, A_4$를 구성합니다(인덱스는 0부터 시작).
그런 다음 [<strong>이 실제 바운딩 박스와 앵커 박스를 이미지에 그립니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab all
ground_truth = d2l.tensor([[0, 0.1, 0.08, 0.52, 0.92],
                         [1, 0.55, 0.2, 0.9, 0.88]])
anchors = d2l.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                    [0.57, 0.3, 0.92, 0.9]])

fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')
show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);
</code></pre>
<p>위에서 정의한 <code>multibox_target</code> 함수를 사용하여,
개와 고양이에 대한 [<strong>실제 바운딩 박스를 기반으로
이러한 앵커 박스의 클래스와 오프셋을 라벨링</strong>]할 수 있습니다.
이 예에서 배경, 개, 고양이 클래스의 인덱스는 각각 0, 1, 2입니다.
아래에서 앵커 박스 및 실제 바운딩 박스의 예제에 대한 차원을 추가합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
labels = multibox_target(np.expand_dims(anchors, axis=0),
                         np.expand_dims(ground_truth, axis=0))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
labels = multibox_target(anchors.unsqueeze(dim=0),
                         ground_truth.unsqueeze(dim=0))
</code></pre>
<p>반환된 결과에는 세 가지 항목이 있으며 모두 텐서 형식입니다.
세 번째 항목에는 입력 앵커 박스의 라벨링된 클래스가 포함됩니다.</p>
<p>이미지의 앵커 박스 및 실제 바운딩 박스 위치를 기반으로 반환된 클래스 레이블을 분석해 보겠습니다.
먼저, 모든 앵커 박스 및 실제 바운딩 박스 쌍 중에서
앵커 박스 $A_4$와 고양이의 실제 바운딩 박스의 IoU가 가장 큽니다.
따라서 $A_4$의 클래스는 고양이로 라벨링됩니다.
$A_4$ 또는 고양이의 실제 바운딩 박스를 포함하는 쌍을 제외하고, 나머지 중에서
앵커 박스 $A_1$과 개의 실제 바운딩 박스 쌍이 가장 큰 IoU를 가집니다.
따라서 $A_1$의 클래스는 개로 라벨링됩니다.
다음으로, 나머지 세 개의 라벨이 지정되지 않은 앵커 박스 $A_0, A_2, A_3$을 순회해야 합니다.
$A_0$의 경우,
IoU가 가장 큰 실제 바운딩 박스의 클래스는 개이지만,
IoU가 미리 정의된 임계값(0.5) 미만이므로 클래스는 배경으로 라벨링됩니다.
$A_2$의 경우,
IoU가 가장 큰 실제 바운딩 박스의 클래스는 고양이이고 IoU가 임계값을 초과하므로 클래스는 고양이로 라벨링됩니다.
$A_3$의 경우,
IoU가 가장 큰 실제 바운딩 박스의 클래스는 고양이이지만 값이 임계값 미만이므로 클래스는 배경으로 라벨링됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
labels[2]
</code></pre>
<p>두 번째 반환된 항목은 (배치 크기, 앵커 박스 수의 4배) 모양의 마스크 변수입니다.
마스크 변수의 4개 요소마다 각 앵커 박스의 4개 오프셋 값에 해당합니다.
배경 감지에는 신경 쓰지 않으므로,
이 음성 클래스의 오프셋은 목적 함수에 영향을 주지 않아야 합니다.
요소별 곱셈을 통해 마스크 변수의 0은 목적 함수를 계산하기 전에 음성 클래스 오프셋을 필터링합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
labels[1]
</code></pre>
<p>첫 번째 반환된 항목에는 각 앵커 박스에 대해 라벨링된 4개의 오프셋 값이 포함됩니다.
음성 클래스 앵커 박스의 오프셋은 0으로 라벨링된다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">#@tab all
labels[0]
</code></pre>
<h2 id="비최대-억제로-바운딩-박스-예측-predicting-bounding-boxes-with-non-maximum-suppression"><a class="header" href="#비최대-억제로-바운딩-박스-예측-predicting-bounding-boxes-with-non-maximum-suppression">비최대 억제로 바운딩 박스 예측 (Predicting Bounding Boxes with Non-Maximum Suppression)</a></h2>
<p>:label:<code>subsec_predicting-bounding-boxes-nms</code></p>
<p>예측 중에,
우리는 이미지에 대해 여러 앵커 박스를 생성하고 각각에 대한 클래스와 오프셋을 예측합니다.
따라서 <em>예측된 바운딩 박스</em>는 예측된 오프셋이 있는 앵커 박스에 따라 얻어집니다. 아래에서 앵커와 오프셋 예측을 입력으로 받아 [<strong>역 오프셋 변환을 적용하여 예측된 바운딩 박스 좌표를 반환</strong>]하는 <code>offset_inverse</code> 함수를 구현합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def offset_inverse(anchors, offset_preds):
    """예측된 오프셋이 있는 앵커 박스를 기반으로 바운딩 박스를 예측합니다."""
    anc = d2l.box_corner_to_center(anchors)
    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]
    pred_bbox_wh = d2l.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]
    pred_bbox = d2l.concat((pred_bbox_xy, pred_bbox_wh), axis=1)
    predicted_bbox = d2l.box_center_to_corner(pred_bbox)
    return predicted_bbox
</code></pre>
<p>앵커 박스가 많은 경우,
동일한 객체를 둘러싸기 위해 유사한(상당한 겹침이 있는) 예측된 바운딩 박스가 많이 출력될 수 있습니다.
출력을 단순화하기 위해, *비최대 억제(non-maximum suppression, NMS)*를 사용하여
동일한 객체에 속하는 유사한 예측된 바운딩 박스를 병합할 수 있습니다.</p>
<p>비최대 억제 작동 방식은 다음과 같습니다.
예측된 바운딩 박스 $B$에 대해,
객체 감지 모델은 각 클래스에 대한 예측 가능성을 계산합니다.
가장 큰 예측 가능성을 $p$라고 하면, 이 확률에 해당하는 클래스가 $B$의 예측 클래스입니다.
구체적으로, 우리는 $p$를 예측된 바운딩 박스 $B$의 <em>신뢰도(confidence)</em> (점수)라고 합니다.
동일한 이미지에서,
예측된 모든 비배경 바운딩 박스는 신뢰도에 따라 내림차순으로 정렬되어
목록 $L$을 생성합니다.
그런 다음 다음 단계에서 정렬된 목록 $L$을 조작합니다.</p>
<ol>
<li>$L$에서 가장 높은 신뢰도를 가진 예측된 바운딩 박스 $B_1$을 기준으로 선택하고, $B_1$과의 IoU가 미리 정의된 임계값 $\epsilon$을 초과하는 모든 비기준 예측된 바운딩 박스를 $L$에서 제거합니다. 이 시점에서 $L$은 가장 높은 신뢰도를 가진 예측된 바운딩 박스를 유지하지만 너무 유사한 다른 바운딩 박스는 삭제합니다. 한마디로, <em>비최대</em> 신뢰도 점수를 가진 것들은 <em>억제</em>됩니다.</li>
<li>$L$에서 두 번째로 높은 신뢰도를 가진 예측된 바운딩 박스 $B_2$를 다른 기준으로 선택하고, $B_2$과의 IoU가 $\epsilon$을 초과하는 모든 비기준 예측된 바운딩 박스를 $L$에서 제거합니다.</li>
<li>$L$의 모든 예측된 바운딩 박스가 기준으로 사용될 때까지 위의 과정을 반복합니다. 이때 $L$에 있는 예측된 바운딩 박스 쌍의 IoU는 임계값 $\epsilon$ 미만이므로 서로 너무 유사한 쌍은 없습니다.</li>
<li>목록 $L$에 있는 모든 예측된 바운딩 박스를 출력합니다.</li>
</ol>
<p>[<strong>다음 <code>nms</code> 함수는 신뢰도 점수를 내림차순으로 정렬하고 인덱스를 반환합니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def nms(boxes, scores, iou_threshold):
    """예측된 바운딩 박스의 신뢰도 점수를 정렬합니다."""
    B = scores.argsort()[::-1]
    keep = []  # 유지될 예측된 바운딩 박스의 인덱스
    while B.size &gt; 0:
        i = B[0]
        keep.append(i)
        if B.size == 1: break
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        inds = np.nonzero(iou &lt;= iou_threshold)[0]
        B = B[inds + 1]
    return np.array(keep, dtype=np.int32, ctx=boxes.ctx)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def nms(boxes, scores, iou_threshold):
    """예측된 바운딩 박스의 신뢰도 점수를 정렬합니다."""
    B = torch.argsort(scores, dim=-1, descending=True)
    keep = []  # 유지될 예측된 바운딩 박스의 인덱스
    while B.numel() &gt; 0:
        i = B[0]
        keep.append(i)
        if B.numel() == 1: break
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        inds = torch.nonzero(iou &lt;= iou_threshold).reshape(-1)
        B = B[inds + 1]
    return d2l.tensor(keep, device=boxes.device)
</code></pre>
<p>우리는 다음 <code>multibox_detection</code>을 정의하여
[<strong>비최대 억제를 적용하여
바운딩 박스를 예측</strong>]합니다.
구현이 조금 복잡하더라도 걱정하지 마십시오. 구현 직후 구체적인 예제를 통해 작동 방식을 보여드리겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """비최대 억제를 사용하여 바운딩 박스를 예측합니다."""
    device, batch_size = cls_probs.ctx, cls_probs.shape[0]
    anchors = np.squeeze(anchors, axis=0)
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    out = []
    for i in range(batch_size):
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        conf, class_id = np.max(cls_prob[1:], 0), np.argmax(cls_prob[1:], 0)
        predicted_bb = offset_inverse(anchors, offset_pred)
        keep = nms(predicted_bb, conf, nms_threshold)
        # 모든 비 `keep` 인덱스를 찾아 클래스를 배경으로 설정
        all_idx = np.arange(num_anchors, dtype=np.int32, ctx=device)
        combined = d2l.concat((keep, all_idx))
        unique, counts = np.unique(combined, return_counts=True)
        non_keep = unique[counts == 1]
        all_id_sorted = d2l.concat((keep, non_keep))
        class_id[non_keep] = -1
        class_id = class_id[all_id_sorted].astype('float32')
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # 여기서 `pos_threshold`는 양성(비배경) 예측을 위한 임계값입니다
        below_min_idx = (conf &lt; pos_threshold)
        class_id[below_min_idx] = -1
        conf[below_min_idx] = 1 - conf[below_min_idx]
        pred_info = d2l.concat((np.expand_dims(class_id, axis=1),
                                np.expand_dims(conf, axis=1),
                                predicted_bb), axis=1)
        out.append(pred_info)
    return d2l.stack(out)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """비최대 억제를 사용하여 바운딩 박스를 예측합니다."""
    device, batch_size = cls_probs.device, cls_probs.shape[0]
    anchors = anchors.squeeze(0)
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    out = []
    for i in range(batch_size):
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        conf, class_id = torch.max(cls_prob[1:], 0)
        predicted_bb = offset_inverse(anchors, offset_pred)
        keep = nms(predicted_bb, conf, nms_threshold)
        # 모든 비 `keep` 인덱스를 찾아 클래스를 배경으로 설정
        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)
        combined = torch.cat((keep, all_idx))
        uniques, counts = combined.unique(return_counts=True)
        non_keep = uniques[counts == 1]
        all_id_sorted = torch.cat((keep, non_keep))
        class_id[non_keep] = -1
        class_id = class_id[all_id_sorted]
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # 여기서 `pos_threshold`는 양성(비배경) 예측을 위한 임계값입니다
        below_min_idx = (conf &lt; pos_threshold)
        class_id[below_min_idx] = -1
        conf[below_min_idx] = 1 - conf[below_min_idx]
        pred_info = torch.cat((class_id.unsqueeze(1),
                               conf.unsqueeze(1),
                               predicted_bb), dim=1)
        out.append(pred_info)
    return d2l.stack(out)
</code></pre>
<p>이제 [<strong>위의 구현을 4개의 앵커 박스가 있는 구체적인 예제에 적용</strong>]해 보겠습니다.
간단하게 하기 위해, 예측된 오프셋이 모두 0이라고 가정합니다.
이는 예측된 바운딩 박스가 앵커 박스임을 의미합니다.
배경, 개, 고양이 중 각 클래스에 대해
예측된 가능성도 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
anchors = d2l.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],
                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])
offset_preds = d2l.tensor([0] * d2l.size(anchors))
cls_probs = d2l.tensor([[0] * 4,  # 예측된 배경 가능성 
                      [0.9, 0.8, 0.7, 0.1],  # 예측된 개 가능성 
                      [0.1, 0.2, 0.3, 0.9]])  # 예측된 고양이 가능성
</code></pre>
<p>우리는 [<strong>이미지에 신뢰도와 함께 예측된 바운딩 박스를 그릴 수 있습니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab all
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, anchors * bbox_scale,
            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])
</code></pre>
<p>이제 <code>multibox_detection</code> 함수를 호출하여
비최대 억제를 수행할 수 있습니다.
여기서 임계값은 0.5로 설정됩니다.
텐서 입력에 예제에 대한 차원을 추가한다는 점에 유의하십시오.</p>
<p>[<strong>반환된 결과의 모양</strong>]은
(배치 크기, 앵커 박스 수, 6)임을 알 수 있습니다.
가장 안쪽 차원의 6개 요소는
동일한 예측된 바운딩 박스에 대한 출력 정보를 제공합니다.
첫 번째 요소는 예측된 클래스 인덱스로, 0부터 시작합니다(0은 개, 1은 고양이). 값 -1은 비최대 억제에서의 배경 또는 제거를 나타냅니다.
두 번째 요소는 예측된 바운딩 박스의 신뢰도입니다.
나머지 4개 요소는 예측된 바운딩 박스의 왼쪽 상단 모서리와
오른쪽 하단 모서리의 $(x, y)$축 좌표입니다(범위는 0과 1 사이).</p>
<pre><code class="language-{.python .input}">#@tab mxnet
output = multibox_detection(np.expand_dims(cls_probs, axis=0),
                            np.expand_dims(offset_preds, axis=0),
                            np.expand_dims(anchors, axis=0),
                            nms_threshold=0.5)
output
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
output = multibox_detection(cls_probs.unsqueeze(dim=0),
                            offset_preds.unsqueeze(dim=0),
                            anchors.unsqueeze(dim=0),
                            nms_threshold=0.5)
output
</code></pre>
<p>클래스 -1인 예측된 바운딩 박스를 제거한 후,
[<strong>비최대 억제에 의해 유지된 최종 예측된 바운딩 박스를 출력</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
fig = d2l.plt.imshow(img)
for i in d2l.numpy(output[0]):
    if i[0] == -1:
        continue
    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])
    show_bboxes(fig.axes, [d2l.tensor(i[2:]) * bbox_scale], label)
</code></pre>
<p>실제로, 비최대 억제를 수행하기 전에도 낮은 신뢰도를 가진 예측된 바운딩 박스를 제거하여 이 알고리즘의 계산을 줄일 수 있습니다.
또한 비최대 억제의 출력을 후처리할 수도 있습니다. 예를 들어, 더 높은 신뢰도를 가진 결과만
최종 출력에 유지하는 것입니다.</p>
<h2 id="요약-summary-72"><a class="header" href="#요약-summary-72">요약 (Summary)</a></h2>
<ul>
<li>우리는 이미지의 각 픽셀을 중심으로 다양한 모양의 앵커 박스를 생성합니다.</li>
<li>자카드 지수라고도 하는 IoU(Intersection over Union)는 두 바운딩 박스의 유사성을 측정합니다. 교집합 영역 대 합집합 영역의 비율입니다.</li>
<li>훈련 세트에서, 각 앵커 박스에 대해 두 가지 유형의 레이블이 필요합니다. 하나는 앵커 박스와 관련된 객체의 클래스이고 다른 하나는 앵커 박스에 대한 실제 바운딩 박스의 오프셋입니다.</li>
<li>예측 중에, 비최대 억제(NMS)를 사용하여 유사한 예측된 바운딩 박스를 제거하여 출력을 단순화할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-87"><a class="header" href="#연습-문제-exercises-87">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>multibox_prior</code> 함수에서 <code>sizes</code>와 <code>ratios</code> 값을 변경해 보십시오. 생성된 앵커 박스에 어떤 변화가 있습니까?</li>
<li>IoU가 0.5인 두 바운딩 박스를 구성하고 시각화해 보십시오. 서로 어떻게 겹칩니까?</li>
<li>:numref:<code>subsec_labeling-anchor-boxes</code> 및 :numref:<code>subsec_predicting-bounding-boxes-nms</code>에서 변수 <code>anchors</code>를 수정해 보십시오. 결과가 어떻게 변합니까?</li>
<li>비최대 억제는 <em>제거</em>함으로써 예측된 바운딩 박스를 억제하는 탐욕 알고리즘입니다. 제거된 것 중 일부가 실제로 유용할 수 있습니까? 이 알고리즘을 <em>부드럽게(softly)</em> 억제하도록 어떻게 수정할 수 있습니까? Soft-NMS :cite:<code>Bodla.Singh.Chellappa.ea.2017</code>를 참조할 수 있습니다.</li>
<li>수작업이 아닌, 비최대 억제를 학습할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/370">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1603">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="다중-스케일-객체-감지-multiscale-object-detection"><a class="header" href="#다중-스케일-객체-감지-multiscale-object-detection">다중 스케일 객체 감지 (Multiscale Object Detection)</a></h1>
<p>:label:<code>sec_multiscale-object-detection</code></p>
<p>:numref:<code>sec_anchor</code>에서,
우리는 입력 이미지의 각 픽셀을 중심으로 여러 앵커 박스를 생성했습니다.
본질적으로 이러한 앵커 박스는
이미지의 다른 영역 샘플을 나타냅니다.
그러나,
<em>모든</em> 픽셀에 대해 생성하면 계산할 앵커 박스가 너무 많아질 수 있습니다.
$561 \times 728$ 입력 이미지를 생각해 보십시오.
각 픽셀을 중심으로
다양한 모양의 앵커 박스 5개를 생성하면,
이미지에서 200만 개 이상의 앵커 박스($561 \times 728 \times 5$)를 라벨링하고 예측해야 합니다.</p>
<h2 id="다중-스케일-앵커-박스-multiscale-anchor-boxes"><a class="header" href="#다중-스케일-앵커-박스-multiscale-anchor-boxes">다중 스케일 앵커 박스 (Multiscale Anchor Boxes)</a></h2>
<p>:label:<code>subsec_multiscale-anchor-boxes</code></p>
<p>여러분은
이미지에서 앵커 박스를 줄이는 것이 어렵지 않다는 것을 깨달을 수 있습니다.
예를 들어,
입력 이미지에서 픽셀의 작은 부분만 균일하게 샘플링하여
이를 중심으로 앵커 박스를 생성할 수 있습니다.
또한,
다양한 스케일에서
다양한 크기의 앵커 박스를 생성할 수 있습니다.
직관적으로,
작은 객체는 큰 객체보다
이미지에 나타날 가능성이 더 높습니다.
예를 들어,
$1 \times 1$, $1 \times 2$, $2 \times 2$ 객체는
$2 \times 2$ 이미지에
각각 4, 2, 1가지 가능한 방식으로 나타날 수 있습니다.
따라서 작은 앵커 박스를 사용하여 작은 객체를 감지할 때는 더 많은 영역을 샘플링할 수 있고,
큰 객체의 경우 더 적은 영역을 샘플링할 수 있습니다.</p>
<p>다중 스케일에서 앵커 박스를 생성하는 방법을 보여주기 위해 이미지를 읽어보겠습니다.
높이와 너비는 각각 561과 728 픽셀입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import image, np, npx

npx.set_np()

img = image.imread('../img/catdog.jpg')
h, w = img.shape[:2]
h, w
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

img = d2l.plt.imread('../img/catdog.jpg')
h, w = img.shape[:2]
h, w
</code></pre>
<p>:numref:<code>sec_conv_layer</code>에서
우리는 합성곱 레이어의 2차원 배열 출력을 특징 맵(feature map)이라고 불렀습니다.
특징 맵 모양을 정의함으로써,
모든 이미지에서 균일하게 샘플링된 앵커 박스의 중심을 결정할 수 있습니다.</p>
<p><code>display_anchors</code> 함수는 아래에 정의되어 있습니다.
[<strong>우리는 각 단위(픽셀)를 앵커 박스 중심으로 하여 특징 맵(<code>fmap</code>)에 앵커 박스(<code>anchors</code>)를 생성합니다.</strong>]
앵커 박스(<code>anchors</code>)의 $(x, y)$축 좌표 값은
특징 맵(<code>fmap</code>)의 너비와 높이로 나누어졌으므로,
이 값은 0과 1 사이이며,
이는 특징 맵에서 앵커 박스의 상대적 위치를 나타냅니다.</p>
<p>앵커 박스(<code>anchors</code>)의 중심은
특징 맵(<code>fmap</code>)의 모든 단위에 퍼져 있으므로,
이러한 중심은 상대적 공간 위치 측면에서
모든 입력 이미지에 <em>균일하게</em> 분포되어야 합니다.
더 구체적으로,
특징 맵의 너비와 높이가 각각 <code>fmap_w</code>와 <code>fmap_h</code>로 주어지면,
다음 함수는 모든 입력 이미지에서
<code>fmap_h</code> 행과 <code>fmap_w</code> 열의 픽셀을 <em>균일하게</em> 샘플링합니다.
이러한 균일하게 샘플링된 픽셀을 중심으로,
스케일 <code>s</code>(목록 <code>s</code>의 길이가 1이라고 가정)와 다양한 가로세로 비율(<code>ratios</code>)의 앵커 박스가
생성됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def display_anchors(fmap_w, fmap_h, s):
    d2l.set_figsize()
    # 처음 두 차원의 값은 출력에 영향을 미치지 않습니다
    fmap = np.zeros((1, 10, fmap_h, fmap_w))
    anchors = npx.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])
    bbox_scale = np.array((w, h, w, h))
    d2l.show_bboxes(d2l.plt.imshow(img.asnumpy()).axes,
                    anchors[0] * bbox_scale)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def display_anchors(fmap_w, fmap_h, s):
    d2l.set_figsize()
    # 처음 두 차원의 값은 출력에 영향을 미치지 않습니다
    fmap = d2l.zeros((1, 10, fmap_h, fmap_w))
    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])
    bbox_scale = d2l.tensor((w, h, w, h))
    d2l.show_bboxes(d2l.plt.imshow(img).axes,
                    anchors[0] * bbox_scale)
</code></pre>
<p>먼저, [<strong>작은 객체의 감지를 고려해 봅시다</strong>].
표시할 때 구별하기 쉽도록 하기 위해, 여기서는 중심이 다른 앵커 박스가 겹치지 않습니다:
앵커 박스 스케일은 0.15로 설정되고
특징 맵의 높이와 너비는 4로 설정됩니다.
이미지의 4행 4열에 있는 앵커 박스의 중심이 균일하게 분포되어 있음을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
display_anchors(fmap_w=4, fmap_h=4, s=[0.15])
</code></pre>
<p>우리는 [<strong>특징 맵의 높이와 너비를 절반으로 줄이고 더 큰 앵커 박스를 사용하여 더 큰 객체를 감지하는 것</strong>]으로 이동합니다. 스케일을 0.4로 설정하면,
일부 앵커 박스가 서로 겹칩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
display_anchors(fmap_w=2, fmap_h=2, s=[0.4])
</code></pre>
<p>마지막으로, [<strong>특징 맵의 높이와 너비를 절반으로 더 줄이고 앵커 박스 스케일을 0.8로 늘립니다</strong>]. 이제 앵커 박스의 중심이 이미지의 중심이 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
display_anchors(fmap_w=1, fmap_h=1, s=[0.8])
</code></pre>
<h2 id="다중-스케일-감지-multiscale-detection"><a class="header" href="#다중-스케일-감지-multiscale-detection">다중 스케일 감지 (Multiscale Detection)</a></h2>
<p>다중 스케일 앵커 박스를 생성했으므로,
이를 사용하여 다양한 스케일에서 다양한 크기의 객체를 감지합니다.
다음에서
우리는 :numref:<code>sec_ssd</code>에서 구현할
CNN 기반 다중 스케일 객체 감지 방법을 소개합니다.</p>
<p>어떤 스케일에서,
$h \times w$ 모양의 특징 맵이 $c$개 있다고 가정해 봅시다.
:numref:<code>subsec_multiscale-anchor-boxes</code>의 방법을 사용하여
$hw$ 세트의 앵커 박스를 생성합니다.
여기서 각 세트에는 중심이 같은 $a$개의 앵커 박스가 있습니다.
예를 들어,
:numref:<code>subsec_multiscale-anchor-boxes</code>의 실험 첫 번째 스케일에서,
10개(채널 수)의 $4 \times 4$ 특징 맵이 주어졌을 때,
우리는 16세트의 앵커 박스를 생성했으며,
각 세트에는 중심이 같은 3개의 앵커 박스가 포함되어 있습니다.
다음으로, 각 앵커 박스는 실제 바운딩 박스를 기반으로
클래스와 오프셋으로 라벨링됩니다. 현재 스케일에서 객체 감지 모델은 입력 이미지에 있는 $hw$ 세트의 앵커 박스(서로 다른 세트는 다른 중심을 가짐)의 클래스와 오프셋을 예측해야 합니다.</p>
<p>여기서 $c$개의 특징 맵이
입력 이미지를 기반으로 CNN 순방향 전파에 의해 얻은
중간 출력이라고 가정합니다. 각 특징 맵에는 $hw$개의 서로 다른 공간 위치가 있으므로,
동일한 공간 위치는 $c$개의 단위를 갖는 것으로 생각할 수 있습니다.
:numref:<code>sec_conv_layer</code>의 수용 영역(receptive field) 정의에 따르면,
특징 맵의 동일한 공간 위치에 있는
이 $c$개의 단위는
입력 이미지에서 동일한 수용 영역을 갖습니다:
그들은 동일한 수용 영역 내의 입력 이미지 정보를 나타냅니다.
따라서 우리는 동일한 공간 위치에 있는 특징 맵의 $c$개 단위를
이 공간 위치를 사용하여 생성된 $a$개 앵커 박스의
클래스와 오프셋으로 변환할 수 있습니다.
본질적으로,
우리는 입력 이미지의 특정 수용 영역에 있는 정보를 사용하여
입력 이미지의 해당 수용 영역에 가까운
앵커 박스의 클래스와 오프셋을 예측합니다.</p>
<p>서로 다른 레이어의 특징 맵이
입력 이미지에서 다양한 크기의 수용 영역을 가질 때, 이들은 서로 다른 크기의 객체를 감지하는 데 사용될 수 있습니다.
예를 들어, 우리는 출력 레이어에 더 가까운 특징 맵의 단위가
더 넓은 수용 영역을 갖도록 신경망을 설계하여,
입력 이미지에서 더 큰 객체를 감지할 수 있도록 할 수 있습니다.</p>
<p>한마디로, 우리는 다중 스케일 객체 감지를 위해
심층 신경망에 의한 여러 수준의 이미지 계층별 표현을 활용할 수 있습니다.
:numref:<code>sec_ssd</code>의 구체적인 예를 통해 이것이 어떻게 작동하는지 보여줄 것입니다.</p>
<h2 id="요약-summary-73"><a class="header" href="#요약-summary-73">요약 (Summary)</a></h2>
<ul>
<li>다중 스케일에서, 우리는 다양한 크기의 객체를 감지하기 위해 다양한 크기의 앵커 박스를 생성할 수 있습니다.</li>
<li>특징 맵의 모양을 정의함으로써, 우리는 모든 이미지에서 균일하게 샘플링된 앵커 박스의 중심을 결정할 수 있습니다.</li>
<li>우리는 입력 이미지의 특정 수용 영역에 있는 정보를 사용하여 입력 이미지의 해당 수용 영역에 가까운 앵커 박스의 클래스와 오프셋을 예측합니다.</li>
<li>딥러닝을 통해, 우리는 다중 스케일 객체 감지를 위해 여러 수준의 이미지 계층별 표현을 활용할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-88"><a class="header" href="#연습-문제-exercises-88">연습 문제 (Exercises)</a></h2>
<ol>
<li>:numref:<code>sec_alexnet</code>에서의 논의에 따르면, 심층 신경망은 이미지에 대해 추상화 수준이 증가하는 계층적 특징을 학습합니다. 다중 스케일 객체 감지에서, 서로 다른 스케일의 특징 맵은 서로 다른 추상화 수준에 해당합니까? 그 이유는 무엇입니까?</li>
<li>:numref:<code>subsec_multiscale-anchor-boxes</code>의 실험 첫 번째 스케일(<code>fmap_w=4, fmap_h=4</code>)에서, 겹칠 수 있는 균일하게 분포된 앵커 박스를 생성하십시오.</li>
<li>모양이 $1 \times c \times h \times w$인 특징 맵 변수가 주어졌을 때(여기서 $c, h, w$는 각각 특징 맵의 채널 수, 높이, 너비임), 이 변수를 앵커 박스의 클래스와 오프셋으로 어떻게 변환할 수 있습니까? 출력의 모양은 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/371">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1607">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="객체-감지-데이터셋-the-object-detection-dataset"><a class="header" href="#객체-감지-데이터셋-the-object-detection-dataset">객체 감지 데이터셋 (The Object Detection Dataset)</a></h1>
<p>:label:<code>sec_object-detection-dataset</code></p>
<p>객체 감지 분야에는 MNIST나 Fashion-MNIST와 같은 작은 데이터셋이 없습니다.
객체 감지 모델을 빠르게 시연하기 위해,
[<strong>우리는 작은 데이터셋을 수집하고 라벨링했습니다</strong>].
먼저, 사무실에서 무료 바나나 사진을 찍어
회전과 크기가 다른 1000개의 바나나 이미지를 생성했습니다.
그런 다음 각 바나나 이미지를
일부 배경 이미지의 임의 위치에 배치했습니다.
마지막으로, 이미지에 있는 바나나에 대한 바운딩 박스를 라벨링했습니다.</p>
<h2 id="데이터셋-다운로드"><a class="header" href="#데이터셋-다운로드">[<strong>데이터셋 다운로드</strong>]</a></h2>
<p>모든 이미지 및 csv 라벨 파일이 포함된 바나나 감지 데이터셋은 인터넷에서 직접 다운로드할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, np, npx
import os
import pandas as pd

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
import torchvision
import os
import pandas as pd
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['banana-detection'] = (
    d2l.DATA_URL + 'banana-detection.zip',
    '5de26c8fce5ccdea9f91267273464dc968d20d72')
</code></pre>
<h2 id="데이터셋-읽기-reading-the-dataset-5"><a class="header" href="#데이터셋-읽기-reading-the-dataset-5">데이터셋 읽기 (Reading the Dataset)</a></h2>
<p>우리는 아래의 <code>read_data_bananas</code> 함수에서 [<strong>바나나 감지 데이터셋을 읽을 것입니다</strong>].
데이터셋에는 객체 클래스 레이블과
왼쪽 상단 및 오른쪽 하단 모서리의 실제 바운딩 박스 좌표에 대한 csv 파일이 포함되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def read_data_bananas(is_train=True):
    """바나나 감지 데이터셋 이미지와 라벨을 읽습니다."""
    data_dir = d2l.download_extract('banana-detection')
    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
                             else 'bananas_val', 'label.csv')
    csv_data = pd.read_csv(csv_fname)
    csv_data = csv_data.set_index('img_name')
    images, targets = [], []
    for img_name, target in csv_data.iterrows():
        images.append(image.imread(
            os.path.join(data_dir, 'bananas_train' if is_train else
                         'bananas_val', 'images', f'{img_name}')))
        # 여기서 `target`은 (클래스, 왼쪽 상단 x, 왼쪽 상단 y, 오른쪽 하단 x, 오른쪽 하단 y)를 포함합니다.
        # 모든 이미지는 동일한 바나나 클래스(인덱스 0)를 갖습니다.
        targets.append(list(target))
    return images, np.expand_dims(np.array(targets), 1) / 256
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def read_data_bananas(is_train=True):
    """바나나 감지 데이터셋 이미지와 라벨을 읽습니다."""
    data_dir = d2l.download_extract('banana-detection')
    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
                             else 'bananas_val', 'label.csv')
    csv_data = pd.read_csv(csv_fname)
    csv_data = csv_data.set_index('img_name')
    images, targets = [], []
    for img_name, target in csv_data.iterrows():
        images.append(torchvision.io.read_image(
            os.path.join(data_dir, 'bananas_train' if is_train else
                         'bananas_val', 'images', f'{img_name}')))
        # 여기서 `target`은 (클래스, 왼쪽 상단 x, 왼쪽 상단 y, 오른쪽 하단 x, 오른쪽 하단 y)를 포함합니다.
        # 모든 이미지는 동일한 바나나 클래스(인덱스 0)를 갖습니다.
        targets.append(list(target))
    return images, torch.tensor(targets).unsqueeze(1) / 256
</code></pre>
<p><code>read_data_bananas</code> 함수를 사용하여 이미지와 라벨을 읽음으로써,
다음 <code>BananasDataset</code> 클래스는 바나나 감지 데이터셋을 로드하기 위한
[<strong>사용자 정의 <code>Dataset</code> 인스턴스를 생성</strong>]할 수 있게 해줍니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
class BananasDataset(gluon.data.Dataset):
    """바나나 감지 데이터셋을 로드하기 위한 사용자 정의 데이터셋."""
    def __init__(self, is_train):
        self.features, self.labels = read_data_bananas(is_train)
        print('read ' + str(len(self.features)) + (f' training examples' if
              is_train else f' validation examples'))

    def __getitem__(self, idx):
        return (self.features[idx].astype('float32').transpose(2, 0, 1),
                self.labels[idx])

    def __len__(self):
        return len(self.features)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
class BananasDataset(torch.utils.data.Dataset):
    """바나나 감지 데이터셋을 로드하기 위한 사용자 정의 데이터셋."""
    def __init__(self, is_train):
        self.features, self.labels = read_data_bananas(is_train)
        print('read ' + str(len(self.features)) + (f' training examples' if
              is_train else f' validation examples'))

    def __getitem__(self, idx):
        return (self.features[idx].float(), self.labels[idx])

    def __len__(self):
        return len(self.features)
</code></pre>
<p>마지막으로, [<strong>훈련 및 테스트 세트 모두에 대해 두 개의 데이터 반복자 인스턴스를 반환</strong>]하는 <code>load_data_bananas</code> 함수를 정의합니다.
테스트 데이터셋의 경우, 무작위 순서로 읽을 필요가 없습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def load_data_bananas(batch_size):
    """바나나 감지 데이터셋을 로드합니다."""
    train_iter = gluon.data.DataLoader(BananasDataset(is_train=True),
                                       batch_size, shuffle=True)
    val_iter = gluon.data.DataLoader(BananasDataset(is_train=False),
                                     batch_size)
    return train_iter, val_iter
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def load_data_bananas(batch_size):
    """바나나 감지 데이터셋을 로드합니다."""
    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),
                                             batch_size, shuffle=True)
    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),
                                           batch_size)
    return train_iter, val_iter
</code></pre>
<p>[<strong>미니배치를 읽고 이 미니배치에 있는 이미지와 라벨의 모양을 인쇄</strong>]해 봅시다.
이미지 미니배치의 모양 (배치 크기, 채널 수, 높이, 너비)은 익숙해 보입니다. 이전 이미지 분류 작업과 동일합니다.
라벨 미니배치의 모양은 (배치 크기, $m$, 5)입니다. 여기서 $m$은 데이터셋의 이미지에 있을 수 있는 가장 큰 바운딩 박스 수입니다.</p>
<p>미니배치에서의 계산이 더 효율적이지만, 연결을 통해 미니배치를 형성하려면 모든 이미지 예제에 동일한 수의 바운딩 박스가 포함되어야 합니다.
일반적으로 이미지는 다양한 수의 바운딩 박스를 가질 수 있습니다. 따라서 $m$개 미만의 바운딩 박스를 가진 이미지는 $m$개에 도달할 때까지 불법 바운딩 박스로 채워집니다.
그런 다음 각 바운딩 박스의 라벨은 길이 5의 배열로 표현됩니다.
배열의 첫 번째 요소는 바운딩 박스에 있는 객체의 클래스이며, -1은 패딩을 위한 불법 바운딩 박스를 나타냅니다.
배열의 나머지 4개 요소는 바운딩 박스의 왼쪽 상단 모서리와 오른쪽 하단 모서리의 ($x$, $y$)-좌표 값입니다(범위는 0과 1 사이).
바나나 데이터셋의 경우, 각 이미지에 하나의 바운딩 박스만 있으므로 $m=1$입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
batch_size, edge_size = 32, 256
train_iter, _ = load_data_bananas(batch_size)
batch = next(iter(train_iter))
batch[0].shape, batch[1].shape
</code></pre>
<h2 id="시연-demonstration"><a class="header" href="#시연-demonstration">[<strong>시연 (Demonstration)</strong>]</a></h2>
<p>라벨링된 실제 바운딩 박스와 함께 10개의 이미지를 시연해 봅시다.
이 모든 이미지에서 바나나의 회전, 크기, 위치가 다양하다는 것을 알 수 있습니다.
물론 이것은 단순한 인공 데이터셋일 뿐입니다.
실제로 실제 데이터셋은 일반적으로 훨씬 더 복잡합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
imgs = (batch[0][:10].transpose(0, 2, 3, 1)) / 255
axes = d2l.show_images(imgs, 2, 5, scale=2)
for ax, label in zip(axes, batch[1][:10]):
    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
imgs = (batch[0][:10].permute(0, 2, 3, 1)) / 255
axes = d2l.show_images(imgs, 2, 5, scale=2)
for ax, label in zip(axes, batch[1][:10]):
    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])
</code></pre>
<h2 id="요약-summary-74"><a class="header" href="#요약-summary-74">요약 (Summary)</a></h2>
<ul>
<li>우리가 수집한 바나나 감지 데이터셋은 객체 감지 모델을 시연하는 데 사용할 수 있습니다.</li>
<li>객체 감지를 위한 데이터 로딩은 이미지 분류와 유사합니다. 그러나 객체 감지에서 라벨에는 이미지 분류에는 없는 실제 바운딩 박스 정보도 포함됩니다.</li>
</ul>
<h2 id="연습-문제-exercises-89"><a class="header" href="#연습-문제-exercises-89">연습 문제 (Exercises)</a></h2>
<ol>
<li>바나나 감지 데이터셋에서 실제 바운딩 박스가 있는 다른 이미지를 시연하십시오. 바운딩 박스와 객체 측면에서 어떻게 다릅니까?</li>
<li>무작위 자르기와 같은 데이터 증강을 객체 감지에 적용하고 싶다고 가정해 봅시다. 이미지 분류와 어떻게 다를 수 있습니까? 힌트: 잘린 이미지에 객체의 일부만 포함된 경우 어떻게 됩니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/372">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1608">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="단일-샷-멀티박스-감지-single-shot-multibox-detection"><a class="header" href="#단일-샷-멀티박스-감지-single-shot-multibox-detection">단일 샷 멀티박스 감지 (Single Shot Multibox Detection)</a></h1>
<p>:label:<code>sec_ssd</code></p>
<p>:numref:<code>sec_bbox</code>--:numref:<code>sec_object-detection-dataset</code>에서,
우리는 바운딩 박스, 앵커 박스,
다중 스케일 객체 감지 및 객체 감지를 위한 데이터셋을 소개했습니다.
이제 우리는 이러한 배경 지식을 사용하여 객체 감지 모델을 설계할 준비가 되었습니다:
바로 단일 샷 멀티박스 감지
(SSD) :cite:<code>Liu.Anguelov.Erhan.ea.2016</code>입니다.
이 모델은 간단하고 빠르며 널리 사용됩니다.
이것은 방대한 양의 객체 감지 모델 중 하나일 뿐이지만,
이 섹션의 설계 원칙과 구현 세부 사항 중 일부는
다른 모델에도 적용될 수 있습니다.</p>
<h2 id="모델-model-4"><a class="header" href="#모델-model-4">모델 (Model)</a></h2>
<p>:numref:<code>fig_ssd</code>는 단일 샷 멀티박스 감지의 설계를 개략적으로 보여줍니다.
이 모델은 주로 기본 네트워크(base network)와 그 뒤를 따르는
여러 다중 스케일 특징 맵 블록으로 구성됩니다.
기본 네트워크는 입력 이미지에서 특징을 추출하기 위한 것이므로
심층 CNN을 사용할 수 있습니다.
예를 들어,
원래의 단일 샷 멀티박스 감지 논문은
분류 레이어 이전에 잘린 VGG 네트워크를 채택했지만 :cite:<code>Liu.Anguelov.Erhan.ea.2016</code>,
ResNet도 일반적으로 사용되었습니다.
우리의 설계를 통해
우리는 기본 네트워크가 더 큰 특징 맵을 출력하도록 하여
더 작은 객체를 감지하기 위해 더 많은 앵커 박스를 생성할 수 있습니다.
그 후,
각 다중 스케일 특징 맵 블록은
이전 블록의 특징 맵의 높이와 너비를 줄이고(예: 절반으로),
특징 맵의 각 단위가 입력 이미지에서 수용 영역을 늘릴 수 있도록 합니다.</p>
<p>:numref:<code>sec_multiscale-object-detection</code>의
심층 신경망에 의한 이미지의 계층별 표현을 통한
다중 스케일 객체 감지의 설계를 상기하십시오.
:numref:<code>fig_ssd</code>의 상단에 더 가까운 다중 스케일 특징 맵은
더 작지만 더 큰 수용 영역을 가지므로,
더 적지만 더 큰 객체를 감지하는 데 적합합니다.</p>
<p>한마디로,
기본 네트워크와 여러 다중 스케일 특징 맵 블록을 통해,
단일 샷 멀티박스 감지는
다양한 크기의 앵커 박스를 생성하고,
이러한 앵커 박스의 클래스와 오프셋(따라서 바운딩 박스)을 예측하여
다양한 크기의 객체를 감지합니다;
따라서 이것은 다중 스케일 객체 감지 모델입니다.</p>
<p><img src="chapter_computer-vision/../img/ssd.svg" alt="다중 스케일 객체 감지 모델로서, 단일 샷 멀티박스 감지는 주로 기본 네트워크와 그 뒤를 따르는 여러 다중 스케일 특징 맵 블록으로 구성됩니다." />
:label:<code>fig_ssd</code></p>
<p>다음에서,
우리는 :numref:<code>fig_ssd</code>의 다른 블록들의 구현 세부 사항을 설명할 것입니다. 우선, 클래스 및 바운딩 박스 예측을 구현하는 방법에 대해 논의합니다.</p>
<h3 id="클래스-예측-레이어-class-prediction-layer"><a class="header" href="#클래스-예측-레이어-class-prediction-layer">[<strong>클래스 예측 레이어 (Class Prediction Layer)</strong>]</a></h3>
<p>객체 클래스의 수를 $q$라고 합시다.
그러면 앵커 박스에는 $q+1$개의 클래스가 있으며,
여기서 클래스 0은 배경입니다.
어떤 스케일에서,
특징 맵의 높이와 너비가 각각 $h$와 $w$라고 가정합니다.
이러한 특징 맵의 각 공간 위치를 중심으로 $a$개의 앵커 박스가 생성되면,
총 $hwa$개의 앵커 박스를 분류해야 합니다.
이것은 종종 무거운 파라미터화 비용으로 인해 완전 연결 레이어로 분류하는 것을 불가능하게 만듭니다.
:numref:<code>sec_nin</code>에서 클래스를 예측하기 위해 합성곱 레이어의 채널을 사용한 방법을 상기하십시오.
단일 샷 멀티박스 감지는 모델 복잡성을 줄이기 위해 동일한 기술을 사용합니다.</p>
<p>구체적으로,
클래스 예측 레이어는 특징 맵의 너비나 높이를 변경하지 않고 합성곱 레이어를 사용합니다.
이런 식으로,
특징 맵의 동일한 공간 차원(너비와 높이)에서
출력과 입력 사이에 일대일 대응이 있을 수 있습니다.
더 구체적으로,
어떤 공간 위치 ($x$, $y$)에서 출력 특징 맵의 채널은
입력 특징 맵의 ($x$, $y$)를 중심으로 하는
모든 앵커 박스에 대한 클래스 예측을 나타냅니다.
유효한 예측을 생성하려면 $a(q+1)$개의 출력 채널이 있어야 합니다.
여기서 동일한 공간 위치에 대해
인덱스 $i(q+1) + j$를 가진 출력 채널은
앵커 박스 $i$ ($0 \leq i &lt; a$)에 대한
클래스 $j$ ($0 \leq j \leq q$)의 예측을 나타냅니다.</p>
<p>아래에서 우리는 <code>num_anchors</code> 및 <code>num_classes</code> 인수를 통해 각각 $a$와 $q$를 지정하여 이러한 클래스 예측 레이어를 정의합니다.
이 레이어는 패딩이 1인 $3\times3$ 합성곱 레이어를 사용합니다.
이 합성곱 레이어의 입력과 출력의 너비와 높이는 변경되지 않습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, image, init, np, npx
from mxnet.gluon import nn

npx.set_np()

def cls_predictor(num_anchors, num_classes):
    return nn.Conv2D(num_anchors * (num_classes + 1), kernel_size=3,
                     padding=1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
import torchvision
from torch import nn
from torch.nn import functional as F

def cls_predictor(num_inputs, num_anchors, num_classes):
    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),
                     kernel_size=3, padding=1)
</code></pre>
<h3 id="바운딩-박스-예측-레이어-bounding-box-prediction-layer"><a class="header" href="#바운딩-박스-예측-레이어-bounding-box-prediction-layer">(<strong>바운딩 박스 예측 레이어 (Bounding Box Prediction Layer)</strong>)</a></h3>
<p>바운딩 박스 예측 레이어의 설계는 클래스 예측 레이어와 유사합니다.
유일한 차이점은 각 앵커 박스에 대한 출력 수에 있습니다:
여기서는 $q+1$개의 클래스가 아니라 4개의 오프셋을 예측해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def bbox_predictor(num_anchors):
    return nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def bbox_predictor(num_inputs, num_anchors):
    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)
</code></pre>
<h3 id="다중-스케일-예측-연결-concatenating-predictions-for-multiple-scales"><a class="header" href="#다중-스케일-예측-연결-concatenating-predictions-for-multiple-scales">[<strong>다중 스케일 예측 연결 (Concatenating Predictions for Multiple Scales)</strong>]</a></h3>
<p>우리가 언급했듯이, 단일 샷 멀티박스 감지는
다중 스케일 특징 맵을 사용하여 앵커 박스를 생성하고 클래스와 오프셋을 예측합니다.
서로 다른 스케일에서,
특징 맵의 모양이나
동일한 단위를 중심으로 하는 앵커 박스의 수가 다를 수 있습니다.
따라서,
서로 다른 스케일에서 예측 출력의 모양이 다를 수 있습니다.</p>
<p>다음 예제에서,
우리는 동일한 미니배치에 대해 두 가지 다른 스케일의 특징 맵 <code>Y1</code>과 <code>Y2</code>를 구성합니다.
여기서 <code>Y2</code>의 높이와 너비는 <code>Y1</code>의 절반입니다.
클래스 예측을 예로 들어 보겠습니다.
<code>Y1</code>과 <code>Y2</code>의 각 단위에 대해
각각 5개와 3개의 앵커 박스가 생성된다고 가정합니다.
또한 객체 클래스의 수가 10개라고 가정합니다.
특징 맵 <code>Y1</code>과 <code>Y2</code>에 대해
클래스 예측 출력의 채널 수는 각각 $5\times(10+1)=55$와 $3\times(10+1)=33$이며,
두 출력 모양은 (배치 크기, 채널 수, 높이, 너비)입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def forward(x, block):
    block.initialize()
    return block(x)

Y1 = forward(np.zeros((2, 8, 20, 20)), cls_predictor(5, 10))
Y2 = forward(np.zeros((2, 16, 10, 10)), cls_predictor(3, 10))
Y1.shape, Y2.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def forward(x, block):
    return block(x)

Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))
Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))
Y1.shape, Y2.shape
</code></pre>
<p>보시다시피, 배치 크기 차원을 제외하고
나머지 세 차원은 모두 크기가 다릅니다.
더 효율적인 계산을 위해 이 두 예측 출력을 연결하려면,
이 텐서들을 더 일관된 형식으로 변환해야 합니다.</p>
<p>채널 차원은 동일한 중심을 가진 앵커 박스에 대한 예측을 보유합니다.
우리는 먼저 이 차원을 가장 안쪽으로 이동합니다.
배치 크기는 다른 스케일에서도 동일하게 유지되므로,
우리는 예측 출력을
(배치 크기, 높이 $\times$ 너비 $\times$ 채널 수) 모양의
2차원 텐서로 변환할 수 있습니다.
그런 다음 차원 1을 따라
서로 다른 스케일의 출력을 연결할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def flatten_pred(pred):
    return npx.batch_flatten(pred.transpose(0, 2, 3, 1))

def concat_preds(preds):
    return np.concatenate([flatten_pred(p) for p in preds], axis=1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def flatten_pred(pred):
    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)

def concat_preds(preds):
    return torch.cat([flatten_pred(p) for p in preds], dim=1)
</code></pre>
<p>이런 식으로,
<code>Y1</code>과 <code>Y2</code>의 채널, 높이, 너비가 다르더라도,
우리는 여전히 동일한 미니배치에 대해 두 가지 다른 스케일의 예측 출력을 연결할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
concat_preds([Y1, Y2]).shape
</code></pre>
<h3 id="다운샘플링-블록-downsampling-block"><a class="header" href="#다운샘플링-블록-downsampling-block">[<strong>다운샘플링 블록 (Downsampling Block)</strong>]</a></h3>
<p>다중 스케일에서 객체를 감지하기 위해,
우리는 입력 특징 맵의 높이와 너비를 반으로 줄이는
다운샘플링 블록 <code>down_sample_blk</code>를 정의합니다.
사실,
이 블록은 :numref:<code>subsec_vgg-blocks</code>의 VGG 블록 설계를 적용합니다.
더 구체적으로,
각 다운샘플링 블록은
패딩이 1인 두 개의 $3\times3$ 합성곱 레이어와
그 뒤를 따르는 스트라이드가 2인 $2\times2$ 최대 풀링 레이어로 구성됩니다.
우리가 알다시피, 패딩이 1인 $3\times3$ 합성곱 레이어는 특징 맵의 모양을 변경하지 않습니다.
그러나 후속 $2\times2$ 최대 풀링은 입력 특징 맵의 높이와 너비를 반으로 줄입니다.
이 다운샘플링 블록의 입력 및 출력 특징 맵 모두에 대해,
$1\times 2+(3-1)+(3-1)=6$이기 때문에,
출력의 각 단위는
입력에 대해 $6\times6$ 수용 영역을 갖습니다.
따라서 다운샘플링 블록은 출력 특징 맵의 각 단위의 수용 영역을 확대합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def down_sample_blk(num_channels):
    blk = nn.Sequential()
    for _ in range(2):
        blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1),
                nn.BatchNorm(in_channels=num_channels),
                nn.Activation('relu'))
    blk.add(nn.MaxPool2D(2))
    return blk
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def down_sample_blk(in_channels, out_channels):
    blk = []
    for _ in range(2):
        blk.append(nn.Conv2d(in_channels, out_channels,
                             kernel_size=3, padding=1))
        blk.append(nn.BatchNorm2d(out_channels))
        blk.append(nn.ReLU())
        in_channels = out_channels
    blk.append(nn.MaxPool2d(2))
    return nn.Sequential(*blk)
</code></pre>
<p>다음 예제에서, 우리가 구성한 다운샘플링 블록은 입력 채널 수를 변경하고 입력 특징 맵의 높이와 너비를 반으로 줄입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
forward(np.zeros((2, 3, 20, 20)), down_sample_blk(10)).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
forward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape
</code></pre>
<h3 id="기본-네트워크-블록-base-network-block"><a class="header" href="#기본-네트워크-블록-base-network-block">[<strong>기본 네트워크 블록 (Base Network Block)</strong>]</a></h3>
<p>기본 네트워크 블록은 입력 이미지에서 특징을 추출하는 데 사용됩니다.
간단하게 하기 위해,
우리는 각 블록에서 채널 수를 두 배로 늘리는
세 개의 다운샘플링 블록으로 구성된 작은 기본 네트워크를 구성합니다.
$256\times256$ 입력 이미지가 주어지면,
이 기본 네트워크 블록은 $32 \times 32$ 특징 맵을 출력합니다 ($256/2^3=32$).</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def base_net():
    blk = nn.Sequential()
    for num_filters in [16, 32, 64]:
        blk.add(down_sample_blk(num_filters))
    return blk

forward(np.zeros((2, 3, 256, 256)), base_net()).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def base_net():
    blk = []
    num_filters = [3, 16, 32, 64]
    for i in range(len(num_filters) - 1):
        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))
    return nn.Sequential(*blk)

forward(torch.zeros((2, 3, 256, 256)), base_net()).shape
</code></pre>
<h3 id="전체-모델-the-complete-model"><a class="header" href="#전체-모델-the-complete-model">전체 모델 (The Complete Model)</a></h3>
<p>[<strong>완전한 단일 샷 멀티박스 감지 모델은 5개의 블록으로 구성됩니다.</strong>]
각 블록에서 생성된 특징 맵은
(i) 앵커 박스 생성
및 (ii) 이러한 앵커 박스의 클래스와 오프셋 예측 모두에 사용됩니다.
이 5개의 블록 중,
첫 번째 블록은 기본 네트워크 블록이고,
두 번째부터 네 번째 블록은 다운샘플링 블록이며,
마지막 블록은 글로벌 최대 풀링을 사용하여 높이와 너비를 모두 1로 줄입니다.
기술적으로,
두 번째부터 다섯 번째 블록은 모두
:numref:<code>fig_ssd</code>의
다중 스케일 특징 맵 블록입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def get_blk(i):
    if i == 0:
        blk = base_net()
    elif i == 4:
        blk = nn.GlobalMaxPool2D()
    else:
        blk = down_sample_blk(128)
    return blk
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def get_blk(i):
    if i == 0:
        blk = base_net()
    elif i == 1:
        blk = down_sample_blk(64, 128)
    elif i == 4:
        blk = nn.AdaptiveMaxPool2d((1,1))
    else:
        blk = down_sample_blk(128, 128)
    return blk
</code></pre>
<p>이제 각 블록에 대한 [<strong>순방향 전파를 정의</strong>]합니다.
이미지 분류 작업과 달리,
여기서의 출력에는
(i) CNN 특징 맵 <code>Y</code>,
(ii) 현재 스케일에서 <code>Y</code>를 사용하여 생성된 앵커 박스,
(iii) 이러한 앵커 박스에 대해 (<code>Y</code>를 기반으로) 예측된 클래스와 오프셋이 포함됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):
    Y = blk(X)
    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)
    cls_preds = cls_predictor(Y)
    bbox_preds = bbox_predictor(Y)
    return (Y, anchors, cls_preds, bbox_preds)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):
    Y = blk(X)
    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)
    cls_preds = cls_predictor(Y)
    bbox_preds = bbox_predictor(Y)
    return (Y, anchors, cls_preds, bbox_preds)
</code></pre>
<p>:numref:<code>fig_ssd</code>에서
상단에 더 가까운 다중 스케일 특징 맵 블록은
더 큰 객체를 감지하기 위한 것입니다;
따라서 더 큰 앵커 박스를 생성해야 합니다.
위의 순방향 전파에서,
각 다중 스케일 특징 맵 블록에서
우리는 호출된 <code>multibox_prior</code> 함수(:numref:<code>sec_anchor</code>에서 설명됨)의
<code>sizes</code> 인수를 통해 두 스케일 값 목록을 전달합니다.
다음에서,
0.2와 1.05 사이의 구간은
5개의 섹션으로 균등하게 분할되어
5개 블록에서 더 작은 스케일 값: 0.2, 0.37, 0.54, 0.71, 0.88을 결정합니다.
그런 다음 더 큰 스케일 값은
$\sqrt{0.2 \times 0.37} = 0.272$, $\sqrt{0.37 \times 0.54} = 0.447$ 등으로 주어집니다.</p>
<p>[<del>각 블록에 대한 하이퍼파라미터</del>]</p>
<pre><code class="language-{.python .input}">#@tab all
sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],
         [0.88, 0.961]]
ratios = [[1, 2, 0.5]] * 5
num_anchors = len(sizes[0]) + len(ratios[0]) - 1
</code></pre>
<p>이제 우리는 다음과 같이 [<strong>전체 모델</strong>] <code>TinySSD</code>를 정의할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class TinySSD(nn.Block):
    def __init__(self, num_classes, **kwargs):
        super(TinySSD, self).__init__(**kwargs)
        self.num_classes = num_classes
        for i in range(5):
            # 할당 문 `self.blk_i = get_blk(i)`와 동일
            setattr(self, f'blk_{i}', get_blk(i))
            setattr(self, f'cls_{i}', cls_predictor(num_anchors, num_classes))
            setattr(self, f'bbox_{i}', bbox_predictor(num_anchors))

    def forward(self, X):
        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5
        for i in range(5):
            # 여기서 `getattr(self, 'blk_%d' % i)`는 `self.blk_i`에 액세스합니다
            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(
                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],
                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))
        anchors = np.concatenate(anchors, axis=1)
        cls_preds = concat_preds(cls_preds)
        cls_preds = cls_preds.reshape(
            cls_preds.shape[0], -1, self.num_classes + 1)
        bbox_preds = concat_preds(bbox_preds)
        return anchors, cls_preds, bbox_preds
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class TinySSD(nn.Module):
    def __init__(self, num_classes, **kwargs):
        super(TinySSD, self).__init__(**kwargs)
        self.num_classes = num_classes
        idx_to_in_channels = [64, 128, 128, 128, 128]
        for i in range(5):
            # 할당 문 `self.blk_i = get_blk(i)`와 동일
            setattr(self, f'blk_{i}', get_blk(i))
            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],
                                                    num_anchors, num_classes))
            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],
                                                      num_anchors))

    def forward(self, X):
        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5
        for i in range(5):
            # 여기서 `getattr(self, 'blk_%d' % i)`는 `self.blk_i`에 액세스합니다
            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(
                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],
                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))
        anchors = torch.cat(anchors, dim=1)
        cls_preds = concat_preds(cls_preds)
        cls_preds = cls_preds.reshape(
            cls_preds.shape[0], -1, self.num_classes + 1)
        bbox_preds = concat_preds(bbox_preds)
        return anchors, cls_preds, bbox_preds
</code></pre>
<p>우리는 [<strong>모델 인스턴스를 생성하고 이를 사용하여</strong>]
$256 \times 256$ 이미지의 미니배치 <code>X</code>에 대해 [<strong>순방향 전파를 수행합니다</strong>].</p>
<p>이 섹션의 앞부분에서 본 것처럼,
첫 번째 블록은 $32 \times 32$ 특징 맵을 출력합니다.
두 번째부터 네 번째 다운샘플링 블록은
높이와 너비를 반으로 줄이고
다섯 번째 블록은 글로벌 풀링을 사용한다는 것을 상기하십시오.
공간 차원의 각 단위에 대해 4개의 앵커 박스가 생성되므로,
5개의 스케일 모두에서
각 이미지에 대해 총 $(32^2 + 16^2 + 8^2 + 4^2 + 1)\times 4 = 5444$개의 앵커 박스가 생성됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = TinySSD(num_classes=1)
net.initialize()
X = np.zeros((32, 3, 256, 256))
anchors, cls_preds, bbox_preds = net(X)

print('output anchors:', anchors.shape)
print('output class preds:', cls_preds.shape)
print('output bbox preds:', bbox_preds.shape)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = TinySSD(num_classes=1)
X = torch.zeros((32, 3, 256, 256))
anchors, cls_preds, bbox_preds = net(X)

print('output anchors:', anchors.shape)
print('output class preds:', cls_preds.shape)
print('output bbox preds:', bbox_preds.shape)
</code></pre>
<h2 id="훈련-training-25"><a class="header" href="#훈련-training-25">훈련 (Training)</a></h2>
<p>이제 객체 감지를 위한 단일 샷 멀티박스 감지 모델을 훈련하는 방법을 설명하겠습니다.</p>
<h3 id="데이터셋-읽기-및-모델-초기화-reading-the-dataset-and-initializing-the-model"><a class="header" href="#데이터셋-읽기-및-모델-초기화-reading-the-dataset-and-initializing-the-model">데이터셋 읽기 및 모델 초기화 (Reading the Dataset and Initializing the Model)</a></h3>
<p>우선,
:numref:<code>sec_object-detection-dataset</code>에서 설명한
[<strong>바나나 감지 데이터셋을 읽어</strong>] 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
batch_size = 32
train_iter, _ = d2l.load_data_bananas(batch_size)
</code></pre>
<p>바나나 감지 데이터셋에는 클래스가 하나뿐입니다. 모델을 정의한 후,
(<strong>파라미터를 초기화하고 최적화 알고리즘을 정의</strong>)해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
device, net = d2l.try_gpu(), TinySSD(num_classes=1)
net.initialize(init=init.Xavier(), ctx=device)
trainer = gluon.Trainer(net.collect_params(), 'sgd',
                        {'learning_rate': 0.2, 'wd': 5e-4})
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
device, net = d2l.try_gpu(), TinySSD(num_classes=1)
trainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)
</code></pre>
<h3 id="손실-및-평가-함수-정의-defining-loss-and-evaluation-functions"><a class="header" href="#손실-및-평가-함수-정의-defining-loss-and-evaluation-functions">[<strong>손실 및 평가 함수 정의 (Defining Loss and Evaluation Functions)</strong>]</a></h3>
<p>객체 감지에는 두 가지 유형의 손실이 있습니다.
첫 번째 손실은 앵커 박스의 클래스와 관련이 있습니다:
이 계산은
이미지 분류에 사용한 크로스 엔트로피 손실 함수를
간단히 재사용할 수 있습니다.
두 번째 손실은
양성(비배경) 앵커 박스의 오프셋과 관련이 있습니다:
이것은 회귀 문제입니다.
하지만 이 회귀 문제의 경우,
여기서는 :numref:<code>subsec_normal_distribution_and_squared_loss</code>에서 설명한 제곱 손실을 사용하지 않습니다.
대신,
우리는 예측과 실제 값 사이의 차이의 절대값인
$\ell_1$ 노름 손실을 사용합니다.
마스크 변수 <code>bbox_masks</code>는 손실 계산에서
음성 앵커 박스와 불법(패딩된) 앵커 박스를 필터링합니다.
결국, 우리는 앵커 박스 클래스 손실과 앵커 박스 오프셋 손실을 합산하여
모델의 손실 함수를 얻습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()
bbox_loss = gluon.loss.L1Loss()

def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):
    cls = cls_loss(cls_preds, cls_labels)
    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks)
    return cls + bbox
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
cls_loss = nn.CrossEntropyLoss(reduction='none')
bbox_loss = nn.L1Loss(reduction='none')

def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):
    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]
    cls = cls_loss(cls_preds.reshape(-1, num_classes),
                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)
    bbox = bbox_loss(bbox_preds * bbox_masks,
                     bbox_labels * bbox_masks).mean(dim=1)
    return cls + bbox
</code></pre>
<p>우리는 분류 결과를 평가하기 위해 정확도를 사용할 수 있습니다.
오프셋에 사용된 $\ell_1$ 노름 손실로 인해,
예측된 바운딩 박스를 평가하기 위해 *평균 절대 오차(mean absolute error)*를 사용합니다.
이러한 예측 결과는
생성된 앵커 박스와 이에 대해 예측된 오프셋에서 얻어집니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def cls_eval(cls_preds, cls_labels):
    # 클래스 예측 결과가 마지막 차원에 있으므로 `argmax`는 이 차원을 지정해야 합니다.
    return float((cls_preds.argmax(axis=-1).astype(
        cls_labels.dtype) == cls_labels).sum())

def bbox_eval(bbox_preds, bbox_labels, bbox_masks):
    return float((np.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def cls_eval(cls_preds, cls_labels):
    # 클래스 예측 결과가 마지막 차원에 있으므로 `argmax`는 이 차원을 지정해야 합니다.
    return float((cls_preds.argmax(dim=-1).type(
        cls_labels.dtype) == cls_labels).sum())

def bbox_eval(bbox_preds, bbox_labels, bbox_masks):
    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())
</code></pre>
<h3 id="모델-훈련-training-the-model"><a class="header" href="#모델-훈련-training-the-model">[<strong>모델 훈련 (Training the Model)</strong>]</a></h3>
<p>모델을 훈련할 때,
순방향 전파에서 다중 스케일 앵커 박스(<code>anchors</code>)를 생성하고
클래스(<code>cls_preds</code>)와 오프셋(<code>bbox_preds</code>)을 예측해야 합니다.
그런 다음 레이블 정보 <code>Y</code>를 기반으로 생성된 앵커 박스의 클래스(<code>cls_labels</code>)와 오프셋(<code>bbox_labels</code>)을 라벨링합니다.
마지막으로, 클래스와 오프셋의 예측된 값과 라벨링된 값을 사용하여
손실 함수를 계산합니다.
간결한 구현을 위해,
여기서는 테스트 데이터셋의 평가를 생략합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
num_epochs, timer = 20, d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                        legend=['class error', 'bbox mae'])
for epoch in range(num_epochs):
    # 훈련 정확도의 합, 훈련 정확도 합의 예제 수,
    # 절대 오차의 합, 절대 오차 합의 예제 수
    metric = d2l.Accumulator(4)
    for features, target in train_iter:
        timer.start()
        X = features.as_in_ctx(device)
        Y = target.as_in_ctx(device)
        with autograd.record():
            # 다중 스케일 앵커 박스 생성 및 클래스와 오프셋 예측
            anchors, cls_preds, bbox_preds = net(X)
            # 이러한 앵커 박스의 클래스와 오프셋 라벨링
            bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors,
                                                                      Y)
            # 클래스와 오프셋의 예측된 값과 라벨링된 값을 사용하여 손실 함수 계산
            l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,
                          bbox_masks)
        l.backward()
        trainer.step(batch_size)
        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.size,
                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),
                   bbox_labels.size)
    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]
    animator.add(epoch + 1, (cls_err, bbox_mae))
print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')
print(f'{len(train_iter._dataset) / timer.stop():.1f} examples/sec on '
      f'{str(device)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
num_epochs, timer = 20, d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                        legend=['class error', 'bbox mae'])
net = net.to(device)
for epoch in range(num_epochs):
    # 훈련 정확도의 합, 훈련 정확도 합의 예제 수,
    # 절대 오차의 합, 절대 오차 합의 예제 수
    metric = d2l.Accumulator(4)
    net.train()
    for features, target in train_iter:
        timer.start()
        trainer.zero_grad()
        X, Y = features.to(device), target.to(device)
        # 다중 스케일 앵커 박스 생성 및 클래스와 오프셋 예측
        anchors, cls_preds, bbox_preds = net(X)
        # 이러한 앵커 박스의 클래스와 오프셋 라벨링
        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)
        # 클래스와 오프셋의 예측된 값과 라벨링된 값을 사용하여 손실 함수 계산
        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,
                      bbox_masks)
        l.mean().backward()
        trainer.step()
        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),
                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),
                   bbox_labels.numel())
    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]
    animator.add(epoch + 1, (cls_err, bbox_mae))
print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')
print(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on '
      f'{str(device)}')
</code></pre>
<h2 id="예측-prediction-3"><a class="header" href="#예측-prediction-3">[<strong>예측 (Prediction)</strong>]</a></h2>
<p>예측 중에,
목표는 이미지에서 관심 있는 모든 객체를 감지하는 것입니다. 아래
우리는 테스트 이미지를 읽고 크기를 조정하여,
합성곱 레이어에 필요한 4차원 텐서로 변환합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
img = image.imread('../img/banana.jpg')
feature = image.imresize(img, 256, 256).astype('float32')
X = np.expand_dims(feature.transpose(2, 0, 1), axis=0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
X = torchvision.io.read_image('../img/banana.jpg').unsqueeze(0).float()
img = X.squeeze(0).permute(1, 2, 0).long()
</code></pre>
<p>아래 <code>multibox_detection</code> 함수를 사용하여,
예측된 바운딩 박스는
앵커 박스와 예측된 오프셋에서 얻어집니다.
그런 다음 비최대 억제를 사용하여
유사한 예측된 바운딩 박스를 제거합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def predict(X):
    anchors, cls_preds, bbox_preds = net(X.as_in_ctx(device))
    cls_probs = npx.softmax(cls_preds).transpose(0, 2, 1)
    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)
    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]
    return output[0, idx]

output = predict(X)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def predict(X):
    net.eval()
    anchors, cls_preds, bbox_preds = net(X.to(device))
    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)
    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)
    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]
    return output[0, idx]

output = predict(X)
</code></pre>
<p>마지막으로, 우리는 [<strong>신뢰도가 0.9 이상인 모든 예측된 바운딩 박스를 출력으로 표시</strong>]합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def display(img, output, threshold):
    d2l.set_figsize((5, 5))
    fig = d2l.plt.imshow(img.asnumpy())
    for row in output:
        score = float(row[1])
        if score &lt; threshold:
            continue
        h, w = img.shape[:2]
        bbox = [row[2:6] * np.array((w, h, w, h), ctx=row.ctx)]
        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')

display(img, output, threshold=0.9)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def display(img, output, threshold):
    d2l.set_figsize((5, 5))
    fig = d2l.plt.imshow(img)
    for row in output:
        score = float(row[1])
        if score &lt; threshold:
            continue
        h, w = img.shape[:2]
        bbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]
        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')

display(img, output.cpu(), threshold=0.9)
</code></pre>
<h2 id="요약-summary-75"><a class="header" href="#요약-summary-75">요약 (Summary)</a></h2>
<ul>
<li>단일 샷 멀티박스 감지는 다중 스케일 객체 감지 모델입니다. 기본 네트워크와 여러 다중 스케일 특징 맵 블록을 통해, 단일 샷 멀티박스 감지는 다양한 크기의 앵커 박스를 생성하고, 이러한 앵커 박스의 클래스와 오프셋(따라서 바운딩 박스)을 예측하여 다양한 크기의 객체를 감지합니다.</li>
<li>단일 샷 멀티박스 감지 모델을 훈련할 때, 손실 함수는 앵커 박스 클래스와 오프셋의 예측된 값과 라벨링된 값을 기반으로 계산됩니다.</li>
</ul>
<h2 id="연습-문제-exercises-90"><a class="header" href="#연습-문제-exercises-90">연습 문제 (Exercises)</a></h2>
<ol>
<li>손실 함수를 개선하여 단일 샷 멀티박스 감지를 개선할 수 있습니까? 예를 들어, 예측된 오프셋에 대해 $\ell_1$ 노름 손실을 부드러운(smooth) $\ell_1$ 노름 손실로 대체합니다. 이 손실 함수는 하이퍼파라미터 $\sigma$에 의해 제어되는 부드러움을 위해 0 주변에서 제곱 함수를 사용합니다:</li>
</ol>
<p>$$ f(x) =
\begin{cases}
(\sigma x)^2/2,&amp; \textrm{if }|x| &lt; 1/\sigma^2\
|x|-0.5/\sigma^2,&amp; \textrm{otherwise}
\end{cases}
$$</p>
<p>$\sigma$가 매우 클 때, 이 손실은 $\ell_1$ 노름 손실과 유사합니다. 값이 작을 때 손실 함수는 더 부드럽습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
sigmas = [10, 1, 0.5]
lines = ['-', '--', '-.']
x = np.arange(-2, 2, 0.1)
d2l.set_figsize()

for l, s in zip(lines, sigmas):
    y = npx.smooth_l1(x, scalar=s)
    d2l.plt.plot(x.asnumpy(), y.asnumpy(), l, label='sigma=%.1f' % s)
d2l.plt.legend();
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def smooth_l1(data, scalar):
    out = []
    for i in data:
        if abs(i) &lt; 1 / (scalar ** 2):
            out.append(((scalar * i) ** 2) / 2)
        else:
            out.append(abs(i) - 0.5 / (scalar ** 2))
    return torch.tensor(out)

sigmas = [10, 1, 0.5]
lines = ['-', '--', '-.']
x = torch.arange(-2, 2, 0.1)
d2l.set_figsize()

for l, s in zip(lines, sigmas):
    y = smooth_l1(x, scalar=s)
    d2l.plt.plot(x, y, l, label='sigma=%.1f' % s)
d2l.plt.legend();
</code></pre>
<p>또한 실험에서 우리는 클래스 예측을 위해 크로스 엔트로피 손실을 사용했습니다:
denoting by $p_j$ the predicted probability for the ground-truth class $j$, the cross-entropy loss is $-\log p_j$. We can also use the focal loss
:cite:<code>Lin.Goyal.Girshick.ea.2017</code>: given hyperparameters $\gamma &gt; 0$
and $\alpha &gt; 0$, this loss is defined as:</p>
<p>$$ - \alpha (1-p_j)^{\gamma} \log p_j.$$</p>
<p>As we can see, increasing $\gamma$ can effectively reduce the relative loss
for well-classified examples (e.g., $p_j &gt; 0.5$)
so the training
can focus more on those difficult examples that are misclassified.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def focal_loss(gamma, x):
    return -(1 - x) ** gamma * np.log(x)

x = np.arange(0.01, 1, 0.01)
for l, gamma in zip(lines, [0, 1, 5]):
    y = d2l.plt.plot(x.asnumpy(), focal_loss(gamma, x).asnumpy(), l,
                     label='gamma=%.1f' % gamma)
d2l.plt.legend();
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def focal_loss(gamma, x):
    return -(1 - x) ** gamma * torch.log(x)

x = torch.arange(0.01, 1, 0.01)
for l, gamma in zip(lines, [0, 1, 5]):
    y = d2l.plt.plot(x, focal_loss(gamma, x), l, label='gamma=%.1f' % gamma)
d2l.plt.legend();
</code></pre>
<ol start="2">
<li>공간 제한으로 인해 이 섹션에서는 단일 샷 멀티박스 감지 모델의 일부 구현 세부 정보를 생략했습니다. 다음 측면에서 모델을 더 개선할 수 있습니까?
<ol>
<li>객체가 이미지에 비해 훨씬 작을 때, 모델은 입력 이미지의 크기를 더 크게 조정할 수 있습니다.</li>
<li>일반적으로 음성 앵커 박스의 수는 엄청나게 많습니다. 클래스 분포를 더 균형 있게 만들기 위해 음성 앵커 박스를 다운샘플링할 수 있습니다.</li>
<li>손실 함수에서 클래스 손실과 오프셋 손실에 서로 다른 가중치 하이퍼파라미터를 할당합니다.</li>
<li>단일 샷 멀티박스 감지 논문 :cite:<code>Liu.Anguelov.Erhan.ea.2016</code>에 있는 것과 같은 다른 방법을 사용하여 객체 감지 모델을 평가합니다.</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/373">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1604">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="지역-기반-cnn-r-cnns"><a class="header" href="#지역-기반-cnn-r-cnns">지역 기반 CNN (R-CNNs)</a></h1>
<p>:label:<code>sec_rcnn</code></p>
<p>:numref:<code>sec_ssd</code>에서 설명한 단일 샷 멀티박스 감지 외에도,
지역 기반 CNN 또는 CNN 특징을 가진 지역(R-CNN) 또한
딥러닝을 객체 감지에 적용하는 많은 선구적인 접근 방식 중 하나입니다
:cite:<code>Girshick.Donahue.Darrell.ea.2014</code>.
이 섹션에서는 R-CNN과 그 일련의 개선 사항인 고속 R-CNN(Fast R-CNN) :cite:<code>Girshick.2015</code>, 더 빠른 R-CNN(Faster R-CNN) :cite:<code>Ren.He.Girshick.ea.2015</code>, 마스크 R-CNN(Mask R-CNN) :cite:<code>He.Gkioxari.Dollar.ea.2017</code>을 소개합니다.
제한된 공간으로 인해, 우리는 이러한 모델의 설계에만 초점을 맞출 것입니다.</p>
<h2 id="r-cnns"><a class="header" href="#r-cnns">R-CNNs</a></h2>
<p><em>R-CNN</em>은 먼저 입력 이미지에서 많은(예: 2000개) *지역 제안(region proposals)*을 추출합니다(예: 앵커 박스도 지역 제안으로 간주될 수 있음). 그리고 클래스와 바운딩 박스(예: 오프셋)를 라벨링합니다 :cite:<code>Girshick.Donahue.Darrell.ea.2014</code>.
그런 다음 CNN을 사용하여 각 지역 제안에 대해 순방향 전파를 수행하여 특징을 추출합니다.
다음으로, 각 지역 제안의 특징을 사용하여 이 지역 제안의 클래스와 바운딩 박스를 예측합니다.</p>
<p><img src="chapter_computer-vision/../img/r-cnn.svg" alt="R-CNN 모델." />
:label:<code>fig_r-cnn</code></p>
<p>:numref:<code>fig_r-cnn</code>은 R-CNN 모델을 보여줍니다. 더 구체적으로 R-CNN은 다음 네 단계로 구성됩니다:</p>
<ol>
<li>*선택적 검색(selective search)*을 수행하여 입력 이미지에서 여러 고품질 지역 제안을 추출합니다 :cite:<code>Uijlings.Van-De-Sande.Gevers.ea.2013</code>. 이러한 제안된 지역은 일반적으로 다양한 모양과 크기의 다중 스케일로 선택됩니다. 각 지역 제안은 클래스와 실제 바운딩 박스로 라벨링됩니다.</li>
<li>사전 훈련된 CNN을 선택하고 출력 레이어 이전에 자릅니다. 각 지역 제안을 네트워크에 필요한 입력 크기로 조정하고 순방향 전파를 통해 지역 제안에 대한 추출된 특징을 출력합니다.</li>
<li>각 지역 제안의 추출된 특징과 라벨링된 클래스를 예제로 사용합니다. 객체를 분류하기 위해 여러 서포트 벡터 머신(SVM)을 훈련합니다. 여기서 각 서포트 벡터 머신은 예제에 특정 클래스가 포함되어 있는지 여부를 개별적으로 결정합니다.</li>
<li>각 지역 제안의 추출된 특징과 라벨링된 바운딩 박스를 예제로 사용합니다. 선형 회귀 모델을 훈련하여 실제 바운딩 박스를 예측합니다.</li>
</ol>
<p>R-CNN 모델은 사전 훈련된 CNN을 사용하여 이미지 특징을 효과적으로 추출하지만 속도가 느립니다.
단일 입력 이미지에서 수천 개의 지역 제안을 선택한다고 상상해 보십시오:
객체 감지를 수행하려면 수천 번의 CNN 순방향 전파가 필요합니다.
이 막대한 컴퓨팅 부하로 인해 실제 응용 프로그램에서 R-CNN을 널리 사용하는 것은 불가능합니다.</p>
<h2 id="고속-r-cnn-fast-r-cnn"><a class="header" href="#고속-r-cnn-fast-r-cnn">고속 R-CNN (Fast R-CNN)</a></h2>
<p>R-CNN의 주요 성능 병목 현상은 계산 공유 없이 각 지역 제안에 대해 독립적인 CNN 순방향 전파가 있다는 것입니다.
이러한 지역은 일반적으로 겹치기 때문에 독립적인 특징 추출은 많은 반복 계산으로 이어집니다.
R-CNN에서 *고속 R-CNN(Fast R-CNN)*으로의 주요 개선 사항 중 하나는 CNN 순방향 전파가 전체 이미지에 대해서만 수행된다는 것입니다 :cite:<code>Girshick.2015</code>.</p>
<p><img src="chapter_computer-vision/../img/fast-rcnn.svg" alt="고속 R-CNN 모델." />
:label:<code>fig_fast_r-cnn</code></p>
<p>:numref:<code>fig_fast_r-cnn</code>은 고속 R-CNN 모델을 설명합니다. 주요 계산은 다음과 같습니다:</p>
<ol>
<li>R-CNN과 비교하여 고속 R-CNN에서 특징 추출을 위한 CNN의 입력은 개별 지역 제안이 아닌 전체 이미지입니다. 또한 이 CNN은 훈련 가능합니다. 입력 이미지가 주어졌을 때 CNN 출력의 모양을 $1 \times c \times h_1 \times w_1$이라고 합시다.</li>
<li>선택적 검색이 $n$개의 지역 제안을 생성한다고 가정합니다. 이러한 지역 제안(서로 다른 모양)은 CNN 출력에 관심 영역(서로 다른 모양)을 표시합니다. 그런 다음 이러한 관심 영역은 쉽게 연결될 수 있도록 동일한 모양(높이 $h_2$와 너비 $w_2$가 지정됨)의 특징을 추가로 추출합니다. 이를 달성하기 위해 고속 R-CNN은 <em>관심 영역(RoI) 풀링</em> 레이어를 도입합니다: CNN 출력과 지역 제안이 이 레이어에 입력되어 모든 지역 제안에 대해 추가로 추출되는 $n \times c \times h_2 \times w_2$ 모양의 연결된 특징을 출력합니다.</li>
<li>완전 연결 레이어를 사용하여 연결된 특징을 $n \times d$ 모양의 출력으로 변환합니다. 여기서 $d$는 모델 설계에 따라 다릅니다.</li>
<li>$n$개의 각 지역 제안에 대한 클래스와 바운딩 박스를 예측합니다. 더 구체적으로 클래스 및 바운딩 박스 예측에서 완전 연결 레이어 출력을 각각 $n \times q$ ($q$는 클래스 수) 모양의 출력과 $n \times 4$ 모양의 출력으로 변환합니다. 클래스 예측은 소프트맥스 회귀를 사용합니다.</li>
</ol>
<p>고속 R-CNN에서 제안된 관심 영역 풀링 레이어는 :numref:<code>sec_pooling</code>에서 소개한 풀링 레이어와 다릅니다.
풀링 레이어에서는 풀링 윈도우, 패딩, 스트라이드의 크기를 지정하여 출력 모양을 간접적으로 제어합니다.
반면, 관심 영역 풀링 레이어에서는 출력 모양을 직접 지정할 수 있습니다.</p>
<p>예를 들어, 각 영역의 출력 높이와 너비를 각각 $h_2$와 $w_2$로 지정해 보겠습니다.
모양이 $h \times w$인 관심 영역 윈도우의 경우,
이 윈도우는 $h_2 \times w_2$ 그리드의 하위 윈도우로 나뉩니다.
여기서 각 하위 윈도우의 모양은 대략 $(h/h_2) \times (w/w_2)$입니다.
실제로 모든 하위 윈도우의 높이와 너비는 올림 처리되어야 하며, 가장 큰 요소가 하위 윈도우의 출력으로 사용되어야 합니다.
따라서 관심 영역 풀링 레이어는 관심 영역의 모양이 다르더라도 동일한 모양의 특징을 추출할 수 있습니다.</p>
<p>예를 들어, :numref:<code>fig_roi</code>에서 $4 \times 4$ 입력에서 왼쪽 상단 $3\times 3$ 관심 영역이 선택됩니다.
이 관심 영역에 대해 $2\times 2$ 관심 영역 풀링 레이어를 사용하여 $2\times 2$ 출력을 얻습니다.
네 개의 나뉜 하위 윈도우 각각에는 0, 1, 4, 5 (5가 최대); 2와 6 (6이 최대); 8과 9 (9가 최대); 그리고 10이 포함됩니다.</p>
<p><img src="chapter_computer-vision/../img/roi.svg" alt="$2\times 2$ 관심 영역 풀링 레이어." />
:label:<code>fig_roi</code></p>
<p>아래에서 관심 영역 풀링 레이어의 계산을 보여줍니다. CNN 추출 특징 <code>X</code>의 높이와 너비가 모두 4이고 채널이 하나뿐이라고 가정합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from mxnet import np, npx

npx.set_np()

X = np.arange(16).reshape(1, 1, 4, 4)
X
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
import torch
import torchvision

X = torch.arange(16.).reshape(1, 1, 4, 4)
X
</code></pre>
<p>입력 이미지의 높이와 너비가 모두 40 픽셀이고 선택적 검색이 이 이미지에서 두 개의 지역 제안을 생성한다고 가정해 보겠습니다.
각 지역 제안은 5개의 요소로 표현됩니다: 객체 클래스와 그 뒤를 따르는 왼쪽 상단 및 오른쪽 하단 모서리의 $(x, y)$-좌표입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
rois = np.array([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
rois = torch.Tensor([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])
</code></pre>
<p><code>X</code>의 높이와 너비가 입력 이미지의 높이와 너비의 $1/10$이기 때문에,
두 지역 제안의 좌표에는 지정된 <code>spatial_scale</code> 인수에 따라 0.1이 곱해집니다.
그런 다음 두 관심 영역은 <code>X</code>에 각각 <code>X[:, :, 0:3, 0:3]</code> 및 <code>X[:, :, 1:4, 0:4]</code>로 표시됩니다.
마지막으로 $2\times 2$ 관심 영역 풀링에서 각 관심 영역은 하위 윈도우 그리드로 나뉘어 동일한 모양 $2\times 2$의 특징을 추가로 추출합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
npx.roi_pooling(X, rois, pooled_size=(2, 2), spatial_scale=0.1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
torchvision.ops.roi_pool(X, rois, output_size=(2, 2), spatial_scale=0.1)
</code></pre>
<h2 id="더-빠른-r-cnn-faster-r-cnn"><a class="header" href="#더-빠른-r-cnn-faster-r-cnn">더 빠른 R-CNN (Faster R-CNN)</a></h2>
<p>객체 감지에서 더 정확해지기 위해, 고속 R-CNN 모델은 일반적으로 선택적 검색에서 많은 지역 제안을 생성해야 합니다.
정확도 손실 없이 지역 제안을 줄이기 위해, *더 빠른 R-CNN(Faster R-CNN)*은 선택적 검색을 *지역 제안 네트워크(region proposal network)*로 대체할 것을 제안합니다 :cite:<code>Ren.He.Girshick.ea.2015</code>.</p>
<p><img src="chapter_computer-vision/../img/faster-rcnn.svg" alt="더 빠른 R-CNN 모델." />
:label:<code>fig_faster_r-cnn</code></p>
<p>:numref:<code>fig_faster_r-cnn</code>은 더 빠른 R-CNN 모델을 보여줍니다. 고속 R-CNN과 비교하여,
더 빠른 R-CNN은 지역 제안 방법만 선택적 검색에서 지역 제안 네트워크로 변경합니다.
모델의 나머지는 변경되지 않습니다.
지역 제안 네트워크는 다음 단계로 작동합니다:</p>
<ol>
<li>패딩이 1인 $3\times 3$ 합성곱 레이어를 사용하여 CNN 출력을 $c$ 채널을 가진 새로운 출력으로 변환합니다. 이런 식으로 CNN 추출 특징 맵의 공간 차원을 따른 각 단위는 길이 $c$의 새로운 특징 벡터를 얻습니다.</li>
<li>특징 맵의 각 픽셀을 중심으로 다양한 스케일과 가로세로 비율의 여러 앵커 박스를 생성하고 라벨링합니다.</li>
<li>각 앵커 박스 중심의 길이 $c$ 특징 벡터를 사용하여 이 앵커 박스에 대한 이진 클래스(배경 또는 객체)와 바운딩 박스를 예측합니다.</li>
<li>예측된 클래스가 객체인 예측된 바운딩 박스를 고려합니다. 비최대 억제를 사용하여 중복된 결과를 제거합니다. 객체에 대한 나머지 예측된 바운딩 박스가 관심 영역 풀링 레이어에 필요한 지역 제안입니다.</li>
</ol>
<p>주목할 점은 더 빠른 R-CNN 모델의 일부로서 지역 제안 네트워크가 모델의 나머지 부분과 함께 공동으로 훈련된다는 것입니다.
즉, 더 빠른 R-CNN의 목적 함수에는 객체 감지에서의 클래스 및 바운딩 박스 예측뿐만 아니라 지역 제안 네트워크에서의 앵커 박스의 이진 클래스 및 바운딩 박스 예측도 포함됩니다.
종단간 훈련의 결과로, 지역 제안 네트워크는 고품질 지역 제안을 생성하는 방법을 학습하여 데이터에서 학습된 감소된 수의 지역 제안으로 객체 감지에서 정확도를 유지할 수 있습니다.</p>
<h2 id="마스크-r-cnn-mask-r-cnn"><a class="header" href="#마스크-r-cnn-mask-r-cnn">마스크 R-CNN (Mask R-CNN)</a></h2>
<p>훈련 데이터셋에서 이미지에 객체의 픽셀 수준 위치도 라벨링되어 있다면,
*마스크 R-CNN(Mask R-CNN)*은 이러한 상세한 라벨을 효과적으로 활용하여 객체 감지의 정확도를 더욱 향상시킬 수 있습니다 :cite:<code>He.Gkioxari.Dollar.ea.2017</code>.</p>
<p><img src="chapter_computer-vision/../img/mask-rcnn.svg" alt="마스크 R-CNN 모델." />
:label:<code>fig_mask_r-cnn</code></p>
<p>:numref:<code>fig_mask_r-cnn</code>에 표시된 것처럼, 마스크 R-CNN은 더 빠른 R-CNN을 기반으로 수정되었습니다.
구체적으로, 마스크 R-CNN은 관심 영역 풀링 레이어를 <em>관심 영역(RoI) 정렬</em> 레이어로 대체합니다.
이 관심 영역 정렬 레이어는 이중 선형 보간법(bilinear interpolation)을 사용하여 특징 맵의 공간 정보를 보존하는데, 이는 픽셀 수준 예측에 더 적합합니다.
이 레이어의 출력에는 모든 관심 영역에 대해 동일한 모양의 특징 맵이 포함됩니다.
이들은 각 관심 영역에 대한 클래스와 바운딩 박스뿐만 아니라 추가적인 완전 합성곱 네트워크를 통해 객체의 픽셀 수준 위치를 예측하는 데 사용됩니다.
이미지의 픽셀 수준 의미를 예측하기 위해 완전 합성곱 네트워크를 사용하는 것에 대한 자세한 내용은 이 장의 후속 섹션에서 제공됩니다.</p>
<h2 id="요약-summary-76"><a class="header" href="#요약-summary-76">요약 (Summary)</a></h2>
<ul>
<li>R-CNN은 입력 이미지에서 많은 지역 제안을 추출하고, CNN을 사용하여 각 지역 제안에 대해 순방향 전파를 수행하여 특징을 추출한 다음, 이 특징을 사용하여 이 지역 제안의 클래스와 바운딩 박스를 예측합니다.</li>
<li>R-CNN에서 고속 R-CNN으로의 주요 개선 사항 중 하나는 CNN 순방향 전파가 전체 이미지에 대해서만 수행된다는 것입니다. 또한 관심 영역 풀링 레이어를 도입하여 모양이 다른 관심 영역에 대해 동일한 모양의 특징을 추가로 추출할 수 있습니다.</li>
<li>더 빠른 R-CNN은 고속 R-CNN에서 사용되는 선택적 검색을 공동으로 훈련된 지역 제안 네트워크로 대체하여, 감소된 수의 지역 제안으로 객체 감지에서 정확도를 유지할 수 있습니다.</li>
<li>더 빠른 R-CNN을 기반으로 하는 마스크 R-CNN은 추가적으로 완전 합성곱 네트워크를 도입하여 픽셀 수준 라벨을 활용하여 객체 감지의 정확도를 더욱 향상시킵니다.</li>
</ul>
<h2 id="연습-문제-exercises-91"><a class="header" href="#연습-문제-exercises-91">연습 문제 (Exercises)</a></h2>
<ol>
<li>객체 감지를 바운딩 박스 및 클래스 확률 예측과 같은 단일 회귀 문제로 구성할 수 있습니까? YOLO 모델 :cite:<code>Redmon.Divvala.Girshick.ea.2016</code>의 설계를 참조할 수 있습니다.</li>
<li>단일 샷 멀티박스 감지를 이 섹션에서 소개한 방법과 비교하십시오. 주요 차이점은 무엇입니까? :citet:<code>Zhao.Zheng.Xu.ea.2019</code>의 그림 2를 참조할 수 있습니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/374">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1409">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="시맨틱-분할과-데이터셋-semantic-segmentation-and-the-dataset"><a class="header" href="#시맨틱-분할과-데이터셋-semantic-segmentation-and-the-dataset">시맨틱 분할과 데이터셋 (Semantic Segmentation and the Dataset)</a></h1>
<p>:label:<code>sec_semantic_segmentation</code></p>
<p>:numref:<code>sec_bbox</code>--:numref:<code>sec_rcnn</code>에서 객체 감지 작업을 논의할 때,
직사각형 바운딩 박스를 사용하여 이미지의 객체를 라벨링하고 예측했습니다.
이 섹션에서는 이미지를 서로 다른 의미 클래스에 속하는 영역으로 나누는 방법에 초점을 맞춘 <em>시맨틱 분할(semantic segmentation)</em> 문제에 대해 논의할 것입니다.
다른 객체 감지와 달리,
시맨틱 분할은
이미지에 무엇이 있는지 픽셀 수준에서 인식하고 이해합니다:
그 의미 영역의 라벨링 및 예측은 픽셀 수준입니다.
:numref:<code>fig_segmentation</code>은 시맨틱 분할에서 이미지의 개, 고양이, 배경에 대한 라벨을 보여줍니다.
객체 감지와 비교할 때,
시맨틱 분할에서 라벨링된 픽셀 수준 경계는 분명히 더 세분화되어 있습니다.</p>
<p><img src="chapter_computer-vision/../img/segmentation.svg" alt="시맨틱 분할에서 이미지의 개, 고양이, 배경의 라벨." />
:label:<code>fig_segmentation</code></p>
<h2 id="이미지-분할-및-인스턴스-분할-image-segmentation-and-instance-segmentation"><a class="header" href="#이미지-분할-및-인스턴스-분할-image-segmentation-and-instance-segmentation">이미지 분할 및 인스턴스 분할 (Image Segmentation and Instance Segmentation)</a></h2>
<p>컴퓨터 비전 분야에는 시맨틱 분할과 유사한 두 가지 중요한 작업,
즉 이미지 분할과 인스턴스 분할도 있습니다.
우리는 다음과 같이 시맨틱 분할과 구별하여 간략하게 설명할 것입니다.</p>
<ul>
<li>*이미지 분할(Image segmentation)*은 이미지를 여러 구성 영역으로 나눕니다. 이러한 유형의 문제에 대한 방법은 일반적으로 이미지 픽셀 간의 상관 관계를 사용합니다. 훈련 중 이미지 픽셀에 대한 라벨 정보가 필요하지 않으며, 예측 중에 분할된 영역이 우리가 얻고자 하는 의미를 갖는다고 보장할 수 없습니다. :numref:<code>fig_segmentation</code>의 이미지를 입력으로 사용하면, 이미지 분할은 개를 두 영역으로 나눌 수 있습니다. 하나는 주로 검은색인 입과 눈을 덮고, 다른 하나는 주로 노란색인 몸의 나머지 부분을 덮습니다.</li>
<li>*인스턴스 분할(Instance segmentation)*은 <em>동시 감지 및 분할</em>이라고도 합니다. 이미지 내의 각 객체 인스턴스의 픽셀 수준 영역을 인식하는 방법을 연구합니다. 시맨틱 분할과 달리 인스턴스 분할은 의미뿐만 아니라 다른 객체 인스턴스도 구별해야 합니다. 예를 들어 이미지에 두 마리의 개가 있는 경우, 인스턴스 분할은 픽셀이 두 마리의 개 중 어느 것에 속하는지 구별해야 합니다.</li>
</ul>
<h2 id="pascal-voc2012-시맨틱-분할-데이터셋-the-pascal-voc2012-semantic-segmentation-dataset"><a class="header" href="#pascal-voc2012-시맨틱-분할-데이터셋-the-pascal-voc2012-semantic-segmentation-dataset">Pascal VOC2012 시맨틱 분할 데이터셋 (The Pascal VOC2012 Semantic Segmentation Dataset)</a></h2>
<p>[<strong>가장 중요한 시맨틱 분할 데이터셋 중 하나는 <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal VOC2012</a>입니다.</strong>]
다음에서,
우리는 이 데이터셋을 살펴볼 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, np, npx
import os

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
import torchvision
import os
</code></pre>
<p>데이터셋의 tar 파일은 약 2GB이므로,
파일을 다운로드하는 데 시간이 좀 걸릴 수 있습니다.
추출된 데이터셋은 <code>../data/VOCdevkit/VOC2012</code>에 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')

voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
</code></pre>
<p><code>../data/VOCdevkit/VOC2012</code> 경로로 들어가면,
데이터셋의 다양한 구성 요소를 볼 수 있습니다.
<code>ImageSets/Segmentation</code> 경로에는 훈련 및 테스트 샘플을 지정하는 텍스트 파일이 포함되어 있고,
<code>JPEGImages</code> 및 <code>SegmentationClass</code> 경로에는
각 예제에 대한 입력 이미지와 라벨이 각각 저장되어 있습니다.
여기서 라벨도 이미지 형식이며,
라벨링된 입력 이미지와 크기가 같습니다.
또한,
모든 라벨 이미지에서 동일한 색상을 가진 픽셀은 동일한 의미 클래스에 속합니다.
다음은 <code>read_voc_images</code> 함수를 정의하여 [<strong>모든 입력 이미지와 라벨을 메모리로 읽어옵니다</strong>].</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def read_voc_images(voc_dir, is_train=True):
    """모든 VOC 특징 및 라벨 이미지를 읽습니다."""
    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',
                             'train.txt' if is_train else 'val.txt')
    with open(txt_fname, 'r') as f:
        images = f.read().split()
    features, labels = [], []
    for i, fname in enumerate(images):
        features.append(image.imread(os.path.join(
            voc_dir, 'JPEGImages', f'{fname}.jpg')))
        labels.append(image.imread(os.path.join(
            voc_dir, 'SegmentationClass', f'{fname}.png')))
    return features, labels

train_features, train_labels = read_voc_images(voc_dir, True)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def read_voc_images(voc_dir, is_train=True):
    """모든 VOC 특징 및 라벨 이미지를 읽습니다."""
    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',
                             'train.txt' if is_train else 'val.txt')
    mode = torchvision.io.image.ImageReadMode.RGB
    with open(txt_fname, 'r') as f:
        images = f.read().split()
    features, labels = [], []
    for i, fname in enumerate(images):
        features.append(torchvision.io.read_image(os.path.join(
            voc_dir, 'JPEGImages', f'{fname}.jpg')))
        labels.append(torchvision.io.read_image(os.path.join(
            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))
    return features, labels

train_features, train_labels = read_voc_images(voc_dir, True)
</code></pre>
<p>우리는 [<strong>처음 5개의 입력 이미지와 해당 라벨을 그립니다</strong>].
라벨 이미지에서 흰색과 검은색은 각각 경계와 배경을 나타내며, 다른 색상은 서로 다른 클래스에 해당합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
n = 5
imgs = train_features[:n] + train_labels[:n]
d2l.show_images(imgs, 2, n);
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
n = 5
imgs = train_features[:n] + train_labels[:n]
imgs = [img.permute(1,2,0) for img in imgs]
d2l.show_images(imgs, 2, n);
</code></pre>
<p>다음으로, 우리는 이 데이터셋의 모든 라벨에 대해 [<strong>RGB 색상 값과 클래스 이름을 열거합니다</strong>].</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],
                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
                [0, 64, 128]]

#@save
VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',
               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
               'diningtable', 'dog', 'horse', 'motorbike', 'person',
               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']
</code></pre>
<p>위에서 정의한 두 가지 상수를 사용하여,
우리는 [<strong>라벨의 각 픽셀에 대한 클래스 인덱스를 편리하게 찾을 수 있습니다</strong>].
우리는 위 RGB 색상 값에서 클래스 인덱스로의 매핑을 구축하기 위해 <code>voc_colormap2label</code> 함수를 정의하고,
이 Pascal VOC2012 데이터셋의 모든 RGB 값을 해당 클래스 인덱스로 매핑하기 위해 <code>voc_label_indices</code> 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def voc_colormap2label():
    """VOC 라벨을 위해 RGB에서 클래스 인덱스로 매핑을 구축합니다."""
    colormap2label = np.zeros(256 ** 3)
    for i, colormap in enumerate(VOC_COLORMAP):
        colormap2label[
            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
    return colormap2label

#@save
def voc_label_indices(colormap, colormap2label):
    """VOC 라벨의 모든 RGB 값을 클래스 인덱스로 매핑합니다."""
    colormap = colormap.astype(np.int32)
    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256
           + colormap[:, :, 2])
    return colormap2label[idx]
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def voc_colormap2label():
    """VOC 라벨을 위해 RGB에서 클래스 인덱스로 매핑을 구축합니다."""
    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)
    for i, colormap in enumerate(VOC_COLORMAP):
        colormap2label[
            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
    return colormap2label

#@save
def voc_label_indices(colormap, colormap2label):
    """VOC 라벨의 모든 RGB 값을 클래스 인덱스로 매핑합니다."""
    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')
    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256
           + colormap[:, :, 2])
    return colormap2label[idx]
</code></pre>
<p>[<strong>예를 들어</strong>], 첫 번째 예제 이미지에서,
비행기 앞부분의 클래스 인덱스는 1이고,
배경 인덱스는 0입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
y = voc_label_indices(train_labels[0], voc_colormap2label())
y[105:115, 130:140], VOC_CLASSES[1]
</code></pre>
<h3 id="데이터-전처리-data-preprocessing-2"><a class="header" href="#데이터-전처리-data-preprocessing-2">데이터 전처리 (Data Preprocessing)</a></h3>
<p>:numref:<code>sec_alexnet</code>--:numref:<code>sec_googlenet</code>과 같은 이전 실험에서,
이미지는 모델의 필수 입력 모양에 맞게 크기가 조정(rescaling)되었습니다.
그러나 시맨틱 분할에서는
그렇게 하면 예측된 픽셀 클래스를
원래 입력 이미지 모양으로 다시 크기 조정해야 합니다.
이러한 크기 조정은 특히 클래스가 다른 분할된 영역의 경우 부정확할 수 있습니다. 이 문제를 피하기 위해,
우리는 크기 조정 대신 이미지를 <em>고정된</em> 모양으로 자릅니다. 구체적으로, [<strong>이미지 증강의 무작위 자르기를 사용하여 입력 이미지와 라벨의 동일한 영역을 자릅니다</strong>].</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def voc_rand_crop(feature, label, height, width):
    """특징 및 라벨 이미지를 모두 무작위로 자릅니다."""
    feature, rect = image.random_crop(feature, (width, height))
    label = image.fixed_crop(label, *rect)
    return feature, label
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def voc_rand_crop(feature, label, height, width):
    """특징 및 라벨 이미지를 모두 무작위로 자릅니다."""
    rect = torchvision.transforms.RandomCrop.get_params(
        feature, (height, width))
    feature = torchvision.transforms.functional.crop(feature, *rect)
    label = torchvision.transforms.functional.crop(label, *rect)
    return feature, label
</code></pre>
<pre><code class="language-{.python .input}">#@tab mxnet
imgs = []
for _ in range(n):
    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)
d2l.show_images(imgs[::2] + imgs[1::2], 2, n);
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
imgs = []
for _ in range(n):
    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)

imgs = [img.permute(1, 2, 0) for img in imgs]
d2l.show_images(imgs[::2] + imgs[1::2], 2, n);
</code></pre>
<h3 id="사용자-정의-시맨틱-분할-데이터셋-클래스-custom-semantic-segmentation-dataset-class"><a class="header" href="#사용자-정의-시맨틱-분할-데이터셋-클래스-custom-semantic-segmentation-dataset-class">[<strong>사용자 정의 시맨틱 분할 데이터셋 클래스 (Custom Semantic Segmentation Dataset Class)</strong>]</a></h3>
<p>우리는 고수준 API에서 제공하는 <code>Dataset</code> 클래스를 상속하여 사용자 정의 시맨틱 분할 데이터셋 클래스 <code>VOCSegDataset</code>을 정의합니다.
<code>__getitem__</code> 함수를 구현함으로써,
우리는 데이터셋에서 <code>idx</code>로 인덱싱된 입력 이미지와 이 이미지의 각 픽셀의 클래스 인덱스에 임의로 액세스할 수 있습니다.
데이터셋의 일부 이미지는
무작위 자르기의 출력 크기보다 작기 때문에,
이러한 예제는 사용자 정의 <code>filter</code> 함수에 의해 필터링됩니다.
또한, 입력 이미지의 세 가지 RGB 채널 값을 표준화하기 위해
<code>normalize_image</code> 함수도 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
class VOCSegDataset(gluon.data.Dataset):
    """VOC 데이터셋을 로드하기 위한 사용자 정의 데이터셋."""
    def __init__(self, is_train, crop_size, voc_dir):
        self.rgb_mean = np.array([0.485, 0.456, 0.406])
        self.rgb_std = np.array([0.229, 0.224, 0.225])
        self.crop_size = crop_size
        features, labels = read_voc_images(voc_dir, is_train=is_train)
        self.features = [self.normalize_image(feature)
                         for feature in self.filter(features)]
        self.labels = self.filter(labels)
        self.colormap2label = voc_colormap2label()
        print('read ' + str(len(self.features)) + ' examples')

    def normalize_image(self, img):
        return (img.astype('float32') / 255 - self.rgb_mean) / self.rgb_std

    def filter(self, imgs):
        return [img for img in imgs if (
            img.shape[0] &gt;= self.crop_size[0] and
            img.shape[1] &gt;= self.crop_size[1])]

    def __getitem__(self, idx):
        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],
                                       *self.crop_size)
        return (
                feature.transpose(2, 0, 1),
                voc_label_indices(label, self.colormap2label))

    def __len__(self):
        return len(self.features)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
class VOCSegDataset(torch.utils.data.Dataset):
    """VOC 데이터셋을 로드하기 위한 사용자 정의 데이터셋."""

    def __init__(self, is_train, crop_size, voc_dir):
        self.transform = torchvision.transforms.Normalize(
            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        self.crop_size = crop_size
        features, labels = read_voc_images(voc_dir, is_train=is_train)
        self.features = [self.normalize_image(feature)
                         for feature in self.filter(features)]
        self.labels = self.filter(labels)
        self.colormap2label = voc_colormap2label()
        print('read ' + str(len(self.features)) + ' examples')

    def normalize_image(self, img):
        return self.transform(img.float() / 255)

    def filter(self, imgs):
        return [img for img in imgs if (
            img.shape[1] &gt;= self.crop_size[0] and
            img.shape[2] &gt;= self.crop_size[1])]

    def __getitem__(self, idx):
        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],
                                       *self.crop_size)
        return (feature, voc_label_indices(label, self.colormap2label))

    def __len__(self):
        return len(self.features)
</code></pre>
<h3 id="데이터셋-읽기-reading-the-dataset-6"><a class="header" href="#데이터셋-읽기-reading-the-dataset-6">[<strong>데이터셋 읽기 (Reading the Dataset)</strong>]</a></h3>
<p>우리는 <code>VOCSegDatase</code>t 클래스를 사용하여
훈련 세트와 테스트 세트의 인스턴스를 각각 생성합니다.
무작위로 자른 이미지의 출력 모양을 $320	imes 480$으로 지정한다고 가정합니다.
아래에서 훈련 세트와 테스트 세트에 유지되는 예제의 수를 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
crop_size = (320, 480)
voc_train = VOCSegDataset(True, crop_size, voc_dir)
voc_test = VOCSegDataset(False, crop_size, voc_dir)
</code></pre>
<p>배치 크기를 64로 설정하고,
훈련 세트에 대한 데이터 반복자를 정의합니다.
첫 번째 미니배치의 모양을 인쇄해 봅시다.
이미지 분류나 객체 감지와 달리, 여기서 라벨은 3차원 텐서입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
batch_size = 64
train_iter = gluon.data.DataLoader(voc_train, batch_size, shuffle=True,
                                   last_batch='discard',
                                   num_workers=d2l.get_dataloader_workers())
for X, Y in train_iter:
    print(X.shape)
    print(Y.shape)
    break
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
batch_size = 64
train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,
                                    drop_last=True,
                                    num_workers=d2l.get_dataloader_workers())
for X, Y in train_iter:
    print(X.shape)
    print(Y.shape)
    break
</code></pre>
<h3 id="종합하기-putting-it-all-together-1"><a class="header" href="#종합하기-putting-it-all-together-1">[<strong>종합하기 (Putting It All Together)</strong>]</a></h3>
<p>마지막으로, Pascal VOC2012 시맨틱 분할 데이터셋을 다운로드하고 읽기 위해
다음 <code>load_data_voc</code> 함수를 정의합니다.
이 함수는 훈련 및 테스트 데이터셋 모두에 대한 데이터 반복자를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def load_data_voc(batch_size, crop_size):
    """VOC 시맨틱 분할 데이터셋을 로드합니다."""
    voc_dir = d2l.download_extract('voc2012', os.path.join(
        'VOCdevkit', 'VOC2012'))
    num_workers = d2l.get_dataloader_workers()
    train_iter = gluon.data.DataLoader(
        VOCSegDataset(True, crop_size, voc_dir), batch_size,
        shuffle=True, last_batch='discard', num_workers=num_workers)
    test_iter = gluon.data.DataLoader(
        VOCSegDataset(False, crop_size, voc_dir), batch_size,
        last_batch='discard', num_workers=num_workers)
    return train_iter, test_iter
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def load_data_voc(batch_size, crop_size):
    """VOC 시맨틱 분할 데이터셋을 로드합니다."""
    voc_dir = d2l.download_extract('voc2012', os.path.join(
        'VOCdevkit', 'VOC2012'))
    num_workers = d2l.get_dataloader_workers()
    train_iter = torch.utils.data.DataLoader(
        VOCSegDataset(True, crop_size, voc_dir), batch_size,
        shuffle=True, drop_last=True, num_workers=num_workers)
    test_iter = torch.utils.data.DataLoader(
        VOCSegDataset(False, crop_size, voc_dir), batch_size,
        drop_last=True, num_workers=num_workers)
    return train_iter, test_iter
</code></pre>
<h2 id="요약-summary-77"><a class="header" href="#요약-summary-77">요약 (Summary)</a></h2>
<ul>
<li>시맨틱 분할은 이미지를 다른 의미 클래스에 속하는 영역으로 나누어 픽셀 수준에서 이미지에 무엇이 있는지 인식하고 이해합니다.</li>
<li>가장 중요한 시맨틱 분할 데이터셋 중 하나는 Pascal VOC2012입니다.</li>
<li>시맨틱 분할에서는 입력 이미지와 라벨이 픽셀에서 일대일로 대응하므로, 입력 이미지는 크기가 조정되는 것이 아니라 고정된 모양으로 무작위로 잘립니다.</li>
</ul>
<h2 id="연습-문제-exercises-92"><a class="header" href="#연습-문제-exercises-92">연습 문제 (Exercises)</a></h2>
<ol>
<li>자율 주행 차량 및 의료 영상 진단에 시맨틱 분할을 어떻게 적용할 수 있습니까? 다른 응용 분야를 생각할 수 있습니까?</li>
<li>:numref:<code>sec_image_augmentation</code>의 데이터 증강 설명을 상기하십시오. 이미지 분류에 사용되는 이미지 증강 방법 중 시맨틱 분할에 적용할 수 없는 것은 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/375">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1480">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="전치-합성곱-transposed-convolution"><a class="header" href="#전치-합성곱-transposed-convolution">전치 합성곱 (Transposed Convolution)</a></h1>
<p>:label:<code>sec_transposed_conv</code></p>
<p>지금까지 우리가 살펴본 합성곱 레이어(:numref:<code>sec_conv_layer</code>)와 풀링 레이어(:numref:<code>sec_pooling</code>)와 같은 CNN 레이어들은 일반적으로 입력의 공간적 차원(높이와 너비)을 줄이거나(다운샘플링) 유지합니다. 픽셀 수준에서 분류하는 시맨틱 분할(semantic segmentation)에서는 입력과 출력의 공간적 차원이 동일한 것이 편리할 것입니다. 예를 들어, 한 출력 픽셀의 채널 차원이 동일한 공간 위치에 있는 입력 픽셀에 대한 분류 결과를 보유할 수 있습니다.</p>
<p>이를 달성하기 위해, 특히 CNN 레이어에 의해 공간적 차원이 줄어든 후에, 중간 특성 맵의 공간적 차원을 늘릴(업샘플링) 수 있는 다른 유형의 CNN 레이어를 사용할 수 있습니다. 이 섹션에서는 합성곱에 의한 다운샘플링 연산을 되돌리기 위한 *전치 합성곱(transposed convolution)*을 소개합니다. 이는 *분수 스트라이드 합성곱(fractionally-strided convolution)*이라고도 불립니다 :cite:<code>Dumoulin.Visin.2016</code>.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from mxnet import np, npx, init
from mxnet.gluon import nn
from d2l import mxnet as d2l

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
import torch
from torch import nn
from d2l import torch as d2l
</code></pre>
<h2 id="기본-연산-basic-operation"><a class="header" href="#기본-연산-basic-operation">기본 연산 (Basic Operation)</a></h2>
<p>지금은 채널을 무시하고, 스트라이드가 1이고 패딩이 없는 기본 전치 합성곱 연산부터 시작해 봅시다. $n_h \times n_w$ 입력 텐서와 $k_h \times k_w$ 커널이 주어졌다고 가정합니다. 스트라이드 1로 각 행에서 $n_w$번, 각 열에서 $n_h$번 커널 윈도우를 미끄러뜨리면 총 $n_h n_w$개의 중간 결과가 생성됩니다. 각 중간 결과는 0으로 초기화된 $(n_h + k_h - 1) \times (n_w + k_w - 1)$ 텐서입니다. 각 중간 텐서를 계산하기 위해, 입력 텐서의 각 요소에 커널을 곱하여 결과로 나오는 $k_h \times k_w$ 텐서가 각 중간 텐서의 일부를 대체하도록 합니다. 이때 대체된 부분의 위치는 계산에 사용된 입력 텐서 요소의 위치에 대응합니다. 마지막으로, 모든 중간 결과가 합산되어 출력을 생성합니다.</p>
<p>예를 들어, :numref:<code>fig_trans_conv</code>는 $2\times 2$ 입력 텐서에 대해 $2\times 2$ 커널을 사용한 전치 합성곱이 어떻게 계산되는지 보여줍니다.</p>
<p><img src="chapter_computer-vision/../img/trans_conv.svg" alt="2x2 커널을 사용한 전치 합성곱. 음영 처리된 부분은 중간 텐서의 일부와 계산에 사용된 입력 및 커널 텐서 요소입니다." />
:label:<code>fig_trans_conv</code></p>
<p>우리는 입력 행렬 <code>X</code>와 커널 행렬 <code>K</code>에 대해 이 기본 전치 합성곱 연산 <code>trans_conv</code>를 (<strong>구현</strong>)할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def trans_conv(X, K):
    h, w = K.shape
    Y = d2l.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            Y[i: i + h, j: j + w] += X[i, j] * K
    return Y
</code></pre>
<p>커널을 통해 입력 요소를 <em>줄이는</em> 일반 합성곱(:numref:<code>sec_conv_layer</code>)과 대조적으로, 전치 합성곱은 커널을 통해 입력 요소를 <em>브로드캐스트</em>하여 입력보다 큰 출력을 생성합니다. 우리는 :numref:<code>fig_trans_conv</code>의 입력 텐서 <code>X</code>와 커널 텐서 <code>K</code>를 구성하여 기본 2차원 전치 합성곱 연산의 [<strong>위 구현 출력을 검증</strong>]할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
X = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])
K = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])
trans_conv(X, K)
</code></pre>
<p>대안으로, 입력 <code>X</code>와 커널 <code>K</code>가 모두 4차원 텐서인 경우, [<strong>고수준 API를 사용하여 동일한 결과를 얻을 수 있습니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab mxnet
X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)
tconv = nn.Conv2DTranspose(1, kernel_size=2)
tconv.initialize(init.Constant(K))
tconv(X)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)
tconv.weight.data = K
tconv(X)
</code></pre>
<h2 id="패딩-스트라이드-그리고-다중-채널"><a class="header" href="#패딩-스트라이드-그리고-다중-채널">[<strong>패딩, 스트라이드, 그리고 다중 채널</strong>]</a></h2>
<p>패딩이 입력에 적용되는 일반 합성곱과 달리, 전치 합성곱에서는 패딩이 출력에 적용됩니다. 예를 들어, 높이와 너비의 양쪽에 패딩 수를 1로 지정하면, 전치 합성곱 출력에서 첫 번째와 마지막 행 및 열이 제거됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
tconv = nn.Conv2DTranspose(1, kernel_size=2, padding=1)
tconv.initialize(init.Constant(K))
tconv(X)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)
tconv.weight.data = K
tconv(X)
</code></pre>
<p>전치 합성곱에서 스트라이드는 입력이 아니라 중간 결과(따라서 출력)에 대해 지정됩니다. :numref:<code>fig_trans_conv</code>와 동일한 입력 및 커널 텐서를 사용하여 스트라이드를 1에서 2로 변경하면, :numref:<code>fig_trans_conv_stride2</code>와 같이 중간 텐서와 출력 텐서의 높이와 너비가 모두 증가합니다.</p>
<p><img src="chapter_computer-vision/../img/trans_conv_stride2.svg" alt="스트라이드 2인 2x2 커널을 사용한 전치 합성곱. 음영 처리된 부분은 중간 텐서의 일부와 계산에 사용된 입력 및 커널 텐서 요소입니다." />
:label:<code>fig_trans_conv_stride2</code></p>
<p>다음 코드 스니펫은 :numref:<code>fig_trans_conv_stride2</code>에서 스트라이드 2에 대한 전치 합성곱 출력을 검증할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
tconv = nn.Conv2DTranspose(1, kernel_size=2, strides=2)
tconv.initialize(init.Constant(K))
tconv(X)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)
tconv.weight.data = K
tconv(X)
</code></pre>
<p>다중 입력 및 출력 채널의 경우, 전치 합성곱은 일반 합성곱과 동일한 방식으로 작동합니다. 입력에 $c_i$ 채널이 있고 전치 합성곱이 각 입력 채널에 $k_h\times k_w$ 커널 텐서를 할당한다고 가정합니다. 다중 출력 채널이 지정되면 각 출력 채널에 대해 $c_i\times k_h\times k_w$ 커널을 갖게 됩니다.</p>
<p>종합하자면, $\mathsf{X}$를 합성곱 레이어 $f$에 공급하여 $\mathsf{Y}=f(\mathsf{X})$를 출력하고, $\mathsf{X}$의 채널 수가 출력 채널 수가 되는 것을 제외하고 $f$와 동일한 하이퍼파라미터를 가진 전치 합성곱 레이어 $g$를 생성하면, $g(Y)$는 $\mathsf{X}$와 동일한 모양을 갖게 됩니다. 이는 다음 예제에서 설명할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
X = np.random.uniform(size=(1, 10, 16, 16))
conv = nn.Conv2D(20, kernel_size=5, padding=2, strides=3)
tconv = nn.Conv2DTranspose(10, kernel_size=5, padding=2, strides=3)
conv.initialize()
tconv.initialize()
tconv(conv(X)).shape == X.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
X = torch.rand(size=(1, 10, 16, 16))
conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)
tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)
tconv(conv(X)).shape == X.shape
</code></pre>
<h2 id="행렬-전치와의-연결"><a class="header" href="#행렬-전치와의-연결">[<strong>행렬 전치와의 연결</strong>]</a></h2>
<p>:label:<code>subsec-connection-to-mat-transposition</code></p>
<p>전치 합성곱은 행렬 전치(matrix transposition)의 이름을 따서 명명되었습니다. 이를 설명하기 위해, 먼저 행렬 곱셈을 사용하여 합성곱을 구현하는 방법을 살펴봅시다. 아래 예제에서는 $3\times 3$ 입력 <code>X</code>와 $2\times 2$ 합성곱 커널 <code>K</code>를 정의한 다음, <code>corr2d</code> 함수를 사용하여 합성곱 출력 <code>Y</code>를 계산합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
X = d2l.arange(9.0).reshape(3, 3)
K = d2l.tensor([[1.0, 2.0], [3.0, 4.0]])
Y = d2l.corr2d(X, K)
Y
</code></pre>
<p>다음으로, 합성곱 커널 <code>K</code>를 많은 0을 포함하는 희소 가중치 행렬 <code>W</code>로 다시 씁니다. 가중치 행렬의 모양은 ($4$, $9$)이며, 여기서 0이 아닌 요소는 합성곱 커널 <code>K</code>에서 나옵니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def kernel2matrix(K):
    k, W = d2l.zeros(5), d2l.zeros((4, 9))
    k[:2], k[3:5] = K[0, :], K[1, :]
    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k
    return W

W = kernel2matrix(K)
W
</code></pre>
<p>입력 <code>X</code>를 행별로 연결하여 길이 9의 벡터를 얻습니다. 그런 다음 <code>W</code>와 벡터화된 <code>X</code>의 행렬 곱셈은 길이 4의 벡터를 제공합니다. 이를 재구성한 후, 위의 원래 합성곱 연산에서와 동일한 결과 <code>Y</code>를 얻을 수 있습니다: 우리는 방금 행렬 곱셈을 사용하여 합성곱을 구현했습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
Y == d2l.matmul(W, d2l.reshape(X, -1)).reshape(2, 2)
</code></pre>
<p>마찬가지로, 우리는 행렬 곱셈을 사용하여 전치 합성곱을 구현할 수 있습니다. 다음 예제에서는 위의 일반 합성곱의 $2 \times 2$ 출력 <code>Y</code>를 전치 합성곱의 입력으로 취합니다. 행렬을 곱하여 이 연산을 구현하려면, 모양이 $(9, 4)$인 전치된 가중치 행렬 <code>W</code>만 있으면 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
Z = trans_conv(Y, K)
Z == d2l.matmul(W.T, d2l.reshape(Y, -1)).reshape(3, 3)
</code></pre>
<p>행렬을 곱하여 합성곱을 구현하는 것을 고려해 보십시오. 입력 벡터 $\mathbf{x}$와 가중치 행렬 $\mathbf{W}$가 주어졌을 때, 합성곱의 순전파 함수는 입력을 가중치 행렬과 곱하여 벡터 $\mathbf{y}=\mathbf{W}\mathbf{x}$를 출력함으로써 구현될 수 있습니다. 역전파는 연쇄 법칙을 따르고 $\nabla_{\mathbf{x}}\mathbf{y}=\mathbf{W}^\top$이므로, 합성곱의 역전파 함수는 입력을 전치된 가중치 행렬 $\mathbf{W}^\top$과 곱함으로써 구현될 수 있습니다. 따라서 전치 합성곱 레이어는 합성곱 레이어의 순전파 함수와 역전파 함수를 교환하기만 하면 됩니다: 그 순전파 및 역전파 함수는 입력 벡터를 각각 $\mathbf{W}^\top$ 및 $\mathbf{W}$와 곱합니다.</p>
<h2 id="요약-summary-78"><a class="header" href="#요약-summary-78">요약 (Summary)</a></h2>
<ul>
<li>커널을 통해 입력 요소를 줄이는 일반 합성곱과 달리, 전치 합성곱은 커널을 통해 입력 요소를 브로드캐스트하여 입력보다 큰 출력을 생성합니다.</li>
<li>$\mathsf{X}$를 합성곱 레이어 $f$에 공급하여 $\mathsf{Y}=f(\mathsf{X})$를 출력하고, $\mathsf{X}$의 채널 수가 출력 채널 수가 되는 것을 제외하고 $f$와 동일한 하이퍼파라미터를 가진 전치 합성곱 레이어 $g$를 생성하면, $g(Y)$는 $\mathsf{X}$와 동일한 모양을 갖게 됩니다.</li>
<li>우리는 행렬 곱셈을 사용하여 합성곱을 구현할 수 있습니다. 전치 합성곱 레이어는 합성곱 레이어의 순전파 함수와 역전파 함수를 교환하기만 하면 됩니다.</li>
</ul>
<h2 id="연습-문제-exercises-93"><a class="header" href="#연습-문제-exercises-93">연습 문제 (Exercises)</a></h2>
<ol>
<li>:numref:<code>subsec-connection-to-mat-transposition</code>에서 합성곱 입력 <code>X</code>와 전치 합성곱 출력 <code>Z</code>는 동일한 모양을 갖습니다. 그들이 동일한 값을 가집니까? 왜 그런가요?</li>
<li>합성곱을 구현하는 데 행렬 곱셈을 사용하는 것이 효율적입니까? 왜 그런가요?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/376">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1450">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="완전-합성곱-네트워크-fully-convolutional-networks"><a class="header" href="#완전-합성곱-네트워크-fully-convolutional-networks">완전 합성곱 네트워크 (Fully Convolutional Networks)</a></h1>
<p>:label:<code>sec_fcn</code></p>
<p>:numref:<code>sec_semantic_segmentation</code>에서 논의한 바와 같이,
시맨틱 분할은
이미지를 픽셀 수준에서 분류합니다.
완전 합성곱 네트워크(FCN)는
이미지 픽셀을 픽셀 클래스로 변환하기 위해 합성곱 신경망을 사용합니다 :cite:<code>Long.Shelhamer.Darrell.2015</code>.
이미지 분류 또는 객체 감지를 위해
이전에 접했던 CNN과 달리,
완전 합성곱 네트워크는
중간 특징 맵의 높이와 너비를
입력 이미지의 높이와 너비로 다시 변환합니다.
이것은 :numref:<code>sec_transposed_conv</code>에서 소개한
전치 합성곱 레이어에 의해 달성됩니다.
결과적으로,
분류 출력과 입력 이미지는
픽셀 수준에서 일대일 대응을 가집니다.
어떤 출력 픽셀의 채널 차원은
동일한 공간 위치에 있는 입력 픽셀에 대한 분류 결과를 보유합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, init, np, npx
from mxnet.gluon import nn

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
</code></pre>
<h2 id="모델-the-model-1"><a class="header" href="#모델-the-model-1">모델 (The Model)</a></h2>
<p>여기서는 완전 합성곱 네트워크 모델의 기본 설계를 설명합니다.
:numref:<code>fig_fcn</code>에 표시된 것처럼,
이 모델은 먼저 CNN을 사용하여 이미지 특징을 추출하고,
그 다음 $1\times 1$ 합성곱 레이어를 통해 채널 수를 클래스 수로 변환하며,
마지막으로 :numref:<code>sec_transposed_conv</code>에서 소개한 전치 합성곱을 통해
특징 맵의 높이와 너비를
입력 이미지의 높이와 너비로 변환합니다.
결과적으로,
모델 출력은 입력 이미지와 동일한 높이와 너비를 가지며,
출력 채널에는 동일한 공간 위치에 있는 입력 픽셀에 대한 예측 클래스가 포함됩니다.</p>
<p><img src="chapter_computer-vision/../img/fcn.svg" alt="완전 합성곱 네트워크." />
:label:<code>fig_fcn</code></p>
<p>아래에서, 우리는 [<strong>ImageNet 데이터셋에서 사전 훈련된 ResNet-18 모델을 사용하여 이미지 특징을 추출</strong>]하고
모델 인스턴스를 <code>pretrained_net</code>으로 표시합니다.
이 모델의 마지막 몇 개 레이어에는
글로벌 평균 풀링 레이어와 완전 연결 레이어가 포함되어 있습니다.
이들은 완전 합성곱 네트워크에서 필요하지 않습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)
pretrained_net.features[-3:], pretrained_net.output
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
pretrained_net = torchvision.models.resnet18(pretrained=True)
list(pretrained_net.children())[-3:]
</code></pre>
<p>다음으로, 우리는 [<strong>완전 합성곱 네트워크 인스턴스 <code>net</code>을 생성</strong>]합니다.
이것은 ResNet-18의 모든 사전 훈련된 레이어를 복사합니다.
단, 출력에 가장 가까운 최종 글로벌 평균 풀링 레이어와 완전 연결 레이어는 제외합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = nn.HybridSequential()
for layer in pretrained_net.features[:-2]:
    net.add(layer)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = nn.Sequential(*list(pretrained_net.children())[:-2])
</code></pre>
<p>높이와 너비가 각각 320과 480인 입력이 주어지면,
<code>net</code>의 순방향 전파는
입력 높이와 너비를 원래의 1/32, 즉 10과 15로 줄입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
X = np.random.uniform(size=(1, 3, 320, 480))
net(X).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
X = torch.rand(size=(1, 3, 320, 480))
net(X).shape
</code></pre>
<p>다음으로, 우리는 [<strong>$1\times 1$ 합성곱 레이어를 사용하여 출력 채널 수를 Pascal VOC2012 데이터셋의 클래스 수(21)로 변환합니다.</strong>]
마지막으로, 특징 맵을 입력 이미지의 높이와 너비로 다시 변경하기 위해 (<strong>특징 맵의 높이와 너비를 32배로 늘려야 합니다</strong>).
:numref:<code>sec_padding</code>에서 합성곱 레이어의 출력 모양을 계산하는 방법을 상기하십시오.
$(320-64+16\times2+32)/32=10$이고 $(480-64+16\times2+32)/32=15$이므로, 우리는 스트라이드 $32$인 전치 합성곱 레이어를 구성하고,
커널의 높이와 너비를 $64$로, 패딩을 $16$으로 설정합니다.
일반적으로,
스트라이드 $s$,
패딩 $s/2$($s/2$가 정수라고 가정),
커널의 높이와 너비 $2s$에 대해,
전치 합성곱은 입력의 높이와 너비를 $s$배로 늘립니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
num_classes = 21
net.add(nn.Conv2D(num_classes, kernel_size=1),
        nn.Conv2DTranspose(
            num_classes, kernel_size=64, padding=16, strides=32))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
num_classes = 21
net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))
net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,
                                    kernel_size=64, padding=16, stride=32))
</code></pre>
<h2 id="전치-합성곱-레이어-초기화-initializing-transposed-convolutional-layers"><a class="header" href="#전치-합성곱-레이어-초기화-initializing-transposed-convolutional-layers">[<strong>전치 합성곱 레이어 초기화 (Initializing Transposed Convolutional Layers)</strong>]</a></h2>
<p>우리는 이미 전치 합성곱 레이어가
특징 맵의 높이와 너비를 늘릴 수 있다는 것을 알고 있습니다.
이미지 처리에서, 우리는 이미지를 확대해야 할 수도 있습니다. 즉, *업샘플링(upsampling)*입니다.
*이중 선형 보간법(Bilinear interpolation)*은
일반적으로 사용되는 업샘플링 기술 중 하나입니다.
이것은 또한 전치 합성곱 레이어를 초기화하는 데 자주 사용됩니다.</p>
<p>이중 선형 보간법을 설명하기 위해,
입력 이미지가 주어졌을 때
업샘플링된 출력 이미지의 각 픽셀을
계산하고 싶다고 가정해 봅시다.
좌표 $(x, y)$에 있는 출력 이미지의 픽셀을 계산하기 위해,
먼저 입력 크기 대 출력 크기의 비율에 따라 $(x, y)$를 입력 이미지의 좌표 $(x', y')$로 매핑합니다.
매핑된 $x'$와 $y'$는 실수입니다.
그런 다음, 입력 이미지에서 좌표 $(x', y')$에 가장 가까운 4개의 픽셀을 찾습니다.
마지막으로, 좌표 $(x, y)$에 있는 출력 이미지의 픽셀은 입력 이미지의 이 4개의 가장 가까운 픽셀과
$(x', y')$로부터의 상대적 거리를 기반으로 계산됩니다.</p>
<p>이중 선형 보간법의 업샘플링은
다음 <code>bilinear_kernel</code> 함수에 의해 구성된 커널을 가진 전치 합성곱 레이어로 구현될 수 있습니다.
공간 제한으로 인해, 알고리즘 설계에 대한 논의 없이 아래에 <code>bilinear_kernel</code> 함수의 구현만 제공합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def bilinear_kernel(in_channels, out_channels, kernel_size):
    factor = (kernel_size + 1) // 2
    if kernel_size % 2 == 1:
        center = factor - 1
    else:
        center = factor - 0.5
    og = (np.arange(kernel_size).reshape(-1, 1),
          np.arange(kernel_size).reshape(1, -1))
    filt = (1 - np.abs(og[0] - center) / factor) * \
           (1 - np.abs(og[1] - center) / factor)
    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size))
    weight[range(in_channels), range(out_channels), :, :] = filt
    return np.array(weight)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def bilinear_kernel(in_channels, out_channels, kernel_size):
    factor = (kernel_size + 1) // 2
    if kernel_size % 2 == 1:
        center = factor - 1
    else:
        center = factor - 0.5
    og = (torch.arange(kernel_size).reshape(-1, 1),
          torch.arange(kernel_size).reshape(1, -1))
    filt = (1 - torch.abs(og[0] - center) / factor) * \
           (1 - torch.abs(og[1] - center) / factor)
    weight = torch.zeros((in_channels, out_channels,
                          kernel_size, kernel_size))
    weight[range(in_channels), range(out_channels), :, :] = filt
    return weight
</code></pre>
<p>전치 합성곱 레이어에 의해 구현된 [<strong>이중 선형 보간법의 업샘플링을 실험</strong>]해 봅시다.
우리는 높이와 너비를 두 배로 늘리는 전치 합성곱 레이어를 구성하고,
<code>bilinear_kernel</code> 함수로 커널을 초기화합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
conv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2)
conv_trans.initialize(init.Constant(bilinear_kernel(3, 3, 4)))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,
                                bias=False)
conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4));
</code></pre>
<p>이미지 <code>X</code>를 읽고 업샘플링 출력을 <code>Y</code>에 할당합니다. 이미지를 인쇄하려면 채널 차원의 위치를 조정해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
img = image.imread('../img/catdog.jpg')
X = np.expand_dims(img.astype('float32').transpose(2, 0, 1), axis=0) / 255
Y = conv_trans(X)
out_img = Y[0].transpose(1, 2, 0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
img = torchvision.transforms.ToTensor()(d2l.Image.open('../img/catdog.jpg'))
X = img.unsqueeze(0)
Y = conv_trans(X)
out_img = Y[0].permute(1, 2, 0).detach()
</code></pre>
<p>보시다시피, 전치 합성곱 레이어는 이미지의 높이와 너비를 모두 2배로 늘립니다.
좌표의 스케일이 다른 것을 제외하고,
이중 선형 보간법으로 확대된 이미지와 :numref:<code>sec_bbox</code>에서 인쇄된 원본 이미지는 동일하게 보입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.set_figsize()
print('input image shape:', img.shape)
d2l.plt.imshow(img.asnumpy());
print('output image shape:', out_img.shape)
d2l.plt.imshow(out_img.asnumpy());
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
d2l.set_figsize()
print('input image shape:', img.permute(1, 2, 0).shape)
d2l.plt.imshow(img.permute(1, 2, 0));
print('output image shape:', out_img.shape)
d2l.plt.imshow(out_img);
</code></pre>
<p>완전 합성곱 네트워크에서, 우리는 [<strong>이중 선형 보간법의 업샘플링으로 전치 합성곱 레이어를 초기화합니다. $1\times 1$ 합성곱 레이어의 경우 Xavier 초기화를 사용합니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab mxnet
W = bilinear_kernel(num_classes, num_classes, 64)
net[-1].initialize(init.Constant(W))
net[-2].initialize(init=init.Xavier())
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
W = bilinear_kernel(num_classes, num_classes, 64)
net.transpose_conv.weight.data.copy_(W);
</code></pre>
<h2 id="데이터셋-읽기-reading-the-dataset-7"><a class="header" href="#데이터셋-읽기-reading-the-dataset-7">[<strong>데이터셋 읽기 (Reading the Dataset)</strong>]</a></h2>
<p>우리는 :numref:<code>sec_semantic_segmentation</code>에서 소개한
시맨틱 분할 데이터셋을 읽습니다.
무작위 자르기의 출력 이미지 모양은 $320\times 480$으로 지정됩니다. 높이와 너비 모두 32로 나누어떨어집니다.</p>
<pre><code class="language-{.python .input}">#@tab all
batch_size, crop_size = 32, (320, 480)
train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)
</code></pre>
<h2 id="훈련-training-26"><a class="header" href="#훈련-training-26">[<strong>훈련 (Training)</strong>]</a></h2>
<p>이제 우리가 구성한
완전 합성곱 네트워크를 훈련할 수 있습니다.
여기서의 손실 함수와 정확도 계산은
이전 장의 이미지 분류와 본질적으로 다르지 않습니다.
우리는 전치 합성곱 레이어의 출력 채널을 사용하여
각 픽셀에 대한 클래스를 예측하기 때문에,
손실 계산에서 채널 차원이 지정됩니다.
또한, 정확도는 모든 픽셀에 대해 예측된 클래스의 정확성을 기반으로 계산됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
num_epochs, lr, wd, devices = 5, 0.1, 1e-3, d2l.try_all_gpus()
loss = gluon.loss.SoftmaxCrossEntropyLoss(axis=1)
net.collect_params().reset_ctx(devices)
trainer = gluon.Trainer(net.collect_params(), 'sgd',
                        {'learning_rate': lr, 'wd': wd})
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def loss(inputs, targets):
    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)

num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()
trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<h2 id="예측-prediction-4"><a class="header" href="#예측-prediction-4">[<strong>예측 (Prediction)</strong>]</a></h2>
<p>예측할 때, 우리는 각 채널에서 입력 이미지를 표준화하고
이미지를 CNN에 필요한 4차원 입력 형식으로 변환해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def predict(img):
    X = test_iter._dataset.normalize_image(img)
    X = np.expand_dims(X.transpose(2, 0, 1), axis=0)
    pred = net(X.as_in_ctx(devices[0])).argmax(axis=1)
    return pred.reshape(pred.shape[1], pred.shape[2])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def predict(img):
    X = test_iter.dataset.normalize_image(img).unsqueeze(0)
    pred = net(X.to(devices[0])).argmax(dim=1)
    return pred.reshape(pred.shape[1], pred.shape[2])
</code></pre>
<p>각 픽셀의 [<strong>예측된 클래스를 시각화</strong>]하기 위해, 예측된 클래스를 데이터셋의 라벨 색상으로 다시 매핑합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def label2image(pred):
    colormap = np.array(d2l.VOC_COLORMAP, ctx=devices[0], dtype='uint8')
    X = pred.astype('int32')
    return colormap[X, :]
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def label2image(pred):
    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])
    X = pred.long()
    return colormap[X, :]
</code></pre>
<p>테스트 데이터셋의 이미지는 크기와 모양이 다양합니다.
모델은 스트라이드가 32인 전치 합성곱 레이어를 사용하므로,
입력 이미지의 높이나 너비가 32로 나누어떨어지지 않으면
전치 합성곱 레이어의 출력 높이나 너비가 입력 이미지의 모양에서 벗어날 것입니다.
이 문제를 해결하기 위해,
우리는 이미지에서 높이와 너비가 32의 정수 배수인 여러 직사각형 영역을 자르고,
이 영역의 픽셀에 대해 개별적으로 순방향 전파를 수행할 수 있습니다.
이러한 직사각형 영역의 합집합은 입력 이미지를 완전히 덮어야 합니다.
픽셀이 여러 직사각형 영역에 의해 덮일 때,
이 동일한 픽셀에 대한 별도 영역의 전치 합성곱 출력 평균을
소프트맥스 연산에 입력하여 클래스를 예측할 수 있습니다.</p>
<p>간단하게 하기 위해, 우리는 몇 개의 더 큰 테스트 이미지만 읽고,
이미지의 왼쪽 상단 모서리에서 시작하여 예측을 위해 $320\times 480$ 영역을 자릅니다.
이 테스트 이미지들에 대해, 우리는
자른 영역,
예측 결과,
그리고 실제(ground-truth)를 줄별로 인쇄합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
test_images, test_labels = d2l.read_voc_images(voc_dir, False)
n, imgs = 4, []
for i in range(n):
    crop_rect = (0, 0, 480, 320)
    X = image.fixed_crop(test_images[i], *crop_rect)
    pred = label2image(predict(X))
    imgs += [X, pred, image.fixed_crop(test_labels[i], *crop_rect)]
d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
test_images, test_labels = d2l.read_voc_images(voc_dir, False)
n, imgs = 4, []
for i in range(n):
    crop_rect = (0, 0, 320, 480)
    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)
    pred = label2image(predict(X))
    imgs += [X.permute(1,2,0), pred.cpu(),
             torchvision.transforms.functional.crop(
                 test_labels[i], *crop_rect).permute(1,2,0)]
d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);
</code></pre>
<h2 id="요약-summary-79"><a class="header" href="#요약-summary-79">요약 (Summary)</a></h2>
<ul>
<li>완전 합성곱 네트워크는 먼저 CNN을 사용하여 이미지 특징을 추출하고, 그 다음 $1\times 1$ 합성곱 레이어를 통해 채널 수를 클래스 수로 변환하며, 마지막으로 전치 합성곱을 통해 특징 맵의 높이와 너비를 입력 이미지의 높이와 너비로 변환합니다.</li>
<li>완전 합성곱 네트워크에서, 우리는 전치 합성곱 레이어를 초기화하기 위해 이중 선형 보간법의 업샘플링을 사용할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-94"><a class="header" href="#연습-문제-exercises-94">연습 문제 (Exercises)</a></h2>
<ol>
<li>실험에서 전치 합성곱 레이어에 Xavier 초기화를 사용하면 결과가 어떻게 변합니까?</li>
<li>하이퍼파라미터를 조정하여 모델의 정확도를 더 향상시킬 수 있습니까?</li>
<li>테스트 이미지의 모든 픽셀 클래스를 예측하십시오.</li>
<li>원래의 완전 합성곱 네트워크 논문은 또한 일부 중간 CNN 레이어의 출력을 사용합니다 :cite:<code>Long.Shelhamer.Darrell.2015</code>. 이 아이디어를 구현해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/377">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1582">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="신경-스타일-전송-neural-style-transfer"><a class="header" href="#신경-스타일-전송-neural-style-transfer">신경 스타일 전송 (Neural Style Transfer)</a></h1>
<p>사진 애호가라면 필터에 익숙할 것입니다.
필터는 사진의 색상 스타일을 변경하여
풍경 사진을 더 선명하게 만들거나
인물 사진의 피부를 하얗게 만들 수 있습니다.
그러나,
하나의 필터는 일반적으로 사진의 한 측면만 변경합니다.
사진에 이상적인 스타일을 적용하려면,
아마도 다양한 필터 조합을 시도해야 할 것입니다.
이 과정은 모델의 하이퍼파라미터를 튜닝하는 것만큼 복잡합니다.</p>
<p>이 섹션에서는
CNN의 계층별 표현을 활용하여 한 이미지의 스타일을
다른 이미지에 자동으로 적용하는 <em>스타일 전송(style transfer)</em> :cite:<code>Gatys.Ecker.Bethge.2016</code>을 소개합니다.
이 작업에는 두 개의 입력 이미지가 필요합니다:
하나는 *콘텐츠 이미지(content image)*이고
다른 하나는 *스타일 이미지(style image)*입니다.
우리는 신경망을 사용하여
콘텐츠 이미지를 수정하여 스타일 이미지의 스타일과 가깝게 만들 것입니다.
예를 들어,
:numref:<code>fig_style_transfer</code>의 콘텐츠 이미지는 시애틀 교외의 레이니어 산 국립공원에서 우리가 찍은 풍경 사진이고, 스타일 이미지는 가을 참나무를 주제로 한 유화입니다.
출력 합성 이미지에서는
스타일 이미지의 유화 붓터치가 적용되어 더 생생한 색상을 띠면서도
콘텐츠 이미지에 있는 객체의 주요 모양은 보존됩니다.</p>
<p><img src="chapter_computer-vision/../img/style-transfer.svg" alt="콘텐츠 및 스타일 이미지가 주어지면 스타일 전송은 합성 이미지를 출력합니다." />
:label:<code>fig_style_transfer</code></p>
<h2 id="방법-method"><a class="header" href="#방법-method">방법 (Method)</a></h2>
<p>:numref:<code>fig_style_transfer_model</code>은
CNN 기반 스타일 전송 방법을 단순화된 예제로 설명합니다.
먼저, 합성 이미지를 초기화합니다.
예를 들어 콘텐츠 이미지로 초기화할 수 있습니다.
이 합성 이미지는 스타일 전송 과정 중에 업데이트해야 할 유일한 변수입니다.
즉, 훈련 중에 업데이트할 모델 파라미터입니다.
그런 다음 사전 훈련된 CNN을 선택하여 이미지 특징을 추출하고
훈련 중에 모델 파라미터를 고정(동결)합니다.
이 심층 CNN은 여러 레이어를 사용하여
이미지에 대한 계층적 특징을 추출합니다.
우리는 이러한 레이어 중 일부의 출력을 콘텐츠 특징 또는 스타일 특징으로 선택할 수 있습니다.
:numref:<code>fig_style_transfer_model</code>을 예로 들어보겠습니다.
여기서 사전 훈련된 신경망에는 3개의 합성곱 레이어가 있으며,
두 번째 레이어는 콘텐츠 특징을 출력하고,
첫 번째와 세 번째 레이어는 스타일 특징을 출력합니다.</p>
<p><img src="chapter_computer-vision/../img/neural-style.svg" alt="CNN 기반 스타일 전송 프로세스. 실선은 순방향 전파 방향을 나타내고 점선은 역방향 전파를 나타냅니다." />
:label:<code>fig_style_transfer_model</code></p>
<p>다음으로, 순방향 전파(실선 화살표 방향)를 통해 스타일 전송의 손실 함수를 계산하고, 역전파(점선 화살표 방향)를 통해 모델 파라미터(출력을 위한 합성 이미지)를 업데이트합니다.
스타일 전송에서 일반적으로 사용되는 손실 함수는 세 부분으로 구성됩니다:
(i) *콘텐츠 손실(content loss)*은 합성 이미지와 콘텐츠 이미지를 콘텐츠 특징에서 가깝게 만듭니다;
(ii) *스타일 손실(style loss)*은 합성 이미지와 스타일 이미지를 스타일 특징에서 가깝게 만듭니다;
(iii) *총 변동 손실(total variation loss)*은 합성 이미지의 노이즈를 줄이는 데 도움이 됩니다.
마지막으로, 모델 훈련이 끝나면 스타일 전송의 모델 파라미터를 출력하여
최종 합성 이미지를 생성합니다.</p>
<p>다음에서,
우리는 구체적인 실험을 통해 스타일 전송의 기술적 세부 사항을 설명할 것입니다.</p>
<h2 id="콘텐츠-및-스타일-이미지-읽기"><a class="header" href="#콘텐츠-및-스타일-이미지-읽기">[<strong>콘텐츠 및 스타일 이미지 읽기</strong>]</a></h2>
<p>먼저, 콘텐츠 및 스타일 이미지를 읽습니다.
인쇄된 좌표 축에서
이 이미지들의 크기가 다르다는 것을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, image, init, np, npx
from mxnet.gluon import nn

npx.set_np()

d2l.set_figsize()
content_img = image.imread('../img/rainier.jpg')
d2l.plt.imshow(content_img.asnumpy());
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
import torchvision
from torch import nn

d2l.set_figsize()
content_img = d2l.Image.open('../img/rainier.jpg')
d2l.plt.imshow(content_img);
</code></pre>
<pre><code class="language-{.python .input}">#@tab mxnet
style_img = image.imread('../img/autumn-oak.jpg')
d2l.plt.imshow(style_img.asnumpy());
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
style_img = d2l.Image.open('../img/autumn-oak.jpg')
d2l.plt.imshow(style_img);
</code></pre>
<h2 id="전처리-및-후처리-preprocessing-and-postprocessing"><a class="header" href="#전처리-및-후처리-preprocessing-and-postprocessing">[<strong>전처리 및 후처리 (Preprocessing and Postprocessing)</strong>]</a></h2>
<p>아래에서는 이미지를 전처리하고 후처리하는 두 가지 함수를 정의합니다.
<code>preprocess</code> 함수는 입력 이미지의 세 RGB 채널 각각을 표준화하고 결과를 CNN 입력 형식으로 변환합니다.
<code>postprocess</code> 함수는 출력 이미지의 픽셀 값을 표준화 이전의 원래 값으로 복원합니다.
이미지 인쇄 함수는 각 픽셀이 0에서 1 사이의 부동 소수점 값을 가질 것을 요구하므로,
0보다 작거나 1보다 큰 값은 각각 0 또는 1로 대체합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
rgb_mean = np.array([0.485, 0.456, 0.406])
rgb_std = np.array([0.229, 0.224, 0.225])

def preprocess(img, image_shape):
    img = image.imresize(img, *image_shape)
    img = (img.astype('float32') / 255 - rgb_mean) / rgb_std
    return np.expand_dims(img.transpose(2, 0, 1), axis=0)

def postprocess(img):
    img = img[0].as_in_ctx(rgb_std.ctx)
    return (img.transpose(1, 2, 0) * rgb_std + rgb_mean).clip(0, 1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
rgb_mean = torch.tensor([0.485, 0.456, 0.406])
rgb_std = torch.tensor([0.229, 0.224, 0.225])

def preprocess(img, image_shape):
    transforms = torchvision.transforms.Compose([
        torchvision.transforms.Resize(image_shape),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])
    return transforms(img).unsqueeze(0)

def postprocess(img):
    img = img[0].to(rgb_std.device)
    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)
    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))
</code></pre>
<h2 id="특징-추출-extracting-features"><a class="header" href="#특징-추출-extracting-features">[<strong>특징 추출 (Extracting Features)</strong>]</a></h2>
<p>우리는 이미지 특징을 추출하기 위해 ImageNet 데이터셋에서 사전 훈련된 VGG-19 모델을 사용합니다 :cite:<code>Gatys.Ecker.Bethge.2016</code>.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
pretrained_net = gluon.model_zoo.vision.vgg19(pretrained=True)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
pretrained_net = torchvision.models.vgg19(pretrained=True)
</code></pre>
<p>이미지의 콘텐츠 특징과 스타일 특징을 추출하기 위해, VGG 네트워크에서 특정 레이어의 출력을 선택할 수 있습니다.
일반적으로 입력 레이어에 가까울수록 이미지의 세부 사항을 추출하기 쉽고, 반대로 출력 레이어에 가까울수록 이미지의 전역 정보를 추출하기 쉽습니다.
합성 이미지에서 콘텐츠 이미지의 세부 사항을 과도하게 유지하는 것을 피하기 위해,
우리는 출력에 더 가까운 VGG 레이어를 <em>콘텐츠 레이어</em>로 선택하여 이미지의 콘텐츠 특징을 출력합니다.
또한 로컬 및 전역 스타일 특징을 추출하기 위해 서로 다른 VGG 레이어의 출력을 선택합니다.
이러한 레이어를 <em>스타일 레이어</em>라고도 합니다.
:numref:<code>sec_vgg</code>에서 언급했듯이,
VGG 네트워크는 5개의 합성곱 블록을 사용합니다.
실험에서 우리는 네 번째 합성곱 블록의 마지막 합성곱 레이어를 콘텐츠 레이어로 선택하고, 각 합성곱 블록의 첫 번째 합성곱 레이어를 스타일 레이어로 선택합니다.
이 레이어들의 인덱스는 <code>pretrained_net</code> 인스턴스를 인쇄하여 얻을 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
style_layers, content_layers = [0, 5, 10, 19, 28], [25]
</code></pre>
<p>VGG 레이어를 사용하여 특징을 추출할 때,
입력 레이어부터 출력 레이어에 가장 가까운 콘텐츠 레이어 또는 스타일 레이어까지만 사용하면 됩니다.
특징 추출에 사용할 모든 VGG 레이어만 유지하는 새 네트워크 인스턴스 <code>net</code>을 구성해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = nn.Sequential()
for i in range(max(content_layers + style_layers) + 1):
    net.add(pretrained_net.features[i])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = nn.Sequential(*[pretrained_net.features[i] for i in
                      range(max(content_layers + style_layers) + 1)])
</code></pre>
<p>입력 <code>X</code>가 주어졌을 때, 단순히 순방향 전파 <code>net(X)</code>를 호출하면 마지막 레이어의 출력만 얻을 수 있습니다.
중간 레이어의 출력도 필요하므로,
레이어별 계산을 수행하고 콘텐츠 및 스타일 레이어 출력을 유지해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def extract_features(X, content_layers, style_layers):
    contents = []
    styles = []
    for i in range(len(net)):
        X = net[i](X)
        if i in style_layers:
            styles.append(X)
        if i in content_layers:
            contents.append(X)
    return contents, styles
</code></pre>
<p>아래에 두 함수가 정의되어 있습니다:
<code>get_contents</code> 함수는 콘텐츠 이미지에서 콘텐츠 특징을 추출하고,
<code>get_styles</code> 함수는 스타일 이미지에서 스타일 특징을 추출합니다.
훈련 중에는 사전 훈련된 VGG의 모델 파라미터를 업데이트할 필요가 없으므로,
훈련이 시작되기 전에도 콘텐츠와 스타일 특징을 추출할 수 있습니다.
합성 이미지는 스타일 전송을 위해 업데이트해야 할 모델 파라미터 세트이므로,
훈련 중에 <code>extract_features</code> 함수를 호출해야만 합성 이미지의 콘텐츠 및 스타일 특징을 추출할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def get_contents(image_shape, device):
    content_X = preprocess(content_img, image_shape).copyto(device)
    contents_Y, _ = extract_features(content_X, content_layers, style_layers)
    return content_X, contents_Y

def get_styles(image_shape, device):
    style_X = preprocess(style_img, image_shape).copyto(device)
    _, styles_Y = extract_features(style_X, content_layers, style_layers)
    return style_X, styles_Y
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def get_contents(image_shape, device):
    content_X = preprocess(content_img, image_shape).to(device)
    contents_Y, _ = extract_features(content_X, content_layers, style_layers)
    return content_X, contents_Y

def get_styles(image_shape, device):
    style_X = preprocess(style_img, image_shape).to(device)
    _, styles_Y = extract_features(style_X, content_layers, style_layers)
    return style_X, styles_Y
</code></pre>
<h2 id="손실-함수-정의-defining-the-loss-function"><a class="header" href="#손실-함수-정의-defining-the-loss-function">[<strong>손실 함수 정의 (Defining the Loss Function)</strong>]</a></h2>
<p>이제 스타일 전송을 위한 손실 함수를 설명하겠습니다. 손실 함수는 콘텐츠 손실, 스타일 손실, 총 변동 손실로 구성됩니다.</p>
<h3 id="콘텐츠-손실-content-loss"><a class="header" href="#콘텐츠-손실-content-loss">콘텐츠 손실 (Content Loss)</a></h3>
<p>선형 회귀의 손실 함수와 유사하게,
콘텐츠 손실은 제곱 오차 함수를 통해
합성 이미지와 콘텐츠 이미지 간의 콘텐츠 특징 차이를 측정합니다.
제곱 오차 함수의 두 입력은
모두 <code>extract_features</code> 함수로 계산된 콘텐츠 레이어의 출력입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def content_loss(Y_hat, Y):
    return np.square(Y_hat - Y).mean()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def content_loss(Y_hat, Y):
    # 기울기를 동적으로 계산하는 데 사용되는 트리에서 타겟 콘텐츠를 분리합니다:
    # 이것은 변수가 아니라 명시된 값입니다. 그렇지 않으면 손실에서 오류가 발생합니다.
    return torch.square(Y_hat - Y.detach()).mean()
</code></pre>
<h3 id="스타일-손실-style-loss"><a class="header" href="#스타일-손실-style-loss">스타일 손실 (Style Loss)</a></h3>
<p>스타일 손실은 콘텐츠 손실과 유사하게,
제곱 오차 함수를 사용하여 합성 이미지와 스타일 이미지 간의 스타일 차이를 측정합니다.
어떤 스타일 레이어의 스타일 출력을 표현하기 위해,
먼저 <code>extract_features</code> 함수를 사용하여 스타일 레이어 출력을 계산합니다.
출력이 1개의 예제, $c$ 채널, 높이 $h$, 너비 $w$를 갖는다고 가정하면,
이 출력을 $c$ 행과 $hw$ 열을 가진 행렬 $\mathbf{X}$로 변환할 수 있습니다.
이 행렬은 각각 길이가 $hw$인 $c$개의 벡터 $\mathbf{x}_1, \ldots, \mathbf{x}_c$의 연결로 생각할 수 있습니다.
여기서 벡터 $\mathbf{x}_i$는 채널 $i$의 스타일 특징을 나타냅니다.</p>
<p>이 벡터들의 <em>그램 행렬(Gram matrix)</em> $\mathbf{X}\mathbf{X}^\top \in \mathbb{R}^{c \times c}$에서, 행 $i$와 열 $j$의 요소 $x_{ij}$는 벡터 $\mathbf{x}_i$와 $\mathbf{x}_j$의 내적입니다.
그것은 채널 $i$와 $j$의 스타일 특징의 상관 관계를 나타냅니다.
우리는 이 그램 행렬을 사용하여 모든 스타일 레이어의 스타일 출력을 나타냅니다.
$hw$ 값이 클수록 그램 행렬에서 더 큰 값으로 이어질 가능성이 높다는 점에 유의하십시오.
또한 그램 행렬의 높이와 너비는 모두 채널 수 $c$입니다.
스타일 손실이 이러한 값에 영향을 받지 않도록 하기 위해,
아래의 <code>gram</code> 함수는 그램 행렬을 요소 수, 즉 $chw$로 나눕니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def gram(X):
    num_channels, n = X.shape[1], d2l.size(X) // X.shape[1]
    X = d2l.reshape(X, (num_channels, n))
    return d2l.matmul(X, X.T) / (num_channels * n)
</code></pre>
<p>분명히,
스타일 손실에 대한 제곱 오차 함수의 두 그램 행렬 입력은
합성 이미지와 스타일 이미지에 대한 스타일 레이어 출력을 기반으로 합니다.
여기서는 스타일 이미지를 기반으로 한 그램 행렬 <code>gram_Y</code>가 미리 계산되어 있다고 가정합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def style_loss(Y_hat, gram_Y):
    return np.square(gram(Y_hat) - gram_Y).mean()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def style_loss(Y_hat, gram_Y):
    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()
</code></pre>
<h3 id="총-변동-손실-total-variation-loss"><a class="header" href="#총-변동-손실-total-variation-loss">총 변동 손실 (Total Variation Loss)</a></h3>
<p>때때로 학습된 합성 이미지에는 고주파 노이즈,
즉 특히 밝거나 어두운 픽셀이 많이 포함됩니다.
일반적인 노이즈 감소 방법 중 하나는 *총 변동 노이즈 제거(total variation denoising)*입니다.
좌표 $(i, j)$의 픽셀 값을 $x_{i, j}$라고 합시다.
총 변동 손실을 줄이면</p>
<p>$$\sum_{i, j} \left|x_{i, j} - x_{i+1, j}\right| + \left|x_{i, j} - x_{i, j+1}\right|$$</p>
<p>합성 이미지의 인접 픽셀 값이 더 가까워집니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def tv_loss(Y_hat):
    return 0.5 * (d2l.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +
                  d2l.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())
</code></pre>
<h3 id="손실-함수-loss-function-2"><a class="header" href="#손실-함수-loss-function-2">손실 함수 (Loss Function)</a></h3>
<p>[<strong>스타일 전송의 손실 함수는 콘텐츠 손실, 스타일 손실 및 총 변동 손실의 가중 합입니다</strong>].
이러한 가중치 하이퍼파라미터를 조정하여,
합성 이미지의 콘텐츠 보존, 스타일 전송, 노이즈 감소 간의 균형을 맞출 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
content_weight, style_weight, tv_weight = 1, 1e4, 10

def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):
    # 콘텐츠, 스타일 및 총 변동 손실을 각각 계산합니다
    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(
        contents_Y_hat, contents_Y)]
    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(
        styles_Y_hat, styles_Y_gram)]
    tv_l = tv_loss(X) * tv_weight
    # 모든 손실을 더합니다
    l = sum(styles_l + contents_l + [tv_l])
    return contents_l, styles_l, tv_l, l
</code></pre>
<h2 id="합성-이미지-초기화-initializing-the-synthesized-image"><a class="header" href="#합성-이미지-초기화-initializing-the-synthesized-image">[<strong>합성 이미지 초기화 (Initializing the Synthesized Image)</strong>]</a></h2>
<p>스타일 전송에서,
합성 이미지는 훈련 중에 업데이트해야 할 유일한 변수입니다.
따라서 간단한 모델 <code>SynthesizedImage</code>를 정의하고 합성 이미지를 모델 파라미터로 취급할 수 있습니다.
이 모델에서 순방향 전파는 모델 파라미터를 반환하기만 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class SynthesizedImage(nn.Block):
    def __init__(self, img_shape, **kwargs):
        super(SynthesizedImage, self).__init__(**kwargs)
        self.weight = self.params.get('weight', shape=img_shape)

    def forward(self):
        return self.weight.data()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class SynthesizedImage(nn.Module):
    def __init__(self, img_shape, **kwargs):
        super(SynthesizedImage, self).__init__(**kwargs)
        self.weight = nn.Parameter(torch.rand(*img_shape))

    def forward(self):
        return self.weight
</code></pre>
<p>다음으로 <code>get_inits</code> 함수를 정의합니다.
이 함수는 합성 이미지 모델 인스턴스를 생성하고 이미지 <code>X</code>로 초기화합니다.
다양한 스타일 레이어에서의 스타일 이미지에 대한 그램 행렬 <code>styles_Y_gram</code>은 훈련 전에 계산됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def get_inits(X, device, lr, styles_Y):
    gen_img = SynthesizedImage(X.shape)
    gen_img.initialize(init.Constant(X), ctx=device, force_reinit=True)
    trainer = gluon.Trainer(gen_img.collect_params(), 'adam',
                            {'learning_rate': lr})
    styles_Y_gram = [gram(Y) for Y in styles_Y]
    return gen_img(), styles_Y_gram, trainer
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def get_inits(X, device, lr, styles_Y):
    gen_img = SynthesizedImage(X.shape).to(device)
    gen_img.weight.data.copy_(X.data)
    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)
    styles_Y_gram = [gram(Y) for Y in styles_Y]
    return gen_img(), styles_Y_gram, trainer
</code></pre>
<h2 id="훈련-training-27"><a class="header" href="#훈련-training-27">[<strong>훈련 (Training)</strong>]</a></h2>
<p>스타일 전송을 위해 모델을 훈련할 때,
우리는 합성 이미지의 콘텐츠 특징과 스타일 특징을 지속적으로 추출하고 손실 함수를 계산합니다.
아래는 훈련 루프를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):
    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[10, num_epochs], ylim=[0, 20],
                            legend=['content', 'style', 'TV'],
                            ncols=2, figsize=(7, 2.5))
    for epoch in range(num_epochs):
        with autograd.record():
            contents_Y_hat, styles_Y_hat = extract_features(
                X, content_layers, style_layers)
            contents_l, styles_l, tv_l, l = compute_loss(
                X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)
        l.backward()
        trainer.step(1)
        if (epoch + 1) % lr_decay_epoch == 0:
            trainer.set_learning_rate(trainer.learning_rate * 0.8)
        if (epoch + 1) % 10 == 0:
            animator.axes[1].imshow(postprocess(X).asnumpy())
            animator.add(epoch + 1, [float(sum(contents_l)),
                                     float(sum(styles_l)), float(tv_l)])
    return X
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):
    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[10, num_epochs],
                            legend=['content', 'style', 'TV'],
                            ncols=2, figsize=(7, 2.5))
    for epoch in range(num_epochs):
        trainer.zero_grad()
        contents_Y_hat, styles_Y_hat = extract_features(
            X, content_layers, style_layers)
        contents_l, styles_l, tv_l, l = compute_loss(
            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)
        l.backward()
        trainer.step()
        scheduler.step()
        if (epoch + 1) % 10 == 0:
            animator.axes[1].imshow(postprocess(X))
            animator.add(epoch + 1, [float(sum(contents_l)),
                                     float(sum(styles_l)), float(tv_l)])
    return X
</code></pre>
<p>이제 [<strong>모델 훈련을 시작합니다</strong>].
우리는 콘텐츠 및 스타일 이미지의 높이와 너비를 300 x 450 픽셀로 재조정합니다.
콘텐츠 이미지를 사용하여 합성 이미지를 초기화합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
device, image_shape = d2l.try_gpu(), (450, 300)
net.collect_params().reset_ctx(device)
content_X, contents_Y = get_contents(image_shape, device)
_, styles_Y = get_styles(image_shape, device)
output = train(content_X, contents_Y, styles_Y, device, 0.9, 500, 50)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
device, image_shape = d2l.try_gpu(), (300, 450)  # PIL 이미지 (h, w)
net = net.to(device)
content_X, contents_Y = get_contents(image_shape, device)
_, styles_Y = get_styles(image_shape, device)
output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)
</code></pre>
<p>우리는 합성 이미지가 콘텐츠 이미지의 풍경과 객체를 유지하면서
동시에 스타일 이미지의 색상을 전송한다는 것을 볼 수 있습니다.
예를 들어,
합성 이미지에는 스타일 이미지와 같은 색상 블록이 있습니다.
이 블록 중 일부에는 붓터치의 미묘한 질감도 있습니다.</p>
<h2 id="요약-summary-80"><a class="header" href="#요약-summary-80">요약 (Summary)</a></h2>
<ul>
<li>스타일 전송에 일반적으로 사용되는 손실 함수는 세 부분으로 구성됩니다: (i) 콘텐츠 손실은 합성 이미지와 콘텐츠 이미지를 콘텐츠 특징에서 가깝게 만듭니다; (ii) 스타일 손실은 합성 이미지와 스타일 이미지를 스타일 특징에서 가깝게 만듭니다; (iii) 총 변동 손실은 합성 이미지의 노이즈를 줄이는 데 도움이 됩니다.</li>
<li>우리는 사전 훈련된 CNN을 사용하여 이미지 특징을 추출하고 손실 함수를 최소화하여 훈련 중에 합성 이미지를 모델 파라미터로 지속적으로 업데이트할 수 있습니다.</li>
<li>우리는 스타일 레이어의 스타일 출력을 나타내기 위해 그램 행렬을 사용합니다.</li>
</ul>
<h2 id="연습-문제-exercises-95"><a class="header" href="#연습-문제-exercises-95">연습 문제 (Exercises)</a></h2>
<ol>
<li>다른 콘텐츠 및 스타일 레이어를 선택하면 출력이 어떻게 변합니까?</li>
<li>손실 함수의 가중치 하이퍼파라미터를 조정하십시오. 출력이 더 많은 콘텐츠를 유지합니까 아니면 노이즈가 적습니까?</li>
<li>다른 콘텐츠 및 스타일 이미지를 사용하십시오. 더 흥미로운 합성 이미지를 만들 수 있습니까?</li>
<li>텍스트에 스타일 전송을 적용할 수 있습니까? 힌트: :citet:<code>10.1145/3544903.3544906</code>의 설문 조사 논문을 참조할 수 있습니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/378">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1476">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kaggle의-이미지-분류-cifar-10"><a class="header" href="#kaggle의-이미지-분류-cifar-10">Kaggle의 이미지 분류 (CIFAR-10)</a></h1>
<p>:label:<code>sec_kaggle_cifar10</code></p>
<p>지금까지 우리는 딥러닝 프레임워크의 고수준 API를 사용하여 텐서 형식의 이미지 데이터셋을 직접 얻었습니다.
그러나 사용자 정의 이미지 데이터셋은
종종 이미지 파일 형태로 제공됩니다.
이 섹션에서는 원시 이미지 파일에서 시작하여
단계별로 정리하고, 읽고, 텐서 형식으로 변환합니다.</p>
<p>우리는 :numref:<code>sec_image_augmentation</code>에서 CIFAR-10 데이터셋을 실험했는데,
이는 컴퓨터 비전에서 중요한 데이터셋입니다.
이 섹션에서는
이전 섹션에서 배운 지식을 적용하여
CIFAR-10 이미지 분류의 Kaggle 대회를 연습할 것입니다.
(<strong>대회의 웹 주소는 https://www.kaggle.com/c/cifar-10입니다</strong>)</p>
<p>:numref:<code>fig_kaggle_cifar10</code>은 대회 웹페이지의 정보를 보여줍니다.
결과를 제출하려면 Kaggle 계정을 등록해야 합니다.</p>
<p><img src="chapter_computer-vision/../img/kaggle-cifar10.png" alt="CIFAR-10 이미지 분류 대회 웹페이지 정보. &quot;Data&quot; 탭을 클릭하여 대회 데이터셋을 얻을 수 있습니다." />
:width:<code>600px</code>
:label:<code>fig_kaggle_cifar10</code></p>
<pre><code class="language-{.python .input}">#@tab mxnet
import collections
from d2l import mxnet as d2l
import math
from mxnet import gluon, init, npx
from mxnet.gluon import nn
import os
import pandas as pd
import shutil

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
import collections
from d2l import torch as d2l
import math
import torch
import torchvision
from torch import nn
import os
import pandas as pd
import shutil
</code></pre>
<h2 id="데이터셋-획득-및-정리-obtaining-and-organizing-the-dataset"><a class="header" href="#데이터셋-획득-및-정리-obtaining-and-organizing-the-dataset">데이터셋 획득 및 정리 (Obtaining and Organizing the Dataset)</a></h2>
<p>대회 데이터셋은 훈련 세트와 테스트 세트로 나뉘며,
각각 50,000개와 300,000개의 이미지를 포함합니다.
테스트 세트에서,
10,000개 이미지가 평가에 사용되며,
나머지 290,000개 이미지는 평가되지 않습니다:
이들은 단지 <em>수동으로</em> 라벨링된 테스트 세트 결과로 부정행위를 하기 어렵게 만들기 위해 포함되었습니다.
이 데이터셋의 이미지는
모두 png 컬러(RGB 채널) 이미지 파일이며,
높이와 너비는 모두 32 픽셀입니다.
이미지는 비행기, 자동차, 새, 고양이, 사슴, 개, 개구리, 말, 배, 트럭의 총 10개 범주를 다룹니다.
:numref:<code>fig_kaggle_cifar10</code>의 왼쪽 상단 모서리에는 데이터셋의 비행기, 자동차, 새 이미지 일부가 표시되어 있습니다.</p>
<h3 id="데이터셋-다운로드-downloading-the-dataset"><a class="header" href="#데이터셋-다운로드-downloading-the-dataset">데이터셋 다운로드 (Downloading the Dataset)</a></h3>
<p>Kaggle에 로그인한 후, :numref:<code>fig_kaggle_cifar10</code>에 표시된 CIFAR-10 이미지 분류 대회 웹페이지에서 "Data" 탭을 클릭하고 "Download All" 버튼을 클릭하여 데이터셋을 다운로드할 수 있습니다.
다운로드한 파일을 <code>../data</code>에 압축 해제하고 그 안의 <code>train.7z</code>와 <code>test.7z</code> 압축을 해제하면 다음 경로에서 전체 데이터셋을 찾을 수 있습니다.</p>
<ul>
<li><code>../data/cifar-10/train/[1-50000].png</code></li>
<li><code>../data/cifar-10/test/[1-300000].png</code></li>
<li><code>../data/cifar-10/trainLabels.csv</code></li>
<li><code>../data/cifar-10/sampleSubmission.csv</code></li>
</ul>
<p>여기서 <code>train</code> 및 <code>test</code> 디렉터리에는 각각 훈련 및 테스트 이미지가 포함되어 있고, <code>trainLabels.csv</code>는 훈련 이미지에 대한 라벨을 제공하며, <code>sample_submission.csv</code>는 샘플 제출 파일입니다.</p>
<p>더 쉽게 시작할 수 있도록, [<strong>처음 1000개의 훈련 이미지와 5개의 무작위 테스트 이미지가 포함된 소규모 샘플 데이터셋을 제공합니다.</strong>]
Kaggle 대회의 전체 데이터셋을 사용하려면 다음 <code>demo</code> 변수를 <code>False</code>로 설정해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
                                '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')

# Kaggle 대회용으로 다운로드한 전체 데이터셋을 사용하는 경우 `demo`를 False로 설정하십시오
demo = True

if demo:
    data_dir = d2l.download_extract('cifar10_tiny')
else:
    data_dir = '../data/cifar-10/'
</code></pre>
<h3 id="데이터셋-정리-organizing-the-dataset"><a class="header" href="#데이터셋-정리-organizing-the-dataset">[<strong>데이터셋 정리 (Organizing the Dataset)</strong>]</a></h3>
<p>모델 훈련 및 테스트를 용이하게 하기 위해 데이터셋을 정리해야 합니다.
먼저 csv 파일에서 라벨을 읽어봅시다.
다음 함수는 파일 이름의 확장자가 없는 부분을 라벨에 매핑하는 사전을 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def read_csv_labels(fname):
    """`fname`을 읽어 파일 이름 대 라벨 사전을 반환합니다."""
    with open(fname, 'r') as f:
        # 파일 헤더 라인(열 이름) 건너뛰기
        lines = f.readlines()[1:]
    tokens = [l.rstrip().split(',') for l in lines]
    return dict(((name, label) for name, label in tokens))

labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
print('# 훈련 예제:', len(labels))
print('# 클래스:', len(set(labels.values())))
</code></pre>
<p>다음으로, [<strong>원래 훈련 세트에서 검증 세트를 분리</strong>]하는 <code>reorg_train_valid</code> 함수를 정의합니다.
이 함수의 <code>valid_ratio</code> 인수는 원래 훈련 세트의 예제 수에 대한 검증 세트의 예제 수 비율입니다.
더 구체적으로,
$n$을 예제가 가장 적은 클래스의 이미지 수라고 하고 $r$을 비율이라고 합시다.
검증 세트는 각 클래스에 대해 $\max(\lfloor nr\rfloor,1)$개의 이미지를 분리합니다.
<code>valid_ratio=0.1</code>을 예로 들어보겠습니다. 원래 훈련 세트에 50,000개의 이미지가 있으므로,
<code>train_valid_test/train</code> 경로에 45,000개의 이미지가 훈련에 사용되고,
나머지 5,000개의 이미지는 <code>train_valid_test/valid</code> 경로에 검증 세트로 분리됩니다. 데이터셋을 정리한 후, 동일한 클래스의 이미지는 동일한 폴더 아래에 배치됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def copyfile(filename, target_dir):
    """파일을 대상 디렉터리로 복사합니다."""
    os.makedirs(target_dir, exist_ok=True)
    shutil.copy(filename, target_dir)

#@save
def reorg_train_valid(data_dir, labels, valid_ratio):
    """원래 훈련 세트에서 검증 세트를 분리합니다."""
    # 훈련 데이터셋에서 예제 수가 가장 적은 클래스의 예제 수
    n = collections.Counter(labels.values()).most_common()[-1][1]
    # 검증 세트의 클래스당 예제 수
    n_valid_per_label = max(1, math.floor(n * valid_ratio))
    label_count = {}
    for train_file in os.listdir(os.path.join(data_dir, 'train')):
        label = labels[train_file.split('.')[0]]
        fname = os.path.join(data_dir, 'train', train_file)
        copyfile(fname, os.path.join(data_dir, 'train_valid_test',
                                     'train_valid', label))
        if label not in label_count or label_count[label] &lt; n_valid_per_label:
            copyfile(fname, os.path.join(data_dir, 'train_valid_test',
                                         'valid', label))
            label_count[label] = label_count.get(label, 0) + 1
        else:
            copyfile(fname, os.path.join(data_dir, 'train_valid_test',
                                         'train', label))
    return n_valid_per_label
</code></pre>
<p>아래의 <code>reorg_test</code> 함수는 [<strong>예측 중 데이터 로딩을 위해 테스트 세트를 정리합니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def reorg_test(data_dir):
    """예측 중 데이터 로딩을 위해 테스트 세트를 정리합니다."""
    for test_file in os.listdir(os.path.join(data_dir, 'test')):
        copyfile(os.path.join(data_dir, 'test', test_file),
                 os.path.join(data_dir, 'train_valid_test', 'test',
                              'unknown'))
</code></pre>
<p>마지막으로, 함수를 사용하여 (<strong>위에서 정의한</strong>) <code>read_csv_labels</code>, <code>reorg_train_valid</code>, <code>reorg_test</code> [<strong>함수를 호출합니다.</strong>]</p>
<pre><code class="language-{.python .input}">#@tab all
def reorg_cifar10_data(data_dir, valid_ratio):
    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
    reorg_train_valid(data_dir, labels, valid_ratio)
    reorg_test(data_dir)
</code></pre>
<p>여기서는 데이터셋의 소규모 샘플에 대해 배치 크기를 32로만 설정합니다.
Kaggle 대회의 전체 데이터셋을 훈련하고 테스트할 때,
<code>batch_size</code>는 128과 같이 더 큰 정수로 설정해야 합니다.
하이퍼파라미터 튜닝을 위해 훈련 예제의 10%를 검증 세트로 분리합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
batch_size = 32 if demo else 128
valid_ratio = 0.1
reorg_cifar10_data(data_dir, valid_ratio)
</code></pre>
<h2 id="이미지-증강-image-augmentation-1"><a class="header" href="#이미지-증강-image-augmentation-1">[<strong>이미지 증강 (Image Augmentation)</strong>]</a></h2>
<p>우리는 과대적합을 해결하기 위해 이미지 증강을 사용합니다.
예를 들어, 훈련 중에 이미지를 무작위로 수평 뒤집을 수 있습니다.
또한 컬러 이미지의 세 가지 RGB 채널에 대해 표준화를 수행할 수 있습니다. 아래에는 조정할 수 있는 몇 가지 작업이 나열되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
transform_train = gluon.data.vision.transforms.Compose([
    # 이미지를 높이와 너비 모두 40 픽셀의 정사각형으로 확대
    gluon.data.vision.transforms.Resize(40),
    # 높이와 너비 모두 40 픽셀인 정사각형 이미지를 무작위로 자르기하여
    # 원본 이미지 면적의 0.64에서 1배인 작은 정사각형을 생성한 다음,
    # 높이와 너비 모두 32 픽셀인 정사각형으로 축소
    gluon.data.vision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),
                                                   ratio=(1.0, 1.0)),
    gluon.data.vision.transforms.RandomFlipLeftRight(),
    gluon.data.vision.transforms.ToTensor(),
    # 이미지의 각 채널 표준화
    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                           [0.2023, 0.1994, 0.2010])])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
transform_train = torchvision.transforms.Compose([
    # 이미지를 높이와 너비 모두 40 픽셀의 정사각형으로 확대
    torchvision.transforms.Resize(40),
    # 높이와 너비 모두 40 픽셀인 정사각형 이미지를 무작위로 자르기하여
    # 원본 이미지 면적의 0.64에서 1배인 작은 정사각형을 생성한 다음,
    # 높이와 너비 모두 32 픽셀인 정사각형으로 축소
    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),
                                                   ratio=(1.0, 1.0)),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    # 이미지의 각 채널 표준화
    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                     [0.2023, 0.1994, 0.2010])])
</code></pre>
<p>테스트 중에는
평가 결과의 무작위성을 제거하기 위해
이미지에 대해 표준화만 수행합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
transform_test = gluon.data.vision.transforms.Compose([
    gluon.data.vision.transforms.ToTensor(),
    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                           [0.2023, 0.1994, 0.2010])])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
transform_test = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                     [0.2023, 0.1994, 0.2010])])
</code></pre>
<h2 id="데이터셋-읽기-reading-the-dataset-8"><a class="header" href="#데이터셋-읽기-reading-the-dataset-8">데이터셋 읽기 (Reading the Dataset)</a></h2>
<p>다음으로, [<strong>원시 이미지 파일로 구성된 정리된 데이터셋을 읽습니다</strong>]. 각 예제에는 이미지와 라벨이 포함됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train_ds, valid_ds, train_valid_ds, test_ds = [
    gluon.data.vision.ImageFolderDataset(
        os.path.join(data_dir, 'train_valid_test', folder))
    for folder in ['train', 'valid', 'train_valid', 'test']]
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(
    os.path.join(data_dir, 'train_valid_test', folder),
    transform=transform_train) for folder in ['train', 'train_valid']]

valid_ds, test_ds = [torchvision.datasets.ImageFolder(
    os.path.join(data_dir, 'train_valid_test', folder),
    transform=transform_test) for folder in ['valid', 'test']]
</code></pre>
<p>훈련 중에,
우리는 [<strong>위에서 정의한 모든 이미지 증강 작업을 지정</strong>]해야 합니다.
하이퍼파라미터 튜닝 중 모델 평가에 검증 세트가 사용될 때는
이미지 증강으로 인한 무작위성을 도입해서는 안 됩니다.
최종 예측 전에, 라벨이 지정된 모든 데이터를 최대한 활용하기 위해 훈련 세트와 검증 세트를 합친 데이터로 모델을 훈련합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train_iter, train_valid_iter = [gluon.data.DataLoader(
    dataset.transform_first(transform_train), batch_size, shuffle=True,
    last_batch='discard') for dataset in (train_ds, train_valid_ds)]

valid_iter = gluon.data.DataLoader(
    valid_ds.transform_first(transform_test), batch_size, shuffle=False,
    last_batch='discard')

test_iter = gluon.data.DataLoader(
    test_ds.transform_first(transform_test), batch_size, shuffle=False,
    last_batch='keep')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train_iter, train_valid_iter = [torch.utils.data.DataLoader(
    dataset, batch_size, shuffle=True, drop_last=True)
    for dataset in (train_ds, train_valid_ds)]

valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,
                                         drop_last=True)

test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,
                                        drop_last=False)
</code></pre>
<h2 id="모델-정의-defining-the-model"><a class="header" href="#모델-정의-defining-the-model">[<strong>모델</strong>] 정의 (Defining the Model)</a></h2>
<p>:begin_tab:<code>mxnet</code>
여기서는 <code>HybridBlock</code> 클래스를 기반으로 잔차 블록을 구축하는데, 이는
:numref:<code>sec_resnet</code>에서 설명한 구현과 약간 다릅니다.
이는 계산 효율성을 향상시키기 위함입니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class Residual(nn.HybridBlock):
    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
        super(Residual, self).__init__(**kwargs)
        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,
                               strides=strides)
        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,
                                   strides=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm()
        self.bn2 = nn.BatchNorm()

    def hybrid_forward(self, F, X):
        Y = F.npx.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        return F.npx.relu(Y + X)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
다음으로 ResNet-18 모델을 정의합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def resnet18(num_classes):
    net = nn.HybridSequential()
    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),
            nn.BatchNorm(), nn.Activation('relu'))

    def resnet_block(num_channels, num_residuals, first_block=False):
        blk = nn.HybridSequential()
        for i in range(num_residuals):
            if i == 0 and not first_block:
                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
            else:
                blk.add(Residual(num_channels))
        return blk

    net.add(resnet_block(64, 2, first_block=True),
            resnet_block(128, 2),
            resnet_block(256, 2),
            resnet_block(512, 2))
    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))
    return net
</code></pre>
<p>:begin_tab:<code>mxnet</code>
훈련이 시작되기 전에 :numref:<code>subsec_xavier</code>에 설명된 Xavier 초기화를 사용합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
:numref:<code>sec_resnet</code>에 설명된 ResNet-18 모델을 정의합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def get_net(devices):
    num_classes = 10
    net = resnet18(num_classes)
    net.initialize(ctx=devices, init=init.Xavier())
    return net

loss = gluon.loss.SoftmaxCrossEntropyLoss()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def get_net():
    num_classes = 10
    net = d2l.resnet18(num_classes, 3)
    return net

loss = nn.CrossEntropyLoss(reduction="none")
</code></pre>
<h2 id="훈련-함수-정의-defining-the-training-function"><a class="header" href="#훈련-함수-정의-defining-the-training-function">[<strong>훈련 함수</strong>] 정의 (Defining the Training Function)</a></h2>
<p>검증 세트에서의 모델 성능에 따라 모델을 선택하고 하이퍼파라미터를 조정할 것입니다.
다음에서 모델 훈련 함수 <code>train</code>을 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
          lr_decay):
    trainer = gluon.Trainer(net.collect_params(), 'sgd',
                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})
    num_batches, timer = len(train_iter), d2l.Timer()
    legend = ['train loss', 'train acc']
    if valid_iter is not None:
        legend.append('valid acc')
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=legend)
    for epoch in range(num_epochs):
        metric = d2l.Accumulator(3)
        if epoch &gt; 0 and epoch % lr_period == 0:
            trainer.set_learning_rate(trainer.learning_rate * lr_decay)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = d2l.train_batch_ch13(
                net, features, labels.astype('float32'), loss, trainer,
                devices, d2l.split_batch)
            metric.add(l, acc, labels.shape[0])
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[2],
                              None))
        if valid_iter is not None:
            valid_acc = d2l.evaluate_accuracy_gpus(net, valid_iter,
                                                   d2l.split_batch)
            animator.add(epoch + 1, (None, None, valid_acc))
    measures = (f'train loss {metric[0] / metric[2]:.3f}, '
                f'train acc {metric[1] / metric[2]:.3f}')
    if valid_iter is not None:
        measures += f', valid acc {valid_acc:.3f}'
    print(measures + f'\n{metric[2] * num_epochs / timer.sum():.1f}'
          f' examples/sec on {str(devices)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
          lr_decay):
    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,
                              weight_decay=wd)
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)
    num_batches, timer = len(train_iter), d2l.Timer()
    legend = ['train loss', 'train acc']
    if valid_iter is not None:
        legend.append('valid acc')
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=legend)
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    for epoch in range(num_epochs):
        net.train()
        metric = d2l.Accumulator(3)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = d2l.train_batch_ch13(net, features, labels,
                                          loss, trainer, devices)
            metric.add(l, acc, labels.shape[0])
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[2],
                              None))
        if valid_iter is not None:
            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)
            animator.add(epoch + 1, (None, None, valid_acc))
        scheduler.step()
    measures = (f'train loss {metric[0] / metric[2]:.3f}, '
                f'train acc {metric[1] / metric[2]:.3f}')
    if valid_iter is not None:
        measures += f', valid acc {valid_acc:.3f}'
    print(measures + f'\n{metric[2] * num_epochs / timer.sum():.1f}'
          f' examples/sec on {str(devices)}')
</code></pre>
<h2 id="모델-훈련-및-검증-training-and-validating-the-model"><a class="header" href="#모델-훈련-및-검증-training-and-validating-the-model">[<strong>모델 훈련 및 검증 (Training and Validating the Model)</strong>]</a></h2>
<p>이제 모델을 훈련하고 검증할 수 있습니다.
다음의 모든 하이퍼파라미터는 조정 가능합니다.
예를 들어 에포크 수를 늘릴 수 있습니다.
<code>lr_period</code>와 <code>lr_decay</code>가 각각 4와 0.9로 설정되면 최적화 알고리즘의 학습률은 4 에포크마다 0.9배가 됩니다. 시연의 편의를 위해,
여기서는 20 에포크만 훈련합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 0.02, 5e-4
lr_period, lr_decay, net = 4, 0.9, get_net(devices)
net.hybridize()
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
      lr_decay)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4
lr_period, lr_decay, net = 4, 0.9, get_net()
net(next(iter(train_iter))[0])
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
      lr_decay)
</code></pre>
<h2 id="테스트-세트-분류-classifying-the-testing-set-및-kaggle에-결과-제출"><a class="header" href="#테스트-세트-분류-classifying-the-testing-set-및-kaggle에-결과-제출">[<strong>테스트 세트 분류 (Classifying the Testing Set)</strong>] 및 Kaggle에 결과 제출</a></h2>
<p>유망한 모델과 하이퍼파라미터를 얻은 후,
모든 라벨이 지정된 데이터(검증 세트 포함)를 사용하여 모델을 다시 훈련하고 테스트 세트를 분류합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net, preds = get_net(devices), []
net.hybridize()
train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
      lr_decay)

for X, _ in test_iter:
    y_hat = net(X.as_in_ctx(devices[0]))
    preds.extend(y_hat.argmax(axis=1).astype(int).asnumpy())
sorted_ids = list(range(1, len(test_ds) + 1))
sorted_ids.sort(key=lambda x: str(x))
df = pd.DataFrame({'id': sorted_ids, 'label': preds})
df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])
df.to_csv('submission.csv', index=False)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net, preds = get_net(), []
net(next(iter(train_valid_iter))[0])
train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
      lr_decay)

for X, _ in test_iter:
    y_hat = net(X.to(devices[0]))
    preds.extend(y_hat.argmax(dim=1).type(torch.int32).cpu().numpy())
sorted_ids = list(range(1, len(test_ds) + 1))
sorted_ids.sort(key=lambda x: str(x))
df = pd.DataFrame({'id': sorted_ids, 'label': preds})
df['label'] = df['label'].apply(lambda x: train_valid_ds.classes[x])
df.to_csv('submission.csv', index=False)
</code></pre>
<p>위의 코드는
<code>submission.csv</code> 파일을 생성하며,
그 형식은
Kaggle 대회의 요구 사항을 충족합니다.
Kaggle에 결과를 제출하는 방법은
:numref:<code>sec_kaggle_house</code>와 유사합니다.</p>
<h2 id="요약-summary-81"><a class="header" href="#요약-summary-81">요약 (Summary)</a></h2>
<ul>
<li>우리는 원시 이미지 파일을 포함하는 데이터셋을 필요한 형식으로 정리한 후 읽을 수 있습니다.</li>
</ul>
<p>:begin_tab:<code>mxnet</code></p>
<ul>
<li>우리는 이미지 분류 대회에서 합성곱 신경망, 이미지 증강 및 하이브리드 프로그래밍을 사용할 수 있습니다.
:end_tab:</li>
</ul>
<p>:begin_tab:<code>pytorch</code></p>
<ul>
<li>우리는 이미지 분류 대회에서 합성곱 신경망과 이미지 증강을 사용할 수 있습니다.
:end_tab:</li>
</ul>
<h2 id="연습-문제-exercises-96"><a class="header" href="#연습-문제-exercises-96">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 Kaggle 대회에 전체 CIFAR-10 데이터셋을 사용하십시오. 하이퍼파라미터를 <code>batch_size = 128</code>, <code>num_epochs = 100</code>, <code>lr = 0.1</code>, <code>lr_period = 50</code>, <code>lr_decay = 0.1</code>로 설정하십시오.  이 대회에서 어떤 정확도와 순위를 얻을 수 있는지 확인하십시오. 더 향상시킬 수 있습니까?</li>
<li>이미지 증강을 사용하지 않을 때 어떤 정확도를 얻을 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/379">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1479">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kaggle의-개-품종-식별-dog-breed-identification-on-kaggle"><a class="header" href="#kaggle의-개-품종-식별-dog-breed-identification-on-kaggle">Kaggle의 개 품종 식별 (Dog Breed Identification on Kaggle)</a></h1>
<p>이 섹션에서는 Kaggle에서
개 품종 식별 문제를 연습할 것입니다. (<strong>이 대회의 웹 주소는 https://www.kaggle.com/c/dog-breed-identification입니다</strong>)</p>
<p>이 대회에서는
120가지 다른 품종의 개를 인식해야 합니다.
사실,
이 대회의 데이터셋은
ImageNet 데이터셋의 하위 집합입니다.
:numref:<code>sec_kaggle_cifar10</code>의 CIFAR-10 데이터셋 이미지와 달리,
ImageNet 데이터셋의 이미지는 높이와 너비가 다양하고 더 큽니다.
:numref:<code>fig_kaggle_dog</code>는 대회 웹페이지의 정보를 보여줍니다. 결과를 제출하려면 Kaggle 계정이 필요합니다.</p>
<p><img src="chapter_computer-vision/../img/kaggle-dog.jpg" alt="개 품종 식별 대회 웹사이트. &quot;Data&quot; 탭을 클릭하여 대회 데이터셋을 얻을 수 있습니다." />
:width:<code>400px</code>
:label:<code>fig_kaggle_dog</code></p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, npx
from mxnet.gluon import nn
import os

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
import torchvision
from torch import nn
import os
</code></pre>
<h2 id="데이터셋-획득-및-정리-obtaining-and-organizing-the-dataset-1"><a class="header" href="#데이터셋-획득-및-정리-obtaining-and-organizing-the-dataset-1">데이터셋 획득 및 정리 (Obtaining and Organizing the Dataset)</a></h2>
<p>대회 데이터셋은 훈련 세트와 테스트 세트로 나뉘며, 각각 10,222개와 10,357개의 세 RGB(컬러) 채널 JPEG 이미지를 포함합니다.
훈련 데이터셋에는
래브라도, 푸들, 닥스훈트, 사모예드, 허스키, 치와와, 요크셔 테리어 등 120종의 개가 있습니다.</p>
<h3 id="데이터셋-다운로드-downloading-the-dataset-1"><a class="header" href="#데이터셋-다운로드-downloading-the-dataset-1">데이터셋 다운로드 (Downloading the Dataset)</a></h3>
<p>Kaggle에 로그인한 후,
:numref:<code>fig_kaggle_dog</code>에 표시된 대회 웹페이지에서 "Data" 탭을 클릭하고 "Download All" 버튼을 클릭하여 데이터셋을 다운로드할 수 있습니다.
다운로드한 파일을 <code>../data</code>에 압축 해제하면 다음 경로에서 전체 데이터셋을 찾을 수 있습니다.</p>
<ul>
<li>../data/dog-breed-identification/labels.csv</li>
<li>../data/dog-breed-identification/sample_submission.csv</li>
<li>../data/dog-breed-identification/train</li>
<li>../data/dog-breed-identification/test</li>
</ul>
<p>위의 구조는
:numref:<code>sec_kaggle_cifar10</code>의 CIFAR-10 대회와 유사하다는 것을 알 수 있습니다. 여기서 <code>train/</code> 및 <code>test/</code> 폴더에는 각각 훈련 및 테스트 개 이미지가 포함되어 있고, <code>labels.csv</code>에는
훈련 이미지에 대한 라벨이 포함되어 있습니다.
마찬가지로, 더 쉽게 시작할 수 있도록, 위에서 언급한 [<strong>데이터셋의 작은 샘플을 제공합니다</strong>]: <code>train_valid_test_tiny.zip</code>.
Kaggle 대회의 전체 데이터셋을 사용하려면 아래 <code>demo</code> 변수를 <code>False</code>로 변경해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',
                            '0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')

# Kaggle 대회용으로 다운로드한 전체 데이터셋을 사용하는 경우,
# 아래 변수를 `False`로 변경하십시오.
demo = True
if demo:
    data_dir = d2l.download_extract('dog_tiny')
else:
    data_dir = os.path.join('..', 'data', 'dog-breed-identification')
</code></pre>
<h3 id="데이터셋-정리-organizing-the-dataset-1"><a class="header" href="#데이터셋-정리-organizing-the-dataset-1">[<strong>데이터셋 정리 (Organizing the Dataset)</strong>]</a></h3>
<p>:numref:<code>sec_kaggle_cifar10</code>에서 했던 것과 유사하게 데이터셋을 정리할 수 있습니다. 즉, 원래 훈련 세트에서 검증 세트를 분리하고 이미지를 라벨별로 그룹화된 하위 폴더로 이동합니다.</p>
<p>아래의 <code>reorg_dog_data</code> 함수는
훈련 데이터 라벨을 읽고, 검증 세트를 분리하고, 훈련 세트를 정리합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def reorg_dog_data(data_dir, valid_ratio):
    labels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv'))
    d2l.reorg_train_valid(data_dir, labels, valid_ratio)
    d2l.reorg_test(data_dir)


batch_size = 32 if demo else 128
valid_ratio = 0.1
reorg_dog_data(data_dir, valid_ratio)
</code></pre>
<h2 id="이미지-증강-image-augmentation-2"><a class="header" href="#이미지-증강-image-augmentation-2">[<strong>이미지 증강 (Image Augmentation)</strong>]</a></h2>
<p>이 개 품종 데이터셋은
ImageNet 데이터셋의 하위 집합이며,
이미지는 :numref:<code>sec_kaggle_cifar10</code>의 CIFAR-10 데이터셋보다 큽니다.
다음은 상대적으로 큰 이미지에 유용할 수 있는 몇 가지 이미지 증강 작업을 나열합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
transform_train = gluon.data.vision.transforms.Compose([
    # 이미지를 무작위로 잘라 원래 면적의 0.08에서 1배이고 높이 대 너비 비율이 3/4에서 4/3 사이인 이미지를 얻습니다.
    # 그런 다음 이미지를 스케일링하여 새로운 224 x 224 이미지를 만듭니다.
    gluon.data.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),
                                                   ratio=(3.0/4.0, 4.0/3.0)),
    gluon.data.vision.transforms.RandomFlipLeftRight(),
    # 밝기, 대비 및 채도를 무작위로 변경
    gluon.data.vision.transforms.RandomColorJitter(brightness=0.4,
                                                   contrast=0.4,
                                                   saturation=0.4),
    # 무작위 노이즈 추가
    gluon.data.vision.transforms.RandomLighting(0.1),
    gluon.data.vision.transforms.ToTensor(),
    # 이미지의 각 채널 표준화
    gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],
                                           [0.229, 0.224, 0.225])])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
transform_train = torchvision.transforms.Compose([
    # 이미지를 무작위로 잘라 원래 면적의 0.08에서 1배이고 높이 대 너비 비율이 3/4에서 4/3 사이인 이미지를 얻습니다.
    # 그런 다음 이미지를 스케일링하여 새로운 224 x 224 이미지를 만듭니다.
    torchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),
                                             ratio=(3.0/4.0, 4.0/3.0)),
    torchvision.transforms.RandomHorizontalFlip(),
    # 밝기, 대비 및 채도를 무작위로 변경
    torchvision.transforms.ColorJitter(brightness=0.4,
                                       contrast=0.4,
                                       saturation=0.4),
    # 무작위 노이즈 추가
    torchvision.transforms.ToTensor(),
    # 이미지의 각 채널 표준화
    torchvision.transforms.Normalize([0.485, 0.456, 0.406],
                                     [0.229, 0.224, 0.225])])
</code></pre>
<p>예측 중에는
무작위성이 없는 이미지 전처리 작업만 사용합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
transform_test = gluon.data.vision.transforms.Compose([
    gluon.data.vision.transforms.Resize(256),
    # 이미지 중심에서 224 x 224 정사각형 영역 자르기
    gluon.data.vision.transforms.CenterCrop(224),
    gluon.data.vision.transforms.ToTensor(),
    gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],
                                           [0.229, 0.224, 0.225])])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
transform_test = torchvision.transforms.Compose([
    torchvision.transforms.Resize(256),
    # 이미지 중심에서 224 x 224 정사각형 영역 자르기
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.485, 0.456, 0.406],
                                     [0.229, 0.224, 0.225])])
</code></pre>
<h2 id="데이터셋-읽기-reading-the-dataset-9"><a class="header" href="#데이터셋-읽기-reading-the-dataset-9">[<strong>데이터셋 읽기 (Reading the Dataset)</strong>]</a></h2>
<p>:numref:<code>sec_kaggle_cifar10</code>에서와 같이,
우리는 원시 이미지 파일로 구성된 정리된 데이터셋을 읽을 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train_ds, valid_ds, train_valid_ds, test_ds = [
    gluon.data.vision.ImageFolderDataset(
        os.path.join(data_dir, 'train_valid_test', folder))
    for folder in ('train', 'valid', 'train_valid', 'test')]
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(
    os.path.join(data_dir, 'train_valid_test', folder),
    transform=transform_train) for folder in ['train', 'train_valid']]

valid_ds, test_ds = [torchvision.datasets.ImageFolder(
    os.path.join(data_dir, 'train_valid_test', folder),
    transform=transform_test) for folder in ['valid', 'test']]
</code></pre>
<p>아래에서 :numref:<code>sec_kaggle_cifar10</code>과 동일한 방식으로 데이터 반복자 인스턴스를 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train_iter, train_valid_iter = [gluon.data.DataLoader(
    dataset.transform_first(transform_train), batch_size, shuffle=True,
    last_batch='discard') for dataset in (train_ds, train_valid_ds)]

valid_iter = gluon.data.DataLoader(
    valid_ds.transform_first(transform_test), batch_size, shuffle=False,
    last_batch='discard')

test_iter = gluon.data.DataLoader(
    test_ds.transform_first(transform_test), batch_size, shuffle=False,
    last_batch='keep')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train_iter, train_valid_iter = [torch.utils.data.DataLoader(
    dataset, batch_size, shuffle=True, drop_last=True)
    for dataset in (train_ds, train_valid_ds)]

valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,
                                         drop_last=True)

test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,
                                        drop_last=False)
</code></pre>
<h2 id="사전-훈련된-모델-미세-조정-fine-tuning-a-pretrained-model"><a class="header" href="#사전-훈련된-모델-미세-조정-fine-tuning-a-pretrained-model">[<strong>사전 훈련된 모델 미세 조정 (Fine-Tuning a Pretrained Model)</strong>]</a></h2>
<p>다시 말하지만,
이 대회의 데이터셋은 ImageNet 데이터셋의 하위 집합입니다.
따라서 우리는 :numref:<code>sec_fine_tuning</code>에서 논의된 접근 방식을 사용하여
전체 ImageNet 데이터셋에서 사전 훈련된 모델을 선택하고 이를 사용하여 이미지 특징을 추출하여
사용자 정의 소규모 출력 네트워크에 공급할 수 있습니다.
딥러닝 프레임워크의 고수준 API는
ImageNet 데이터셋에서 사전 훈련된 다양한 모델을 제공합니다.
여기서는 사전 훈련된 ResNet-34 모델을 선택합니다.
여기서 우리는 단순히
이 모델의 출력 레이어의 입력(즉, 추출된 특징)을 재사용합니다.
그런 다음 원래 출력 레이어를 훈련 가능한 작은 사용자 정의 출력 네트워크로 대체할 수 있습니다.
예를 들어 두 개의 완전 연결 레이어를 쌓는 것입니다.
:numref:<code>sec_fine_tuning</code>의 실험과 달리,
다음은 특징 추출에 사용되는 사전 훈련된 모델을 다시 훈련하지 않습니다.
이렇게 하면 훈련 시간과 기울기를 저장하기 위한 메모리가 줄어듭니다.</p>
<p>전체 ImageNet 데이터셋에 대해 세 가지 RGB 채널의 평균과 표준 편차를 사용하여 이미지를 표준화했다는 점을 상기하십시오.
사실,
이는 ImageNet에서 사전 훈련된 모델에 의한 표준화 작업과도 일치합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def get_net(devices):
    finetune_net = gluon.model_zoo.vision.resnet34_v2(pretrained=True)
    # 새로운 출력 네트워크 정의
    finetune_net.output_new = nn.HybridSequential(prefix='')
    finetune_net.output_new.add(nn.Dense(256, activation='relu'))
    # 120개의 출력 범주가 있습니다.
    finetune_net.output_new.add(nn.Dense(120))
    # 출력 네트워크 초기화
    finetune_net.output_new.initialize(init.Xavier(), ctx=devices)
    # 계산에 사용되는 CPU 또는 GPU에 모델 파라미터 배포
    finetune_net.collect_params().reset_ctx(devices)
    return finetune_net
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def get_net(devices):
    finetune_net = nn.Sequential()
    finetune_net.features = torchvision.models.resnet34(pretrained=True)
    # 새로운 출력 네트워크 정의 (120개의 출력 범주가 있습니다)
    finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256),
                                            nn.ReLU(),
                                            nn.Linear(256, 120))
    # 모델을 장치로 이동
    finetune_net = finetune_net.to(devices[0])
    # 특징 레이어의 파라미터 고정
    for param in finetune_net.features.parameters():
        param.requires_grad = False
    return finetune_net
</code></pre>
<p>[<strong>손실을 계산</strong>]하기 전에,
우리는 먼저 사전 훈련된 모델의 출력 레이어 입력, 즉 추출된 특징을 얻습니다.
그런 다음 이 특징을 작은 사용자 정의 출력 네트워크의 입력으로 사용하여 손실을 계산합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
loss = gluon.loss.SoftmaxCrossEntropyLoss()

def evaluate_loss(data_iter, net, devices):
    l_sum, n = 0.0, 0
    for features, labels in data_iter:
        X_shards, y_shards = d2l.split_batch(features, labels, devices)
        output_features = [net.features(X_shard) for X_shard in X_shards]
        outputs = [net.output_new(feature) for feature in output_features]
        ls = [loss(output, y_shard).sum() for output, y_shard
              in zip(outputs, y_shards)]
        l_sum += sum([float(l.sum()) for l in ls])
        n += labels.size
    return l_sum / n
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
loss = nn.CrossEntropyLoss(reduction='none')

def evaluate_loss(data_iter, net, devices):
    l_sum, n = 0.0, 0
    for features, labels in data_iter:
        features, labels = features.to(devices[0]), labels.to(devices[0])
        outputs = net(features)
        l = loss(outputs, labels)
        l_sum += l.sum()
        n += labels.numel()
    return l_sum / n
</code></pre>
<h2 id="훈련-함수-정의-defining-the-training-function-1"><a class="header" href="#훈련-함수-정의-defining-the-training-function-1">[<strong>훈련 함수</strong>] 정의 (Defining the Training Function)</a></h2>
<p>우리는 검증 세트에서의 모델 성능에 따라 모델을 선택하고 하이퍼파라미터를 조정할 것입니다. 모델 훈련 함수 <code>train</code>은 작은 사용자 정의 출력 네트워크의 파라미터만 반복합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
          lr_decay):
    # 작은 사용자 정의 출력 네트워크만 훈련
    trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd',
                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})
    num_batches, timer = len(train_iter), d2l.Timer()
    legend = ['train loss']
    if valid_iter is not None:
        legend.append('valid loss')
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=legend)
    for epoch in range(num_epochs):
        metric = d2l.Accumulator(2)
        if epoch &gt; 0 and epoch % lr_period == 0:
            trainer.set_learning_rate(trainer.learning_rate * lr_decay)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            X_shards, y_shards = d2l.split_batch(features, labels, devices)
            output_features = [net.features(X_shard) for X_shard in X_shards]
            with autograd.record():
                outputs = [net.output_new(feature)
                           for feature in output_features]
                ls = [loss(output, y_shard).sum() for output, y_shard
                      in zip(outputs, y_shards)]
            for l in ls:
                l.backward()
            trainer.step(batch_size)
            metric.add(sum([float(l.sum()) for l in ls]), labels.shape[0])
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[1], None))
        if valid_iter is not None:
            valid_loss = evaluate_loss(valid_iter, net, devices)
            animator.add(epoch + 1, (None, valid_loss))
    measures = f'train loss {metric[0] / metric[1]:.3f}'
    if valid_iter is not None:
        measures += f', valid loss {valid_loss:.3f}'
    print(measures + f'\n{metric[1] * num_epochs / timer.sum():.1f}'
          f' examples/sec on {str(devices)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
          lr_decay):
    # 작은 사용자 정의 출력 네트워크만 훈련
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    trainer = torch.optim.SGD((param for param in net.parameters()
                               if param.requires_grad), lr=lr,
                              momentum=0.9, weight_decay=wd)
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)
    num_batches, timer = len(train_iter), d2l.Timer()
    legend = ['train loss']
    if valid_iter is not None:
        legend.append('valid loss')
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=legend)
    for epoch in range(num_epochs):
        metric = d2l.Accumulator(2)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            features, labels = features.to(devices[0]), labels.to(devices[0])
            trainer.zero_grad()
            output = net(features)
            l = loss(output, labels).sum()
            l.backward()
            trainer.step()
            metric.add(l, labels.shape[0])
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[1], None))
        measures = f'train loss {metric[0] / metric[1]:.3f}'
        if valid_iter is not None:
            valid_loss = evaluate_loss(valid_iter, net, devices)
            animator.add(epoch + 1, (None, valid_loss.detach().cpu()))
        scheduler.step()
    if valid_iter is not None:
        measures += f', valid loss {valid_loss:.3f}'
    print(measures + f'\n{metric[1] * num_epochs / timer.sum():.1f}'
          f' examples/sec on {str(devices)}')
</code></pre>
<h2 id="모델-훈련-및-검증-training-and-validating-the-model-1"><a class="header" href="#모델-훈련-및-검증-training-and-validating-the-model-1">[<strong>모델 훈련 및 검증 (Training and Validating the Model)</strong>]</a></h2>
<p>이제 모델을 훈련하고 검증할 수 있습니다.
다음의 모든 하이퍼파라미터는 조정 가능합니다.
예를 들어 에포크 수를 늘릴 수 있습니다. <code>lr_period</code>와 <code>lr_decay</code>가 각각 2와 0.9로 설정되어 있으므로 최적화 알고리즘의 학습률은 2 에포크마다 0.9배가 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 5e-3, 1e-4
lr_period, lr_decay, net = 2, 0.9, get_net(devices)
net.hybridize()
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
      lr_decay)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 1e-4, 1e-4
lr_period, lr_decay, net = 2, 0.9, get_net(devices)
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
      lr_decay)
</code></pre>
<h2 id="테스트-세트-분류-classifying-the-testing-set-및-kaggle에-결과-제출-1"><a class="header" href="#테스트-세트-분류-classifying-the-testing-set-및-kaggle에-결과-제출-1">[<strong>테스트 세트 분류 (Classifying the Testing Set)</strong>] 및 Kaggle에 결과 제출</a></h2>
<p>:numref:<code>sec_kaggle_cifar10</code>의 마지막 단계와 유사하게,
결국 모든 라벨이 지정된 데이터(검증 세트 포함)는 모델을 훈련하고 테스트 세트를 분류하는 데 사용됩니다.
우리는 훈련된 사용자 정의 출력 네트워크를 사용하여 분류를 수행합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = get_net(devices)
net.hybridize()
train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
      lr_decay)

preds = []
for data, label in test_iter:
    output_features = net.features(data.as_in_ctx(devices[0]))
    output = npx.softmax(net.output_new(output_features))
    preds.extend(output.asnumpy())
ids = sorted(os.listdir(
    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))
with open('submission.csv', 'w') as f:
    f.write('id,' + ','.join(train_valid_ds.synsets) + '\n')
    for i, output in zip(ids, preds):
        f.write(i.split('.')[0] + ',' + ','.join(
            [str(num) for num in output]) + '\n')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = get_net(devices)
train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
      lr_decay)

preds = []
for data, label in test_iter:
    output = torch.nn.functional.softmax(net(data.to(devices[0])), dim=1)
    preds.extend(output.cpu().detach().numpy())
ids = sorted(os.listdir(
    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))
with open('submission.csv', 'w') as f:
    f.write('id,' + ','.join(train_valid_ds.classes) + '\n')
    for i, output in zip(ids, preds):
        f.write(i.split('.')[0] + ',' + ','.join(
            [str(num) for num in output]) + '\n')
</code></pre>
<p>위의 코드는
:numref:<code>sec_kaggle_house</code>에서 설명한 것과 동일한 방식으로
Kaggle에 제출할 <code>submission.csv</code> 파일을 생성합니다.</p>
<h2 id="요약-summary-82"><a class="header" href="#요약-summary-82">요약 (Summary)</a></h2>
<ul>
<li>ImageNet 데이터셋의 이미지는 CIFAR-10 이미지보다 큽니다(다양한 크기). 다른 데이터셋의 작업에 대해 이미지 증강 작업을 수정할 수 있습니다.</li>
<li>ImageNet 데이터셋의 하위 집합을 분류하기 위해, 우리는 전체 ImageNet 데이터셋에서 사전 훈련된 모델을 활용하여 특징을 추출하고 작은 사용자 정의 출력 네트워크만 훈련할 수 있습니다. 이렇게 하면 계산 시간과 메모리 비용이 줄어듭니다.</li>
</ul>
<h2 id="연습-문제-exercises-97"><a class="header" href="#연습-문제-exercises-97">연습 문제 (Exercises)</a></h2>
<ol>
<li>전체 Kaggle 대회 데이터셋을 사용할 때, <code>batch_size</code>(배치 크기)와 <code>num_epochs</code>(에포크 수)를 늘리고 다른 하이퍼파라미터를 <code>lr = 0.01</code>, <code>lr_period = 10</code>, <code>lr_decay = 0.1</code>로 설정하면 어떤 결과를 얻을 수 있습니까?</li>
<li>더 깊은 사전 훈련된 모델을 사용하면 더 나은 결과를 얻습니까? 하이퍼파라미터를 어떻게 조정합니까? 결과를 더 향상시킬 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/380">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1481">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="자연어-처리-사전-훈련-natural-language-processing-pretraining"><a class="header" href="#자연어-처리-사전-훈련-natural-language-processing-pretraining">자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a></h1>
<p>:label:<code>chap_nlp_pretrain</code></p>
<p>인간은 소통해야 합니다.
이러한 인간 조건의 기본적인 욕구에서, 매일 방대한 양의 텍스트가 생성되고 있습니다.
소셜 미디어, 채팅 앱, 이메일, 제품 리뷰, 뉴스 기사, 연구 논문 및 책의 풍부한 텍스트를 고려할 때, 컴퓨터가 이를 이해하여 도움을 제공하거나 인간 언어를 기반으로 결정을 내릴 수 있도록 하는 것이 중요합니다.</p>
<p>*자연어 처리(Natural language processing)*는 자연어를 사용하여 컴퓨터와 인간 간의 상호 작용을 연구합니다.
실제로 :numref:<code>sec_language-model</code>의 언어 모델 및 :numref:<code>sec_machine_translation</code>의 기계 번역 모델과 같이 자연어 처리 기술을 사용하여 텍스트(인간 자연어) 데이터를 처리하고 분석하는 것은 매우 일반적입니다.</p>
<p>텍스트를 이해하기 위해, 우리는 그 표현을 학습하는 것으로 시작할 수 있습니다.
대규모 코퍼스(corpora)의 기존 텍스트 시퀀스를 활용하여,
*자기 지도 학습(self-supervised learning)*은
주변 텍스트의 다른 부분을 사용하여 텍스트의 일부 숨겨진 부분을 예측하는 것과 같이
텍스트 표현을 사전 훈련(pretrain)하는 데 광범위하게 사용되었습니다.
이런 식으로,
모델은 <em>비싼</em> 라벨링 노력 없이
<em>방대한</em> 텍스트 데이터로부터
감독(supervision)을 통해 학습합니다!</p>
<p>이 장에서 보게 되겠지만,
각 단어 또는 하위 단어를 개별 토큰으로 취급할 때,
각 토큰의 표현은 대규모 코퍼스에서
word2vec, GloVe 또는 하위 단어 임베딩 모델을 사용하여 사전 훈련될 수 있습니다.
사전 훈련 후, 각 토큰의 표현은 벡터가 될 수 있지만,
문맥이 무엇이든 동일하게 유지됩니다.
예를 들어, "bank"의 벡터 표현은
"go to the bank to deposit some money" (돈을 입금하러 은행에 가다)와
"go to the bank to sit down" (앉으러 둑에 가다)에서 동일합니다.
따라서 더 많은 최근의 사전 훈련 모델은 동일한 토큰의 표현을
다른 문맥에 적응시킵니다.
그중에는 Transformer 인코더를 기반으로 한 훨씬 더 깊은 자기 지도 모델인 BERT가 있습니다.
이 장에서는 :numref:<code>fig_nlp-map-pretrain</code>에서 강조된 것처럼 텍스트에 대한 그러한 표현을 사전 훈련하는 방법에 초점을 맞출 것입니다.</p>
<p><img src="chapter_natural-language-processing-pretraining/../img/nlp-map-pretrain.svg" alt="사전 훈련된 텍스트 표현은 다양한 다운스트림 자연어 처리 응용 프로그램을 위해 다양한 딥러닝 아키텍처에 공급될 수 있습니다. 이 장에서는 업스트림 텍스트 표현 사전 훈련에 중점을 둡니다." />
:label:<code>fig_nlp-map-pretrain</code></p>
<p>큰 그림을 보기 위해,
:numref:<code>fig_nlp-map-pretrain</code>은
사전 훈련된 텍스트 표현이 다양한 다운스트림 자연어 처리 응용 프로그램을 위해
다양한 딥러닝 아키텍처에 공급될 수 있음을 보여줍니다.
우리는 :numref:<code>chap_nlp_app</code>에서 그것들을 다룰 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

word2vec
approx-training
word-embedding-dataset
word2vec-pretraining
glove
subword-embedding
similarity-analogy
bert
bert-dataset
bert-pretraining

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="단어-임베딩-word2vec"><a class="header" href="#단어-임베딩-word2vec">단어 임베딩 (word2vec)</a></h1>
<p>:label:<code>sec_word2vec</code></p>
<p>자연어는 의미를 표현하기 위해 사용되는 복잡한 시스템입니다. 이 시스템에서 단어는 의미의 기본 단위입니다. 이름에서 알 수 있듯이, *단어 벡터(word vectors)*는 단어를 나타내기 위해 사용되는 벡터이며, 단어의 특성 벡터 또는 표현으로 간주될 수도 있습니다. 단어를 실제 벡터에 매핑하는 기술을 *단어 임베딩(word embedding)*이라고 합니다. 최근 몇 년 동안 단어 임베딩은 점차 자연어 처리의 기본 지식이 되었습니다.</p>
<h2 id="원-핫-벡터는-나쁜-선택입니다"><a class="header" href="#원-핫-벡터는-나쁜-선택입니다">원-핫 벡터는 나쁜 선택입니다</a></h2>
<p>우리는 :numref:<code>sec_rnn-scratch</code>에서 단어(문자가 단어임)를 나타내기 위해 원-핫 벡터를 사용했습니다. 사전에 있는 서로 다른 단어의 수(사전 크기)를 $N$이라고 하고, 각 단어가 $0$에서 $N-1$까지의 서로 다른 정수(인덱스)에 대응한다고 가정해 봅시다. 인덱스 $i$를 가진 임의의 단어에 대한 원-핫 벡터 표현을 얻기 위해, 우리는 모든 값이 0인 길이 $N$의 벡터를 생성하고 $i$번째 위치의 요소를 1로 설정합니다. 이런 식으로 각 단어는 길이 $N$의 벡터로 표현되며 신경망에서 직접 사용될 수 있습니다.</p>
<p>원-핫 단어 벡터는 구성하기 쉽지만, 일반적으로 좋은 선택은 아닙니다. 주된 이유는 원-핫 단어 벡터가 우리가 자주 사용하는 *코사인 유사도(cosine similarity)*와 같이 서로 다른 단어 간의 유사성을 정확하게 표현할 수 없기 때문입니다. 벡터 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^d$에 대해, 그들의 코사인 유사도는 두 벡터 사이 각도의 코사인 값입니다:</p>
<p>$$\frac{\mathbf{x}^\top \mathbf{y}}{|\mathbf{x}| |\mathbf{y}|} \in [-1, 1].$$</p>
<p>임의의 서로 다른 두 단어의 원-핫 벡터 간의 코사인 유사도는 0이므로, 원-핫 벡터는 단어 간의 유사성을 인코딩할 수 없습니다.</p>
<h2 id="자기-지도-학습-word2vec"><a class="header" href="#자기-지도-학습-word2vec">자기 지도 학습 word2vec</a></h2>
<p>위의 문제를 해결하기 위해 <a href="https://code.google.com/archive/p/word2vec/">word2vec</a> 도구가 제안되었습니다. 이는 각 단어를 고정 길이 벡터로 매핑하며, 이 벡터들은 서로 다른 단어 간의 유사성 및 유추 관계를 더 잘 표현할 수 있습니다. word2vec 도구에는 <em>스킵-그램(skip-gram)</em> :cite:<code>Mikolov.Sutskever.Chen.ea.2013</code>과 <em>CBOW(continuous bag of words)</em> :cite:<code>Mikolov.Chen.Corrado.ea.2013</code>이라는 두 가지 모델이 포함되어 있습니다. 의미론적으로 의미 있는 표현을 위해, 그들의 훈련은 코퍼스에서 주변 단어의 일부를 사용하여 일부 단어를 예측하는 것으로 볼 수 있는 조건부 확률에 의존합니다. 감독(supervision)이 레이블 없는 데이터에서 오기 때문에 스킵-그램과 CBOW는 모두 자기 지도(self-supervised) 모델입니다.</p>
<p>다음에서는 이 두 모델과 그 훈련 방법을 소개합니다.</p>
<h2 id="스킵-그램skip-gram-모델"><a class="header" href="#스킵-그램skip-gram-모델">스킵-그램(Skip-Gram) 모델</a></h2>
<p>:label:<code>subsec_skip-gram</code></p>
<p><em>스킵-그램(skip-gram)</em> 모델은 텍스트 시퀀스에서 한 단어가 주변 단어들을 생성하는 데 사용될 수 있다고 가정합니다. "the", "man", "loves", "his", "son"이라는 텍스트 시퀀스를 예로 들어 보겠습니다. "loves"를 *중심 단어(center word)*로 선택하고 문맥 윈도우 크기를 2로 설정합시다. :numref:<code>fig_skip_gram</code>에 표시된 것처럼, 중심 단어 "loves"가 주어졌을 때 스킵-그램 모델은 중심 단어에서 2단어 이내에 있는 *문맥 단어(context words)*인 "the", "man", "his", "son"을 생성할 조건부 확률을 고려합니다:</p>
<p>$$P(\textrm{"the"},\textrm{"man"},\textrm{"his"},\textrm{"son"}\mid\textrm{"loves"}).$$</p>
<p>중심 단어가 주어졌을 때 문맥 단어들이 독립적으로 생성된다고 가정합니다(즉, 조건부 독립). 이 경우 위의 조건부 확률은 다음과 같이 다시 쓸 수 있습니다.</p>
<p>$$P(\textrm{"the"}\mid\textrm{"loves"})\cdot P(\textrm{"man"}\mid\textrm{"loves"})\cdot P(\textrm{"his"}\mid\textrm{"loves"})\cdot P(\textrm{"son"}\mid\textrm{"loves"}).$$</p>
<p><img src="chapter_natural-language-processing-pretraining/../img/skip-gram.svg" alt="스킵-그램 모델은 중심 단어가 주어졌을 때 주변 문맥 단어를 생성할 조건부 확률을 고려합니다." />
:label:<code>fig_skip_gram</code></p>
<p>스킵-그램 모델에서 각 단어는 조건부 확률을 계산하기 위해 두 개의 $d$차원 벡터 표현을 갖습니다. 더 구체적으로, 사전에 있는 인덱스 $i$를 가진 임의의 단어에 대해, 각각 <em>중심</em> 단어와 <em>문맥</em> 단어로 사용될 때의 두 벡터를 $\mathbf{v}_i\in\mathbb{R}^d$와 $\mathbf{u}_i\in\mathbb{R}^d$라고 표시합시다. 중심 단어 $w_c$(사전 인덱스 $c$)가 주어졌을 때 임의의 문맥 단어 $w_o$(사전 인덱스 $o$)를 생성할 조건부 확률은 벡터 내적에 대한 소프트맥스 연산으로 모델링될 수 있습니다:</p>
<p>$$P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^\top \mathbf{v}<em>c)}{ \sum</em>{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)},$$
:eqlabel:<code>eq_skip-gram-softmax</code></p>
<p>여기서 어휘 인덱스 집합 $\mathcal{V} = {0, 1, \ldots, |\mathcal{V}|-1}$입니다. 길이 $T$인 텍스트 시퀀스가 주어지고 타임 스텝 $t$에서의 단어를 $w^{(t)}$라고 할 때, 임의의 중심 단어가 주어졌을 때 문맥 단어들이 독립적으로 생성된다고 가정합니다. 문맥 윈도우 크기가 $m$일 때, 스킵-그램 모델의 우도 함수(likelihood function)는 임의의 중심 단어가 주어졌을 때 모든 문맥 단어를 생성할 확률입니다:</p>
<p>$$ \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w^{(t+j)} \mid w^{(t)}),$$</p>
<p>여기서 $1$보다 작거나 $T$보다 큰 타임 스텝은 생략될 수 있습니다.</p>
<h3 id="훈련"><a class="header" href="#훈련">훈련</a></h3>
<p>스킵-그램 모델 파라미터는 어휘의 각 단어에 대한 중심 단어 벡터와 문맥 단어 벡터입니다. 훈련 시에는 우도 함수를 최대화(즉, 최대 우도 추정)하여 모델 파라미터를 학습합니다. 이는 다음 손실 함수를 최소화하는 것과 같습니다:</p>
<p>$$ - \sum_{t=1}^{T} \sum_{-m \leq j \leq m,\ j \neq 0} \textrm{log}, P(w^{(t+j)} \mid w^{(t)}).$$</p>
<p>손실을 최소화하기 위해 확률적 경사 하강법을 사용할 때, 각 반복에서 무작위로 더 짧은 하위 시퀀스를 샘플링하여 이 하위 시퀀스에 대한 (확률적) 기울기를 계산하여 모델 파라미터를 업데이트할 수 있습니다. 이 (확률적) 기울기를 계산하려면 중심 단어 벡터와 문맥 단어 벡터에 대한 로그 조건부 확률의 기울기를 얻어야 합니다. 일반적으로 :eqref:<code>eq_skip-gram-softmax</code>에 따라 임의의 중심 단어 $w_c$와 문맥 단어 $w_o$ 쌍을 포함하는 로그 조건부 확률은 다음과 같습니다.</p>
<p>$$\log P(w_o \mid w_c) =\mathbf{u}_o^\top \mathbf{v}<em>c - \log\left(\sum</em>{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)\right).$$
:eqlabel:<code>eq_skip-gram-log</code></p>
<p>미분을 통해 중심 단어 벡터 $\mathbf{v}_c$에 대한 기울기를 다음과 같이 얻을 수 있습니다.</p>
<p>$$\begin{aligned}\frac{\partial \textrm{log}, P(w_o \mid w_c)}{\partial \mathbf{v}_c}&amp;= \mathbf{u}<em>o - \frac{\sum</em>{j \in \mathcal{V}} \exp(\mathbf{u}_j^\top \mathbf{v}_c)\mathbf{u}<em>j}{\sum</em>{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)}\&amp;= \mathbf{u}<em>o - \sum</em>{j \in \mathcal{V}} \left(\frac{\exp(\mathbf{u}_j^\top \mathbf{v}<em>c)}{ \sum</em>{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)}\right) \mathbf{u}_j\&amp;= \mathbf{u}<em>o - \sum</em>{j \in \mathcal{V}} P(w_j \mid w_c) \mathbf{u}_j.\end{aligned}$$
:eqlabel:<code>eq_skip-gram-grad</code></p>
<p>:eqref:<code>eq_skip-gram-grad</code>의 계산에는 $w_c$를 중심 단어로 하는 사전의 모든 단어에 대한 조건부 확률이 필요하다는 점에 유의하십시오. 다른 단어 벡터에 대한 기울기도 같은 방식으로 얻을 수 있습니다.</p>
<p>훈련 후, 사전의 인덱스 $i$를 가진 임의의 단어에 대해 두 단어 벡터 $,\mathbf{v}_i$(중심 단어로서)와 $,\mathbf{u}_i$(문맥 단어로서)를 모두 얻습니다. 자연어 처리 응용 프로그램에서 스킵-그램 모델의 중심 단어 벡터는 일반적으로 단어 표현으로 사용됩니다.</p>
<h2 id="cbowcontinuous-bag-of-words-모델"><a class="header" href="#cbowcontinuous-bag-of-words-모델">CBOW(Continuous Bag of Words) 모델</a></h2>
<p><em>CBOW(continuous bag of words)</em> 모델은 스킵-그램 모델과 유사합니다. 스킵-그램 모델과의 주요 차이점은 CBOW 모델은 텍스트 시퀀스에서 주변 문맥 단어들을 기반으로 중심 단어가 생성된다고 가정한다는 것입니다. 예를 들어 동일한 텍스트 시퀀스 "the", "man", "loves", "his", "son"에서 "loves"를 중심 단어로 하고 문맥 윈도우 크기를 2로 할 때, CBOW 모델은 문맥 단어 "the", "man", "his", "son"을 기반으로 중심 단어 "loves"를 생성할 조건부 확률을 고려합니다(:numref:<code>fig_cbow</code> 참조):</p>
<p>$$P(\textrm{"loves"}\mid\textrm{"the"},\textrm{"man"},\textrm{"his"},\textrm{"son"}).$$</p>
<p><img src="chapter_natural-language-processing-pretraining/../img/cbow.svg" alt="CBOW 모델은 주변 문맥 단어가 주어졌을 때 중심 단어를 생성할 조건부 확률을 고려합니다." />
:label:<code>fig_cbow</code></p>
<p>CBOW 모델에는 여러 문맥 단어가 있으므로, 조건부 확률 계산 시 이러한 문맥 단어 벡터들의 평균을 냅니다. 구체적으로, 사전에 있는 인덱스 $i$를 가진 임의의 단어에 대해, 각각 <em>문맥</em> 단어와 <em>중심</em> 단어로 사용될 때의 두 벡터를 $,\mathbf{v}<em>i\in\mathbb{R}^d$와 $,\mathbf{u}<em>i\in\mathbb{R}^d$라고 표시합시다(스킵-그램 모델과는 의미가 반대임). 주변 문맥 단어 $w</em>{o_1}, \ldots, w</em>{o_{2m}}$(사전 인덱스 $o_1, \ldots, o_{2m}$)이 주어졌을 때 임의의 중심 단어 $w_c$(사전 인덱스 $c$)를 생성할 조건부 확률은 다음과 같이 모델링될 수 있습니다.</p>
<p>$$P(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) = \frac{\exp\left(\frac{1}{2m}\mathbf{u}<em>c^\top (\mathbf{v}</em>{o_1} + \ldots + \mathbf{v}<em>{o</em>{2m}}) \right)}{ \sum_{i \in \mathcal{V}} \exp\left(\frac{1}{2m}\mathbf{u}<em>i^\top (\mathbf{v}</em>{o_1} + \ldots + \mathbf{v}<em>{o</em>{2m}}) \right)}.$$
:eqlabel:<code>fig_cbow-full</code></p>
<p>간결함을 위해 $\mathcal{W}<em>o= {w</em>{o_1}, \ldots, w_{o_{2m}}\ }$ 및 $,\bar{\mathbf{v}}<em>o = \left(\mathbf{v}</em>{o_1} + \ldots + \mathbf{v}<em>{o</em>{2m}} \right)/(2m)$라고 합시다. 그러면 :eqref:<code>fig_cbow-full</code>은 다음과 같이 단순화될 수 있습니다.</p>
<p>$$P(w_c \mid \mathcal{W}_o) = \frac{\exp\left(\mathbf{u}_c^\top \bar{\mathbf{v}}<em>o\right)}{\sum</em>{i \in \mathcal{V}} \exp\left(\mathbf{u}_i^\top \bar{\mathbf{v}}_o\right)}.$$</p>
<p>길이 $T$인 텍스트 시퀀스가 주어지고 타임 스텝 $t$에서의 단어를 $w^{(t)}$라고 할 때, 문맥 윈도우 크기가 $m$인 경우 CBOW 모델의 우도 함수는 문맥 단어들이 주어졌을 때 모든 중심 단어를 생성할 확률입니다:</p>
<p>$$ \prod_{t=1}^{T}  P(w^{(t)} \mid  w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)}).$$</p>
<h3 id="훈련-1"><a class="header" href="#훈련-1">훈련</a></h3>
<p>CBOW 모델을 훈련하는 것은 스킵-그램 모델을 훈련하는 것과 거의 동일합니다. CBOW 모델의 최대 우도 추정은 다음 손실 함수를 최소화하는 것과 같습니다:</p>
<p>$$  -\sum_{t=1}^T  \textrm{log}, P(w^{(t)} \mid  w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)}).$$</p>
<p>다음을 유의하십시오.</p>
<p>$$\log,P(w_c \mid \mathcal{W}_o) = \mathbf{u}_c^\top \bar{\mathbf{v}}<em>o - \log,\left(\sum</em>{i \in \mathcal{V}} \exp\left(\mathbf{u}_i^\top \bar{\mathbf{v}}_o\right)\right).$$</p>
<p>미분을 통해 임의의 문맥 단어 벡터 $,\mathbf{v}_{o_i}$($i = 1, \ldots, 2m$)에 대한 기울기를 다음과 같이 얻을 수 있습니다.</p>
<p>$$\frac{\partial \log, P(w_c \mid \mathcal{W}<em>o)}{\partial \mathbf{v}</em>{o_i}} = \frac{1}{2m} \left(\mathbf{u}<em>c - \sum</em>{j \in \mathcal{V}} \frac{\exp(\mathbf{u}_j^\top \bar{\mathbf{v}}_o)\mathbf{u}<em>j}{ \sum</em>{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \bar{\mathbf{v}}_o)} \right) = \frac{1}{2m}\left(\mathbf{u}<em>c - \sum</em>{j \in \mathcal{V}} P(w_j \mid \mathcal{W}_o) \mathbf{u}_j \right).$$
:eqlabel:<code>eq_cbow-gradient</code></p>
<p>다른 단어 벡터에 대한 기울기도 같은 방식으로 얻을 수 있습니다. 스킵-그램 모델과 달리 CBOW 모델은 일반적으로 문맥 단어 벡터를 단어 표현으로 사용합니다.</p>
<h2 id="요약-summary-83"><a class="header" href="#요약-summary-83">요약 (Summary)</a></h2>
<ul>
<li>단어 벡터는 단어를 나타내기 위해 사용되는 벡터이며, 단어의 특성 벡터 또는 표현으로 간주될 수도 있습니다. 단어를 실제 벡터에 매핑하는 기술을 단어 임베딩이라고 합니다.</li>
<li>word2vec 도구에는 스킵-그램 모델과 CBOW 모델이 모두 포함되어 있습니다.</li>
<li>스킵-그램 모델은 텍스트 시퀀스에서 한 단어가 주변 단어들을 생성하는 데 사용될 수 있다고 가정합니다. 반면 CBOW 모델은 주변 문맥 단어들을 기반으로 중심 단어가 생성된다고 가정합니다.</li>
</ul>
<h2 id="연습-문제-exercises-98"><a class="header" href="#연습-문제-exercises-98">연습 문제 (Exercises)</a></h2>
<ol>
<li>각 기울기를 계산하기 위한 계산 복잡도는 얼마입니까? 사전 크기가 매우 클 때 어떤 문제가 발생할 수 있습니까?</li>
<li>영어의 일부 고정된 구문은 "new york"과 같이 여러 단어로 구성됩니다. 그들의 단어 벡터를 어떻게 훈련합니까? 힌트: word2vec 논문 :cite:<code>Mikolov.Sutskever.Chen.ea.2013</code>의 섹션 4를 참조하십시오.</li>
<li>스킵-그램 모델을 예로 들어 word2vec 설계를 되짚어 봅시다. 스킵-그램 모델에서 두 단어 벡터의 내적과 코사인 유사도 사이의 관계는 무엇입니까? 의미론적으로 유사한 단어 쌍에 대해, 왜 그들의 단어 벡터(스킵-그램 모델로 훈련됨)의 코사인 유사도가 높을 수 있습니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/381">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="근사-훈련-approximate-training"><a class="header" href="#근사-훈련-approximate-training">근사 훈련 (Approximate Training)</a></h1>
<p>:label:<code>sec_approx_train</code></p>
<p>:numref:<code>sec_word2vec</code>에서의 논의를 상기해 봅시다.
스킵-그램 모델의 주요 아이디어는
:eqref:<code>eq_skip-gram-softmax</code>에서
주어진 중심 단어 $w_c$를 기반으로
문맥 단어 $w_o$를 생성할 조건부 확률을 계산하기 위해
소프트맥스 연산을 사용하는 것이며,
이에 해당하는 로그 손실은
:eqref:<code>eq_skip-gram-log</code>의 반대 부호로 주어집니다.</p>
<p>소프트맥스 연산의 특성상,
문맥 단어는 사전 $\mathcal{V}$의 어느 것이든 될 수 있으므로,
:eqref:<code>eq_skip-gram-log</code>의 반대 부호는
전체 어휘 크기만큼의 항목 합계를 포함합니다.
결과적으로,
:eqref:<code>eq_skip-gram-grad</code>의 스킵-그램 모델에 대한 기울기 계산과
:eqref:<code>eq_cbow-gradient</code>의 CBOW 모델에 대한 기울기 계산 모두
합계를 포함합니다.
불행히도,
(종종 수십만 또는 수백만 단어를 포함하는) 큰 사전에 대해 합산하는
그러한 기울기에 대한 계산 비용은
엄청납니다!</p>
<p>앞서 언급한 계산 복잡성을 줄이기 위해, 이 섹션에서는 두 가지 근사 훈련 방법인
*네거티브 샘플링(negative sampling)*과 *계층적 소프트맥스(hierarchical softmax)*를 소개합니다.
스킵-그램 모델과 CBOW 모델 간의 유사성 때문에,
이 두 가지 근사 훈련 방법을 설명하기 위해
스킵-그램 모델을 예로 들 것입니다.</p>
<h2 id="네거티브-샘플링-negative-sampling"><a class="header" href="#네거티브-샘플링-negative-sampling">네거티브 샘플링 (Negative Sampling)</a></h2>
<p>:label:<code>subsec_negative-sampling</code></p>
<p>네거티브 샘플링은 원래 목적 함수를 수정합니다.
중심 단어 $w_c$의 문맥 윈도우가 주어졌을 때,
어떤 (문맥) 단어 $w_o$가
이 문맥 윈도우에서 나온다는 사실은
다음과 같이 모델링된 확률을 갖는 사건으로 간주됩니다.</p>
<p>$$P(D=1\mid w_c, w_o) = \sigma(\mathbf{u}_o^\top \mathbf{v}_c),$$</p>
<p>여기서 $\sigma$는 시그모이드 활성화 함수의 정의를 사용합니다:</p>
<p>$$\sigma(x) = \frac{1}{1+\exp(-x)}.$$
:eqlabel:<code>eq_sigma-f</code></p>
<p>텍스트 시퀀스에서 이러한 모든 사건의 결합 확률을 최대화하여
단어 임베딩을 훈련하는 것으로 시작하겠습니다.
구체적으로,
길이 $T$인 텍스트 시퀀스가 주어졌을 때,
타임 스텝 $t$에서의 단어를 $w^{(t)}$로 표시하고
문맥 윈도우 크기를 $m$이라 할 때, 다음 결합 확률을 최대화하는 것을 고려하십시오.</p>
<p>$$ \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(D=1\mid w^{(t)}, w^{(t+j)}).$$
:eqlabel:<code>eq-negative-sample-pos</code></p>
<p>그러나
:eqref:<code>eq-negative-sample-pos</code>는
긍정 예제(positive examples)와 관련된 사건만 고려합니다.
결과적으로,
:eqref:<code>eq-negative-sample-pos</code>의 결합 확률은
모든 단어 벡터가 무한대와 같을 때만
1로 최대화됩니다.
물론,
그러한 결과는 무의미합니다.
목적 함수를 더 의미 있게 만들기 위해,
<em>네거티브 샘플링</em>은
미리 정의된 분포에서 샘플링된 부정 예제(negative examples)를 추가합니다.</p>
<p>문맥 단어 $w_o$가 중심 단어 $w_c$의 문맥 윈도우에서 나온다는 사건을 $S$라고 표시합시다.
$w_o$와 관련된 이 사건에 대해,
미리 정의된 분포 $P(w)$에서
이 문맥 윈도우에 속하지 않는 $K$개의 *노이즈 단어(noise words)*를 샘플링합니다.
노이즈 단어 $w_k$ ($k=1, \ldots, K$)가
$w_c$의 문맥 윈도우에서 나오지 않는다는 사건을 $N_k$라고 표시합시다.
긍정 예제와 부정 예제 $S, N_1, \ldots, N_K$를 포함하는
이러한 사건들이 상호 독립적이라고 가정합니다.
네거티브 샘플링은
:eqref:<code>eq-negative-sample-pos</code>의 (긍정 예제만 포함하는) 결합 확률을
다음과 같이 다시 씁니다.</p>
<p>$$ \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} P(w^{(t+j)} \mid w^{(t)}),$$</p>
<p>여기서 조건부 확률은 사건 $S, N_1, \ldots, N_K$를 통해 근사됩니다:</p>
<p>$$ P(w^{(t+j)} \mid w^{(t)}) =P(D=1\mid w^{(t)}, w^{(t+j)})\prod_{k=1,\ w_k \sim P(w)}^K P(D=0\mid w^{(t)}, w_k).$$
:eqlabel:<code>eq-negative-sample-conditional-prob</code></p>
<p>텍스트 시퀀스의 타임 스텝 $t$에서의 단어 $w^{(t)}$와
노이즈 단어 $w_k$의 인덱스를
각각 $i_t$와 $h_k$라고 표시합시다.
:eqref:<code>eq-negative-sample-conditional-prob</code>의 조건부 확률에 대한 로그 손실은 다음과 같습니다.</p>
<p>$$
\begin{aligned}
-\log P(w^{(t+j)} \mid w^{(t)})
=&amp; -\log P(D=1\mid w^{(t)}, w^{(t+j)}) - \sum_{k=1,\ w_k \sim P(w)}^K \log P(D=0\mid w^{(t)}, w_k)<br />
=&amp;-  \log, \sigma\left(\mathbf{u}<em>{i</em>{t+j}}^\top \mathbf{v}<em>{i_t}\right) - \sum</em>{k=1,\ w_k \sim P(w)}^K \log\left(1-\sigma\left(\mathbf{u}<em>{h_k}^\top \mathbf{v}</em>{i_t}\right)\right)<br />
=&amp;-  \log, \sigma\left(\mathbf{u}<em>{i</em>{t+j}}^\top \mathbf{v}<em>{i_t}\right) - \sum</em>{k=1,\ w_k \sim P(w)}^K \log\sigma\left(-\mathbf{u}<em>{h_k}^\top \mathbf{v}</em>{i_t}\right).
\end{aligned}
$$</p>
<p>이제 각 훈련 단계에서의 기울기 계산 비용은
사전 크기와 무관하며,
$K$에 선형적으로 의존한다는 것을 알 수 있습니다.
하이퍼파라미터 $K$를 더 작은 값으로 설정하면,
네거티브 샘플링을 사용한 각 훈련 단계에서의 기울기 계산 비용이 더 작아집니다.</p>
<h2 id="계층적-소프트맥스-hierarchical-softmax"><a class="header" href="#계층적-소프트맥스-hierarchical-softmax">계층적 소프트맥스 (Hierarchical Softmax)</a></h2>
<p>대안적인 근사 훈련 방법으로,
*계층적 소프트맥스(hierarchical softmax)*는
:numref:<code>fig_hi_softmax</code>에 묘사된 데이터 구조인
이진 트리를 사용합니다.
여기서 트리의 각 리프 노드(leaf node)는
사전 $\mathcal{V}$의 단어를 나타냅니다.</p>
<p><img src="chapter_natural-language-processing-pretraining/../img/hi-softmax.svg" alt="근사 훈련을 위한 계층적 소프트맥스. 트리의 각 리프 노드는 사전에 있는 단어를 나타냅니다." />
:label:<code>fig_hi_softmax</code></p>
<p>이진 트리에서 루트 노드로부터 단어 $w$를 나타내는 리프 노드까지의 경로에 있는
노드(양 끝 포함)의 수를 $L(w)$라고 표시합시다.
이 경로상의 $j$번째 노드를 $n(w,j)$라고 하고,
그 문맥 단어 벡터를 $\mathbf{u}_{n(w, j)}$라고 합시다.
예를 들어, :numref:<code>fig_hi_softmax</code>에서 $L(w_3) = 4$입니다.
계층적 소프트맥스는 :eqref:<code>eq_skip-gram-softmax</code>의 조건부 확률을 다음과 같이 근사합니다.</p>
<p>$$P(w_o \mid w_c) = \prod_{j=1}^{L(w_o)-1} \sigma\left( [![  n(w_o, j+1) = \textrm{leftChild}(n(w_o, j)) ]!] \cdot \mathbf{u}_{n(w_o, j)}^\top \mathbf{v}_c\right),$$</p>
<p>여기서 함수 $\sigma$는 :eqref:<code>eq_sigma-f</code>에 정의되어 있으며,
$\textrm{leftChild}(n)$은 노드 $n$의 왼쪽 자식 노드입니다. 만약 $x$가 참이면 $[[x]] = 1$; 그렇지 않으면 $[[x]] = -1$입니다.</p>
<p>설명하자면,
:numref:<code>fig_hi_softmax</code>에서 단어 $w_c$가 주어졌을 때
단어 $w_3$을 생성할 조건부 확률을 계산해 봅시다.
이를 위해서는 $w_c$의 단어 벡터 $\mathbf{v}_c$와
루트에서 $w_3$까지의 경로(:numref:<code>fig_hi_softmax</code>의 굵은 경로)에 있는 비-리프(non-leaf) 노드 벡터들 간의 내적이 필요하며,
이 경로는 왼쪽, 오른쪽, 그리고 왼쪽으로 이동합니다.</p>
<p>$$P(w_3 \mid w_c) = \sigma(\mathbf{u}_{n(w_3, 1)}^\top \mathbf{v}<em>c) \cdot \sigma(-\mathbf{u}</em>{n(w_3, 2)}^\top \mathbf{v}<em>c) \cdot \sigma(\mathbf{u}</em>{n(w_3, 3)}^\top \mathbf{v}_c).$$</p>
<p>$\sigma(x)+\sigma(-x) = 1$이므로,
임의의 단어 $w_c$를 기반으로
사전 $\mathcal{V}$의 모든 단어를 생성할
조건부 확률의 합은 1이 됩니다:</p>
<p>$$\sum_{w \in \mathcal{V}} P(w \mid w_c) = 1.$$
:eqlabel:<code>eq_hi-softmax-sum-one</code></p>
<p>다행히도 이진 트리 구조로 인해 $L(w_o)-1$은 $\mathcal{O}(\textrm{log}_2|\mathcal{V}|)$ 수준이므로,
사전 크기 $\mathcal{V}$가 매우 클 때,
계층적 소프트맥스를 사용하는 각 훈련 단계의 계산 비용은
근사 훈련을 사용하지 않는 경우에 비해
상당히 줄어듭니다.</p>
<h2 id="요약-summary-84"><a class="header" href="#요약-summary-84">요약 (Summary)</a></h2>
<ul>
<li>네거티브 샘플링은 긍정 예제와 부정 예제를 모두 포함하는 상호 독립적인 사건을 고려하여 손실 함수를 구성합니다. 훈련을 위한 계산 비용은 각 단계의 노이즈 단어 수에 선형적으로 의존합니다.</li>
<li>계층적 소프트맥스는 이진 트리에서 루트 노드로부터 리프 노드까지의 경로를 사용하여 손실 함수를 구성합니다. 훈련을 위한 계산 비용은 각 단계의 사전 크기의 로그에 의존합니다.</li>
</ul>
<h2 id="연습-문제-exercises-99"><a class="header" href="#연습-문제-exercises-99">연습 문제 (Exercises)</a></h2>
<ol>
<li>네거티브 샘플링에서 노이즈 단어를 어떻게 샘플링할 수 있습니까?</li>
<li>:eqref:<code>eq_hi-softmax-sum-one</code>이 성립함을 확인하십시오.</li>
<li>네거티브 샘플링과 계층적 소프트맥스를 사용하여 CBOW 모델을 각각 어떻게 훈련합니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/382">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="단어-임베딩-사전-훈련을-위한-데이터셋-the-dataset-for-pretraining-word-embeddings"><a class="header" href="#단어-임베딩-사전-훈련을-위한-데이터셋-the-dataset-for-pretraining-word-embeddings">단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings</a></h1>
<p>:label:<code>sec_word2vec_data</code></p>
<p>이제 word2vec 모델과 근사 훈련 방법의 기술적 세부 사항을 알았으니, 그 구현 과정을 살펴보겠습니다. 구체적으로, :numref:<code>sec_word2vec</code>의 스킵-그램(skip-gram) 모델과 :numref:<code>sec_approx_train</code>의 네거티브 샘플링(negative sampling)을 예로 들어 보겠습니다. 이 섹션에서는 단어 임베딩 모델 사전 훈련을 위한 데이터셋부터 시작합니다. 데이터의 원래 형식이 훈련 중에 반복해서 불러올 수 있는 미니배치(minibatches)로 변환될 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
import collections
from d2l import mxnet as d2l
import math
from mxnet import gluon, np
import os
import random
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
import collections
from d2l import torch as d2l
import math
import torch
import os
import random
</code></pre>
<h2 id="데이터셋-읽기-reading-the-dataset-10"><a class="header" href="#데이터셋-읽기-reading-the-dataset-10">데이터셋 읽기 (Reading the Dataset)</a></h2>
<p>여기서 사용할 데이터셋은 <a href="https://catalog.ldc.upenn.edu/LDC99T42">Penn Tree Bank (PTB)</a>입니다. 이 코퍼스(corpus)는 Wall Street Journal 기사에서 샘플링되었으며, 훈련, 검증, 테스트 세트로 나뉩니다. 원래 형식에서 텍스트 파일의 각 줄은 공백으로 구분된 단어들로 구성된 문장을 나타냅니다. 여기서는 각 단어를 하나의 토큰(token)으로 취급합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
                       '319d85e578af0cdc590547f26231e4e31cdf1e42')

#@save
def read_ptb():
    """PTB 데이터셋을 텍스트 줄의 리스트로 로드합니다."""
    data_dir = d2l.download_extract('ptb')
    # 훈련 세트 읽기
    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:
        raw_text = f.read()
    return [line.split() for line in raw_text.split('\n')]

sentences = read_ptb()
f'# 문장 수: {len(sentences)}'
</code></pre>
<p>훈련 세트를 읽은 후 코퍼스에 대한 어휘 사전(vocabulary)을 구축합니다. 여기서 10번 미만으로 나타나는 단어는 "<unk>" 토큰으로 대체됩니다. 원래 데이터셋에도 드문(알 수 없는) 단어를 나타내는 "<unk>" 토큰이 포함되어 있음에 유의하십시오.</p>
<pre><code class="language-{.python .input}">#@tab all
vocab = d2l.Vocab(sentences, min_freq=10)
f'어휘 사전 크기: {len(vocab)}'
</code></pre>
<h2 id="서브샘플링-subsampling"><a class="header" href="#서브샘플링-subsampling">서브샘플링 (Subsampling)</a></h2>
<p>텍스트 데이터에는 일반적으로 "the", "a", "in"과 같은 고빈도 단어가 있습니다. 이러한 단어들은 매우 큰 코퍼스에서 수십억 번 나타날 수도 있습니다. 그러나 이러한 단어들은 종종 문맥 윈도우(context windows) 내에서 많은 다른 단어들과 함께 나타나며, 유용한 신호를 거의 제공하지 않습니다. 예를 들어, 문맥 윈도우에서 "chip"이라는 단어를 고려해 보십시오. 직관적으로 고빈도 단어인 "a"와 함께 나타나는 것보다 저빈도 단어인 "intel"과 함께 나타나는 것이 훈련에 더 유용합니다. 게다가 엄청난 양의 (고빈도) 단어로 훈련하는 것은 느립니다. 따라서 단어 임베딩 모델을 훈련할 때 고빈도 단어는 *서브샘플링(subsampled)*될 수 있습니다 :cite:<code>Mikolov.Sutskever.Chen.ea.2013</code>. 구체적으로, 데이터셋의 각 인덱싱된 단어 $w_i$는 다음 확률로 폐기됩니다.</p>
<p>$$ P(w_i) = \max\left(1 - \sqrt{\frac{t}{f(w_i)}}, 0\right),$$</p>
<p>여기서 $f(w_i)$는 데이터셋의 총 단어 수에 대한 단어 $w_i$ 수의 비율이고, 상수 $t$는 하이퍼파라미터입니다(실험에서는 $10^{-4}$). 상대 빈도 $f(w_i) &gt; t$일 때만 (고빈도) 단어 $w_i$가 폐기될 수 있으며, 단어의 상대 빈도가 높을수록 폐기될 확률이 커짐을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def subsample(sentences, vocab):
    """고빈도 단어를 서브샘플링합니다."""
    # 알 수 없는 토큰('&lt;unk&gt;') 제외
    sentences = [[token for token in line if vocab[token] != vocab.unk]
                 for line in sentences]
    counter = collections.Counter([
        token for line in sentences for token in line])
    num_tokens = sum(counter.values())

    # 서브샘플링 중에 `token`이 유지되면 True 반환
    def keep(token):
        return(random.uniform(0, 1) &lt;
               math.sqrt(1e-4 / counter[token] * num_tokens))

    return ([[token for token in line if keep(token)] for line in sentences],
            counter)

subsampled, counter = subsample(sentences, vocab)
</code></pre>
<p>다음 코드 스니펫은 서브샘플링 전후의 문장당 토큰 수 히스토그램을 그립니다. 예상대로 서브샘플링은 고빈도 단어를 제거함으로써 문장을 크게 단축시키며, 이는 훈련 속도 향상으로 이어집니다.</p>
<pre><code class="language-{.python .input}">#@tab all
d2l.show_list_len_pair_hist(['원래 데이터', '서브샘플링됨'], '# 문장당 토큰 수',
                            '개수', sentences, subsampled);
</code></pre>
<p>개별 토큰의 경우, 고빈도 단어 "the"의 샘플링 속도는 1/20 미만입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def compare_counts(token):
    return (
            f'"{{token}}"의 수: '
            f'이전={sum([l.count(token) for l in sentences])}, '
            f'이후={sum([l.count(token) for l in subsampled])}')

compare_counts('the')
</code></pre>
<p>반면, 저빈도 단어 "join"은 완전히 유지됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
compare_counts('join')
</code></pre>
<p>서브샘플링 후, 토큰을 코퍼스의 인덱스로 매핑합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
corpus = [vocab[line] for line in subsampled]
corpus[:3]
</code></pre>
<h2 id="중심-단어와-문맥-단어-추출-extracting-center-words-and-context-words"><a class="header" href="#중심-단어와-문맥-단어-추출-extracting-center-words-and-context-words">중심 단어와 문맥 단어 추출 (Extracting Center Words and Context Words)</a></h2>
<p>다음 <code>get_centers_and_contexts</code> 함수는 <code>corpus</code>에서 모든 중심 단어와 그 문맥 단어들을 추출합니다. 이 함수는 1과 <code>max_window_size</code> 사이의 정수를 문맥 윈도우 크기로 균일하게 무작위 샘플링합니다. 임의의 중심 단어에 대해, 샘플링된 문맥 윈도우 크기를 초과하지 않는 거리에 있는 단어들이 문맥 단어가 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def get_centers_and_contexts(corpus, max_window_size):
    """스킵-그램에서의 중심 단어와 문맥 단어를 반환합니다."""
    centers, contexts = [], []
    for line in corpus:
        # "중심 단어-문맥 단어" 쌍을 형성하려면 각 문장에 최소 2개의 단어가 있어야 합니다
        if len(line) &lt; 2:
            continue
        centers += line
        for i in range(len(line)):  # `i`를 중심으로 하는 문맥 윈도우
            window_size = random.randint(1, max_window_size)
            indices = list(range(max(0, i - window_size),
                                 min(len(line), i + 1 + window_size)))
            # 문맥 단어에서 중심 단어 제외
            indices.remove(i)
            contexts.append([line[idx] for idx in indices])
    return centers, contexts
</code></pre>
<p>다음으로, 각각 7단어와 3단어인 두 문장을 포함하는 인공 데이터셋을 만듭니다. 최대 문맥 윈도우 크기를 2로 설정하고 모든 중심 단어와 그 문맥 단어들을 출력해 봅니다.</p>
<pre><code class="language-{.python .input}">#@tab all
tiny_dataset = [list(range(7)), list(range(7, 10))]
print('데이터셋', tiny_dataset)
for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):
    print('중심 단어', center, '의 문맥 단어:', context)
</code></pre>
<p>PTB 데이터셋에서 훈련할 때는 최대 문맥 윈도우 크기를 5로 설정합니다. 다음은 데이터셋의 모든 중심 단어와 그 문맥 단어들을 추출합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
all_centers, all_contexts = get_centers_and_contexts(corpus, 5)
f'# 중심 단어-문맥 단어 쌍의 수: {sum([len(contexts) for contexts in all_contexts])}'
</code></pre>
<h2 id="네거티브-샘플링-negative-sampling-1"><a class="header" href="#네거티브-샘플링-negative-sampling-1">네거티브 샘플링 (Negative Sampling)</a></h2>
<p>근사 훈련을 위해 네거티브 샘플링을 사용합니다. 미리 정의된 분포에 따라 노이즈 단어(noise words)를 샘플링하기 위해 다음 <code>RandomGenerator</code> 클래스를 정의합니다. 여기서 (아마도 정규화되지 않은) 샘플링 분포는 <code>sampling_weights</code> 인수를 통해 전달됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
class RandomGenerator:
    """n개의 샘플링 가중치에 따라 {1, ..., n} 중에서 무작위로 추출합니다."""
    def __init__(self, sampling_weights):
        # 제외 
        self.population = list(range(1, len(sampling_weights) + 1))
        self.sampling_weights = sampling_weights
        self.candidates = []
        self.i = 0

    def draw(self):
        if self.i == len(self.candidates):
            # `k`개의 무작위 샘플링 결과 캐시
            self.candidates = random.choices(
                self.population, self.sampling_weights, k=10000)
            self.i = 0
        self.i += 1
        return self.candidates[self.i - 1]
</code></pre>
<p>예를 들어, 샘플링 확률 $P(X=1)=2/9, P(X=2)=3/9, P(X=3)=4/9$인 인덱스 1, 2, 3 중에서 10개의 확률 변수 $X$를 다음과 같이 추출할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
generator = RandomGenerator([2, 3, 4])
[generator.draw() for _ in range(10)]
</code></pre>
<p>중심 단어와 문맥 단어 쌍에 대해, <code>K</code>(실험에서는 5)개의 노이즈 단어를 무작위로 샘플링합니다. word2vec 논문의 제안에 따라, 노이즈 단어 $w$의 샘플링 확률 $P(w)$는 사전에서의 상대 빈도에 0.75승을 한 값으로 설정됩니다 :cite:<code>Mikolov.Sutskever.Chen.ea.2013</code>.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def get_negatives(all_contexts, vocab, counter, K):
    """네거티브 샘플링에서의 노이즈 단어를 반환합니다."""
    # 어휘 사전에서 인덱스 1, 2, ... (인덱스 0은 제외된 알 수 없는 토큰)인 단어들에 대한 샘플링 가중치
    sampling_weights = [counter[vocab.to_tokens(i)]**0.75
                        for i in range(1, len(vocab))]
    all_negatives, generator = [], RandomGenerator(sampling_weights)
    for contexts in all_contexts:
        negatives = []
        while len(negatives) &lt; len(contexts) * K:
            neg = generator.draw()
            # 노이즈 단어는 문맥 단어가 될 수 없음
            if neg not in contexts:
                negatives.append(neg)
        all_negatives.append(negatives)
    return all_negatives

all_negatives = get_negatives(all_contexts, vocab, counter, 5)
</code></pre>
<h2 id="미니배치로-훈련-예제-로드하기-loading-training-examples-in-minibatches"><a class="header" href="#미니배치로-훈련-예제-로드하기-loading-training-examples-in-minibatches">미니배치로 훈련 예제 로드하기 (Loading Training Examples in Minibatches)</a></h2>
<p>:label:<code>subsec_word2vec-minibatch-loading</code></p>
<p>모든 중심 단어와 그 문맥 단어 및 샘플링된 노이즈 단어가 추출된 후, 이들은 훈련 중에 반복해서 로드될 수 있는 예제의 미니배치로 변환됩니다.</p>
<p>미니배치에서 $i^\textrm{th}$번째 예제는 중심 단어와 그 $n_i$개의 문맥 단어 및 $m_i$개의 노이즈 단어를 포함합니다. 문맥 윈도우 크기가 다르기 때문에 $n_i+m_i$는 $i$마다 다릅니다. 따라서 각 예제에 대해 <code>contexts_negatives</code> 변수에서 문맥 단어와 노이즈 단어를 연결하고, 연결된 길이가 $\max_i n_i+m_i$ (<code>max_len</code>)에 도달할 때까지 0을 추가(패딩)합니다. 손실 계산에서 패딩을 제외하기 위해 마스크 변수 <code>masks</code>를 정의합니다. <code>masks</code>의 요소와 <code>contexts_negatives</code>의 요소 사이에는 일대일 대응 관계가 있으며, <code>masks</code>의 0(그 외에는 1)은 <code>contexts_negatives</code>의 패딩에 대응합니다.</p>
<p>긍정 예제와 부정 예제를 구별하기 위해 <code>labels</code> 변수를 통해 <code>contexts_negatives</code>에서 문맥 단어와 노이즈 단어를 분리합니다. <code>masks</code>와 유사하게 <code>labels</code>의 요소와 <code>contexts_negatives</code>의 요소 사이에도 일대일 대응 관계가 있으며, <code>labels</code>의 1(그 외에는 0)은 <code>contexts_negatives</code>의 문맥 단어(긍정 예제)에 대응합니다.</p>
<p>위의 아이디어는 다음 <code>batchify</code> 함수에서 구현됩니다. 입력 <code>data</code>는 배치 크기와 동일한 길이를 가진 리스트이며, 각 요소는 중심 단어 <code>center</code>, 그 문맥 단어 <code>context</code>, 그리고 노이즈 단어 <code>negative</code>로 구성된 예제입니다. 이 함수는 마스크 변수를 포함하여 훈련 중에 계산을 위해 로드될 수 있는 미니배치를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def batchify(data):
    """네거티브 샘플링을 사용한 스킵-그램을 위한 예제 미니배치를 반환합니다."""
    max_len = max(len(c) + len(n) for _, c, n in data)
    centers, contexts_negatives, masks, labels = [], [], [], []
    for center, context, negative in data:
        cur_len = len(context) + len(negative)
        centers += [center]
        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]
        masks += [[1] * cur_len + [0] * (max_len - cur_len)]
        labels += [[1] * len(context) + [0] * (max_len - len(context))]
    return (d2l.reshape(d2l.tensor(centers), (-1, 1)), d2l.tensor(
        contexts_negatives), d2l.tensor(masks), d2l.tensor(labels))
</code></pre>
<p>두 개의 예제로 구성된 미니배치를 사용하여 이 함수를 테스트해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
x_1 = (1, [2, 2], [3, 3, 3, 3])
x_2 = (1, [2, 2, 2], [3, 3])
batch = batchify((x_1, x_2))

names = ['중심 단어', '문맥_부정_단어', '마스크', '레이블']
for name, data in zip(names, batch):
    print(name, '=', data)
</code></pre>
<h2 id="종합하기-putting-it-all-together-2"><a class="header" href="#종합하기-putting-it-all-together-2">종합하기 (Putting It All Together)</a></h2>
<p>마지막으로, PTB 데이터셋을 읽고 데이터 반복자와 어휘 사전을 반환하는 <code>load_data_ptb</code> 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def load_data_ptb(batch_size, max_window_size, num_noise_words):
    """PTB 데이터셋을 다운로드한 후 메모리로 로드합니다."""
    sentences = read_ptb()
    vocab = d2l.Vocab(sentences, min_freq=10)
    subsampled, counter = subsample(sentences, vocab)
    corpus = [vocab[line] for line in subsampled]
    all_centers, all_contexts = get_centers_and_contexts(
        corpus, max_window_size)
    all_negatives = get_negatives(
        all_contexts, vocab, counter, num_noise_words)
    dataset = gluon.data.ArrayDataset(
        all_centers, all_contexts, all_negatives)
    data_iter = gluon.data.DataLoader(
        dataset, batch_size, shuffle=True,batchify_fn=batchify,
        num_workers=d2l.get_dataloader_workers())
    return data_iter, vocab
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def load_data_ptb(batch_size, max_window_size, num_noise_words):
    """PTB 데이터셋을 다운로드한 후 메모리로 로드합니다."""
    num_workers = d2l.get_dataloader_workers()
    sentences = read_ptb()
    vocab = d2l.Vocab(sentences, min_freq=10)
    subsampled, counter = subsample(sentences, vocab)
    corpus = [vocab[line] for line in subsampled]
    all_centers, all_contexts = get_centers_and_contexts(
        corpus, max_window_size)
    all_negatives = get_negatives(
        all_contexts, vocab, counter, num_noise_words)

    class PTBDataset(torch.utils.data.Dataset):
        def __init__(self, centers, contexts, negatives):
            assert len(centers) == len(contexts) == len(negatives)
            self.centers = centers
            self.contexts = contexts
            self.negatives = negatives

        def __getitem__(self, index):
            return (self.centers[index], self.contexts[index],
                    self.negatives[index])

        def __len__(self):
            return len(self.centers)

    dataset = PTBDataset(all_centers, all_contexts, all_negatives)

    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,
                                      collate_fn=batchify,
                                      num_workers=num_workers)
    return data_iter, vocab
</code></pre>
<p>데이터 반복자의 첫 번째 미니배치를 출력해 봅니다.</p>
<pre><code class="language-{.python .input}">#@tab all
data_iter, vocab = load_data_ptb(512, 5, 5)
for batch in data_iter:
    for name, data in zip(names, batch):
        print(name, '모양:', data.shape)
    break
</code></pre>
<h2 id="요약-summary-85"><a class="header" href="#요약-summary-85">요약 (Summary)</a></h2>
<ul>
<li>고빈도 단어는 훈련에 그렇게 유용하지 않을 수 있습니다. 훈련 속도를 높이기 위해 이를 서브샘플링할 수 있습니다.</li>
<li>계산 효율성을 위해 예제를 미니배치로 로드합니다. 패딩과 패딩이 아닌 것을 구별하고, 긍정 예제와 부정 예제를 구별하기 위해 다른 변수들을 정의할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-100"><a class="header" href="#연습-문제-exercises-100">연습 문제 (Exercises)</a></h2>
<ol>
<li>서브샘플링을 사용하지 않을 경우 이 섹션의 코드 실행 시간이 어떻게 변합니까?</li>
<li><code>RandomGenerator</code> 클래스는 <code>k</code>개의 무작위 샘플링 결과를 캐시합니다. <code>k</code>를 다른 값으로 설정해 보고 데이터 로딩 속도에 어떤 영향을 미치는지 확인해 보십시오.</li>
<li>이 섹션의 코드에서 데이터 로딩 속도에 영향을 줄 수 있는 다른 하이퍼파라미터는 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/383">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1330">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="word2vec-사전-훈련-pretraining-word2vec"><a class="header" href="#word2vec-사전-훈련-pretraining-word2vec">word2vec 사전 훈련 (Pretraining word2vec)</a></h1>
<p>:label:<code>sec_word2vec_pretraining</code></p>
<p>우리는 :numref:<code>sec_word2vec</code>에 정의된
스킵-그램 모델을 구현하기 위해 계속 진행합니다.
그런 다음 PTB 데이터셋에서 네거티브 샘플링을 사용하여
word2vec을 사전 훈련할 것입니다.
우선,
:numref:<code>sec_word2vec_data</code>에서 설명한
<code>d2l.load_data_ptb</code> 함수를 호출하여
이 데이터셋에 대한 데이터 반복자와 어휘를 얻습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
import math
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
npx.set_np()

batch_size, max_window_size, num_noise_words = 512, 5, 5
data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,
                                     num_noise_words)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import math
import torch
from torch import nn

batch_size, max_window_size, num_noise_words = 512, 5, 5
data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,
                                     num_noise_words)
</code></pre>
<h2 id="스킵-그램-모델-the-skip-gram-model"><a class="header" href="#스킵-그램-모델-the-skip-gram-model">스킵-그램 모델 (The Skip-Gram Model)</a></h2>
<p>우리는 임베딩 레이어와 배치 행렬 곱셈을 사용하여
스킵-그램 모델을 구현합니다.
먼저, 임베딩 레이어가 어떻게 작동하는지 검토해 봅시다.</p>
<h3 id="임베딩-레이어-embedding-layer"><a class="header" href="#임베딩-레이어-embedding-layer">임베딩 레이어 (Embedding Layer)</a></h3>
<p>:numref:<code>sec_seq2seq</code>에서 설명한 대로,
임베딩 레이어는 토큰의 인덱스를 특징 벡터에 매핑합니다.
이 레이어의 가중치는
행 수가 사전 크기(<code>input_dim</code>)와 같고
열 수가 각 토큰의 벡터 차원(<code>output_dim</code>)과 같은 행렬입니다.
단어 임베딩 모델이 훈련된 후,
이 가중치가 우리에게 필요한 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
embed = nn.Embedding(input_dim=20, output_dim=4)
embed.initialize()
embed.weight
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
embed = nn.Embedding(num_embeddings=20, embedding_dim=4)
print(f'Parameter embedding_weight ({embed.weight.shape}, '
      f'dtype={embed.weight.dtype})')
</code></pre>
<p>임베딩 레이어의 입력은 토큰(단어)의 인덱스입니다.
임의의 토큰 인덱스 $i$에 대해,
그 벡터 표현은
임베딩 레이어의 가중치 행렬의 $i$번째 행에서
얻을 수 있습니다.
벡터 차원(<code>output_dim</code>)이 4로 설정되었으므로,
임베딩 레이어는
(2, 3) 모양의 토큰 인덱스 미니배치에 대해
(2, 3, 4) 모양의 벡터를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
x = d2l.tensor([[1, 2, 3], [4, 5, 6]])
embed(x)
</code></pre>
<h3 id="순전파-정의-defining-the-forward-propagation"><a class="header" href="#순전파-정의-defining-the-forward-propagation">순전파 정의 (Defining the Forward Propagation)</a></h3>
<p>순전파에서, 스킵-그램 모델의 입력은
(배치 크기, 1) 모양의 중심 단어 인덱스 <code>center</code>와
(배치 크기, <code>max_len</code>) 모양의 연결된 문맥 및 노이즈 단어 인덱스 <code>contexts_and_negatives</code>를 포함합니다.
여기서 <code>max_len</code>은 :numref:<code>subsec_word2vec-minibatch-loading</code>에 정의되어 있습니다.
이 두 변수는 먼저 임베딩 레이어를 통해
토큰 인덱스에서 벡터로 변환된 다음,
배치 행렬 곱셈(:numref:<code>subsec_batch_dot</code>에 설명됨)을 통해
(배치 크기, 1, <code>max_len</code>) 모양의 출력을 반환합니다.
출력의 각 요소는 중심 단어 벡터와 문맥 또는 노이즈 단어 벡터의 내적입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def skip_gram(center, contexts_and_negatives, embed_v, embed_u):
    v = embed_v(center)
    u = embed_u(contexts_and_negatives)
    pred = npx.batch_dot(v, u.swapaxes(1, 2))
    return pred
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def skip_gram(center, contexts_and_negatives, embed_v, embed_u):
    v = embed_v(center)
    u = embed_u(contexts_and_negatives)
    pred = torch.bmm(v, u.permute(0, 2, 1))
    return pred
</code></pre>
<p>몇 가지 예제 입력에 대해 이 <code>skip_gram</code> 함수의 출력 모양을 인쇄해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
skip_gram(np.ones((2, 1)), np.ones((2, 4)), embed, embed).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
skip_gram(torch.ones((2, 1), dtype=torch.long),
          torch.ones((2, 4), dtype=torch.long), embed, embed).shape
</code></pre>
<h2 id="훈련-training-28"><a class="header" href="#훈련-training-28">훈련 (Training)</a></h2>
<p>네거티브 샘플링으로 스킵-그램 모델을 훈련하기 전에,
먼저 손실 함수를 정의해 봅시다.</p>
<h3 id="이진-크로스-엔트로피-손실-binary-cross-entropy-loss"><a class="header" href="#이진-크로스-엔트로피-손실-binary-cross-entropy-loss">이진 크로스 엔트로피 손실 (Binary Cross-Entropy Loss)</a></h3>
<p>:numref:<code>subsec_negative-sampling</code>의 네거티브 샘플링에 대한
손실 함수의 정의에 따라,
우리는 이진 크로스 엔트로피 손실을 사용할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
loss = gluon.loss.SigmoidBCELoss()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class SigmoidBCELoss(nn.Module):
    # 마스킹이 있는 이진 크로스 엔트로피 손실
    def __init__(self):
        super().__init__()

    def forward(self, inputs, target, mask=None):
        out = nn.functional.binary_cross_entropy_with_logits(
            inputs, target, weight=mask, reduction="none")
        return out.mean(dim=1)

loss = SigmoidBCELoss()
</code></pre>
<p>:numref:<code>subsec_word2vec-minibatch-loading</code>에서의
마스크 변수와 레이블 변수에 대한 설명을 상기하십시오.
다음은 주어진 변수에 대해
이진 크로스 엔트로피 손실을 계산합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
pred = d2l.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)
label = d2l.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])
mask = d2l.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])
loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)
</code></pre>
<p>아래는 이진 크로스 엔트로피 손실에서
시그모이드 활성화 함수를 사용하여
위의 결과가 (덜 효율적인 방식으로) 어떻게 계산되는지 보여줍니다.
우리는 두 출력을 마스킹되지 않은 예측에 대해 평균화된
두 개의 정규화된 손실로 간주할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def sigmd(x):
    return -math.log(1 / (1 + math.exp(-x)))

print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')
print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')
</code></pre>
<h3 id="모델-파라미터-초기화-initializing-model-parameters-2"><a class="header" href="#모델-파라미터-초기화-initializing-model-parameters-2">모델 파라미터 초기화 (Initializing Model Parameters)</a></h3>
<p>우리는 사전에 있는 모든 단어에 대해
각각 중심 단어와 문맥 단어로 사용될 때를 위한
두 개의 임베딩 레이어를 정의합니다.
단어 벡터 차원 <code>embed_size</code>는 100으로 설정됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
embed_size = 100
net = nn.Sequential()
net.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),
        nn.Embedding(input_dim=len(vocab), output_dim=embed_size))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
embed_size = 100
net = nn.Sequential(nn.Embedding(num_embeddings=len(vocab),
                                 embedding_dim=embed_size),
                    nn.Embedding(num_embeddings=len(vocab),
                                 embedding_dim=embed_size))
</code></pre>
<h3 id="훈련-루프-정의-defining-the-training-loop"><a class="header" href="#훈련-루프-정의-defining-the-training-loop">훈련 루프 정의 (Defining the Training Loop)</a></h3>
<p>훈련 루프는 아래와 같이 정의됩니다. 패딩의 존재로 인해 손실 함수 계산은 이전 훈련 함수와 비교하여 약간 다릅니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):
    net.initialize(ctx=device, force_reinit=True)
    trainer = gluon.Trainer(net.collect_params(), 'adam',
                            {'learning_rate': lr})
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[1, num_epochs])
    # 정규화된 손실의 합, 정규화된 손실의 수
    metric = d2l.Accumulator(2)
    for epoch in range(num_epochs):
        timer, num_batches = d2l.Timer(), len(data_iter)
        for i, batch in enumerate(data_iter):
            center, context_negative, mask, label = [
                data.as_in_ctx(device) for data in batch]
            with autograd.record():
                pred = skip_gram(center, context_negative, net[0], net[1])
                l = (loss(pred.reshape(label.shape), label, mask) *
                     mask.shape[1] / mask.sum(axis=1))
            l.backward()
            trainer.step(batch_size)
            metric.add(l.sum(), l.size)
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[1],))
    print(f'loss {metric[0] / metric[1]:.3f}, '
          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):
    def init_weights(module):
        if type(module) == nn.Embedding:
            nn.init.xavier_uniform_(module.weight)
    net.apply(init_weights)
    net = net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[1, num_epochs])
    # 정규화된 손실의 합, 정규화된 손실의 수
    metric = d2l.Accumulator(2)
    for epoch in range(num_epochs):
        timer, num_batches = d2l.Timer(), len(data_iter)
        for i, batch in enumerate(data_iter):
            optimizer.zero_grad()
            center, context_negative, mask, label = [
                data.to(device) for data in batch]

            pred = skip_gram(center, context_negative, net[0], net[1])
            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)
                     / mask.sum(axis=1) * mask.shape[1])
            l.sum().backward()
            optimizer.step()
            metric.add(l.sum(), l.numel())
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[1],))
    print(f'loss {metric[0] / metric[1]:.3f}, '
          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')
</code></pre>
<p>이제 네거티브 샘플링을 사용하여 스킵-그램 모델을 훈련할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
lr, num_epochs = 0.002, 5
train(net, data_iter, lr, num_epochs)
</code></pre>
<h2 id="단어-임베딩-적용-applying-word-embeddings"><a class="header" href="#단어-임베딩-적용-applying-word-embeddings">단어 임베딩 적용 (Applying Word Embeddings)</a></h2>
<p>:label:<code>subsec_apply-word-embed</code></p>
<p>word2vec 모델을 훈련한 후,
훈련된 모델의 단어 벡터의 코사인 유사도를 사용하여
입력 단어와 의미적으로 가장 유사한 단어를
사전에서 찾을 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def get_similar_tokens(query_token, k, embed):
    W = embed.weight.data()
    x = W[vocab[query_token]]
    # 코사인 유사도를 계산합니다. 수치적 안정성을 위해 1e-9를 더합니다
    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)
    topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')
    for i in topk[1:]:  # 입력 단어를 제거합니다
        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')

get_similar_tokens('chip', 3, net[0])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def get_similar_tokens(query_token, k, embed):
    W = embed.weight.data
    x = W[vocab[query_token]]
    # 코사인 유사도를 계산합니다. 수치적 안정성을 위해 1e-9를 더합니다
    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *
                                      torch.sum(x * x) + 1e-9)
    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')
    for i in topk[1:]:  # 입력 단어를 제거합니다
        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')

get_similar_tokens('chip', 3, net[0])
</code></pre>
<h2 id="요약-summary-86"><a class="header" href="#요약-summary-86">요약 (Summary)</a></h2>
<ul>
<li>임베딩 레이어와 이진 크로스 엔트로피 손실을 사용하여 네거티브 샘플링으로 스킵-그램 모델을 훈련할 수 있습니다.</li>
<li>단어 임베딩의 응용 분야에는 단어 벡터의 코사인 유사도를 기반으로 주어진 단어와 의미적으로 유사한 단어를 찾는 것이 포함됩니다.</li>
</ul>
<h2 id="연습-문제-exercises-101"><a class="header" href="#연습-문제-exercises-101">연습 문제 (Exercises)</a></h2>
<ol>
<li>훈련된 모델을 사용하여 다른 입력 단어에 대해 의미적으로 유사한 단어를 찾으십시오. 하이퍼파라미터를 조정하여 결과를 개선할 수 있습니까?</li>
<li>훈련 코퍼스가 매우 클 때, 우리는 종종 <em>모델 파라미터를 업데이트할 때</em> 현재 미니배치의 중심 단어에 대해 문맥 단어와 노이즈 단어를 샘플링합니다. 즉, 동일한 중심 단어라도 훈련 에포크마다 다른 문맥 단어나 노이즈 단어를 가질 수 있습니다. 이 방법의 이점은 무엇입니까? 이 훈련 방법을 구현해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/384">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1335">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="glove를-이용한-단어-임베딩-word-embedding-with-global-vectors-glove"><a class="header" href="#glove를-이용한-단어-임베딩-word-embedding-with-global-vectors-glove">GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></h1>
<p>:label:<code>sec_glove</code></p>
<p>문맥 윈도우 내의 단어-단어 공생(co-occurrences)은 풍부한 의미론적 정보를 담고 있을 수 있습니다. 예를 들어 대규모 코퍼스에서 "solid"라는 단어는 "steam"보다 "ice"와 함께 나타날 가능성이 높지만, "gas"라는 단어는 아마도 "ice"보다 "steam"과 더 자주 함께 나타날 것입니다. 게다가 이러한 공생에 대한 글로벌 코퍼스 통계는 미리 계산될 수 있으며, 이는 더 효율적인 훈련으로 이어질 수 있습니다. 단어 임베딩을 위해 전체 코퍼스의 통계 정보를 활용하기 위해, 먼저 :numref:<code>subsec_skip-gram</code>의 스킵-그램 모델을 공생 횟수와 같은 글로벌 코퍼스 통계를 사용하여 해석하는 것부터 다시 살펴보겠습니다.</p>
<h2 id="글로벌-코퍼스-통계를-이용한-스킵-그램-skip-gram-with-global-corpus-statistics"><a class="header" href="#글로벌-코퍼스-통계를-이용한-스킵-그램-skip-gram-with-global-corpus-statistics">글로벌 코퍼스 통계를 이용한 스킵-그램 (Skip-Gram with Global Corpus Statistics)</a></h2>
<p>:label:<code>subsec_skipgram-global</code></p>
<p>스킵-그램 모델에서 단어 $w_i$가 주어졌을 때 단어 $w_j$의 조건부 확률 $P(w_j
mid w_i)$를 $q_{ij}$라고 표시하면 다음과 같습니다.</p>
<p>$$q_{ij}=\frac{\exp(\mathbf{u}_j^\top \mathbf{v}<em>i)}{ \sum</em>{k \in \mathcal{V}} \exp(\mathbf{u}_k^\top \mathbf{v}_i)},$$</p>
<p>여기서 임의의 인덱스 $i$에 대해 벡터 $\mathbf{v}_i$와 $\mathbf{u}_i$는 각각 단어 $w_i$를 중심 단어와 문맥 단어로 나타내며, $\mathcal{V} = {0, 1, \ldots, |\mathcal{V}|-1}$은 어휘의 인덱스 집합입니다.</p>
<p>코퍼스에서 여러 번 나타날 수 있는 단어 $w_i$를 고려해 봅시다. 전체 코퍼스에서 $w_i$가 중심 단어로 사용된 모든 문맥 윈도우에서 나타나는 문맥 단어들은 <em>동일한 요소의 여러 인스턴스를 허용하는</em> 단어 인덱스의 <em>멀티집합(multiset)</em> $\mathcal{C}_i$를 형성합니다. 임의의 요소에 대해 그 인스턴스 수를 *중복도(multiplicity)*라고 합니다. 예를 들어 설명하자면, 단어 $w_i$가 코퍼스에서 두 번 나타나고 두 문맥 윈도우에서 $w_i$를 중심 단어로 취하는 문맥 단어의 인덱스가 $k, j, m, k$ 및 $k, l, k, j$라고 가정해 봅시다. 그러면 멀티집합 $\mathcal{C}_i = {j, j, k, k, k, k, l, m}$이며, 요소 $j, k, l, m$의 중복도는 각각 2, 4, 1, 1입니다.</p>
<p>이제 멀티집합 $\mathcal{C}<em>i$에서 요소 $j$의 중복도를 $x</em>{ij}$라고 표시합시다. 이는 전체 코퍼스에서 동일한 문맥 윈도우에 있는 단어 $w_j$(문맥 단어로)와 단어 $w_i$(중심 단어로)의 글로벌 공생 횟수입니다. 이러한 글로벌 코퍼스 통계를 사용하면 스킵-그램 모델의 손실 함수는 다음과 동등합니다.</p>
<p>$$-\sum_{i\in\mathcal{V}}\sum_{j\in\mathcal{V}} x_{ij} \log,q_{ij}.$$ :eqlabel:<code>eq_skipgram-x_ij</code></p>
<p>우리는 또한 $w_i$가 중심 단어로 나타나는 문맥 윈도우 내의 모든 문맥 단어의 수를 $x_i$로 표시하며, 이는 $|\mathcal{C}<em>i|$와 같습니다. 중심 단어 $w_i$가 주어졌을 때 문맥 단어 $w_j$를 생성할 조건부 확률 $x</em>{ij}/x_i$를 $p_{ij}$라고 하면, :eqref:<code>eq_skipgram-x_ij</code>는 다음과 같이 다시 쓸 수 있습니다.</p>
<p>$$-\sum_{i\in\mathcal{V}} x_i \sum_{j\in\mathcal{V}} p_{ij} \log,q_{ij}.$$ :eqlabel:<code>eq_skipgram-p_ij</code></p>
<p>:eqref:<code>eq_skipgram-p_ij</code>에서 $-\sum_{j\in\mathcal{V}} p_{ij} \log,q_{ij}$는 글로벌 코퍼스 통계의 조건부 분포 $p_{ij}$와 모델 예측의 조건부 분포 $q_{ij}$ 사이의 크로스 엔트로피를 계산합니다. 이 손실은 위에서 설명한 대로 $x_i$에 의해 가중치가 부여됩니다. :eqref:<code>eq_skipgram-p_ij</code>의 손실 함수를 최소화하면 예측된 조건부 분포가 글로벌 코퍼스 통계의 조건부 분포에 가까워질 수 있습니다.</p>
<p>확률 분포 간의 거리를 측정하는 데 흔히 사용되지만, 크로스 엔트로피 손실 함수는 여기서 좋은 선택이 아닐 수 있습니다. 한편으로 :numref:<code>sec_approx_train</code>에서 언급했듯이, $q_{ij}$를 적절하게 정규화하는 비용은 전체 어휘에 대한 합산으로 이어져 계산 비용이 많이 들 수 있습니다. 다른 한편으로 대규모 코퍼스에서 발생하는 수많은 드문 사건들은 크로스 엔트로피 손실에 의해 종종 너무 많은 가중치가 할당되도록 모델링됩니다.</p>
<h2 id="glove-모델-the-glove-model"><a class="header" href="#glove-모델-the-glove-model">GloVe 모델 (The GloVe Model)</a></h2>
<p>이러한 점을 고려하여 <em>GloVe</em> 모델은 제곱 손실을 기반으로 스킵-그램 모델에 세 가지 변경을 가했습니다 :cite:<code>Pennington.Socher.Manning.2014</code>:</p>
<ol>
<li>확률 분포가 아닌 변수 $p'<em>{ij}=x</em>{ij}$와 $q'_{ij}=\exp(\mathbf{u}<em>j^\top \mathbf{v}<em>i)$를 사용하고 두 변수 모두에 로그를 취하므로, 제곱 손실 항은 $\left(\log,p'</em>{ij} - \log,q'</em>{ij}\right)^2 = \left(\mathbf{u}_j^\top \mathbf{v}<em>i - \log,x</em>{ij}\right)^2$이 됩니다.</li>
<li>각 단어 $w_i$에 대해 두 개의 스칼라 모델 파라미터인 중심 단어 편향 $b_i$와 문맥 단어 편향 $c_i$를 추가합니다.</li>
<li>각 손실 항의 가중치를 가중치 함수 $h(x_{ij})$로 대체합니다. 여기서 $h(x)$는 구간 $[0, 1]$에서 증가하는 함수입니다.</li>
</ol>
<p>모든 것을 종합하면, GloVe를 훈련하는 것은 다음 손실 함수를 최소화하는 것입니다:</p>
<p>$$\sum_{i\in\mathcal{V}} \sum_{j\in\mathcal{V}} h(x_{ij}) \left(\mathbf{u}_j^\top \mathbf{v}<em>i + b_i + c_j - \log,x</em>{ij}\right)^2.$$ :eqlabel:<code>eq_glove-loss</code></p>
<p>가중치 함수에 대한 권장 선택은 다음과 같습니다: $x &lt; c$ (예: $c = 100$)이면 $h(x) = (x/c) ^\alpha$ (예: $\alpha = 0.75$)이고, 그렇지 않으면 $h(x) = 1$입니다. 이 경우 $h(0)=0$이므로 $x_{ij}=0$인 임의의 제곱 손실 항은 계산 효율성을 위해 생략될 수 있습니다. 예를 들어 훈련을 위해 미니배치 확률적 경사 하강법을 사용할 때, 각 반복에서 <em>0이 아닌</em> $x_{ij}$의 미니배치를 무작위로 샘플링하여 기울기를 계산하고 모델 파라미터를 업데이트합니다. 이러한 0이 아닌 $x_{ij}$는 미리 계산된 글로벌 코퍼스 통계이므로, 이 모델을 글로벌 벡터를 의미하는 GloVe(Global Vectors)라고 부릅니다.</p>
<p>단어 $w_i$가 단어 $w_j$의 문맥 윈도우에 나타나면 그 반대도 마찬가지라는 점을 강조해야 합니다. 따라서 $x_{ij}=x_{ji}$입니다. 비대칭 조건부 확률 $p_{ij}$를 피팅하는 word2vec과 달리 GloVe는 대칭인 $\log , x_{ij}$를 피팅합니다. 따라서 GloVe 모델에서 임의의 단어의 중심 단어 벡터와 문맥 단어 벡터는 수학적으로 동등합니다. 그러나 실제로는 초기화 값이 다르기 때문에 훈련 후에 동일한 단어라도 이 두 벡터에서 서로 다른 값을 가질 수 있습니다. GloVe는 이들을 더하여 출력 벡터로 사용합니다.</p>
<h2 id="공생-확률의-비율을-통한-glove-해석-interpreting-glove-from-the-ratio-of-co-occurrence-probabilities"><a class="header" href="#공생-확률의-비율을-통한-glove-해석-interpreting-glove-from-the-ratio-of-co-occurrence-probabilities">공생 확률의 비율을 통한 GloVe 해석 (Interpreting GloVe from the Ratio of Co-occurrence Probabilities)</a></h2>
<p>우리는 또한 다른 관점에서 GloVe 모델을 해석할 수 있습니다. :numref:<code>subsec_skipgram-global</code>과 동일한 표기법을 사용하여, $p_{ij} \stackrel{\textrm{def}}{=} P(w_j \mid w_i)$를 코퍼스에서 중심 단어 $w_i$가 주어졌을 때 문맥 단어 $w_j$를 생성할 조건부 확률이라고 합시다. :numref:<code>tab_glove</code>는 대규모 코퍼스의 통계를 기반으로 단어 "ice"와 "steam"이 주어졌을 때의 몇 가지 공생 확률과 그 비율을 보여줍니다.</p>
<p>:대규모 코퍼스의 단어-단어 공생 확률 및 그 비율 (:citet:<code>Pennington.Socher.Manning.2014</code>의 표 1 수정)
:label:<code>tab_glove</code></p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">$w_k$=</th><th style="text-align: left">solid</th><th style="text-align: left">gas</th><th style="text-align: left">water</th><th style="text-align: left">fashion</th></tr></thead><tbody>
<tr><td style="text-align: left">$p_1=P(w_k</td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">mid \textrm{ice})$</td><td style="text-align: left">0.00019</td><td style="text-align: left">0.000066</td><td style="text-align: left">0.003</td><td style="text-align: left">0.000017</td></tr>
<tr><td style="text-align: left">$p_2=P(w_k</td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">mid\textrm{steam})$</td><td style="text-align: left">0.000022</td><td style="text-align: left">0.00078</td><td style="text-align: left">0.0022</td><td style="text-align: left">0.000018</td></tr>
<tr><td style="text-align: left">$p_1/p_2$</td><td style="text-align: left">8.9</td><td style="text-align: left">0.085</td><td style="text-align: left">1.36</td><td style="text-align: left">0.96</td></tr>
</tbody></table>
</div>
<p>:numref:<code>tab_glove</code>에서 다음과 같은 사실을 관찰할 수 있습니다:</p>
<ul>
<li>"ice"와 관련이 있지만 "steam"과는 관련이 없는 단어 $w_k$ (예: $w_k=\textrm{solid}$)의 경우, 8.9와 같이 더 큰 공생 확률 비율을 기대합니다.</li>
<li>"steam"과 관련이 있지만 "ice"와는 관련이 없는 단어 $w_k$ (예: $w_k=\textrm{gas}$)의 경우, 0.085와 같이 더 작은 공생 확률 비율을 기대합니다.</li>
<li>"ice"와 "steam" 모두와 관련이 있는 단어 $w_k$ (예: $w_k=\textrm{water}$)의 경우, 1.36과 같이 1에 가까운 공생 확률 비율을 기대합니다.</li>
<li>"ice"와 "steam" 모두와 관련이 없는 단어 $w_k$ (예: $w_k=\textrm{fashion}$)의 경우, 0.96과 같이 1에 가까운 공생 확률 비율을 기대합니다.</li>
</ul>
<p>공생 확률의 비율이 단어 간의 관계를 직관적으로 표현할 수 있음을 알 수 있습니다. 따라서 우리는 이 비율을 피팅하기 위해 세 단어 벡터의 함수를 설계할 수 있습니다. $w_i$가 중심 단어이고 $w_j$와 $w_k$가 문맥 단어일 때의 공생 확률 비율 ${p_{ij}}/{p_{ik}}$에 대해, 어떤 함수 $f$를 사용하여 이 비율을 피팅하고자 합니다:</p>
<p>$$f(\mathbf{u}_j, \mathbf{u}<em>k, {\mathbf{v}}<em>i) \approx \frac{p</em>{ij}}{p</em>{ik}}.$$ :eqlabel:<code>eq_glove-f</code></p>
<p>$f$에 대한 많은 가능한 설계 중에서 여기서는 합리적인 선택 하나만 고릅니다. 공생 확률의 비율이 스칼라이므로 $f$가 $f(\mathbf{u}_j, \mathbf{u}_k, {\mathbf{v}}_i) = f\left((\mathbf{u}_j - \mathbf{u}_k)^\top {\mathbf{v}}_i\right)$와 같은 스칼라 함수여야 함을 요구합니다. :eqref:<code>eq_glove-f</code>에서 단어 인덱스 $j$와 $k$를 바꾸면 $f(x)f(-x)=1$이 성립해야 하므로, 한 가지 가능성은 $f(x)=\exp(x)$입니다. 즉,</p>
<p>$$f(\mathbf{u}_j, \mathbf{u}_k, {\mathbf{v}}_i) = \frac{\exp\left(\mathbf{u}_j^\top {\mathbf{v}}_i\right)}{\exp\left(\mathbf{u}<em>k^\top {\mathbf{v}}<em>i\right)} \approx \frac{p</em>{ij}}{p</em>{ik}}.$$</p>
<p>이제 $\exp\left(\mathbf{u}<em>j^\top {\mathbf{v}}<em>i\right) \approx \alpha p</em>{ij}$라고 합시다. 여기서 $\alpha$는 상수입니다. $p</em>{ij}=x_{ij}/x_i$이므로 양변에 로그를 취하면 $\mathbf{u}_j^\top {\mathbf{v}}<em>i \approx \log,\alpha + \log,x</em>{ij} - \log,x_i$를 얻습니다. 우리는 중심 단어 편향 $b_i$와 문맥 단어 편향 $c_j$와 같은 추가적인 편향 항을 사용하여 $- \log, \alpha + \log, x_i$를 피팅할 수 있습니다:</p>
<p>$$\mathbf{u}_j^\top \mathbf{v}<em>i + b_i + c_j \approx \log, x</em>{ij}.$$ :eqlabel:<code>eq_glove-square</code></p>
<p>가중치를 부여하여 :eqref:<code>eq_glove-square</code>의 제곱 오차를 측정하면 :eqref:<code>eq_glove-loss</code>의 GloVe 손실 함수를 얻게 됩니다.</p>
<h2 id="요약-summary-87"><a class="header" href="#요약-summary-87">요약 (Summary)</a></h2>
<ul>
<li>스킵-그램 모델은 단어-단어 공생 횟수와 같은 글로벌 코퍼스 통계를 사용하여 해석될 수 있습니다.</li>
<li>크로스 엔트로피 손실은 두 확률 분포 사이의 차이를 측정하는 데, 특히 대규모 코퍼스에 대해서는 좋은 선택이 아닐 수 있습니다. GloVe는 미리 계산된 글로벌 코퍼스 통계를 피팅하기 위해 제곱 손실을 사용합니다.</li>
<li>GloVe에서 임의의 단어의 중심 단어 벡터와 문맥 단어 벡터는 수학적으로 동등합니다.</li>
<li>GloVe는 단어-단어 공생 확률의 비율로부터 해석될 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-102"><a class="header" href="#연습-문제-exercises-102">연습 문제 (Exercises)</a></h2>
<ol>
<li>단어 $w_i$와 $w_j$가 동일한 문맥 윈도우에서 공생하는 경우, 텍스트 시퀀스에서의 그들 사이의 거리를 사용하여 조건부 확률 $p_{ij}$를 계산하는 방법을 어떻게 다시 설계할 수 있을까요? 힌트: GloVe 논문 :cite:<code>Pennington.Socher.Manning.2014</code>의 섹션 4.2를 참조하십시오.</li>
<li>임의의 단어에 대해 GloVe에서 중심 단어 편향과 문맥 단어 편향이 수학적으로 동등합니까? 그 이유는 무엇입니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/385">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="하위-단어-임베딩-subword-embedding"><a class="header" href="#하위-단어-임베딩-subword-embedding">하위 단어 임베딩 (Subword Embedding)</a></h1>
<p>:label:<code>sec_fasttext</code></p>
<p>영어에서,
"helps", "helped", "helping"과 같은 단어들은
동일한 단어 "help"의 굴절 형태입니다.
"dog"와 "dogs" 사이의 관계는
"cat"과 "cats" 사이의 관계와 같고,
"boy"와 "boyfriend" 사이의 관계는
"girl"과 "girlfriend" 사이의 관계와 같습니다.
프랑스어나 스페인어와 같은 다른 언어에서는,
많은 동사가 40개 이상의 굴절 형태를 가지며,
핀란드어에서는 명사가 최대 15개의 격(case)을 가질 수 있습니다.
언어학에서 형태론(morphology)은 단어 형성과 단어 관계를 연구합니다.
그러나
단어의 내부 구조는
word2vec에서도 GloVe에서도 탐구되지 않았습니다.</p>
<h2 id="fasttext-모델-the-fasttext-model"><a class="header" href="#fasttext-모델-the-fasttext-model">fastText 모델 (The fastText Model)</a></h2>
<p>word2vec에서 단어가 어떻게 표현되는지 상기해 보십시오.
스킵-그램 모델과 CBOW 모델 모두에서,
동일한 단어의 다른 굴절 형태는
공유 파라미터 없이
서로 다른 벡터로 직접 표현됩니다.
형태학적 정보를 사용하기 위해,
<em>fastText</em> 모델은
<em>하위 단어 임베딩(subword embedding)</em> 접근 방식을 제안했습니다.
여기서 하위 단어는 문자 $n$-그램입니다 :cite:<code>Bojanowski.Grave.Joulin.ea.2017</code>.
단어 수준 벡터 표현을 학습하는 대신,
fastText는 하위 단어 수준 스킵-그램으로 간주될 수 있으며,
여기서 각 <em>중심 단어</em>는
그 하위 단어 벡터들의 합으로 표현됩니다.</p>
<p>"where"라는 단어를 사용하여
fastText에서 각 중심 단어에 대한 하위 단어를 얻는 방법을 설명해 보겠습니다.
먼저, 접두사와 접미사를 다른 하위 단어와 구별하기 위해
단어의 시작과 끝에 특수 문자 “&lt;”와 “&gt;”를 추가합니다.
그런 다음 단어에서 문자 $n$-그램을 추출합니다.
예를 들어 $n=3$일 때,
우리는 길이 3의 모든 하위 단어"&lt;wh", "whe", "her", "ere", "re&gt;"와 특수 하위 단어"&lt;where&gt;"를 얻습니다.</p>
<p>fastText에서, 임의의 단어 $w$에 대해,
길이 3에서 6 사이의 모든 하위 단어와
그 특수 하위 단어의 합집합을 $\mathcal{G}_w$라고 표시합니다.
어휘(vocabulary)는 모든 단어의 하위 단어들의 합집합입니다.
$\mathbf{z}_g$를 사전에 있는 하위 단어 $g$의 벡터라고 할 때,
스킵-그램 모델에서 중심 단어로서의 단어 $w$에 대한 벡터 $\mathbf{v}_w$는
그 하위 단어 벡터들의 합입니다:</p>
<p>$$\mathbf{v}<em>w = \sum</em>{g\in\mathcal{G}_w} \mathbf{z}_g.$$</p>
<p>fastText의 나머지 부분은 스킵-그램 모델과 동일합니다. 스킵-그램 모델과 비교할 때,
fastText의 어휘는 더 크며,
결과적으로 더 많은 모델 파라미터를 갖습니다.
게다가,
단어의 표현을 계산하기 위해,
모든 하위 단어 벡터를 합산해야 하므로,
계산 복잡도가 더 높습니다.
그러나
유사한 구조를 가진 단어들 간의 하위 단어 파라미터 공유 덕분에,
희귀 단어(rare words)와 심지어 어휘 밖의 단어(out-of-vocabulary words)도
fastText에서 더 나은 벡터 표현을 얻을 수 있습니다.</p>
<h2 id="바이트-페어-인코딩-byte-pair-encoding"><a class="header" href="#바이트-페어-인코딩-byte-pair-encoding">바이트 페어 인코딩 (Byte Pair Encoding)</a></h2>
<p>:label:<code>subsec_Byte_Pair_Encoding</code></p>
<p>fastText에서 추출된 모든 하위 단어는 $3$에서 $6$과 같이 지정된 길이를 가져야 하므로, 어휘 크기를 미리 정의할 수 없습니다.
고정 크기 어휘에서 가변 길이 하위 단어를 허용하기 위해,
우리는 하위 단어를 추출하기 위해 *바이트 페어 인코딩(byte pair encoding, BPE)*이라는 압축 알고리즘을 적용할 수 있습니다 :cite:<code>Sennrich.Haddow.Birch.2015</code>.</p>
<p>바이트 페어 인코딩은 훈련 데이터셋의 통계적 분석을 수행하여 단어 내의 공통 기호(예: 임의 길이의 연속 문자)를 발견합니다.
길이 1인 기호부터 시작하여,
바이트 페어 인코딩은 가장 빈번한 연속 기호 쌍을 반복적으로 병합하여 새로운 더 긴 기호를 생성합니다.
효율성을 위해 단어 경계를 넘는 쌍은 고려하지 않는다는 점에 유의하십시오.
결국, 우리는 이러한 기호를 하위 단어로 사용하여 단어를 분할할 수 있습니다.
바이트 페어 인코딩과 그 변형은 GPT-2 :cite:<code>Radford.Wu.Child.ea.2019</code> 및 RoBERTa :cite:<code>Liu.Ott.Goyal.ea.2019</code>와 같은 인기 있는 자연어 처리 사전 훈련 모델의 입력 표현에 사용되었습니다.
다음에서는 바이트 페어 인코딩이 어떻게 작동하는지 설명합니다.</p>
<p>먼저, 기호 어휘를 모든 영어 소문자, 특수 단어 끝 기호 <code>'_'</code>, 특수 알 수 없는 기호 <code>'[UNK]'</code>로 초기화합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
import collections

symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
           '_', '[UNK]']
</code></pre>
<p>우리는 단어 경계를 넘는 기호 쌍을 고려하지 않으므로,
우리는 단어를 데이터셋 내의 빈도(발생 횟수)에 매핑하는 딕셔너리 <code>raw_token_freqs</code>만 있으면 됩니다.
출력 기호 시퀀스(예: "a_ tall er_ man")에서
단어 시퀀스(예: "a taller man")를 쉽게 복구할 수 있도록
각 단어에 특수 기호 <code>'_'</code>가 추가된다는 점에 유의하십시오.
단일 문자와 특수 기호만 있는 어휘에서 병합 프로세스를 시작하므로, 각 단어 내의 모든 연속 문자 쌍 사이에 공백이 삽입됩니다(딕셔너리 <code>token_freqs</code>의 키).
즉, 공백은 단어 내 기호 간의 구분자입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}
token_freqs = {}
for token, freq in raw_token_freqs.items():
    token_freqs[' '.join(list(token))] = raw_token_freqs[token]
token_freqs
</code></pre>
<p>우리는 다음 <code>get_max_freq_pair</code> 함수를 정의합니다. 이 함수는 단어 내에서 가장 빈번한 연속 기호 쌍을 반환하며, 여기서 단어는 입력 딕셔너리 <code>token_freqs</code>의 키에서 나옵니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def get_max_freq_pair(token_freqs):
    pairs = collections.defaultdict(int)
    for token, freq in token_freqs.items():
        symbols = token.split()
        for i in range(len(symbols) - 1):
            # `pairs`의 키는 두 연속 기호의 튜플입니다
            pairs[symbols[i], symbols[i + 1]] += freq
    return max(pairs, key=pairs.get)  # 최댓값을 가진 `pairs`의 키
</code></pre>
<p>연속 기호의 빈도에 기반한 탐욕적 접근 방식으로서,
바이트 페어 인코딩은 다음 <code>merge_symbols</code> 함수를 사용하여 가장 빈번한 연속 기호 쌍을 병합하여 새로운 기호를 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def merge_symbols(max_freq_pair, token_freqs, symbols):
    symbols.append(''.join(max_freq_pair))
    new_token_freqs = dict()
    for token, freq in token_freqs.items():
        new_token = token.replace(' '.join(max_freq_pair),
                                  ''.join(max_freq_pair))
        new_token_freqs[new_token] = token_freqs[token]
    return new_token_freqs
</code></pre>
<p>이제 딕셔너리 <code>token_freqs</code>의 키에 대해 바이트 페어 인코딩 알고리즘을 반복적으로 수행합니다. 첫 번째 반복에서 가장 빈번한 연속 기호 쌍은 <code>'t'</code>와 <code>'a'</code>이므로, 바이트 페어 인코딩은 이들을 병합하여 새로운 기호 <code>'ta'</code>를 생성합니다. 두 번째 반복에서 바이트 페어 인코딩은 <code>'ta'</code>와 <code>'l'</code>을 계속 병합하여 또 다른 새로운 기호 <code>'tal'</code>을 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
num_merges = 10
for i in range(num_merges):
    max_freq_pair = get_max_freq_pair(token_freqs)
    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)
    print(f'merge #{i + 1}:', max_freq_pair)
</code></pre>
<p>바이트 페어 인코딩을 10회 반복한 후, 리스트 <code>symbols</code>에 다른 기호에서 반복적으로 병합된 10개의 기호가 더 포함되어 있음을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
print(symbols)
</code></pre>
<p>딕셔너리 <code>raw_token_freqs</code>의 키에 지정된 동일한 데이터셋에 대해,
바이트 페어 인코딩 알고리즘의 결과로
데이터셋의 각 단어는 이제 하위 단어 "fast_", "fast", "er_", "tall_", "tall"로 분할됩니다.
예를 들어, 단어 "faster_"와 "taller_"는 각각 "fast er_"와 "tall er_"로 분할됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
print(list(token_freqs.keys()))
</code></pre>
<p>바이트 페어 인코딩의 결과는 사용되는 데이터셋에 따라 달라진다는 점에 유의하십시오.
우리는 한 데이터셋에서 학습한 하위 단어를 사용하여
다른 데이터셋의 단어를 분할할 수도 있습니다.
탐욕적 접근 방식으로서, 다음 <code>segment_BPE</code> 함수는 입력 인수 <code>symbols</code>에서 가능한 가장 긴 하위 단어로 단어를 분할하려고 시도합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def segment_BPE(tokens, symbols):
    outputs = []
    for token in tokens:
        start, end = 0, len(token)
        cur_output = []
        # symbols에서 가능한 가장 긴 하위 단어로 토큰을 분할합니다
        while start &lt; len(token) and start &lt; end:
            if token[start: end] in symbols:
                cur_output.append(token[start: end])
                start = end
                end = len(token)
            else:
                end -= 1
        if start &lt; len(token):
            cur_output.append('[UNK]')
        outputs.append(' '.join(cur_output))
    return outputs
</code></pre>
<p>다음에서는 앞서 언급한 데이터셋에서 학습한 리스트 <code>symbols</code>의 하위 단어를 사용하여
다른 데이터셋을 나타내는 <code>tokens</code>를 분할합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
tokens = ['tallest_', 'fatter_']
print(segment_BPE(tokens, symbols))
</code></pre>
<h2 id="요약-summary-88"><a class="header" href="#요약-summary-88">요약 (Summary)</a></h2>
<ul>
<li>fastText 모델은 하위 단어 임베딩 접근 방식을 제안합니다. word2vec의 스킵-그램 모델을 기반으로, 중심 단어를 하위 단어 벡터의 합으로 표현합니다.</li>
<li>바이트 페어 인코딩은 훈련 데이터셋의 통계적 분석을 수행하여 단어 내의 공통 기호를 발견합니다. 탐욕적 접근 방식으로서, 바이트 페어 인코딩은 가장 빈번한 연속 기호 쌍을 반복적으로 병합합니다.</li>
<li>하위 단어 임베딩은 희귀 단어와 사전 밖 단어의 표현 품질을 향상시킬 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-103"><a class="header" href="#연습-문제-exercises-103">연습 문제 (Exercises)</a></h2>
<ol>
<li>예로서, 영어에는 약 $3\times 10^8$개의 가능한 $6$-그램이 있습니다. 하위 단어가 너무 많으면 어떤 문제가 있습니까? 이 문제를 어떻게 해결합니까? 힌트: fastText 논문 :cite:<code>Bojanowski.Grave.Joulin.ea.2017</code>의 섹션 3.2 끝부분을 참조하십시오.</li>
<li>CBOW 모델을 기반으로 하위 단어 임베딩 모델을 어떻게 설계합니까?</li>
<li>초기 기호 어휘 크기가 $n$일 때, 크기 $m$의 어휘를 얻으려면 몇 번의 병합 작업이 필요합니까?</li>
<li>구(phrases)를 추출하기 위해 바이트 페어 인코딩 아이디어를 어떻게 확장할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/386">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/4587">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="단어-유사성과-유추-word-similarity-and-analogy"><a class="header" href="#단어-유사성과-유추-word-similarity-and-analogy">단어 유사성과 유추 (Word Similarity and Analogy)</a></h1>
<p>:label:<code>sec_synonyms</code></p>
<p>:numref:<code>sec_word2vec_pretraining</code>에서,
우리는 작은 데이터셋에서 word2vec 모델을 훈련하고,
입력 단어에 대해 의미적으로 유사한 단어를 찾는 데 적용했습니다.
실제로,
대규모 코퍼스에서 사전 훈련된 단어 벡터는
:numref:<code>chap_nlp_app</code>에서 나중에 다룰
다운스트림 자연어 처리 작업에 적용될 수 있습니다.
대규모 코퍼스에서 사전 훈련된 단어 벡터의 의미를
직관적인 방식으로 입증하기 위해,
단어 유사성 및 유추 작업에 적용해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
import os

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
import os
</code></pre>
<h2 id="사전-훈련된-단어-벡터-로드-loading-pretrained-word-vectors"><a class="header" href="#사전-훈련된-단어-벡터-로드-loading-pretrained-word-vectors">사전 훈련된 단어 벡터 로드 (Loading Pretrained Word Vectors)</a></h2>
<p>아래에는 <a href="https://nlp.stanford.edu/projects/glove/">GloVe 웹사이트</a>에서 다운로드할 수 있는 50, 100, 300차원의 사전 훈련된 GloVe 임베딩이 나열되어 있습니다.
사전 훈련된 fastText 임베딩은 여러 언어로 제공됩니다.
여기서는 <a href="https://fasttext.cc/">fastText 웹사이트</a>에서 다운로드할 수 있는 영어 버전(300차원 "wiki.en") 하나를 고려합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',
                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')

#@save
d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',
                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')

#@save
d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',
                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')

#@save
d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',
                           'c1816da3821ae9f43899be655002f6c723e91b88')
</code></pre>
<p>이러한 사전 훈련된 GloVe 및 fastText 임베딩을 로드하기 위해, 다음 <code>TokenEmbedding</code> 클래스를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
class TokenEmbedding:
    """Token Embedding."""
    def __init__(self, embedding_name):
        self.idx_to_token, self.idx_to_vec = self._load_embedding(
            embedding_name)
        self.unknown_idx = 0
        self.token_to_idx = {token: idx for idx, token in
                             enumerate(self.idx_to_token)}

    def _load_embedding(self, embedding_name):
        idx_to_token, idx_to_vec = ['&lt;unk&gt;'], []
        data_dir = d2l.download_extract(embedding_name)
        # GloVe 웹사이트: https://nlp.stanford.edu/projects/glove/
        # fastText 웹사이트: https://fasttext.cc/
        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:
            for line in f:
                elems = line.rstrip().split(' ')
                token, elems = elems[0], [float(elem) for elem in elems[1:]]
                # 헤더 정보 건너뛰기 (예: fastText의 첫 번째 행)
                if len(elems) &gt; 1:
                    idx_to_token.append(token)
                    idx_to_vec.append(elems)
        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec
        return idx_to_token, d2l.tensor(idx_to_vec)

    def __getitem__(self, tokens):
        indices = [self.token_to_idx.get(token, self.unknown_idx)
                   for token in tokens]
        vecs = self.idx_to_vec[d2l.tensor(indices)]
        return vecs

    def __len__(self):
        return len(self.idx_to_token)
</code></pre>
<p>아래에서는 (위키피디아 하위 집합에서 사전 훈련된)
50차원 GloVe 임베딩을 로드합니다.
<code>TokenEmbedding</code> 인스턴스를 생성할 때,
지정된 임베딩 파일이 아직 다운로드되지 않았다면 다운로드해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
glove_6b50d = TokenEmbedding('glove.6b.50d')
</code></pre>
<p>어휘 크기를 출력합니다. 어휘에는 400,000개의 단어(토큰)와 특수 알 수 없는 토큰이 포함되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
len(glove_6b50d)
</code></pre>
<p>어휘에서 단어의 인덱스를 얻거나 그 반대로 할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]
</code></pre>
<h2 id="사전-훈련된-단어-벡터-적용-applying-pretrained-word-vectors"><a class="header" href="#사전-훈련된-단어-벡터-적용-applying-pretrained-word-vectors">사전 훈련된 단어 벡터 적용 (Applying Pretrained Word Vectors)</a></h2>
<p>로드된 GloVe 벡터를 사용하여,
다음 단어 유사성 및 유추 작업에 적용하여 그 의미를 보여줄 것입니다.</p>
<h3 id="단어-유사성-word-similarity"><a class="header" href="#단어-유사성-word-similarity">단어 유사성 (Word Similarity)</a></h3>
<p>:numref:<code>subsec_apply-word-embed</code>와 유사하게,
단어 벡터 간의 코사인 유사도를 기반으로
입력 단어에 대해 의미적으로 유사한 단어를 찾기 위해,
다음 <code>knn</code> ($k$-최근접 이웃) 함수를 구현합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def knn(W, x, k):
    # 수치적 안정성을 위해 1e-9를 더합니다
    cos = np.dot(W, x.reshape(-1,)) / (
        np.sqrt(np.sum(W * W, axis=1) + 1e-9) * np.sqrt((x * x).sum()))
    topk = npx.topk(cos, k=k, ret_typ='indices')
    return topk, [cos[int(i)] for i in topk]
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def knn(W, x, k):
    # 수치적 안정성을 위해 1e-9를 더합니다
    cos = torch.mv(W, x.reshape(-1,)) / (
        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *
        torch.sqrt((x * x).sum()))
    _, topk = torch.topk(cos, k=k)
    return topk, [cos[int(i)] for i in topk]
</code></pre>
<p>그런 다음 <code>TokenEmbedding</code> 인스턴스 <code>embed</code>에서
사전 훈련된 단어 벡터를 사용하여
유사한 단어를 검색합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def get_similar_tokens(query_token, k, embed):
    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)
    for i, c in zip(topk[1:], cos[1:]):  # 입력 단어 제외
        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')
</code></pre>
<p><code>glove_6b50d</code>의 사전 훈련된 단어 벡터 어휘에는 400,000개의 단어와 특수 알 수 없는 토큰이 포함되어 있습니다.
입력 단어와 알 수 없는 토큰을 제외하고,
이 어휘 중에서
"chip"이라는 단어와 의미적으로 가장 유사한
세 단어를 찾아봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
get_similar_tokens('chip', 3, glove_6b50d)
</code></pre>
<p>아래는 "baby"와 "beautiful"에 유사한 단어를 출력합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
get_similar_tokens('baby', 3, glove_6b50d)
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
get_similar_tokens('beautiful', 3, glove_6b50d)
</code></pre>
<h3 id="단어-유추-word-analogy"><a class="header" href="#단어-유추-word-analogy">단어 유추 (Word Analogy)</a></h3>
<p>유사한 단어를 찾는 것 외에도,
단어 벡터를 단어 유추 작업에 적용할 수도 있습니다.
예를 들어,
“man”:“woman”::“son”:“daughter”는
단어 유추의 형태입니다:
“man”이 “woman”에 해당하는 것은 “son”이 “daughter”에 해당하는 것과 같습니다.
구체적으로,
단어 유추 완성 작업은 다음과 같이 정의할 수 있습니다:
단어 유추 $a : b :: c : d$에 대해, 처음 세 단어 $a$, $b$, $c$가 주어졌을 때 $d$를 찾습니다.
단어 $w$의 벡터를 $	extrm{vec}(w)$라고 표시합시다.
유추를 완성하기 위해,
우리는 벡터가 $	extrm{vec}(c)+	extrm{vec}(b)-	extrm{vec}(a)$의 결과와
가장 유사한 단어를 찾을 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def get_analogy(token_a, token_b, token_c, embed):
    vecs = embed[[token_a, token_b, token_c]]
    x = vecs[1] - vecs[0] + vecs[2]
    topk, cos = knn(embed.idx_to_vec, x, 1)
    return embed.idx_to_token[int(topk[0])]  # 알 수 없는 단어 제거
</code></pre>
<p>로드된 단어 벡터를 사용하여 "male-female" 유추를 확인해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
get_analogy('man', 'woman', 'son', glove_6b50d)
</code></pre>
<p>아래는 “capital-country” 유추를 완성합니다:
“beijing”:“china”::“tokyo”:“japan”.
이것은 사전 훈련된 단어 벡터의 의미론을 보여줍니다.</p>
<pre><code class="language-{.python .input}">#@tab all
get_analogy('beijing', 'china', 'tokyo', glove_6b50d)
</code></pre>
<p>“bad”:“worst”::“big”:“biggest”와 같은
“adjective-superlative adjective” 유추의 경우,
사전 훈련된 단어 벡터가 구문 정보를 캡처할 수 있음을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
get_analogy('bad', 'worst', 'big', glove_6b50d)
</code></pre>
<p>사전 훈련된 단어 벡터에서 캡처된 과거 시제 개념을 보여주기 위해,
"present tense-past tense" 유추를 사용하여 구문을 테스트할 수 있습니다: “do”:“did”::“go”:“went”.</p>
<pre><code class="language-{.python .input}">#@tab all
get_analogy('do', 'did', 'go', glove_6b50d)
</code></pre>
<h2 id="요약-summary-89"><a class="header" href="#요약-summary-89">요약 (Summary)</a></h2>
<ul>
<li>실제로 대규모 코퍼스에서 사전 훈련된 단어 벡터는 다운스트림 자연어 처리 작업에 적용될 수 있습니다.</li>
<li>사전 훈련된 단어 벡터는 단어 유사성 및 유추 작업에 적용될 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-104"><a class="header" href="#연습-문제-exercises-104">연습 문제 (Exercises)</a></h2>
<ol>
<li><code>TokenEmbedding('wiki.en')</code>을 사용하여 fastText 결과를 테스트하십시오.</li>
<li>어휘가 매우 클 때, 유사한 단어를 찾거나 단어 유추를 더 빨리 완료하려면 어떻게 해야 합니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/387">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1336">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="트랜스포머로부터의-양방향-인코더-표현-bert"><a class="header" href="#트랜스포머로부터의-양방향-인코더-표현-bert">트랜스포머로부터의 양방향 인코더 표현 (BERT)</a></h1>
<p>:label:<code>sec_bert</code></p>
<p>우리는 자연어 이해를 위한 여러 단어 임베딩 모델을 소개했습니다.
사전 훈련 후, 출력은 각 행이 미리 정의된 어휘의 단어를 나타내는 벡터인 행렬로 생각할 수 있습니다.
사실, 이러한 단어 임베딩 모델은 모두 *문맥 독립적(context-independent)*입니다.
이 속성을 설명하는 것으로 시작하겠습니다.</p>
<h2 id="문맥-독립적-표현에서-문맥-의존적-표현으로-from-context-independent-to-context-sensitive"><a class="header" href="#문맥-독립적-표현에서-문맥-의존적-표현으로-from-context-independent-to-context-sensitive">문맥 독립적 표현에서 문맥 의존적 표현으로 (From Context-Independent to Context-Sensitive)</a></h2>
<p>:numref:<code>sec_word2vec_pretraining</code>과 :numref:<code>sec_synonyms</code>의 실험을 상기해 보십시오.
예를 들어, word2vec과 GloVe는 단어의 문맥에 관계없이 동일한 단어에 동일한 사전 훈련된 벡터를 할당합니다(문맥이 있는 경우).
공식적으로, 임의의 토큰 $x$의 문맥 독립적 표현은 $x$만을 입력으로 취하는 함수 $f(x)$입니다.
자연어의 다의어와 복잡한 의미론의 풍부함을 고려할 때,
문맥 독립적 표현은 명백한 한계를 가지고 있습니다.
예를 들어, "a crane is flying"과 "a crane driver came"이라는 문맥에서 "crane"이라는 단어는 완전히 다른 의미를 갖습니다(전자는 '학', 후자는 '기중기').
따라서 동일한 단어라도 문맥에 따라 다른 표현이 할당되어야 할 수 있습니다.</p>
<p>이것은 단어의 표현이 문맥에 의존하는 <em>문맥 의존적(context-sensitive)</em> 단어 표현의 개발에 동기를 부여했습니다.
따라서 토큰 $x$의 문맥 의존적 표현은 $x$와 그 문맥 $c(x)$ 모두에 의존하는 함수 $f(x, c(x))$입니다.
인기 있는 문맥 의존적 표현으로는
TagLM(language-model-augmented sequence tagger) :cite:<code>Peters.Ammar.Bhagavatula.ea.2017</code>,
CoVe(Context Vectors) :cite:<code>McCann.Bradbury.Xiong.ea.2017</code>,
ELMo(Embeddings from Language Models) :cite:<code>Peters.Neumann.Iyyer.ea.2018</code>가 있습니다.</p>
<p>예를 들어, 전체 시퀀스를 입력으로 취함으로써,
ELMo는 입력 시퀀스의 각 단어에 표현을 할당하는 함수입니다.
구체적으로, ELMo는 사전 훈련된 양방향 LSTM의 모든 중간 레이어 표현을 출력 표현으로 결합합니다.
그런 다음 ELMo 표현은 다운스트림 작업의 기존 지도 모델에 추가 특징으로 추가됩니다. 예를 들어 기존 모델의 토큰에 대한 원본 표현(예: GloVe)과 ELMo 표현을 연결하는 방식입니다.
한편으로, 사전 훈련된 양방향 LSTM 모델의 모든 가중치는 ELMo 표현이 추가된 후 고정됩니다.
반면, 기존 지도 모델은 주어진 작업에 맞게 특별히 맞춤화됩니다.
당시 서로 다른 작업에 대해 서로 다른 최적의 모델을 활용하고 ELMo를 추가함으로써,
감정 분석, 자연어 추론, 의미역 결정(semantic role labeling), 상호참조 해결(coreference resolution), 개체명 인식(named entity recognition), 질문 응답 등 6가지 자연어 처리 작업 전반에 걸쳐 최첨단 기술을 향상시켰습니다.</p>
<h2 id="작업별에서-작업-불가지론적으로-from-task-specific-to-task-agnostic"><a class="header" href="#작업별에서-작업-불가지론적으로-from-task-specific-to-task-agnostic">작업별에서 작업 불가지론적으로 (From Task-Specific to Task-Agnostic)</a></h2>
<p>ELMo가 다양한 자연어 처리 작업에 대한 솔루션을 크게 개선했지만,
각 솔루션은 여전히 <em>작업별(task-specific)</em> 아키텍처에 의존합니다.
그러나 모든 자연어 처리 작업에 대해 특정 아키텍처를 만드는 것은 실제로 쉽지 않습니다.
GPT(Generative Pre-Training) 모델은 문맥 의존적 표현을 위한 일반적인 <em>작업 불가지론적(task-agnostic)</em> 모델을 설계하려는 노력을 나타냅니다 :cite:<code>Radford.Narasimhan.Salimans.ea.2018</code>.
트랜스포머 디코더를 기반으로 구축된 GPT는 텍스트 시퀀스를 표현하는 데 사용될 언어 모델을 사전 훈련합니다.
GPT를 다운스트림 작업에 적용할 때, 언어 모델의 출력은 작업의 레이블을 예측하기 위해 추가된 선형 출력 레이어에 공급됩니다.
사전 훈련된 모델의 파라미터를 고정하는 ELMo와 극명하게 대조적으로,
GPT는 다운스트림 작업의 지도 학습 중에 사전 훈련된 트랜스포머 디코더의 <em>모든</em> 파라미터를 미세 조정합니다.
GPT는 자연어 추론, 질문 응답, 문장 유사성 및 분류의 12가지 작업에서 평가되었으며,
모델 아키텍처에 최소한의 변경만으로 그중 9가지 작업에서 최첨단 기술을 개선했습니다.</p>
<p>그러나 언어 모델의 자기 회귀 특성으로 인해,
GPT는 앞만 봅니다(왼쪽에서 오른쪽으로).
"i went to the bank to deposit cash"와 "i went to the bank to sit down"이라는 문맥에서,
"bank"는 그 왼쪽 문맥에 민감하므로,
GPT는 "bank"에 대해 동일한 표현을 반환하지만, 실제로는 다른 의미를 갖습니다.</p>
<h2 id="bert-두-세계의-장점-결합-bert-combining-the-best-of-both-worlds"><a class="header" href="#bert-두-세계의-장점-결합-bert-combining-the-best-of-both-worlds">BERT: 두 세계의 장점 결합 (BERT: Combining the Best of Both Worlds)</a></h2>
<p>우리가 보았듯이,
ELMo는 문맥을 양방향으로 인코딩하지만 작업별 아키텍처를 사용합니다.
반면 GPT는 작업 불가지론적이지만 문맥을 왼쪽에서 오른쪽으로 인코딩합니다.
두 세계의 장점을 결합하여,
BERT(Bidirectional Encoder Representations from Transformers)는
문맥을 양방향으로 인코딩하고 광범위한 자연어 처리 작업에 대해 최소한의 아키텍처 변경만 요구합니다 :cite:<code>Devlin.Chang.Lee.ea.2018</code>.
사전 훈련된 트랜스포머 인코더를 사용하여,
BERT는 양방향 문맥을 기반으로 모든 토큰을 표현할 수 있습니다.
다운스트림 작업의 지도 학습 중에,
BERT는 두 가지 측면에서 GPT와 유사합니다.
첫째, BERT 표현은 추가된 출력 레이어에 공급되며, 모든 토큰에 대해 예측하는 것과 전체 시퀀스에 대해 예측하는 것과 같이 작업의 성격에 따라 모델 아키텍처에 최소한의 변경만 가합니다.
둘째, 사전 훈련된 트랜스포머 인코더의 모든 파라미터가 미세 조정되는 반면, 추가 출력 레이어는 처음부터 훈련됩니다.
:numref:<code>fig_elmo-gpt-bert</code>는 ELMo, GPT, BERT 간의 차이점을 묘사합니다.</p>
<p><img src="chapter_natural-language-processing-pretraining/../img/elmo-gpt-bert.svg" alt="ELMo, GPT, BERT 비교." />
:label:<code>fig_elmo-gpt-bert</code></p>
<p>BERT는 (i) 단일 텍스트 분류(예: 감정 분석), (ii) 텍스트 쌍 분류(예: 자연어 추론), (iii) 질문 응답, (iv) 텍스트 태깅(예: 개체명 인식)의 광범위한 범주 아래 11가지 자연어 처리 작업에서 최첨단 기술을 더욱 향상시켰습니다.
모두 2018년에 제안된 문맥 의존적 ELMo에서 작업 불가지론적 GPT와 BERT에 이르기까지,
개념적으로 간단하면서도 경험적으로 강력한 자연어에 대한 심층 표현의 사전 훈련은 다양한 자연어 처리 작업에 대한 솔루션에 혁명을 일으켰습니다.</p>
<p>이 장의 나머지 부분에서는 BERT의 사전 훈련에 대해 자세히 알아볼 것입니다.
:numref:<code>chap_nlp_app</code>에서 자연어 처리 응용 프로그램이 설명될 때,
다운스트림 응용 프로그램을 위한 BERT의 미세 조정을 설명할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
from mxnet.gluon import nn

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<h2 id="입력-표현-input-representation"><a class="header" href="#입력-표현-input-representation">[<strong>입력 표현 (Input Representation)</strong>]</a></h2>
<p>:label:<code>subsec_bert_input_rep</code></p>
<p>자연어 처리에서,
어떤 작업(예: 감정 분석)은 단일 텍스트를 입력으로 취하고,
다른 작업(예: 자연어 추론)에서는 입력이 텍스트 시퀀스 쌍입니다.
BERT 입력 시퀀스는 단일 텍스트와 텍스트 쌍을 명확하게 나타냅니다.
전자의 경우,
BERT 입력 시퀀스는
특수 분류 토큰 “&lt;cls&gt;”,
텍스트 시퀀스의 토큰,
그리고 특수 구분 토큰 “&lt;sep&gt;”의 연결입니다.
후자의 경우,
BERT 입력 시퀀스는
“&lt;cls&gt;”, 첫 번째 텍스트 시퀀스의 토큰,
“&lt;sep&gt;”, 두 번째 텍스트 시퀀스의 토큰, 그리고 “&lt;sep&gt;”의 연결입니다.
우리는 "BERT 입력 시퀀스"라는 용어를 다른 유형의 "시퀀스"와 일관되게 구별할 것입니다.
예를 들어, 하나의 <em>BERT 입력 시퀀스</em>는 하나의 <em>텍스트 시퀀스</em> 또는 두 개의 <em>텍스트 시퀀스</em>를 포함할 수 있습니다.</p>
<p>텍스트 쌍을 구별하기 위해,
학습된 세그먼트 임베딩 $\mathbf{e}_A$와 $\mathbf{e}_B$가
각각 첫 번째 시퀀스와 두 번째 시퀀스의 토큰 임베딩에 추가됩니다.
단일 텍스트 입력의 경우 $\mathbf{e}_A$만 사용됩니다.</p>
<p>다음 <code>get_tokens_and_segments</code>는 한 문장 또는 두 문장을 입력으로 받아
BERT 입력 시퀀스의 토큰과 해당 세그먼트 ID를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def get_tokens_and_segments(tokens_a, tokens_b=None):
    """BERT 입력 시퀀스의 토큰과 세그먼트 ID를 가져옵니다."""
    tokens = ['&lt;cls&gt;'] + tokens_a + ['&lt;sep&gt;']
    # 0과 1은 각각 세그먼트 A와 B를 표시합니다
    segments = [0] * (len(tokens_a) + 2)
    if tokens_b is not None:
        tokens += tokens_b + ['&lt;sep&gt;']
        segments += [1] * (len(tokens_b) + 1)
    return tokens, segments
</code></pre>
<p>BERT는 양방향 아키텍처로 트랜스포머 인코더를 선택합니다.
트랜스포머 인코더에서 흔히 그렇듯이,
위치 임베딩이 BERT 입력 시퀀스의 모든 위치에 추가됩니다.
그러나 원래 트랜스포머 인코더와 달리,
BERT는 <em>학습 가능한</em> 위치 임베딩을 사용합니다.
요약하자면, :numref:<code>fig_bert-input</code>은
BERT 입력 시퀀스의 임베딩이
토큰 임베딩, 세그먼트 임베딩, 위치 임베딩의 합임을 보여줍니다.</p>
<p><img src="chapter_natural-language-processing-pretraining/../img/bert-input.svg" alt="BERT 입력 시퀀스의 임베딩은 토큰 임베딩, 세그먼트 임베딩, 위치 임베딩의 합입니다." />
:label:<code>fig_bert-input</code></p>
<p>다음 [<strong><code>BERTEncoder</code> 클래스</strong>]는 :numref:<code>sec_transformer</code>에서 구현된 <code>TransformerEncoder</code> 클래스와 유사합니다.
<code>TransformerEncoder</code>와 달리, <code>BERTEncoder</code>는
세그먼트 임베딩과 학습 가능한 위치 임베딩을 사용합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
class BERTEncoder(nn.Block):
    """BERT 인코더."""
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_blks, dropout, max_len=1000, **kwargs):
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(2, num_hiddens)
        self.blks = nn.Sequential()
        for _ in range(num_blks):
            self.blks.add(d2l.TransformerEncoderBlock(
                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))
        # BERT에서 위치 임베딩은 학습 가능하므로, 충분히 긴 위치 임베딩 파라미터를 생성합니다
        self.pos_embedding = self.params.get('pos_embedding',
                                             shape=(1, max_len, num_hiddens))

    def forward(self, tokens, segments, valid_lens):
        # `X`의 모양은 다음 코드 스니펫에서 변경되지 않은 상태로 유지됩니다:
        # (배치 크기, 최대 시퀀스 길이, `num_hiddens`)
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data(ctx=X.ctx)[:, :X.shape[1], :]
        for blk in self.blks:
            X = blk(X, valid_lens)
        return X
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
class BERTEncoder(nn.Module):
    """BERT 인코더."""
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_blks, dropout, max_len=1000, **kwargs):
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(2, num_hiddens)
        self.blks = nn.Sequential()
        for i in range(num_blks):
            self.blks.add_module(f"{i}", d2l.TransformerEncoderBlock(
                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))
        # BERT에서 위치 임베딩은 학습 가능하므로, 충분히 긴 위치 임베딩 파라미터를 생성합니다
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,
                                                      num_hiddens))

    def forward(self, tokens, segments, valid_lens):
        # `X`의 모양은 다음 코드 스니펫에서 변경되지 않은 상태로 유지됩니다:
        # (배치 크기, 최대 시퀀스 길이, `num_hiddens`)
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding[:, :X.shape[1], :]
        for blk in self.blks:
            X = blk(X, valid_lens)
        return X
</code></pre>
<p>어휘 크기가 10,000이라고 가정합니다.
[<strong><code>BERTEncoder</code>의 순방향 추론</strong>]을 시연하기 위해,
인스턴스를 생성하고 파라미터를 초기화해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4
num_blks, dropout = 2, 0.2
encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                      num_blks, dropout)
encoder.initialize()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4
ffn_num_input, num_blks, dropout = 768, 2, 0.2
encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                      num_blks, dropout)
</code></pre>
<p><code>tokens</code>를 길이 8의 BERT 입력 시퀀스 2개로 정의합니다.
여기서 각 토큰은 어휘의 인덱스입니다.
입력 <code>tokens</code>를 사용한 <code>BERTEncoder</code>의 순방향 추론은
각 토큰이 하이퍼파라미터 <code>num_hiddens</code>에 의해 미리 정의된 길이의 벡터로 표현되는 인코딩된 결과를 반환합니다.
이 하이퍼파라미터는 일반적으로 트랜스포머 인코더의 <em>은닉 크기(hidden size)</em> (은닉 유닛 수)라고 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
tokens = np.random.randint(0, vocab_size, (2, 8))
segments = np.array([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])
encoded_X = encoder(tokens, segments, None)
encoded_X.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
tokens = torch.randint(0, vocab_size, (2, 8))
segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])
encoded_X = encoder(tokens, segments, None)
encoded_X.shape
</code></pre>
<h2 id="사전-훈련-작업-pretraining-tasks"><a class="header" href="#사전-훈련-작업-pretraining-tasks">사전 훈련 작업 (Pretraining Tasks)</a></h2>
<p>:label:<code>subsec_bert_pretraining_tasks</code></p>
<p><code>BERTEncoder</code>의 순방향 추론은 입력 텍스트의 각 토큰과 삽입된 특수 토큰 “&lt;cls&gt;” 및 “&lt;seq&gt;”의 BERT 표현을 제공합니다.
다음으로, 우리는 이 표현들을 사용하여 BERT 사전 훈련을 위한 손실 함수를 계산할 것입니다.
사전 훈련은 마스킹된 언어 모델링(masked language modeling)과 다음 문장 예측(next sentence prediction)이라는 두 가지 작업으로 구성됩니다.</p>
<h3 id="마스킹된-언어-모델링-masked-language-modeling"><a class="header" href="#마스킹된-언어-모델링-masked-language-modeling">[<strong>마스킹된 언어 모델링 (Masked Language Modeling)</strong>]</a></h3>
<p>:label:<code>subsec_mlm</code></p>
<p>:numref:<code>sec_language-model</code>에 설명된 대로,
언어 모델은 왼쪽의 문맥을 사용하여 토큰을 예측합니다.
각 토큰을 표현하기 위해 문맥을 양방향으로 인코딩하기 위해,
BERT는 무작위로 토큰을 마스킹하고 양방향 문맥의 토큰을 사용하여
자기 지도 방식으로 마스킹된 토큰을 예측합니다.
이 작업을 *마스킹된 언어 모델(masked language model)*이라고 합니다.</p>
<p>이 사전 훈련 작업에서,
토큰의 15%가 예측을 위한 마스킹된 토큰으로 무작위로 선택됩니다.
레이블을 사용하여 부정행위 없이 마스킹된 토큰을 예측하기 위해,
가장 간단한 접근 방식은 BERT 입력 시퀀스에서 항상 특수 “&lt;mask&gt;” 토큰으로 대체하는 것입니다.
그러나 인공적인 특수 토큰 “&lt;mask&gt;”는 미세 조정 단계에서는 절대 나타나지 않습니다.
사전 훈련과 미세 조정 사이의 이러한 불일치를 피하기 위해,
토큰이 예측을 위해 마스킹되는 경우(예: "this movie is great"에서 "great"이 마스킹되고 예측되도록 선택됨),
입력에서 다음과 같이 대체됩니다:</p>
<ul>
<li>80%의 경우 특수 “&lt;mask&gt;” 토큰으로 대체됩니다(예: "this movie is great"가 "this movie is &lt;mask&gt;"가 됨).</li>
<li>10%의 경우 무작위 토큰으로 대체됩니다(예: "this movie is great"가 "this movie is drink"가 됨).</li>
<li>10%의 경우 변경되지 않은 원래 토큰으로 유지됩니다(예: "this movie is great"가 "this movie is great"가 됨).</li>
</ul>
<p>15% 중 10%의 시간에는 무작위 토큰이 삽입된다는 점에 유의하십시오.
이러한 가끔 발생하는 노이즈는 BERT가 양방향 문맥 인코딩에서 마스킹된 토큰에 덜 편향되도록(특히 레이블 토큰이 변경되지 않은 상태로 유지될 때) 장려합니다.</p>
<p>우리는 BERT 사전 훈련의 마스킹된 언어 모델 작업에서 마스킹된 토큰을 예측하기 위해 다음 <code>MaskLM</code> 클래스를 구현합니다.
예측에는 1개의 은닉층 MLP(<code>self.mlp</code>)가 사용됩니다.
순방향 추론에서, 이는 <code>BERTEncoder</code>의 인코딩된 결과와 예측을 위한 토큰 위치라는 두 가지 입력을 받습니다.
출력은 해당 위치에서의 예측 결과입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
class MaskLM(nn.Block):
    """BERT의 마스킹된 언어 모델 작업."""
    def __init__(self, vocab_size, num_hiddens, **kwargs):
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential()
        self.mlp.add(
            nn.Dense(num_hiddens, flatten=False, activation='relu'))
        self.mlp.add(nn.LayerNorm())
        self.mlp.add(nn.Dense(vocab_size, flatten=False))

    def forward(self, X, pred_positions):
        num_pred_positions = pred_positions.shape[1]
        pred_positions = pred_positions.reshape(-1)
        batch_size = X.shape[0]
        batch_idx = np.arange(0, batch_size)
        # `batch_size` = 2, `num_pred_positions` = 3이라고 가정하면,
        # `batch_idx`는 `np.array([0, 0, 0, 1, 1, 1])`입니다
        batch_idx = np.repeat(batch_idx, num_pred_positions)
        masked_X = X[batch_idx, pred_positions]
        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))
        mlm_Y_hat = self.mlp(masked_X)
        return mlm_Y_hat
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
class MaskLM(nn.Module):
    """BERT의 마스킹된 언어 모델 작업."""
    def __init__(self, vocab_size, num_hiddens, **kwargs):
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens),
                                 nn.ReLU(),
                                 nn.LayerNorm(num_hiddens),
                                 nn.LazyLinear(vocab_size))

    def forward(self, X, pred_positions):
        num_pred_positions = pred_positions.shape[1]
        pred_positions = pred_positions.reshape(-1)
        batch_size = X.shape[0]
        batch_idx = torch.arange(0, batch_size)
        # `batch_size` = 2, `num_pred_positions` = 3이라고 가정하면,
        # `batch_idx`는 `torch.tensor([0, 0, 0, 1, 1, 1])`입니다
        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)
        masked_X = X[batch_idx, pred_positions]
        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))
        mlm_Y_hat = self.mlp(masked_X)
        return mlm_Y_hat
</code></pre>
<p>[<strong><code>MaskLM</code>의 순방향 추론</strong>]을 보여주기 위해,
인스턴스 <code>mlm</code>을 생성하고 초기화합니다.
<code>BERTEncoder</code>의 순방향 추론에서 나온 <code>encoded_X</code>는 2개의 BERT 입력 시퀀스를 나타냄을 상기하십시오.
<code>mlm_positions</code>를 <code>encoded_X</code>의 BERT 입력 시퀀스 중 하나에서 예측할 3개의 인덱스로 정의합니다.
<code>mlm</code>의 순방향 추론은 <code>encoded_X</code>의 모든 마스킹된 위치 <code>mlm_positions</code>에서의 예측 결과 <code>mlm_Y_hat</code>을 반환합니다.
각 예측에 대해 결과의 크기는 어휘 크기와 같습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
mlm = MaskLM(vocab_size, num_hiddens)
mlm.initialize()
mlm_positions = np.array([[1, 5, 2], [6, 1, 5]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
mlm_Y_hat.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
mlm = MaskLM(vocab_size, num_hiddens)
mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
mlm_Y_hat.shape
</code></pre>
<p>마스크 아래의 예측된 토큰 <code>mlm_Y_hat</code>의 정답 레이블 <code>mlm_Y</code>를 사용하여,
BERT 사전 훈련에서 마스킹된 언어 모델 작업의 크로스 엔트로피 손실을 계산할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
mlm_Y = np.array([[7, 8, 9], [10, 20, 30]])
loss = gluon.loss.SoftmaxCrossEntropyLoss()
mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))
mlm_l.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])
loss = nn.CrossEntropyLoss(reduction='none')
mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))
mlm_l.shape
</code></pre>
<h3 id="다음-문장-예측-next-sentence-prediction"><a class="header" href="#다음-문장-예측-next-sentence-prediction">[<strong>다음 문장 예측 (Next Sentence Prediction)</strong>]</a></h3>
<p>:label:<code>subsec_nsp</code></p>
<p>마스킹된 언어 모델링은 단어를 표현하기 위해 양방향 문맥을 인코딩할 수 있지만,
텍스트 쌍 간의 논리적 관계를 명시적으로 모델링하지는 않습니다.
두 텍스트 시퀀스 간의 관계를 이해하는 데 도움을 주기 위해,
BERT는 사전 훈련에서 <em>다음 문장 예측</em>이라는 이진 분류 작업을 고려합니다.
사전 훈련을 위한 문장 쌍을 생성할 때,
절반은 실제로 연속된 문장이며 "True" 레이블이 붙고,
나머지 절반은 두 번째 문장이 코퍼스에서 무작위로 샘플링되며 "False" 레이블이 붙습니다.</p>
<p>다음 <code>NextSentencePred</code> 클래스는 1개의 은닉층 MLP를 사용하여
BERT 입력 시퀀스에서 두 번째 문장이 첫 번째 문장의 다음 문장인지 여부를 예측합니다.
트랜스포머 인코더의 셀프 어텐션으로 인해,
특수 토큰 “&lt;cls&gt;”의 BERT 표현은 입력의 두 문장을 모두 인코딩합니다.
따라서 MLP 분류기의 출력 레이어(<code>self.output</code>)는 <code>X</code>를 입력으로 받습니다.
여기서 <code>X</code>는 인코딩된 “&lt;cls&gt;” 토큰을 입력으로 하는 MLP 은닉층의 출력입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
class NextSentencePred(nn.Block):
    """BERT의 다음 문장 예측 작업."""
    def __init__(self, **kwargs):
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.Dense(2)

    def forward(self, X):
        # `X` 모양: (배치 크기, `num_hiddens`)
        return self.output(X)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
class NextSentencePred(nn.Module):
    """BERT의 다음 문장 예측 작업."""
    def __init__(self, **kwargs):
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.LazyLinear(2)

    def forward(self, X):
        # `X` 모양: (배치 크기, `num_hiddens`)
        return self.output(X)
</code></pre>
<p>우리는 [<strong><code>NextSentencePred</code> 인스턴스의 순방향 추론</strong>]이
각 BERT 입력 시퀀스에 대해 이진 예측을 반환함을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
nsp = NextSentencePred()
nsp.initialize()
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# PyTorch는 기본적으로 텐서를 평탄화하지 않지만, mxnet에서는 flatten=True인 경우
# 입력 데이터의 첫 번째 축을 제외한 모든 축이 함께 축소됩니다
encoded_X = torch.flatten(encoded_X, start_dim=1)
# NSP의 input_shape: (배치 크기, `num_hiddens`)
nsp = NextSentencePred()
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape
</code></pre>
<p>2개의 이진 분류에 대한 크로스 엔트로피 손실도 계산할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
nsp_y = np.array([0, 1])
nsp_l = loss(nsp_Y_hat, nsp_y)
nsp_l.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
nsp_y = torch.tensor([0, 1])
nsp_l = loss(nsp_Y_hat, nsp_y)
nsp_l.shape
</code></pre>
<p>앞서 언급한 두 사전 훈련 작업의 모든 레이블은
수동 라벨링 노력 없이 사전 훈련 코퍼스에서 사소하게 얻을 수 있다는 점이 주목할 만합니다.
원래 BERT는 BookCorpus :cite:<code>Zhu.Kiros.Zemel.ea.2015</code>와 영문 위키피디아의 연결에 대해 사전 훈련되었습니다.
이 두 텍스트 코퍼스는 거대합니다:
각각 8억 단어와 25억 단어를 가지고 있습니다.</p>
<h2 id="종합하기-putting-it-all-together-3"><a class="header" href="#종합하기-putting-it-all-together-3">[<strong>종합하기 (Putting It All Together)</strong>]</a></h2>
<p>BERT를 사전 훈련할 때, 최종 손실 함수는
마스킹된 언어 모델링과 다음 문장 예측에 대한 손실 함수의 선형 결합입니다.
이제 우리는 <code>BERTEncoder</code>, <code>MaskLM</code>, <code>NextSentencePred</code>의 세 가지 클래스를 인스턴스화하여 <code>BERTModel</code> 클래스를 정의할 수 있습니다.
순방향 추론은 인코딩된 BERT 표현 <code>encoded_X</code>,
마스킹된 언어 모델링의 예측 <code>mlm_Y_hat</code>,
그리고 다음 문장 예측 <code>nsp_Y_hat</code>을 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
class BERTModel(nn.Block):
    """BERT 모델."""
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_blks, dropout, max_len=1000):
        super(BERTModel, self).__init__()
        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,
                                   num_heads, num_blks, dropout, max_len)
        self.hidden = nn.Dense(num_hiddens, activation='tanh')
        self.mlm = MaskLM(vocab_size, num_hiddens)
        self.nsp = NextSentencePred()

    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):
        encoded_X = self.encoder(tokens, segments, valid_lens)
        if pred_positions is not None:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        else:
            mlm_Y_hat = None
        # 다음 문장 예측을 위한 MLP 분류기의 은닉층.
        # 0은 '&lt;cls&gt;' 토큰의 인덱스입니다
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
        return encoded_X, mlm_Y_hat, nsp_Y_hat
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
class BERTModel(nn.Module):
    """BERT 모델."""
    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, 
                 num_heads, num_blks, dropout, max_len=1000):
        super(BERTModel, self).__init__()
        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,
                                   num_heads, num_blks, dropout,
                                   max_len=max_len)
        self.hidden = nn.Sequential(nn.LazyLinear(num_hiddens),
                                    nn.Tanh())
        self.mlm = MaskLM(vocab_size, num_hiddens)
        self.nsp = NextSentencePred()

    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):
        encoded_X = self.encoder(tokens, segments, valid_lens)
        if pred_positions is not None:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        else:
            mlm_Y_hat = None
        # 다음 문장 예측을 위한 MLP 분류기의 은닉층.
        # 0은 '&lt;cls&gt;' 토큰의 인덱스입니다
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
        return encoded_X, mlm_Y_hat, nsp_Y_hat
</code></pre>
<h2 id="요약-summary-90"><a class="header" href="#요약-summary-90">요약 (Summary)</a></h2>
<ul>
<li>word2vec 및 GloVe와 같은 단어 임베딩 모델은 문맥 독립적입니다. 단어의 문맥에 관계없이(있는 경우) 동일한 단어에 동일한 사전 훈련된 벡터를 할당합니다. 자연어의 다의어나 복잡한 의미론을 잘 처리하기 어렵습니다.</li>
<li>ELMo 및 GPT와 같은 문맥 의존적 단어 표현의 경우, 단어의 표현은 문맥에 따라 달라집니다.</li>
<li>ELMo는 문맥을 양방향으로 인코딩하지만 작업별 아키텍처를 사용합니다(그러나 모든 자연어 처리 작업에 대해 특정 아키텍처를 만드는 것은 실제로 쉽지 않습니다). 반면 GPT는 작업 불가지론적이지만 문맥을 왼쪽에서 오른쪽으로 인코딩합니다.</li>
<li>BERT는 두 세계의 장점을 결합합니다: 문맥을 양방향으로 인코딩하고 광범위한 자연어 처리 작업에 대해 최소한의 아키텍처 변경만 요구합니다.</li>
<li>BERT 입력 시퀀스의 임베딩은 토큰 임베딩, 세그먼트 임베딩, 위치 임베딩의 합입니다.</li>
<li>BERT 사전 훈련은 마스킹된 언어 모델링과 다음 문장 예측이라는 두 가지 작업으로 구성됩니다. 전자는 단어를 표현하기 위해 양방향 문맥을 인코딩할 수 있으며, 후자는 텍스트 쌍 간의 논리적 관계를 명시적으로 모델링합니다.</li>
</ul>
<h2 id="연습-문제-exercises-105"><a class="header" href="#연습-문제-exercises-105">연습 문제 (Exercises)</a></h2>
<ol>
<li>다른 모든 조건이 동일하다면, 마스킹된 언어 모델은 왼쪽에서 오른쪽으로 진행하는 언어 모델보다 수렴하는 데 더 많은 사전 훈련 단계가 필요합니까 아니면 더 적게 필요합니까? 그 이유는 무엇입니까?</li>
<li>BERT의 원래 구현에서, <code>BERTEncoder</code>의 포지션와이즈 피드 포워드 네트워크(<code>d2l.TransformerEncoderBlock</code>를 통해)와 <code>MaskLM</code>의 완전 연결 레이어는 모두 활성화 함수로 GELU(Gaussian error linear unit) :cite:<code>Hendrycks.Gimpel.2016</code>를 사용합니다. GELU와 ReLU의 차이점을 조사하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/388">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1490">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bert-사전-훈련을-위한-데이터셋-the-dataset-for-pretraining-bert"><a class="header" href="#bert-사전-훈련을-위한-데이터셋-the-dataset-for-pretraining-bert">BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></h1>
<p>:label:<code>sec_bert-dataset</code></p>
<p>:numref:<code>sec_bert</code>에서 구현된 BERT 모델을 사전 훈련하기 위해, 두 가지 사전 훈련 작업인 마스킹된 언어 모델링(masked language modeling)과 다음 문장 예측(next sentence prediction)을 용이하게 할 수 있는 이상적인 형식의 데이터셋을 생성해야 합니다. 한편으로 원래 BERT 모델은 BookCorpus와 영어 위키피디아라는 두 거대한 코퍼스의 연결에 대해 사전 훈련되어(:numref:<code>subsec_bert_pretraining_tasks</code> 참조), 이 책의 대부분 독자들이 실행하기 어렵습니다. 다른 한편으로 기성 사전 훈련된 BERT 모델은 의학과 같은 특정 도메인의 응용 프로그램에는 적합하지 않을 수 있습니다. 따라서 사용자 정의 데이터셋에서 BERT를 사전 훈련하는 것이 인기를 얻고 있습니다. BERT 사전 훈련 시연을 용이하게 하기 위해, 우리는 더 작은 코퍼스인 WikiText-2 :cite:<code>Merity.Xiong.Bradbury.ea.2016</code>를 사용합니다.</p>
<p>:numref:<code>sec_word2vec_data</code>에서 word2vec 사전 훈련에 사용된 PTB 데이터셋과 비교할 때, WikiText-2는 (i) 원래의 구두점을 유지하여 다음 문장 예측에 적합하고, (ii) 원래의 대소문자와 숫자를 유지하며, (iii) 두 배 이상 큽니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
import os
import random

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import os
import random
import torch
</code></pre>
<p>[<strong>WikiText-2 데이터셋</strong>]에서 각 줄은 단락을 나타내며, 구두점과 그 앞의 토큰 사이에 공백이 삽입되어 있습니다. 최소 두 문장이 있는 단락만 유지됩니다. 문장을 분할하기 위해 단순함을 위해 마침표만 구분자로 사용합니다. 더 복잡한 문장 분할 기술에 대한 논의는 이 섹션 끝의 연습 문제로 남겨둡니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['wikitext-2'] = (
    'https://s3.amazonaws.com/research.metamind.io/wikitext/'
    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')

#@save
def _read_wiki(data_dir):
    file_name = os.path.join(data_dir, 'wiki.train.tokens')
    with open(file_name, 'r') as f:
        lines = f.readlines()
    # 대문자를 소문자로 변환
    paragraphs = [line.strip().lower().split(' . ')
                  for line in lines if len(line.split(' . ')) &gt;= 2]
    random.shuffle(paragraphs)
    return paragraphs
</code></pre>
<h2 id="사전-훈련-작업을-위한-도우미-함수-정의하기"><a class="header" href="#사전-훈련-작업을-위한-도우미-함수-정의하기">사전 훈련 작업을 위한 도우미 함수 정의하기</a></h2>
<p>다음에서는 두 가지 BERT 사전 훈련 작업인 다음 문장 예측과 마스킹된 언어 모델링을 위한 도우미 함수를 구현하는 것부터 시작합니다. 이러한 도우미 함수는 나중에 원시 텍스트 코퍼스를 BERT 사전 훈련을 위한 이상적인 형식의 데이터셋으로 변환할 때 호출될 것입니다.</p>
<h3 id="다음-문장-예측-작업-생성하기"><a class="header" href="#다음-문장-예측-작업-생성하기">[<strong>다음 문장 예측 작업 생성하기</strong>]</a></h3>
<p>:numref:<code>subsec_nsp</code>의 설명에 따라, <code>_get_next_sentence</code> 함수는 이진 분류 작업을 위한 훈련 예제를 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def _get_next_sentence(sentence, next_sentence, paragraphs):
    if random.random() &lt; 0.5:
        is_next = True
    else:
        # `paragraphs`는 리스트의 리스트의 리스트입니다
        next_sentence = random.choice(random.choice(paragraphs))
        is_next = False
    return sentence, next_sentence, is_next
</code></pre>
<p>다음 함수는 <code>_get_next_sentence</code> 함수를 호출하여 입력 <code>paragraph</code>로부터 다음 문장 예측을 위한 훈련 예제를 생성합니다. 여기서 <code>paragraph</code>는 문장들의 리스트이고 각 문장은 토큰들의 리스트입니다. <code>max_len</code> 인수는 사전 훈련 중 BERT 입력 시퀀스의 최대 길이를 지정합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):
    nsp_data_from_paragraph = []
    for i in range(len(paragraph) - 1):
        tokens_a, tokens_b, is_next = _get_next_sentence(
            paragraph[i], paragraph[i + 1], paragraphs)
        # 1개의 '&lt;cls&gt;' 토큰과 2개의 '&lt;sep&gt;' 토큰 고려
        if len(tokens_a) + len(tokens_b) + 3 &gt; max_len:
            continue
        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
        nsp_data_from_paragraph.append((tokens, segments, is_next))
    return nsp_data_from_paragraph
</code></pre>
<h3 id="마스킹된-언어-모델링-작업-생성하기"><a class="header" href="#마스킹된-언어-모델링-작업-생성하기">[<strong>마스킹된 언어 모델링 작업 생성하기</strong>]</a></h3>
<p>:label:<code>subsec_prepare_mlm_data</code></p>
<p>BERT 입력 시퀀스로부터 마스킹된 언어 모델링 작업을 위한 훈련 예제를 생성하기 위해, 다음 <code>_replace_mlm_tokens</code> 함수를 정의합니다. 입력에서 <code>tokens</code>는 BERT 입력 시퀀스를 나타내는 토큰 리스트이고, <code>candidate_pred_positions</code>는 특수 토큰을 제외한 BERT 입력 시퀀스의 토큰 인덱스 리스트(마스킹된 언어 모델링 작업에서는 특수 토큰을 예측하지 않음)이며, <code>num_mlm_preds</code>는 예측할 수(예측할 무작위 토큰 15% 상기)를 나타냅니다. :numref:<code>subsec_mlm</code>의 마스킹된 언어 모델링 작업 정의에 따라, 각 예측 위치에서 입력은 특수 “<mask>” 토큰이나 무작위 토큰으로 대체되거나 변경되지 않은 상태로 유지될 수 있습니다. 마지막으로 함수는 가능한 대체 후의 입력 토큰, 예측이 일어나는 토큰 인덱스 및 이러한 예측에 대한 레이블을 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
                        vocab):
    # 마스킹된 언어 모델의 입력을 위해, 토큰의 새로운 복사본을 만들고 
    # 그 중 일부를 '&lt;mask&gt;' 또는 무작위 토큰으로 대체합니다
    mlm_input_tokens = [token for token in tokens]
    pred_positions_and_labels = []
    # 마스킹된 언어 모델링 작업에서 예측을 위해 15%의 무작위 토큰을 얻기 위해 섞음
    random.shuffle(candidate_pred_positions)
    for mlm_pred_position in candidate_pred_positions:
        if len(pred_positions_and_labels) &gt;= num_mlm_preds:
            break
        masked_token = None
        # 80%의 경우: 단어를 '&lt;mask&gt;' 토큰으로 교체
        if random.random() &lt; 0.8:
            masked_token = '&lt;mask&gt;'
        else:
            # 10%의 경우: 단어를 변경하지 않고 유지
            if random.random() &lt; 0.5:
                masked_token = tokens[mlm_pred_position]
            # 10%의 경우: 단어를 무작위 단어로 교체
            else:
                masked_token = random.choice(vocab.idx_to_token)
        mlm_input_tokens[mlm_pred_position] = masked_token
        pred_positions_and_labels.append(
            (mlm_pred_position, tokens[mlm_pred_position]))
    return mlm_input_tokens, pred_positions_and_labels
</code></pre>
<p>앞서 언급한 <code>_replace_mlm_tokens</code> 함수를 호출하여, 다음 함수는 BERT 입력 시퀀스(<code>tokens</code>)를 입력으로 받아 (:numref:<code>subsec_mlm</code>에서 설명한 가능한 토큰 대체 후의) 입력 토큰 인덱스, 예측이 일어나는 토큰 인덱스 및 이러한 예측에 대한 레이블 인덱스를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def _get_mlm_data_from_tokens(tokens, vocab):
    candidate_pred_positions = []
    # `tokens`는 문자열 리스트입니다
    for i, token in enumerate(tokens):
        # 마스킹된 언어 모델링 작업에서 특수 토큰은 예측하지 않음
        if token in ['&lt;cls&gt;', '&lt;sep&gt;']:
            continue
        candidate_pred_positions.append(i)
    # 마스킹된 언어 모델링 작업에서 15%의 무작위 토큰을 예측함
    num_mlm_preds = max(1, round(len(tokens) * 0.15))
    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
        tokens, candidate_pred_positions, num_mlm_preds, vocab)
    pred_positions_and_labels = sorted(pred_positions_and_labels,
                                       key=lambda x: x[0])
    pred_positions = [v[0] for v in pred_positions_and_labels]
    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]
    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]
</code></pre>
<h2 id="텍스트를-사전-훈련-데이터셋으로-변환하기"><a class="header" href="#텍스트를-사전-훈련-데이터셋으로-변환하기">텍스트를 사전 훈련 데이터셋으로 변환하기</a></h2>
<p>이제 BERT 사전 훈련을 위한 <code>Dataset</code> 클래스를 사용자 정의할 준비가 거의 다 되었습니다. 그 전에, [<strong>입력에 특수 “<pad>” 토큰을 추가하기 위한</strong>] 도우미 함수 <code>_pad_bert_inputs</code>를 정의해야 합니다. 이 함수의 <code>examples</code> 인수는 두 가지 사전 훈련 작업을 위한 도우미 함수 <code>_get_nsp_data_from_paragraph</code> 및 <code>_get_mlm_data_from_tokens</code>의 출력을 포함합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def _pad_bert_inputs(examples, max_len, vocab):
    max_num_mlm_preds = round(max_len * 0.15)
    all_token_ids, all_segments, valid_lens,  = [], [], []
    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []
    nsp_labels = []
    for (token_ids, pred_positions, mlm_pred_label_ids, segments,
         is_next) in examples:
        all_token_ids.append(np.array(token_ids + [vocab['&lt;pad&gt;']] * (
            max_len - len(token_ids)), dtype='int32'))
        all_segments.append(np.array(segments + [0] * (
            max_len - len(segments)), dtype='int32'))
        # `valid_lens`는 '&lt;pad&gt;' 토큰 수를 제외함
        valid_lens.append(np.array(len(token_ids), dtype='float32'))
        all_pred_positions.append(np.array(pred_positions + [0] * (
            max_num_mlm_preds - len(pred_positions)), dtype='int32'))
        # 패딩된 토큰의 예측은 0 가중치 곱셈을 통해 손실에서 필터링됨
        all_mlm_weights.append(
            np.array([1.0] * len(mlm_pred_label_ids) + [0.0] * (
                max_num_mlm_preds - len(pred_positions)), dtype='float32'))
        all_mlm_labels.append(np.array(mlm_pred_label_ids + [0] * (
            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype='int32'))
        nsp_labels.append(np.array(is_next))
    return (all_token_ids, all_segments, valid_lens, all_pred_positions,
            all_mlm_weights, all_mlm_labels, nsp_labels)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def _pad_bert_inputs(examples, max_len, vocab):
    max_num_mlm_preds = round(max_len * 0.15)
    all_token_ids, all_segments, valid_lens,  = [], [], []
    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []
    nsp_labels = []
    for (token_ids, pred_positions, mlm_pred_label_ids, segments,
         is_next) in examples:
        all_token_ids.append(torch.tensor(token_ids + [vocab['&lt;pad&gt;']] * (
            max_len - len(token_ids)), dtype=torch.long))
        all_segments.append(torch.tensor(segments + [0] * (
            max_len - len(segments)), dtype=torch.long))
        # `valid_lens`는 '&lt;pad&gt;' 토큰 수를 제외함
        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))
        all_pred_positions.append(torch.tensor(pred_positions + [0] * (
            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))
        # 패딩된 토큰의 예측은 0 가중치 곱셈을 통해 손실에서 필터링됨
        all_mlm_weights.append(
            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (
                max_num_mlm_preds - len(pred_positions)),
                dtype=torch.float32))
        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (
            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))
        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))
    return (all_token_ids, all_segments, valid_lens, all_pred_positions,
            all_mlm_weights, all_mlm_labels, nsp_labels)
</code></pre>
<p>두 가지 사전 훈련 작업의 훈련 예제를 생성하기 위한 도우미 함수와 입력을 패딩하기 위한 도우미 함수를 결합하여, [<strong>BERT 사전 훈련을 위한 WikiText-2 데이터셋</strong>]으로서 다음 <code>_WikiTextDataset</code> 클래스를 사용자 정의합니다. <code>__getitem__</code> 함수를 구현함으로써 WikiText-2 코퍼스의 문장 쌍으로부터 생성된 사전 훈련(마스킹된 언어 모델링 및 다음 문장 예측) 예제에 임의로 접근할 수 있습니다.</p>
<p>원래 BERT 모델은 어휘 크기가 30,000인 WordPiece 임베딩을 사용합니다 :cite:<code>Wu.Schuster.Chen.ea.2016</code>. WordPiece의 토큰화 방법은 :numref:<code>subsec_Byte_Pair_Encoding</code>의 원래 바이트 페어 인코딩 알고리즘을 약간 수정한 것입니다. 단순함을 위해 여기서는 <code>d2l.tokenize</code> 함수를 사용하여 토큰화합니다. 5번 미만으로 나타나는 드문 토큰은 필터링됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
class _WikiTextDataset(gluon.data.Dataset):
    def __init__(self, paragraphs, max_len):
        # 입력 `paragraphs[i]`는 단락을 나타내는 문장 문자열들의 리스트입니다;
        # 출력 `paragraphs[i]`는 단락을 나타내는 문장들의 리스트이며 각 문장은 토큰들의 리스트입니다
        paragraphs = [d2l.tokenize(
            paragraph, token='word') for paragraph in paragraphs]
        sentences = [sentence for paragraph in paragraphs
                     for sentence in paragraph]
        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
            '&lt;pad&gt;', '&lt;mask&gt;', '&lt;cls&gt;', '&lt;sep&gt;'])
        # 다음 문장 예측 작업을 위한 데이터 가져오기
        examples = []
        for paragraph in paragraphs:
            examples.extend(_get_nsp_data_from_paragraph(
                paragraph, paragraphs, self.vocab, max_len))
        # 마스킹된 언어 모델 작업을 위한 데이터 가져오기
        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
                      + (segments, is_next))
                     for tokens, segments, is_next in examples]
        # 입력 패딩
        (self.all_token_ids, self.all_segments, self.valid_lens,
         self.all_pred_positions, self.all_mlm_weights,
         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
            examples, max_len, self.vocab)

    def __getitem__(self, idx):
        return (self.all_token_ids[idx], self.all_segments[idx],
                self.valid_lens[idx], self.all_pred_positions[idx],
                self.all_mlm_weights[idx], self.all_mlm_labels[idx],
                self.nsp_labels[idx])

    def __len__(self):
        return len(self.all_token_ids)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
class _WikiTextDataset(torch.utils.data.Dataset):
    def __init__(self, paragraphs, max_len):
        # 입력 `paragraphs[i]`는 단락을 나타내는 문장 문자열들의 리스트입니다;
        # 출력 `paragraphs[i]`는 단락을 나타내는 문장들의 리스트이며 각 문장은 토큰들의 리스트입니다
        paragraphs = [d2l.tokenize(
            paragraph, token='word') for paragraph in paragraphs]
        sentences = [sentence for paragraph in paragraphs
                     for sentence in paragraph]
        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
            '&lt;pad&gt;', '&lt;mask&gt;', '&lt;cls&gt;', '&lt;sep&gt;'])
        # 다음 문장 예측 작업을 위한 데이터 가져오기
        examples = []
        for paragraph in paragraphs:
            examples.extend(_get_nsp_data_from_paragraph(
                paragraph, paragraphs, self.vocab, max_len))
        # 마스킹된 언어 모델 작업을 위한 데이터 가져오기
        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
                      + (segments, is_next))
                     for tokens, segments, is_next in examples]
        # 입력 패딩
        (self.all_token_ids, self.all_segments, self.valid_lens,
         self.all_pred_positions, self.all_mlm_weights,
         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
            examples, max_len, self.vocab)

    def __getitem__(self, idx):
        return (self.all_token_ids[idx], self.all_segments[idx],
                self.valid_lens[idx], self.all_pred_positions[idx],
                self.all_mlm_weights[idx], self.all_mlm_labels[idx],
                self.nsp_labels[idx])

    def __len__(self):
        return len(self.all_token_ids)
</code></pre>
<p><code>_read_wiki</code> 함수와 <code>_WikiTextDataset</code> 클래스를 사용하여, [<strong>WikiText-2 데이터셋을 다운로드하고 그로부터 사전 훈련 예제를 생성하는</strong>] 다음 <code>load_data_wiki</code>를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def load_data_wiki(batch_size, max_len):
    """WikiText-2 데이터셋을 로드합니다."""
    num_workers = d2l.get_dataloader_workers()
    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')
    paragraphs = _read_wiki(data_dir)
    train_set = _WikiTextDataset(paragraphs, max_len)
    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
                                       num_workers=num_workers)
    return train_iter, train_set.vocab
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def load_data_wiki(batch_size, max_len):
    """WikiText-2 데이터셋을 로드합니다."""
    num_workers = d2l.get_dataloader_workers()
    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')
    paragraphs = _read_wiki(data_dir)
    train_set = _WikiTextDataset(paragraphs, max_len)
    train_iter = torch.utils.data.DataLoader(train_set, batch_size,
                                        shuffle=True, num_workers=num_workers)
    return train_iter, train_set.vocab
</code></pre>
<p>배치 크기를 512로, BERT 입력 시퀀스의 최대 길이를 64로 설정하여, [<strong>BERT 사전 훈련 예제의 미니배치 모양을 인쇄해 봅니다.</strong>] 각 BERT 입력 시퀀스에서 마스킹된 언어 모델링 작업을 위해 $10$ ($64 \times 0.15$)개의 위치가 예측됨에 유의하십시오.</p>
<pre><code class="language-{.python .input}">#@tab all
batch_size, max_len = 512, 64
train_iter, vocab = load_data_wiki(batch_size, max_len)

for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,
     mlm_Y, nsp_y) in train_iter:
    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,
          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,
          nsp_y.shape)
    break
</code></pre>
<p>마지막으로 어휘 크기를 살펴봅시다. 드문 토큰을 필터링한 후에도 PTB 데이터셋보다 두 배 이상 큽니다.</p>
<pre><code class="language-{.python .input}">#@tab all
len(vocab)
</code></pre>
<h2 id="요약-summary-91"><a class="header" href="#요약-summary-91">요약 (Summary)</a></h2>
<ul>
<li>PTB 데이터셋과 비교할 때, WikiText-2 데이터셋은 원래의 구두점, 대소문자 및 숫자를 유지하며 두 배 이상 큽니다.</li>
<li>WikiText-2 코퍼스의 문장 쌍으로부터 생성된 사전 훈련(마스킹된 언어 모델링 및 다음 문장 예측) 예제에 임의로 접근할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-106"><a class="header" href="#연습-문제-exercises-106">연습 문제 (Exercises)</a></h2>
<ol>
<li>단순함을 위해 문장 분할을 위한 유일한 구분자로 마침표를 사용했습니다. spaCy나 NLTK와 같은 다른 문장 분할 기술을 시도해 보십시오. NLTK를 예로 들면, 먼저 NLTK를 설치해야 합니다: <code>pip install nltk</code>. 코드에서 먼저 <code>import nltk</code>를 합니다. 그런 다음 Punkt 문장 토크나이저를 다운로드합니다: <code>nltk.download('punkt')</code>. <code>sentences = 'This is great ! Why not ?'</code>와 같은 문장을 분할하려면, <code>nltk.tokenize.sent_tokenize(sentences)</code>를 호출하여 두 문장 문자열의 리스트인 <code>['This is great !', 'Why not ?']</code>을 반환합니다.</li>
<li>드문 토큰을 하나도 필터링하지 않는다면 어휘 크기는 얼마입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/389">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1496">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bert-사전-훈련-pretraining-bert-1"><a class="header" href="#bert-사전-훈련-pretraining-bert-1">BERT 사전 훈련 (Pretraining BERT)</a></h1>
<p>:label:<code>sec_bert-pretraining</code></p>
<p>:numref:<code>sec_bert</code>에서 구현된 BERT 모델과
:numref:<code>sec_bert-dataset</code>에서 WikiText-2 데이터셋으로 생성된 사전 훈련 예제를 사용하여, 이 섹션에서는 WikiText-2 데이터셋에서 BERT를 사전 훈련할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<p>시작하기 위해, 마스킹된 언어 모델링과 다음 문장 예측을 위한 사전 훈련 예제의 미니배치로 WikiText-2 데이터셋을 로드합니다.
배치 크기는 512이고 BERT 입력 시퀀스의 최대 길이는 64입니다.
원래 BERT 모델에서는 최대 길이가 512라는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">#@tab all
batch_size, max_len = 512, 64
train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)
</code></pre>
<h2 id="bert-사전-훈련-pretraining-bert-2"><a class="header" href="#bert-사전-훈련-pretraining-bert-2">BERT 사전 훈련 (Pretraining BERT)</a></h2>
<p>원래 BERT에는 서로 다른 모델 크기의 두 가지 버전이 있습니다 :cite:<code>Devlin.Chang.Lee.ea.2018</code>.
기본 모델($\textrm{BERT}_\textrm{BASE}$)은 768개의 은닉 유닛(은닉 크기)과 12개의 셀프 어텐션 헤드가 있는 12개의 레이어(트랜스포머 인코더 블록)를 사용합니다.
대형 모델($\textrm{BERT}_\textrm{LARGE}$)은 1024개의 은닉 유닛과 16개의 셀프 어텐션 헤드가 있는 24개의 레이어를 사용합니다.
주목할 점은 전자는 1억 1천만 개의 파라미터를 갖는 반면, 후자는 3억 4천만 개의 파라미터를 갖는다는 것입니다.
쉬운 시연을 위해,
우리는 [<strong>2개의 레이어, 128개의 은닉 유닛, 2개의 셀프 어텐션 헤드를 사용하는 작은 BERT</strong>]를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = d2l.BERTModel(len(vocab), num_hiddens=128, ffn_num_hiddens=256,
                    num_heads=2, num_blks=2, dropout=0.2)
devices = d2l.try_all_gpus()
net.initialize(init.Xavier(), ctx=devices)
loss = gluon.loss.SoftmaxCELoss()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = d2l.BERTModel(len(vocab), num_hiddens=128, 
                    ffn_num_hiddens=256, num_heads=2, num_blks=2, dropout=0.2)
devices = d2l.try_all_gpus()
loss = nn.CrossEntropyLoss()
</code></pre>
<p>훈련 루프를 정의하기 전에,
도우미 함수 <code>_get_batch_loss_bert</code>를 정의합니다.
훈련 예제의 샤드가 주어졌을 때,
이 함수는 [<strong>마스킹된 언어 모델링 및 다음 문장 예측 작업 모두에 대한 손실을 계산합니다</strong>].
BERT 사전 훈련의 최종 손실은
마스킹된 언어 모델링 손실과
다음 문장 예측 손실의 합일뿐입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def _get_batch_loss_bert(net, loss, vocab_size, tokens_X_shards,
                         segments_X_shards, valid_lens_x_shards,
                         pred_positions_X_shards, mlm_weights_X_shards,
                         mlm_Y_shards, nsp_y_shards):
    mlm_ls, nsp_ls, ls = [], [], []
    for (tokens_X_shard, segments_X_shard, valid_lens_x_shard,
         pred_positions_X_shard, mlm_weights_X_shard, mlm_Y_shard,
         nsp_y_shard) in zip(
        tokens_X_shards, segments_X_shards, valid_lens_x_shards,
        pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards,
        nsp_y_shards):
        # 순방향 패스
        _, mlm_Y_hat, nsp_Y_hat = net(
            tokens_X_shard, segments_X_shard, valid_lens_x_shard.reshape(-1),
            pred_positions_X_shard)
        # 마스킹된 언어 모델 손실 계산
        mlm_l = loss(
            mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y_shard.reshape(-1),
            mlm_weights_X_shard.reshape((-1, 1)))
        mlm_l = mlm_l.sum() / (mlm_weights_X_shard.sum() + 1e-8)
        # 다음 문장 예측 손실 계산
        nsp_l = loss(nsp_Y_hat, nsp_y_shard)
        nsp_l = nsp_l.mean()
        mlm_ls.append(mlm_l)
        nsp_ls.append(nsp_l)
        ls.append(mlm_l + nsp_l)
        npx.waitall()
    return mlm_ls, nsp_ls, ls
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,
                         segments_X, valid_lens_x,
                         pred_positions_X, mlm_weights_X,
                         mlm_Y, nsp_y):
    # 순방향 패스
    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,
                                  valid_lens_x.reshape(-1),
                                  pred_positions_X)
    # 마스킹된 언어 모델 손실 계산
    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\
    mlm_weights_X.reshape(-1, 1)
    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)
    # 다음 문장 예측 손실 계산
    nsp_l = loss(nsp_Y_hat, nsp_y)
    l = mlm_l + nsp_l
    return mlm_l, nsp_l, l
</code></pre>
<p>앞서 언급한 두 도우미 함수를 호출하여,
다음 <code>train_bert</code> 함수는
[<strong>WikiText-2 (<code>train_iter</code>) 데이터셋에서 BERT (<code>net</code>)를 사전 훈련</strong>]하는 절차를 정의합니다.
BERT 훈련은 매우 오래 걸릴 수 있습니다.
<code>train_ch13</code> 함수(:numref:<code>sec_image_augmentation</code> 참조)에서와 같이 훈련 에포크 수를 지정하는 대신,
다음 함수의 입력 <code>num_steps</code>는
훈련을 위한 반복 단계 수를 지정합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):
    trainer = gluon.Trainer(net.collect_params(), 'adam',
                            {'learning_rate': 0.01})
    step, timer = 0, d2l.Timer()
    animator = d2l.Animator(xlabel='step', ylabel='loss',
                            xlim=[1, num_steps], legend=['mlm', 'nsp'])
    # 마스킹된 언어 모델링 손실의 합, 다음 문장 예측 손실의 합,
    # 문장 쌍의 수, 카운트
    metric = d2l.Accumulator(4)
    num_steps_reached = False
    while step &lt; num_steps and not num_steps_reached:
        for batch in train_iter:
            (tokens_X_shards, segments_X_shards, valid_lens_x_shards,
             pred_positions_X_shards, mlm_weights_X_shards,
             mlm_Y_shards, nsp_y_shards) = [gluon.utils.split_and_load(
                elem, devices, even_split=False) for elem in batch]
            timer.start()
            with autograd.record():
                mlm_ls, nsp_ls, ls = _get_batch_loss_bert(
                    net, loss, vocab_size, tokens_X_shards, segments_X_shards,
                    valid_lens_x_shards, pred_positions_X_shards,
                    mlm_weights_X_shards, mlm_Y_shards, nsp_y_shards)
            for l in ls:
                l.backward()
            trainer.step(1)
            mlm_l_mean = sum([float(l) for l in mlm_ls]) / len(mlm_ls)
            nsp_l_mean = sum([float(l) for l in nsp_ls]) / len(nsp_ls)
            metric.add(mlm_l_mean, nsp_l_mean, batch[0].shape[0], 1)
            timer.stop()
            animator.add(step + 1,
                         (metric[0] / metric[3], metric[1] / metric[3]))
            step += 1
            if step == num_steps:
                num_steps_reached = True
                break

    print(f'MLM loss {metric[0] / metric[3]:.3f}, '
          f'NSP loss {metric[1] / metric[3]:.3f}')
    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '
          f'{str(devices)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):
    net(*next(iter(train_iter))[:4])
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    trainer = torch.optim.Adam(net.parameters(), lr=0.01)
    step, timer = 0, d2l.Timer()
    animator = d2l.Animator(xlabel='step', ylabel='loss',
                            xlim=[1, num_steps], legend=['mlm', 'nsp'])
    # 마스킹된 언어 모델링 손실의 합, 다음 문장 예측 손실의 합,
    # 문장 쌍의 수, 카운트
    metric = d2l.Accumulator(4)
    num_steps_reached = False
    while step &lt; num_steps and not num_steps_reached:
        for tokens_X, segments_X, valid_lens_x, pred_positions_X,
            mlm_weights_X, mlm_Y, nsp_y in train_iter:
            tokens_X = tokens_X.to(devices[0])
            segments_X = segments_X.to(devices[0])
            valid_lens_x = valid_lens_x.to(devices[0])
            pred_positions_X = pred_positions_X.to(devices[0])
            mlm_weights_X = mlm_weights_X.to(devices[0])
            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])
            trainer.zero_grad()
            timer.start()
            mlm_l, nsp_l, l = _get_batch_loss_bert(
                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,
                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)
            l.backward()
            trainer.step()
            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)
            timer.stop()
            animator.add(step + 1,
                         (metric[0] / metric[3], metric[1] / metric[3]))
            step += 1
            if step == num_steps:
                num_steps_reached = True
                break

    print(f'MLM loss {metric[0] / metric[3]:.3f}, '
          f'NSP loss {metric[1] / metric[3]:.3f}')
    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '
          f'{str(devices)}')
</code></pre>
<p>BERT 사전 훈련 중에 마스킹된 언어 모델링 손실과 다음 문장 예측 손실을 모두 그릴 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
train_bert(train_iter, net, loss, len(vocab), devices, 50)
</code></pre>
<h2 id="bert로-텍스트-표현하기-representing-text-with-bert"><a class="header" href="#bert로-텍스트-표현하기-representing-text-with-bert">[<strong>BERT로 텍스트 표현하기 (Representing Text with BERT)</strong>]</a></h2>
<p>BERT를 사전 훈련한 후,
이를 사용하여 단일 텍스트, 텍스트 쌍 또는 그 안의 모든 토큰을 표현할 수 있습니다.
다음 함수는 <code>tokens_a</code>와 <code>tokens_b</code>의 모든 토큰에 대한 BERT(<code>net</code>) 표현을 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def get_bert_encoding(net, tokens_a, tokens_b=None):
    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
    token_ids = np.expand_dims(np.array(vocab[tokens], ctx=devices[0]),
                               axis=0)
    segments = np.expand_dims(np.array(segments, ctx=devices[0]), axis=0)
    valid_len = np.expand_dims(np.array(len(tokens), ctx=devices[0]), axis=0)
    encoded_X, _, _ = net(token_ids, segments, valid_len)
    return encoded_X
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def get_bert_encoding(net, tokens_a, tokens_b=None):
    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)
    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)
    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)
    encoded_X, _, _ = net(token_ids, segments, valid_len)
    return encoded_X
</code></pre>
<p>[<strong>"a crane is flying"이라는 문장을 고려해 봅시다.</strong>]
:numref:<code>subsec_bert_input_rep</code>에서 논의한 BERT의 입력 표현을 상기해 보십시오.
특수 토큰 “&lt;cls&gt;”(분류용)와 “&lt;sep&gt;”(구분용)을 삽입한 후,
BERT 입력 시퀀스의 길이는 6이 됩니다.
0은 “&lt;cls&gt;” 토큰의 인덱스이므로,
<code>encoded_text[:, 0, :]</code>는 전체 입력 문장에 대한 BERT 표현입니다.
다의어 토큰 "crane"을 평가하기 위해,
토큰의 BERT 표현의 처음 세 요소도 출력합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
tokens_a = ['a', 'crane', 'is', 'flying']
encoded_text = get_bert_encoding(net, tokens_a)
# 토큰: '&lt;cls&gt;', 'a', 'crane', 'is', 'flying', '&lt;sep&gt;'
encoded_text_cls = encoded_text[:, 0, :]
encoded_text_crane = encoded_text[:, 2, :]
encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]
</code></pre>
<p>[<strong>이제 문장 쌍 "a crane driver came"과 "he just left"를 고려해 봅시다.</strong>]
마찬가지로 <code>encoded_pair[:, 0, :]</code>는 사전 훈련된 BERT의 전체 문장 쌍의 인코딩된 결과입니다.
다의어 토큰 "crane"의 처음 세 요소가 문맥이 다를 때와 다르다는 점에 유의하십시오.
이는 BERT 표현이 문맥 의존적이라는 것을 뒷받침합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']
encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)
# 토큰: '&lt;cls&gt;', 'a', 'crane', 'driver', 'came', '&lt;sep&gt;', 'he', 'just',
# 'left', '&lt;sep&gt;'
encoded_pair_cls = encoded_pair[:, 0, :]
encoded_pair_crane = encoded_pair[:, 2, :]
encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]
</code></pre>
<p>:numref:<code>chap_nlp_app</code>에서, 우리는 다운스트림 자연어 처리 응용 프로그램을 위해 사전 훈련된 BERT 모델을 미세 조정할 것입니다.</p>
<h2 id="요약-summary-92"><a class="header" href="#요약-summary-92">요약 (Summary)</a></h2>
<ul>
<li>원래 BERT에는 두 가지 버전이 있으며, 기본 모델은 1억 1천만 개의 파라미터를, 대형 모델은 3억 4천만 개의 파라미터를 가지고 있습니다.</li>
<li>BERT를 사전 훈련한 후, 이를 사용하여 단일 텍스트, 텍스트 쌍 또는 그 안의 모든 토큰을 표현할 수 있습니다.</li>
<li>실험에서, 동일한 토큰이라도 문맥이 다르면 BERT 표현이 다릅니다. 이는 BERT 표현이 문맥 의존적이라는 것을 뒷받침합니다.</li>
</ul>
<h2 id="연습-문제-exercises-107"><a class="header" href="#연습-문제-exercises-107">연습 문제 (Exercises)</a></h2>
<ol>
<li>실험에서 마스킹된 언어 모델링 손실이 다음 문장 예측 손실보다 상당히 높다는 것을 알 수 있습니다. 그 이유는 무엇입니까?</li>
<li>BERT 입력 시퀀스의 최대 길이를 512(원래 BERT 모델과 동일)로 설정하십시오. $\textrm{BERT}_\textrm{LARGE}$와 같은 원래 BERT 모델의 구성을 사용하십시오. 이 섹션을 실행할 때 오류가 발생합니까? 그 이유는 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/390">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1497">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="자연어-처리-응용-natural-language-processing-applications"><a class="header" href="#자연어-처리-응용-natural-language-processing-applications">자연어 처리: 응용 (Natural Language Processing: Applications)</a></h1>
<p>:label:<code>chap_nlp_app</code></p>
<p>우리는 :numref:<code>chap_nlp_pretrain</code>에서 텍스트 시퀀스의 토큰을 표현하고 그 표현을 훈련하는 방법을 보았습니다.
이러한 사전 훈련된 텍스트 표현은 다양한 다운스트림 자연어 처리 작업을 위해 다양한 모델에 공급될 수 있습니다.</p>
<p>사실,
이전 챕터에서는 딥러닝 아키텍처를 설명하기 위해
<em>사전 훈련 없이</em> 일부 자연어 처리 응용 프로그램을 이미 논의했습니다.
예를 들어, :numref:<code>chap_rnn</code>에서,
우리는 RNN에 의존하여 단편 소설과 같은 텍스트를 생성하는 언어 모델을 설계했습니다.
:numref:<code>chap_modern_rnn</code> 및 :numref:<code>chap_attention-and-transformers</code>에서,
우리는 또한 기계 번역을 위해 RNN 및 주의 메커니즘을 기반으로 한 모델을 설계했습니다.</p>
<p>그러나 이 책은 그러한 모든 응용 프로그램을 포괄적으로 다루려는 의도는 아닙니다.
대신,
우리의 초점은 <em>자연어 처리 문제를 해결하기 위해 언어의 (심층) 표현 학습을 적용하는 방법</em>에 있습니다.
사전 훈련된 텍스트 표현이 주어졌을 때,
이 장에서는 각각 단일 텍스트와 텍스트 쌍의 관계를 분석하는
두 가지 인기 있고 대표적인
다운스트림 자연어 처리 작업인
감정 분석과 자연어 추론을 탐구할 것입니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/nlp-map-app.svg" alt="사전 훈련된 텍스트 표현은 다양한 다운스트림 자연어 처리 응용 프로그램을 위해 다양한 딥러닝 아키텍처에 공급될 수 있습니다. 이 장에서는 다양한 다운스트림 자연어 처리 응용 프로그램을 위한 모델을 설계하는 방법에 중점을 둡니다." />
:label:<code>fig_nlp-map-app</code></p>
<p>:numref:<code>fig_nlp-map-app</code>에 묘사된 바와 같이,
이 장에서는 MLP, CNN, RNN 및 주의와 같은 다양한 유형의 딥러닝 아키텍처를 사용하여 자연어 처리 모델을 설계하는 기본 아이디어를 설명하는 데 중점을 둡니다.
:numref:<code>fig_nlp-map-app</code>의 모든 응용 프로그램에 대해 모든 사전 훈련된 텍스트 표현을 모든 아키텍처와 결합하는 것이 가능하지만,
우리는 몇 가지 대표적인 조합을 선택합니다.
구체적으로, 우리는 감정 분석을 위해 RNN 및 CNN을 기반으로 한 인기 있는 아키텍처를 탐구할 것입니다.
자연어 추론의 경우, 우리는 텍스트 쌍을 분석하는 방법을 보여주기 위해 주의와 MLP를 선택합니다.
마지막으로, 우리는 시퀀스 수준(단일 텍스트 분류 및 텍스트 쌍 분류)
및 토큰 수준(텍스트 태깅 및 질문 응답)과 같은
광범위한 자연어 처리 응용 프로그램을 위해 사전 훈련된 BERT 모델을 미세 조정하는 방법을 소개합니다.
구체적인 경험적 사례로,
우리는 자연어 추론을 위해 BERT를 미세 조정할 것입니다.</p>
<p>:numref:<code>sec_bert</code>에서 소개했듯이,
BERT는 광범위한 자연어 처리 응용 프로그램을 위해 최소한의 아키텍처 변경을 요구합니다.
그러나 이 이점은 다운스트림 응용 프로그램을 위해
엄청난 수의 BERT 파라미터를 미세 조정하는 비용을 수반합니다.
공간이나 시간이 제한된 경우,
MLP, CNN, RNN 및 주의를 기반으로 한 제작 모델이 더 실현 가능합니다.
다음으로, 감정 분석 응용 프로그램부터 시작하여
각각 RNN 및 CNN을 기반으로 한 모델 설계를 설명합니다.</p>
<pre><code class="language-toc">:maxdepth: 2

sentiment-analysis-and-dataset
sentiment-analysis-rnn
sentiment-analysis-cnn
natural-language-inference-and-dataset
natural-language-inference-attention
finetuning-bert
natural-language-inference-bert
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="감정-분석과-데이터셋-sentiment-analysis-and-the-dataset"><a class="header" href="#감정-분석과-데이터셋-sentiment-analysis-and-the-dataset">감정 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></h1>
<p>:label:<code>sec_sentiment</code></p>
<p>온라인 소셜 미디어와 리뷰 플랫폼의 확산으로,
방대한 양의
의견 데이터가 기록되었으며,
의사 결정 과정을 지원할 수 있는 큰 잠재력을 가지고 있습니다.
*감정 분석(Sentiment analysis)*은
제품 리뷰,
블로그 댓글,
포럼 토론과 같이
사람들이 생산한 텍스트에서 사람들의 감정을 연구합니다.
이는 정치(예: 정책에 대한 대중의 정서 분석),
금융(예: 시장의 정서 분석),
마케팅(예: 제품 연구 및 브랜드 관리)과 같이
다양한 분야에 널리 응용되고 있습니다.</p>
<p>감정은
이산적인 극성이나 척도(예: 긍정 및 부정)로 분류될 수 있으므로,
우리는 감정 분석을
가변 길이 텍스트 시퀀스를
고정 길이 텍스트 범주로 변환하는
텍스트 분류 작업으로 간주할 수 있습니다.
이 장에서는
감정 분석을 위해 스탠포드의 <a href="https://ai.stanford.edu/%7Eamaas/data/sentiment/">대규모 영화 리뷰 데이터셋</a>을 사용할 것입니다.
이것은 훈련 세트와 테스트 세트로 구성되며,
각각 IMDb에서 다운로드한 25,000개의 영화 리뷰를 포함합니다.
두 데이터셋 모두
"긍정(positive)"과 "부정(negative)" 레이블이 동일한 수로 있어
서로 다른 감정 극성을 나타냅니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
import os
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
import os
</code></pre>
<h2 id="데이터셋-읽기-reading-the-dataset-11"><a class="header" href="#데이터셋-읽기-reading-the-dataset-11">데이터셋 읽기 (Reading the Dataset)</a></h2>
<p>먼저, 이 IMDb 리뷰 데이터셋을 다운로드하고
<code>../data/aclImdb</code> 경로에 압축을 풉니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
d2l.DATA_HUB['aclImdb'] = (d2l.DATA_URL + 'aclImdb_v1.tar.gz',
                          '01ada507287d82875905620988597833ad4e0903')

data_dir = d2l.download_extract('aclImdb', 'aclImdb')
</code></pre>
<p>다음으로, 훈련 및 테스트 데이터셋을 읽습니다. 각 예제는 리뷰와 그 레이블입니다: "긍정"은 1, "부정"은 0입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def read_imdb(data_dir, is_train):
    """IMDb 리뷰 데이터셋 텍스트 시퀀스와 레이블을 읽습니다."""
    data, labels = [], []
    for label in ('pos', 'neg'):
        folder_name = os.path.join(data_dir, 'train' if is_train else 'test',
                                   label)
        for file in os.listdir(folder_name):
            with open(os.path.join(folder_name, file), 'rb') as f:
                review = f.read().decode('utf-8').replace('\n', '')
                data.append(review)
                labels.append(1 if label == 'pos' else 0)
    return data, labels

train_data = read_imdb(data_dir, is_train=True)
print('# trainings:', len(train_data[0]))
for x, y in zip(train_data[0][:3], train_data[1][:3]):
    print('label:', y, 'review:', x[:60])
</code></pre>
<h2 id="데이터셋-전처리-preprocessing-the-dataset"><a class="header" href="#데이터셋-전처리-preprocessing-the-dataset">데이터셋 전처리 (Preprocessing the Dataset)</a></h2>
<p>각 단어를 토큰으로 취급하고
5회 미만으로 나타나는 단어를 필터링하여,
훈련 데이터셋에서 어휘(vocabulary)를 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
train_tokens = d2l.tokenize(train_data[0], token='word')
vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['&lt;pad&gt;'])
</code></pre>
<p>토큰화 후,
토큰 단위의 리뷰 길이 히스토그램을 그려봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
d2l.set_figsize()
d2l.plt.xlabel('# tokens per review')
d2l.plt.ylabel('count')
d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));
</code></pre>
<p>예상대로,
리뷰의 길이는 다양합니다.
매번 이러한 리뷰의 미니배치를 처리하기 위해,
우리는 :numref:<code>sec_machine_translation</code>의
기계 번역 데이터셋에 대한 전처리 단계와 유사하게
자르기(truncation) 및 패딩(padding)을 사용하여 각 리뷰의 길이를 500으로 설정합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
num_steps = 500  # 시퀀스 길이
train_features = d2l.tensor([d2l.truncate_pad(
    vocab[line], num_steps, vocab['&lt;pad&gt;']) for line in train_tokens])
print(train_features.shape)
</code></pre>
<h2 id="데이터-반복자-생성-creating-data-iterators"><a class="header" href="#데이터-반복자-생성-creating-data-iterators">데이터 반복자 생성 (Creating Data Iterators)</a></h2>
<p>이제 데이터 반복자를 생성할 수 있습니다.
각 반복마다 예제의 미니배치가 반환됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
train_iter = d2l.load_array((train_features, train_data[1]), 64)

for X, y in train_iter:
    print('X:', X.shape, ', y:', y.shape)
    break
print('# batches:', len(train_iter))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), 64)

for X, y in train_iter:
    print('X:', X.shape, ', y:', y.shape)
    break
print('# batches:', len(train_iter))
</code></pre>
<h2 id="종합하기-putting-it-all-together-4"><a class="header" href="#종합하기-putting-it-all-together-4">종합하기 (Putting It All Together)</a></h2>
<p>마지막으로, 위 단계를 <code>load_data_imdb</code> 함수로 래핑합니다.
이 함수는 훈련 및 테스트 데이터 반복자와 IMDb 리뷰 데이터셋의 어휘를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def load_data_imdb(batch_size, num_steps=500):
    """데이터 반복자와 IMDb 리뷰 데이터셋의 어휘를 반환합니다."""
    data_dir = d2l.download_extract('aclImdb', 'aclImdb')
    train_data = read_imdb(data_dir, True)
    test_data = read_imdb(data_dir, False)
    train_tokens = d2l.tokenize(train_data[0], token='word')
    test_tokens = d2l.tokenize(test_data[0], token='word')
    vocab = d2l.Vocab(train_tokens, min_freq=5)
    train_features = np.array([d2l.truncate_pad(
        vocab[line], num_steps, vocab['&lt;pad&gt;']) for line in train_tokens])
    test_features = np.array([d2l.truncate_pad(
        vocab[line], num_steps, vocab['&lt;pad&gt;']) for line in test_tokens])
    train_iter = d2l.load_array((train_features, train_data[1]), batch_size)
    test_iter = d2l.load_array((test_features, test_data[1]), batch_size,
                               is_train=False)
    return train_iter, test_iter, vocab
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def load_data_imdb(batch_size, num_steps=500):
    """데이터 반복자와 IMDb 리뷰 데이터셋의 어휘를 반환합니다."""
    data_dir = d2l.download_extract('aclImdb', 'aclImdb')
    train_data = read_imdb(data_dir, True)
    test_data = read_imdb(data_dir, False)
    train_tokens = d2l.tokenize(train_data[0], token='word')
    test_tokens = d2l.tokenize(test_data[0], token='word')
    vocab = d2l.Vocab(train_tokens, min_freq=5)
    train_features = torch.tensor([d2l.truncate_pad(
        vocab[line], num_steps, vocab['&lt;pad&gt;']) for line in train_tokens])
    test_features = torch.tensor([d2l.truncate_pad(
        vocab[line], num_steps, vocab['&lt;pad&gt;']) for line in test_tokens])
    train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),
                                batch_size)
    test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),
                               batch_size,
                               is_train=False)
    return train_iter, test_iter, vocab
</code></pre>
<h2 id="요약-summary-93"><a class="header" href="#요약-summary-93">요약 (Summary)</a></h2>
<ul>
<li>감정 분석은 생성된 텍스트에서 사람들의 감정을 연구하며, 이는 가변 길이 텍스트 시퀀스를 고정 길이 텍스트 범주로 변환하는 텍스트 분류 문제로 간주됩니다.</li>
<li>전처리 후, 스탠포드의 대규모 영화 리뷰 데이터셋(IMDb 리뷰 데이터셋)을 어휘와 함께 데이터 반복자로 로드할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-108"><a class="header" href="#연습-문제-exercises-108">연습 문제 (Exercises)</a></h2>
<ol>
<li>감정 분석 모델 훈련을 가속화하기 위해 이 섹션의 어떤 하이퍼파라미터를 수정할 수 있습니까?</li>
<li><a href="https://snap.stanford.edu/data/web-Amazon.html">Amazon 리뷰</a> 데이터셋을 감정 분석을 위한 데이터 반복자와 레이블로 로드하는 함수를 구현할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/391">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1387">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="감정-분석-순환-신경망-사용-sentiment-analysis-using-recurrent-neural-networks"><a class="header" href="#감정-분석-순환-신경망-사용-sentiment-analysis-using-recurrent-neural-networks">감정 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></h1>
<p>:label:<code>sec_sentiment_rnn</code></p>
<p>단어 유사성 및 유추 작업과 마찬가지로,
사전 훈련된 단어 벡터를
감정 분석에도 적용할 수 있습니다.
:numref:<code>sec_sentiment</code>의 IMDb 리뷰 데이터셋은
그리 크지 않으므로,
대규모 코퍼스에서
사전 훈련된 텍스트 표현을 사용하면
모델의 과대적합을 줄일 수 있습니다.
:numref:<code>fig_nlp-map-sa-rnn</code>에 설명된
구체적인 예로서,
우리는 사전 훈련된 GloVe 모델을 사용하여 각 토큰을 표현하고,
이러한 토큰 표현을
다층 양방향 RNN에 공급하여
텍스트 시퀀스 표현을 얻습니다.
이 표현은
감정 분석 출력으로 변환됩니다 :cite:<code>Maas.Daly.Pham.ea.2011</code>.
동일한 다운스트림 응용 프로그램에 대해,
나중에 다른 아키텍처 선택을 고려할 것입니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/nlp-map-sa-rnn.svg" alt="이 섹션에서는 감정 분석을 위해 사전 훈련된 GloVe를 RNN 기반 아키텍처에 공급합니다." />
:label:<code>fig_nlp-map-sa-rnn</code></p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn, rnn
npx.set_np()

batch_size = 64
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn

batch_size = 64
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)
</code></pre>
<h2 id="rnn으로-단일-텍스트-표현하기-representing-single-text-with-rnns"><a class="header" href="#rnn으로-단일-텍스트-표현하기-representing-single-text-with-rnns">RNN으로 단일 텍스트 표현하기 (Representing Single Text with RNNs)</a></h2>
<p>감정 분석과 같은 텍스트 분류 작업에서,
가변 길이 텍스트 시퀀스는
고정 길이 범주로 변환됩니다.
다음 <code>BiRNN</code> 클래스에서,
텍스트 시퀀스의 각 토큰은
임베딩 레이어(<code>self.embedding</code>)를 통해
개별적인 사전 훈련된 GloVe 표현을 얻지만,
전체 시퀀스는
양방향 RNN(<code>self.encoder</code>)에 의해 인코딩됩니다.
더 구체적으로,
초기 및 최종 타임 스텝에서의
양방향 LSTM의 (마지막 레이어의) 은닉 상태가
텍스트 시퀀스의 표현으로 연결(concatenated)됩니다.
이 단일 텍스트 표현은
두 개의 출력("긍정" 및 "부정")이 있는
완전 연결 레이어(<code>self.decoder</code>)에 의해
출력 범주로 변환됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class BiRNN(nn.Block):
    def __init__(self, vocab_size, embed_size, num_hiddens,
                 num_layers, **kwargs):
        super(BiRNN, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # `bidirectional`을 True로 설정하여 양방향 RNN을 얻습니다
        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,
                                bidirectional=True, input_size=embed_size)
        self.decoder = nn.Dense(2)

    def forward(self, inputs):
        # `inputs`의 모양은 (배치 크기, 타임 스텝 수)입니다.
        # LSTM은 입력의 첫 번째 차원이 시간 차원일 것을 요구하므로,
        # 토큰 표현을 얻기 전에 입력을 전치합니다.
        # 출력 모양은 (타임 스텝 수, 배치 크기, 단어 벡터 차원)입니다
        embeddings = self.embedding(inputs.T)
        # 다른 타임 스텝에서의 마지막 은닉층의 은닉 상태를 반환합니다.
        # `outputs`의 모양은 (타임 스텝 수, 배치 크기, 2 * 은닉 유닛 수)입니다
        outputs = self.encoder(embeddings)
        # 초기 및 최종 타임 스텝의 은닉 상태를 연결하여 완전 연결 레이어의 입력으로 사용합니다.
        # 그 모양은 (배치 크기, 4 * 은닉 유닛 수)입니다
        encoding = np.concatenate((outputs[0], outputs[-1]), axis=1)
        outs = self.decoder(encoding)
        return outs
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class BiRNN(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens,
                 num_layers, **kwargs):
        super(BiRNN, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # `bidirectional`을 True로 설정하여 양방향 RNN을 얻습니다
        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,
                                bidirectional=True)
        self.decoder = nn.Linear(4 * num_hiddens, 2)

    def forward(self, inputs):
        # `inputs`의 모양은 (배치 크기, 타임 스텝 수)입니다.
        # LSTM은 입력의 첫 번째 차원이 시간 차원일 것을 요구하므로,
        # 토큰 표현을 얻기 전에 입력을 전치합니다.
        # 출력 모양은 (타임 스텝 수, 배치 크기, 단어 벡터 차원)입니다
        embeddings = self.embedding(inputs.T)
        self.encoder.flatten_parameters()
        # 다른 타임 스텝에서의 마지막 은닉층의 은닉 상태를 반환합니다.
        # `outputs`의 모양은 (타임 스텝 수, 배치 크기, 2 * 은닉 유닛 수)입니다
        outputs, _ = self.encoder(embeddings)
        # 초기 및 최종 타임 스텝의 은닉 상태를 연결하여 완전 연결 레이어의 입력으로 사용합니다.
        # 그 모양은 (배치 크기, 4 * 은닉 유닛 수)입니다
        encoding = torch.cat((outputs[0], outputs[-1]), dim=1) 
        outs = self.decoder(encoding)
        return outs
</code></pre>
<p>감정 분석을 위해 단일 텍스트를 표현하는 두 개의 은닉층이 있는 양방향 RNN을 구성해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
embed_size, num_hiddens, num_layers, devices = 100, 100, 2, d2l.try_all_gpus()
net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)
</code></pre>
<pre><code class="language-{.python .input}">#@tab mxnet
net.initialize(init.Xavier(), ctx=devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def init_weights(module):
    if type(module) == nn.Linear:
        nn.init.xavier_uniform_(module.weight)
    if type(module) == nn.LSTM:
        for param in module._flat_weights_names:
            if "weight" in param:
                nn.init.xavier_uniform_(module._parameters[param])
net.apply(init_weights);
</code></pre>
<h2 id="사전-훈련된-단어-벡터-로드-loading-pretrained-word-vectors-1"><a class="header" href="#사전-훈련된-단어-벡터-로드-loading-pretrained-word-vectors-1">사전 훈련된 단어 벡터 로드 (Loading Pretrained Word Vectors)</a></h2>
<p>아래에서는 어휘의 토큰에 대해 사전 훈련된 100차원(<code>embed_size</code>와 일치해야 함) GloVe 임베딩을 로드합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
</code></pre>
<p>어휘의 모든 토큰에 대한 벡터의 모양을 출력합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
embeds = glove_embedding[vocab.idx_to_token]
embeds.shape
</code></pre>
<p>우리는 이 사전 훈련된 단어 벡터를 사용하여
리뷰의 토큰을 표현하며,
훈련 중에 이 벡터들을 업데이트하지 않을 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net.embedding.weight.set_data(embeds)
net.embedding.collect_params().setattr('grad_req', 'null')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net.embedding.weight.data.copy_(embeds)
net.embedding.weight.requires_grad = False
</code></pre>
<h2 id="모델-훈련-및-평가-training-and-evaluating-the-model"><a class="header" href="#모델-훈련-및-평가-training-and-evaluating-the-model">모델 훈련 및 평가 (Training and Evaluating the Model)</a></h2>
<p>이제 감정 분석을 위해 양방향 RNN을 훈련할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
lr, num_epochs = 0.01, 5
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
lr, num_epochs = 0.01, 5
trainer = torch.optim.Adam(net.parameters(), lr=lr)
loss = nn.CrossEntropyLoss(reduction="none")
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<p>훈련된 모델 <code>net</code>을 사용하여 텍스트 시퀀스의 감정을 예측하는 다음 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def predict_sentiment(net, vocab, sequence):
    """텍스트 시퀀스의 감정을 예측합니다."""
    sequence = np.array(vocab[sequence.split()], ctx=d2l.try_gpu())
    label = np.argmax(net(sequence.reshape(1, -1)), axis=1)
    return 'positive' if label == 1 else 'negative'
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def predict_sentiment(net, vocab, sequence):
    """텍스트 시퀀스의 감정을 예측합니다."""
    sequence = torch.tensor(vocab[sequence.split()], device=d2l.try_gpu())
    label = torch.argmax(net(sequence.reshape(1, -1)), dim=1)
    return 'positive' if label == 1 else 'negative'
</code></pre>
<p>마지막으로, 훈련된 모델을 사용하여 두 개의 간단한 문장에 대한 감정을 예측해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
predict_sentiment(net, vocab, 'this movie is so great')
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
predict_sentiment(net, vocab, 'this movie is so bad')
</code></pre>
<h2 id="요약-summary-94"><a class="header" href="#요약-summary-94">요약 (Summary)</a></h2>
<ul>
<li>사전 훈련된 단어 벡터는 텍스트 시퀀스의 개별 토큰을 나타낼 수 있습니다.</li>
<li>양방향 RNN은 초기 및 최종 타임 스텝의 은닉 상태를 연결하는 것과 같이 텍스트 시퀀스를 나타낼 수 있습니다. 이 단일 텍스트 표현은 완전 연결 레이어를 사용하여 범주로 변환될 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-109"><a class="header" href="#연습-문제-exercises-109">연습 문제 (Exercises)</a></h2>
<ol>
<li>에포크 수를 늘리십시오. 훈련 및 테스트 정확도를 향상시킬 수 있습니까? 다른 하이퍼파라미터를 조정하는 것은 어떻습니까?</li>
<li>300차원 GloVe 임베딩과 같은 더 큰 사전 훈련된 단어 벡터를 사용해 보십시오. 분류 정확도가 향상됩니까?</li>
<li>spaCy 토큰화를 사용하여 분류 정확도를 향상시킬 수 있습니까? spaCy를 설치(<code>pip install spacy</code>)하고 영어 패키지를 설치(<code>python -m spacy download en</code>)해야 합니다. 코드에서 먼저 spaCy를 가져옵니다(<code>import spacy</code>). 그런 다음 spaCy 영어 패키지를 로드합니다(<code>spacy_en = spacy.load('en')</code>). 마지막으로 함수 <code>def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]</code>를 정의하고 원래 <code>tokenizer</code> 함수를 교체합니다. GloVe와 spaCy에서 구문 토큰의 형태가 다르다는 점에 유의하십시오. 예를 들어 구문 토큰 "new york"은 GloVe에서는 "new-york" 형태를 취하고 spaCy 토큰화 후에는 "new york" 형태를 취합니다.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/392">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1424">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="감정-분석-합성곱-신경망-사용하기-sentiment-analysis-using-convolutional-neural-networks"><a class="header" href="#감정-분석-합성곱-신경망-사용하기-sentiment-analysis-using-convolutional-neural-networks">감정 분석: 합성곱 신경망 사용하기 (Sentiment Analysis: Using Convolutional Neural Networks)</a></h1>
<p>:label:<code>sec_sentiment_cnn</code></p>
<p>:numref:<code>chap_cnn</code>에서 우리는 인접한 픽셀과 같은 국소적 특성(local features)에 적용되는 2차원 CNN을 사용하여 2차원 이미지 데이터를 처리하는 메커니즘을 조사했습니다. 원래 컴퓨터 비전을 위해 설계되었지만, CNN은 자연어 처리에도 널리 사용됩니다. 간단히 말해서, 모든 텍스트 시퀀스를 1차원 이미지로 생각하면 됩니다. 이러한 방식으로 1차원 CNN은 텍스트의 $n$-그램과 같은 국소적 특성을 처리할 수 있습니다.</p>
<p>이 섹션에서는 <em>textCNN</em> 모델을 사용하여 단일 텍스트를 표현하기 위한 CNN 아키텍처를 설계하는 방법을 보여줄 것입니다(:cite:<code>Kim.2014</code>). 감정 분석을 위해 GloVe 사전 훈련을 받은 RNN 아키텍처를 사용하는 :numref:<code>fig_nlp-map-sa-rnn</code>과 비교할 때, :numref:<code>fig_nlp-map-sa-cnn</code>에서의 유일한 차이점은 아키텍처 선택에 있습니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/nlp-map-sa-cnn.svg" alt="이 섹션에서는 감정 분석을 위해 사전 훈련된 GloVe를 CNN 기반 아키텍처에 공급합니다." />
:label:<code>fig_nlp-map-sa-cnn</code></p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()

batch_size = 64
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn

batch_size = 64
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)
</code></pre>
<h2 id="1차원-합성곱-one-dimensional-convolutions"><a class="header" href="#1차원-합성곱-one-dimensional-convolutions">1차원 합성곱 (One-Dimensional Convolutions)</a></h2>
<p>모델을 소개하기 전에 1차원 합성곱이 어떻게 작동하는지 살펴봅시다. 이것은 교차 상관(cross-correlation) 연산에 기반한 2차원 합성곱의 특수한 사례일 뿐임을 명심하십시오.</p>
<p><img src="chapter_natural-language-processing-applications/../img/conv1d.svg" alt="1차원 교차 상관 연산. 음영 처리된 부분은 첫 번째 출력 요소와 출력 계산에 사용된 입력 및 커널 텐서 요소입니다: $0\times1+1\times2=2$." />
:label:<code>fig_conv1d</code></p>
<p>:numref:<code>fig_conv1d</code>에 표시된 것처럼, 1차원 사례에서 합성곱 창(convolution window)은 입력 텐서 전체를 왼쪽에서 오른쪽으로 슬라이딩합니다. 슬라이딩하는 동안 특정 위치의 합성곱 창에 포함된 입력 서브텐서(예: :numref:<code>fig_conv1d</code>의 0과 1)와 커널 텐서(예: :numref:<code>fig_conv1d</code>의 1과 2)가 요소별로 곱해집니다. 이러한 곱셈의 합은 출력 텐서의 대응하는 위치에서 단일 스칼라 값(예: :numref:<code>fig_conv1d</code>에서 $0\times1+1\times2=2$)을 제공합니다.</p>
<p>다음 <code>corr1d</code> 함수에서 1차원 교차 상관을 구현합니다. 입력 텐서 <code>X</code>와 커널 텐<code>K</code>가 주어지면 출력 텐서 <code>Y</code>를 반환합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def corr1d(X, K):
    w = K.shape[0]
    Y = d2l.zeros((X.shape[0] - w + 1))
    for i in range(Y.shape[0]):
        Y[i] = (X[i: i + w] * K).sum()
    return Y
</code></pre>
<p>우리는 :numref:<code>fig_conv1d</code>의 입력 텐서 <code>X</code>와 커널 텐서 <code>K</code>를 구성하여 위의 1차원 교차 상관 구현의 출력을 검증할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
X, K = d2l.tensor([0, 1, 2, 3, 4, 5, 6]), d2l.tensor([1, 2])
corr1d(X, K)
</code></pre>
<p>채널이 여러 개인 1차원 입력의 경우, 합성곱 커널은 동일한 수의 입력 채널을 가져야 합니다. 그런 다음 각 채널에 대해 입력의 1차원 텐서와 합성곱 커널의 1차원 텐서에 대해 교차 상관 연산을 수행하고, 모든 채널에 대해 결과를 합산하여 1차원 출력 텐서를 생성합니다. :numref:<code>fig_conv1d_channel</code>은 3개의 입력 채널이 있는 1차원 교차 상관 연산을 보여줍니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/conv1d-channel.svg" alt="3개의 입력 채널이 있는 1차원 교차 상관 연산. 음영 처리된 부분은 첫 번째 출력 요소와 출력 계산에 사용된 입력 및 커널 텐서 요소입니다: $0\times1+1\times2+1\times3+2\times4+2\times(-1)+3\times(-3)=2$." />
:label:<code>fig_conv1d_channel</code></p>
<p>우리는 여러 입력 채널에 대한 1차원 교차 상관 연산을 구현하고 :numref:<code>fig_conv1d_channel</code>의 결과를 검증할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
def corr1d_multi_in(X, K):
    # 먼저 `X`와 `K`의 0번째 차원(채널 차원)을 반복합니다. 그런 다음 그것들을 함께 더합니다.
    return sum(corr1d(x, k) for x, k in zip(X, K))

X = d2l.tensor([[0, 1, 2, 3, 4, 5, 6],
              [1, 2, 3, 4, 5, 6, 7],
              [2, 3, 4, 5, 6, 7, 8]])
K = d2l.tensor([[1, 2], [3, 4], [-1, -3]])
corr1d_multi_in(X, K)
</code></pre>
<p>다중 입력 채널 1차원 교차 상관은 단일 입력 채널 2차원 교차 상관과 동일합니다. 이를 설명하기 위해, :numref:<code>fig_conv1d_channel</code>의 다중 입력 채널 1차원 교차 상관의 등가 형태는 :numref:<code>fig_conv1d_2d</code>의 단일 입력 채널 2차원 교차 상관이며, 여기서 합성곱 커널의 높이는 입력 텐서의 높이와 같아야 합니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/conv1d-2d.svg" alt="단일 입력 채널이 있는 2차원 교차 상관 연산. 음영 처리된 부분은 첫 번째 출력 요소와 출력 계산에 사용된 입력 및 커널 텐서 요소입니다: $2\times(-1)+3\times(-3)+1\times3+2\times4+0\times1+1\times2=2$." />
:label:<code>fig_conv1d_2d</code></p>
<p>:numref:<code>fig_conv1d</code> 및 :numref:<code>fig_conv1d_channel</code>의 출력은 모두 채널이 하나뿐입니다. :numref:<code>subsec_multi-output-channels</code>에서 설명한 다중 출력 채널이 있는 2차원 합성곱과 마찬가지로, 1차원 합성곱에 대해서도 다중 출력 채널을 지정할 수 있습니다.</p>
<h2 id="맥스-오버-타임-풀링-max-over-time-pooling"><a class="header" href="#맥스-오버-타임-풀링-max-over-time-pooling">맥스 오버 타임 풀링 (Max-Over-Time Pooling)</a></h2>
<p>마찬가지로 풀링을 사용하여 시퀀스 표현에서 가장 높은 값을 타임스텝 전체에서 가장 중요한 특성으로 추출할 수 있습니다. textCNN에서 사용되는 *맥스 오버 타임 풀링(max-over-time pooling)*은 1차원 글로벌 맥스 풀링(global max-pooling)처럼 작동합니다(:cite:<code>Collobert.Weston.Bottou.ea.2011</code>). 각 채널이 서로 다른 타임스텝의 값을 저장하는 다중 채널 입력의 경우, 각 채널의 출력은 해당 채널의 최대값입니다. 맥스 오버 타임 풀링을 사용하면 채널마다 타임스텝 수가 달라도 된다는 점에 유의하십시오.</p>
<h2 id="textcnn-모델"><a class="header" href="#textcnn-모델">textCNN 모델</a></h2>
<p>1차원 합성곱과 맥스 오버 타임 풀링을 사용하여 textCNN 모델은 개별 사전 훈련된 토큰 표현을 입력으로 받은 다음, 다운스트림 애플리케이션을 위한 시퀀스 표현을 얻고 변환합니다.</p>
<p>$d$차원 벡터로 표현된 $n$개의 토큰이 있는 단일 텍스트 시퀀스의 경우, 입력 텐서의 너비, 높이 및 채널 수는 각각 $n, 1, d$입니다. textCNN 모델은 다음과 같이 입력을 출력으로 변환합니다:</p>
<ol>
<li>여러 개의 1차원 합성곱 커널을 정의하고 입력에 대해 별도로 합성곱 연산을 수행합니다. 너비가 다른 합성곱 커널은 서로 다른 수의 인접 토큰 사이의 국소적 특성을 포착할 수 있습니다.</li>
<li>모든 출력 채널에 대해 맥스 오버 타임 풀링을 수행한 다음, 모든 스칼라 풀링 출력을 벡터로 연결합니다.</li>
<li>완전 연결 레이어를 사용하여 연결된 벡터를 출력 카테고리로 변환합니다. 과대적합을 줄이기 위해 드롭아웃을 사용할 수 있습니다.</li>
</ol>
<p><img src="chapter_natural-language-processing-applications/../img/textcnn.svg" alt="textCNN의 모델 아키텍처." />
:label:<code>fig_conv1d_textcnn</code></p>
<p>:numref:<code>fig_conv1d_textcnn</code>은 구체적인 예와 함께 textCNN의 모델 아키텍처를 설명합니다. 입력은 11개의 토큰이 있는 문장이며, 각 토큰은 6차원 벡터로 표현됩니다. 따라서 너비 11의 6채널 입력이 있습니다. 너비 2와 4의 두 개의 1차원 합성곱 커널을 정의하며, 각각 4개와 5개의 출력 채널을 갖습니다. 이들은 너비가 $11-2+1=10$인 4개의 출력 채널과 너비가 $11-4+1=8$인 5개의 출력 채널을 생성합니다. 이 9개 채널의 너비가 다르더라도 맥스 오버 타임 풀링은 연결된 9차원 벡터를 제공하며, 이는 최종적으로 이진 감정 예측을 위한 2차원 출력 벡터로 변환됩니다.</p>
<h3 id="모델-정의-defining-the-model-1"><a class="header" href="#모델-정의-defining-the-model-1">모델 정의 (Defining the Model)</a></h3>
<p>우리는 다음 클래스에서 textCNN 모델을 구현합니다. :numref:<code>sec_sentiment_rnn</code>의 양방향 RNN 모델과 비교할 때, 순환 레이어를 합성곱 레이어로 교체하는 것 외에도 두 개의 임베딩 레이어를 사용합니다. 하나는 학습 가능한 가중치가 있고 다른 하나는 고정된 가중치가 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class TextCNN(nn.Block):
    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,
                 **kwargs):
        super(TextCNN, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # 훈련되지 않을 임베딩 레이어
        self.constant_embedding = nn.Embedding(vocab_size, embed_size)
        self.dropout = nn.Dropout(0.5)
        self.decoder = nn.Dense(2)
        # 맥스 오버 타임 풀링 레이어는 파라미터가 없으므로 이 인스턴스를 공유할 수 있습니다.
        self.pool = nn.GlobalMaxPool1D()
        # 여러 개의 1차원 합성곱 레이어 생성
        self.convs = nn.Sequential()
        for c, k in zip(num_channels, kernel_sizes):
            self.convs.add(nn.Conv1D(c, k, activation='relu'))

    def forward(self, inputs):
        # (배치 크기, 토큰 수, 토큰 벡터 차원) 모양의 두 임베딩 레이어 출력을 벡터를 따라 연결합니다.
        embeddings = np.concatenate((
            self.embedding(inputs), self.constant_embedding(inputs)), axis=2)
        # 1차원 합성곱 레이어의 입력 형식에 따라 두 번째 차원이 채널을 저장하도록 텐서를 재정렬합니다.
        embeddings = embeddings.transpose(0, 2, 1)
        # 각 1차원 합성곱 레이어에 대해, 맥스 오버 타임 풀링 후 (배치 크기, 채널 수, 1) 모양의 텐서가 얻어집니다.
        # 마지막 차원을 제거하고 채널을 따라 연결합니다.
        encoding = np.concatenate([ 
            np.squeeze(self.pool(conv(embeddings)), axis=-1)
            for conv in self.convs], axis=1)
        outputs = self.decoder(self.dropout(encoding))
        return outputs
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class TextCNN(nn.Module):
    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,
                 **kwargs):
        super(TextCNN, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # 훈련되지 않을 임베딩 레이어
        self.constant_embedding = nn.Embedding(vocab_size, embed_size)
        self.dropout = nn.Dropout(0.5)
        self.decoder = nn.Linear(sum(num_channels), 2)
        # 맥스 오버 타임 풀링 레이어는 파라미터가 없으므로 이 인스턴스를 공유할 수 있습니다.
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.relu = nn.ReLU()
        # 여러 개의 1차원 합성곱 레이어 생성
        self.convs = nn.ModuleList()
        for c, k in zip(num_channels, kernel_sizes):
            self.convs.append(nn.Conv1d(2 * embed_size, c, k))

    def forward(self, inputs):
        # (배치 크기, 토큰 수, 토큰 벡터 차원) 모양의 두 임베딩 레이어 출력을 벡터를 따라 연결합니다.
        embeddings = torch.cat((
            self.embedding(inputs), self.constant_embedding(inputs)), dim=2)
        # 1차원 합성곱 레이어의 입력 형식에 따라 두 번째 차원이 채널을 저장하도록 텐서를 재정렬합니다.
        embeddings = embeddings.permute(0, 2, 1)
        # 각 1차원 합성곱 레이어에 대해, 맥스 오버 타임 풀링 후 (배치 크기, 채널 수, 1) 모양의 텐서가 얻어집니다.
        # 마지막 차원을 제거하고 채널을 따라 연결합니다.
        encoding = torch.cat([
            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)
            for conv in self.convs], dim=1)
        outputs = self.decoder(self.dropout(encoding))
        return outputs
</code></pre>
<p>textCNN 인스턴스를 생성해 봅시다. 이는 커널 너비가 3, 4, 5이고 모두 100개의 출력 채널을 갖는 3개의 합성곱 레이어를 가집니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]
devices = d2l.try_all_gpus()
net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)
net.initialize(init.Xavier(), ctx=devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]
devices = d2l.try_all_gpus()
net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)

def init_weights(module):
    if type(module) in (nn.Linear, nn.Conv1d):
        nn.init.xavier_uniform_(module.weight)

net.apply(init_weights);
</code></pre>
<h3 id="사전-훈련된-단어-벡터-로드-loading-pretrained-word-vectors-2"><a class="header" href="#사전-훈련된-단어-벡터-로드-loading-pretrained-word-vectors-2">사전 훈련된 단어 벡터 로드 (Loading Pretrained Word Vectors)</a></h3>
<p>:numref:<code>sec_sentiment_rnn</code>과 마찬가지로 사전 훈련된 100차원 GloVe 임베딩을 초기화된 토큰 표현으로 로드합니다. 이러한 토큰 표현(임베딩 가중치)은 <code>embedding</code>에서 훈련되고 <code>constant_embedding</code>에서 고정됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
embeds = glove_embedding[vocab.idx_to_token]
net.embedding.weight.set_data(embeds)
net.constant_embedding.weight.set_data(embeds)
net.constant_embedding.collect_params().setattr('grad_req', 'null')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
embeds = glove_embedding[vocab.idx_to_token]
net.embedding.weight.data.copy_(embeds)
net.constant_embedding.weight.data.copy_(embeds)
net.constant_embedding.weight.requires_grad = False
</code></pre>
<h3 id="모델-훈련-및-평가-training-and-evaluating-the-model-1"><a class="header" href="#모델-훈련-및-평가-training-and-evaluating-the-model-1">모델 훈련 및 평가 (Training and Evaluating the Model)</a></h3>
<p>이제 감정 분석을 위해 textCNN 모델을 훈련할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
lr, num_epochs = 0.001, 5
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
lr, num_epochs = 0.001, 5
trainer = torch.optim.Adam(net.parameters(), lr=lr)
loss = nn.CrossEntropyLoss(reduction="none")
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<p>아래에서는 훈련된 모델을 사용하여 두 개의 간단한 문장에 대한 감정을 예측합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
d2l.predict_sentiment(net, vocab, 'this movie is so great')
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
d2l.predict_sentiment(net, vocab, 'this movie is so bad')
</code></pre>
<h2 id="요약-summary-95"><a class="header" href="#요약-summary-95">요약 (Summary)</a></h2>
<ul>
<li>1차원 CNN은 텍스트의 $n$-그램과 같은 국소적 특성을 처리할 수 있습니다.</li>
<li>다중 입력 채널 1차원 교차 상관은 단일 입력 채널 2차원 교차 상관과 동일합니다.</li>
<li>맥스 오버 타임 풀링을 사용하면 채널마다 타임스텝 수가 달라도 됩니다.</li>
<li>textCNN 모델은 1차원 합성곱 레이어와 맥스 오버 타임 풀링 레이어를 사용하여 개별 토큰 표현을 다운스트림 애플리케이션 출력으로 변환합니다.</li>
</ul>
<h2 id="연습-문제-exercises-110"><a class="header" href="#연습-문제-exercises-110">연습 문제 (Exercises)</a></h2>
<ol>
<li>하이퍼파라미터를 튜닝하고 분류 정확도 및 계산 효율성 측면에서 :numref:<code>sec_sentiment_rnn</code>과 이 섹션의 두 감정 분석 아키텍처를 비교해 보십시오.</li>
<li>:numref:<code>sec_sentiment_rnn</code>의 연습 문제에서 소개된 방법을 사용하여 모델의 분류 정확도를 더 향상시킬 수 있습니까?</li>
<li>입력 표현에 포지셔널 인코딩을 추가하십시오. 분류 정확도가 향상됩니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/393">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1425">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="자연어-추론과-데이터셋-natural-language-inference-and-the-dataset"><a class="header" href="#자연어-추론과-데이터셋-natural-language-inference-and-the-dataset">자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></h1>
<p>:label:<code>sec_natural-language-inference-and-dataset</code></p>
<p>:numref:<code>sec_sentiment</code>에서, 우리는 감정 분석 문제에 대해 논의했습니다.
이 작업은 단일 텍스트 시퀀스를 미리 정의된 범주(예: 감정 극성 집합)로 분류하는 것을 목표로 합니다.
그러나 한 문장이 다른 문장에서 유추될 수 있는지 결정해야 하거나,
의미적으로 동등한 문장을 식별하여 중복을 제거해야 할 때,
하나의 텍스트 시퀀스를 분류하는 방법을 아는 것만으로는 충분하지 않습니다.
대신, 우리는 텍스트 시퀀스 쌍에 대해 추론할 수 있어야 합니다.</p>
<h2 id="자연어-추론-natural-language-inference"><a class="header" href="#자연어-추론-natural-language-inference">자연어 추론 (Natural Language Inference)</a></h2>
<p>*자연어 추론(Natural language inference)*은 *가설(hypothesis)*이
*전제(premise)*로부터 유추될 수 있는지를 연구합니다. 여기서 둘 다 텍스트 시퀀스입니다.
즉, 자연어 추론은 텍스트 시퀀스 쌍 간의 논리적 관계를 결정합니다.
이러한 관계는 일반적으로 다음 세 가지 유형으로 나뉩니다:</p>
<ul>
<li><em>함의(Entailment)</em>: 가설이 전제로부터 유추될 수 있습니다.</li>
<li><em>모순(Contradiction)</em>: 가설의 부정이 전제로부터 유추될 수 있습니다.</li>
<li><em>중립(Neutral)</em>: 다른 모든 경우입니다.</li>
</ul>
<p>자연어 추론은 텍스트 함의 인식(recognizing textual entailment) 작업이라고도 합니다.
예를 들어, 다음 쌍은 <em>함의</em>로 레이블이 지정됩니다. 가설의 "showing affection(애정 표현)"이 전제의 "hugging one another(서로 포옹)"에서 유추될 수 있기 때문입니다.</p>
<blockquote>
<p>전제: Two women are hugging each other. (두 여자가 서로 포옹하고 있다.)</p>
</blockquote>
<blockquote>
<p>가설: Two women are showing affection. (두 여자가 애정을 표현하고 있다.)</p>
</blockquote>
<p>다음은 <em>모순</em>의 예입니다. "running the coding example(코딩 예제 실행)"은 "sleeping(수면)"이 아니라 "not sleeping(자지 않음)"을 나타내기 때문입니다.</p>
<blockquote>
<p>전제: A man is running the coding example from Dive into Deep Learning. (한 남자가 Dive into Deep Learning의 코딩 예제를 실행하고 있다.)</p>
</blockquote>
<blockquote>
<p>가설: The man is sleeping. (남자가 자고 있다.)</p>
</blockquote>
<p>세 번째 예는 <em>중립</em> 관계를 보여줍니다. "famous(유명함)"나 "not famous(유명하지 않음)" 모두 "are performing for us(우리를 위해 공연하고 있다)"라는 사실에서 유추될 수 없기 때문입니다.</p>
<blockquote>
<p>전제: The musicians are performing for us. (음악가들이 우리를 위해 공연하고 있다.)</p>
</blockquote>
<blockquote>
<p>가설: The musicians are famous. (음악가들은 유명하다.)</p>
</blockquote>
<p>자연어 추론은 자연어 이해의 핵심 주제였습니다.
정보 검색에서 개방형 질문 응답에 이르기까지 광범위한 응용 분야를 가지고 있습니다.
이 문제를 연구하기 위해, 인기 있는 자연어 추론 벤치마크 데이터셋을 조사하는 것으로 시작하겠습니다.</p>
<h2 id="스탠포드-자연어-추론-snli-데이터셋"><a class="header" href="#스탠포드-자연어-추론-snli-데이터셋">스탠포드 자연어 추론 (SNLI) 데이터셋</a></h2>
<p>[<strong>Stanford Natural Language Inference (SNLI) Corpus</strong>]는 500,000개 이상의 레이블이 지정된 영어 문장 쌍 모음입니다 :cite:<code>Bowman.Angeli.Potts.ea.2015</code>.
우리는 추출된 SNLI 데이터셋을 다운로드하여 <code>../data/snli_1.0</code> 경로에 저장합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
import os
import re

npx.set_np()

#@save
d2l.DATA_HUB['SNLI'] = (
    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',
    '9fcde07509c7e87ec61c640c1b2753d9041758e4')

data_dir = d2l.download_extract('SNLI')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
import os
import re

#@save
d2l.DATA_HUB['SNLI'] = (
    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',
    '9fcde07509c7e87ec61c640c1b2753d9041758e4')

data_dir = d2l.download_extract('SNLI')
</code></pre>
<h3 id="데이터셋-읽기-reading-the-dataset-12"><a class="header" href="#데이터셋-읽기-reading-the-dataset-12">[<strong>데이터셋 읽기 (Reading the Dataset)</strong>]</a></h3>
<p>원본 SNLI 데이터셋에는 실험에 실제로 필요한 것보다 훨씬 더 풍부한 정보가 포함되어 있습니다. 따라서 데이터셋의 일부만 추출한 다음 전제, 가설 및 레이블 목록을 반환하는 <code>read_snli</code> 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
#@save
def read_snli(data_dir, is_train):
    """SNLI 데이터셋을 전제, 가설, 레이블로 읽습니다."""
    def extract_text(s):
        # 사용하지 않을 정보 제거
        s = re.sub('\\(', '', s) 
        s = re.sub('\\)', '', s)
        # 두 개 이상의 연속 공백을 공백으로 대체
        s = re.sub('\\s{2,}', ' ', s)
        return s.strip()
    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}
    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'
                             if is_train else 'snli_1.0_test.txt')
    with open(file_name, 'r') as f:
        rows = [row.split('\t') for row in f.readlines()[1:]]
    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]
    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]
    labels = [label_set[row[0]] for row in rows if row[0] in label_set]
    return premises, hypotheses, labels
</code></pre>
<p>이제 전제와 가설의 [<strong>처음 3개 쌍과 레이블을 인쇄해 봅시다</strong>] ("0", "1", "2"는 각각 "함의", "모순", "중립"에 해당합니다).</p>
<pre><code class="language-{.python .input}">#@tab all
train_data = read_snli(data_dir, is_train=True)
for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):
    print('premise:', x0)
    print('hypothesis:', x1)
    print('label:', y)
</code></pre>
<p>훈련 세트에는 약 550,000개의 쌍이 있고,
테스트 세트에는 약 10,000개의 쌍이 있습니다.
다음은 훈련 세트와 테스트 세트 모두에서
[<strong>"함의", "모순", "중립"의 세 가지 레이블이 균형을 이루고 있음</strong>]을 보여줍니다.</p>
<pre><code class="language-{.python .input}">#@tab all
test_data = read_snli(data_dir, is_train=False)
for data in [train_data, test_data]:
    print([[row for row in data[2]].count(i) for i in range(3)])
</code></pre>
<h3 id="데이터셋-로드를-위한-클래스-정의-defining-a-class-for-loading-the-dataset"><a class="header" href="#데이터셋-로드를-위한-클래스-정의-defining-a-class-for-loading-the-dataset">[<strong>데이터셋 로드를 위한 클래스 정의 (Defining a Class for Loading the Dataset)</strong>]</a></h3>
<p>아래에서는 Gluon의 <code>Dataset</code> 클래스를 상속하여 SNLI 데이터셋을 로드하는 클래스를 정의합니다. 클래스 생성자의 <code>num_steps</code> 인수는 텍스트 시퀀스의 길이를 지정하여 시퀀스의 각 미니배치가 동일한 모양을 갖도록 합니다.
즉, 더 긴 시퀀스의 처음 <code>num_steps</code> 이후 토큰은 잘리고, 더 짧은 시퀀스에는 길이가 <code>num_steps</code>가 될 때까지 특수 토큰 “&lt;pad&gt;”가 추가됩니다.
<code>__getitem__</code> 함수를 구현함으로써, 인덱스 <code>idx</code>로 전제, 가설 및 레이블에 임의로 접근할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
class SNLIDataset(gluon.data.Dataset):
    """SNLI 데이터셋을 로드하기 위한 사용자 정의 데이터셋."""
    def __init__(self, dataset, num_steps, vocab=None):
        self.num_steps = num_steps
        all_premise_tokens = d2l.tokenize(dataset[0])
        all_hypothesis_tokens = d2l.tokenize(dataset[1])
        if vocab is None:
            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,
                                   min_freq=5, reserved_tokens=['&lt;pad&gt;'])
        else:
            self.vocab = vocab
        self.premises = self._pad(all_premise_tokens)
        self.hypotheses = self._pad(all_hypothesis_tokens)
        self.labels = np.array(dataset[2])
        print('read ' + str(len(self.premises)) + ' examples')

    def _pad(self, lines):
        return np.array([d2l.truncate_pad(
            self.vocab[line], self.num_steps, self.vocab['&lt;pad&gt;'])
                         for line in lines])

    def __getitem__(self, idx):
        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]

    def __len__(self):
        return len(self.premises)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
class SNLIDataset(torch.utils.data.Dataset):
    """SNLI 데이터셋을 로드하기 위한 사용자 정의 데이터셋."""
    def __init__(self, dataset, num_steps, vocab=None):
        self.num_steps = num_steps
        all_premise_tokens = d2l.tokenize(dataset[0])
        all_hypothesis_tokens = d2l.tokenize(dataset[1])
        if vocab is None:
            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,
                                   min_freq=5, reserved_tokens=['&lt;pad&gt;'])
        else:
            self.vocab = vocab
        self.premises = self._pad(all_premise_tokens)
        self.hypotheses = self._pad(all_hypothesis_tokens)
        self.labels = torch.tensor(dataset[2])
        print('read ' + str(len(self.premises)) + ' examples')

    def _pad(self, lines):
        return torch.tensor([d2l.truncate_pad(
            self.vocab[line], self.num_steps, self.vocab['&lt;pad&gt;'])
                         for line in lines])

    def __getitem__(self, idx):
        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]

    def __len__(self):
        return len(self.premises)
</code></pre>
<h3 id="종합하기-putting-it-all-together-5"><a class="header" href="#종합하기-putting-it-all-together-5">[<strong>종합하기 (Putting It All Together)</strong>]</a></h3>
<p>이제 <code>read_snli</code> 함수와 <code>SNLIDataset</code> 클래스를 호출하여 SNLI 데이터셋을 다운로드하고 훈련 및 테스트 세트 모두에 대한 <code>DataLoader</code> 인스턴스와 훈련 세트의 어휘를 반환할 수 있습니다.
훈련 세트에서 구성된 어휘를 테스트 세트의 어휘로 사용해야 한다는 점에 유의해야 합니다.
결과적으로 테스트 세트의 새로운 토큰은 훈련 세트에서 훈련된 모델에 대해 알 수 없는(unknown) 토큰이 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def load_data_snli(batch_size, num_steps=50):
    """SNLI 데이터셋을 다운로드하고 데이터 반복자와 어휘를 반환합니다."""
    num_workers = d2l.get_dataloader_workers()
    data_dir = d2l.download_extract('SNLI')
    train_data = read_snli(data_dir, True)
    test_data = read_snli(data_dir, False)
    train_set = SNLIDataset(train_data, num_steps)
    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)
    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
                                       num_workers=num_workers)
    test_iter = gluon.data.DataLoader(test_set, batch_size, shuffle=False,
                                      num_workers=num_workers)
    return train_iter, test_iter, train_set.vocab
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def load_data_snli(batch_size, num_steps=50):
    """SNLI 데이터셋을 다운로드하고 데이터 반복자와 어휘를 반환합니다."""
    num_workers = d2l.get_dataloader_workers()
    data_dir = d2l.download_extract('SNLI')
    train_data = read_snli(data_dir, True)
    test_data = read_snli(data_dir, False)
    train_set = SNLIDataset(train_data, num_steps)
    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)
    train_iter = torch.utils.data.DataLoader(train_set, batch_size,
                                             shuffle=True,
                                             num_workers=num_workers)
    test_iter = torch.utils.data.DataLoader(test_set, batch_size,
                                            shuffle=False,
                                            num_workers=num_workers)
    return train_iter, test_iter, train_set.vocab
</code></pre>
<p>여기서 배치 크기를 128로, 시퀀스 길이를 50으로 설정하고,
<code>load_data_snli</code> 함수를 호출하여 데이터 반복자와 어휘를 얻습니다.
그런 다음 어휘 크기를 인쇄합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
train_iter, test_iter, vocab = load_data_snli(128, 50)
len(vocab)
</code></pre>
<p>이제 첫 번째 미니배치의 모양을 인쇄합니다.
감정 분석과 달리,
전제와 가설의 쌍을 나타내는 두 개의 입력 <code>X[0]</code>과 <code>X[1]</code>이 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
for X, Y in train_iter:
    print(X[0].shape)
    print(X[1].shape)
    print(Y.shape)
    break
</code></pre>
<h2 id="요약-summary-96"><a class="header" href="#요약-summary-96">요약 (Summary)</a></h2>
<ul>
<li>자연어 추론은 가설이 전제로부터 유추될 수 있는지를 연구합니다. 여기서 둘 다 텍스트 시퀀스입니다.</li>
<li>자연어 추론에서, 전제와 가설 간의 관계에는 함의, 모순, 중립이 포함됩니다.</li>
<li>Stanford Natural Language Inference (SNLI) Corpus는 인기 있는 자연어 추론 벤치마크 데이터셋입니다.</li>
</ul>
<h2 id="연습-문제-exercises-111"><a class="header" href="#연습-문제-exercises-111">연습 문제 (Exercises)</a></h2>
<ol>
<li>기계 번역은 오랫동안 출력 번역과 정답 번역 간의 피상적인 $n$-그램 매칭을 기반으로 평가되어 왔습니다. 자연어 추론을 사용하여 기계 번역 결과를 평가하는 척도를 설계할 수 있습니까?</li>
<li>어휘 크기를 줄이기 위해 하이퍼파라미터를 어떻게 변경할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/394">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1388">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="자연어-추론-어텐션-사용-natural-language-inference-using-attention"><a class="header" href="#자연어-추론-어텐션-사용-natural-language-inference-using-attention">자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></h1>
<p>:label:<code>sec_natural-language-inference-attention</code></p>
<p>우리는 :numref:<code>sec_natural-language-inference-and-dataset</code>에서 자연어 추론 작업과 SNLI 데이터셋을 소개했습니다. 복잡하고 심층적인 아키텍처에 기반한 많은 모델을 고려하여, :citet:<code>Parikh.Tackstrom.Das.ea.2016</code>는 어텐션 메커니즘으로 자연어 추론을 해결할 것을 제안하고 이를 "분해 가능한 어텐션 모델(decomposable attention model)"이라고 불렀습니다.
그 결과 순환 레이어나 합성곱 레이어가 없는 모델이 탄생했으며, 훨씬 적은 파라미터로 SNLI 데이터셋에서 당시 최고의 결과를 달성했습니다.
이 섹션에서는 :numref:<code>fig_nlp-map-nli-attention</code>에 묘사된 대로 자연어 추론을 위한 이 어텐션 기반 방법(MLP 포함)을 설명하고 구현할 것입니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/nlp-map-nli-attention.svg" alt="이 섹션에서는 자연어 추론을 위해 사전 훈련된 GloVe를 어텐션 및 MLP 기반 아키텍처에 공급합니다." />
:label:<code>fig_nlp-map-nli-attention</code></p>
<h2 id="모델-the-model-2"><a class="header" href="#모델-the-model-2">모델 (The Model)</a></h2>
<p>전제와 가설에서 토큰의 순서를 보존하는 것보다 더 간단하게,
우리는 한 텍스트 시퀀스의 토큰을 다른 시퀀스의 모든 토큰에 정렬하고, 그 반대로도 정렬한 다음,
이러한 정보를 비교하고 집계하여 전제와 가설 간의 논리적 관계를 예측할 수 있습니다.
기계 번역에서 소스 문장과 타겟 문장 간의 토큰 정렬과 유사하게,
전제와 가설 간의 토큰 정렬은
어텐션 메커니즘을 통해 깔끔하게 수행될 수 있습니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/nli-attention.svg" alt="어텐션 메커니즘을 사용한 자연어 추론." />
:label:<code>fig_nli_attention</code></p>
<p>:numref:<code>fig_nli_attention</code>은 어텐션 메커니즘을 사용하는 자연어 추론 방법을 묘사합니다.
높은 수준에서, 이것은 참석(attending), 비교(comparing), 집계(aggregating)의 세 가지 공동 훈련 단계로 구성됩니다.
다음에서 단계별로 설명하겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
from torch import nn
from torch.nn import functional as F
</code></pre>
<h3 id="참석-attending"><a class="header" href="#참석-attending">참석 (Attending)</a></h3>
<p>첫 번째 단계는 한 텍스트 시퀀스의 토큰을 다른 시퀀스의 각 토큰에 정렬하는 것입니다.
전제가 "i do need sleep"이고 가설이 "i am tired"라고 가정해 봅시다.
의미적 유사성으로 인해,
우리는 가설의 "i"를 전제의 "i"와 정렬하고,
가설의 "tired"를 전제의 "sleep"과 정렬하고 싶을 수 있습니다.
마찬가지로, 전제의 "i"를 가설의 "i"와 정렬하고,
전제의 "need"와 "sleep"을 가설의 "tired"와 정렬하고 싶을 수 있습니다.
이러한 정렬은 가중 평균을 사용하는 <em>소프트(soft)</em> 정렬이며,
이상적으로는 정렬될 토큰에 큰 가중치가 연관됩니다.
쉬운 시연을 위해, :numref:<code>fig_nli_attention</code>은 이러한 정렬을 <em>하드(hard)</em> 방식으로 보여줍니다.</p>
<p>이제 어텐션 메커니즘을 사용한 소프트 정렬을 더 자세히 설명합니다.
전제와 가설을 각각 $\mathbf{A} = (\mathbf{a}_1, \ldots, \mathbf{a}_m)$
및 $\mathbf{B} = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$으로 표시합니다.
토큰 수는 각각 $m$과 $n$이며,
여기서 $\mathbf{a}_i, \mathbf{b}<em>j \in \mathbb{R}^{d}$ ($i = 1, \ldots, m, j = 1, \ldots, n$)는 $d$차원 단어 벡터입니다.
소프트 정렬을 위해, 우리는 어텐션 가중치 $e</em>{ij} \in \mathbb{R}$를 다음과 같이 계산합니다:</p>
<p>$$e_{ij} = f(\mathbf{a}_i)^{\top} f(\mathbf{b}_j),$$
:eqlabel:<code>eq_nli_e</code></p>
<p>여기서 함수 $f$는 다음 <code>mlp</code> 함수에 정의된 MLP입니다.
$f$의 출력 차원은 <code>mlp</code>의 <code>num_hiddens</code> 인수에 의해 지정됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def mlp(num_hiddens, flatten):
    net = nn.Sequential()
    net.add(nn.Dropout(0.2))
    net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten))
    net.add(nn.Dropout(0.2))
    net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten))
    return net
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def mlp(num_inputs, num_hiddens, flatten):
    net = []
    net.append(nn.Dropout(0.2))
    net.append(nn.Linear(num_inputs, num_hiddens))
    net.append(nn.ReLU())
    if flatten:
        net.append(nn.Flatten(start_dim=1))
    net.append(nn.Dropout(0.2))
    net.append(nn.Linear(num_hiddens, num_hiddens))
    net.append(nn.ReLU())
    if flatten:
        net.append(nn.Flatten(start_dim=1))
    return nn.Sequential(*net)
</code></pre>
<p>:eqref:<code>eq_nli_e</code>에서
$f$는 입력을 쌍으로 함께 받는 것이 아니라 $\mathbf{a}_i$와 $\mathbf{b}_j$를 개별적으로 입력으로 받는다는 점을 강조해야 합니다.
이 <em>분해(decomposition)</em> 트릭은 $f$의 적용 횟수를 $mn$번(이차 복잡도)이 아닌 $m + n$번(선형 복잡도)으로 줄여줍니다.</p>
<p>:eqref:<code>eq_nli_e</code>의 어텐션 가중치를 정규화하여,
우리는 가설의 모든 토큰 벡터의 가중 평균을 계산하여
전제에서 인덱스 $i$로 지정된 토큰과 소프트하게 정렬된 가설의 표현을 얻습니다:</p>
<p>$$
\boldsymbol{\beta}<em>i = \sum</em>{j=1}^{n}\frac{\exp(e_{ij})}{ \sum_{k=1}^{n} \exp(e_{ik})} \mathbf{b}_j.
$$</p>
<p>마찬가지로, 우리는 가설에서 인덱스 $j$로 지정된 각 토큰에 대해 전제 토큰의 소프트 정렬을 계산합니다:</p>
<p>$$
\boldsymbol{\alpha}<em>j = \sum</em>{i=1}^{m}\frac{\exp(e_{ij})}{ \sum_{k=1}^{m} \exp(e_{kj})} \mathbf{a}_i.
$$</p>
<p>아래에서는 <code>Attend</code> 클래스를 정의하여 입력 전제 <code>A</code>와 가설(<code>beta</code>)의 소프트 정렬, 그리고 입력 가설 <code>B</code>와 전제(<code>alpha</code>)의 소프트 정렬을 계산합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class Attend(nn.Block):
    def __init__(self, num_hiddens, **kwargs):
        super(Attend, self).__init__(**kwargs)
        self.f = mlp(num_hiddens=num_hiddens, flatten=False)

    def forward(self, A, B):
        # `A`/`B`의 모양: (`배치 크기`, 시퀀스 A/B의 토큰 수, `embed_size`)
        # `f_A`/`f_B`의 모양: (`배치 크기`, 시퀀스 A/B의 토큰 수, `num_hiddens`)
        f_A = self.f(A)
        f_B = self.f(B)
        # `e`의 모양: (`배치 크기`, 시퀀스 A의 토큰 수, 시퀀스 B의 토큰 수)
        e = npx.batch_dot(f_A, f_B, transpose_b=True)
        # `beta`의 모양: (`배치 크기`, 시퀀스 A의 토큰 수, `embed_size`),
        # 여기서 시퀀스 B는 시퀀스 A의 각 토큰(`beta`의 축 1)과 소프트하게 정렬됩니다
        beta = npx.batch_dot(npx.softmax(e), B)
        # `alpha`의 모양: (`배치 크기`, 시퀀스 B의 토큰 수, `embed_size`),
        # 여기서 시퀀스 A는 시퀀스 B의 각 토큰(`alpha`의 축 1)과 소프트하게 정렬됩니다
        alpha = npx.batch_dot(npx.softmax(e.transpose(0, 2, 1)), A)
        return beta, alpha
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class Attend(nn.Module):
    def __init__(self, num_inputs, num_hiddens, **kwargs):
        super(Attend, self).__init__(**kwargs)
        self.f = mlp(num_inputs, num_hiddens, flatten=False)

    def forward(self, A, B):
        # `A`/`B`의 모양: (`배치 크기`, 시퀀스 A/B의 토큰 수, `embed_size`)
        # `f_A`/`f_B`의 모양: (`배치 크기`, 시퀀스 A/B의 토큰 수, `num_hiddens`)
        f_A = self.f(A)
        f_B = self.f(B)
        # `e`의 모양: (`배치 크기`, 시퀀스 A의 토큰 수, 시퀀스 B의 토큰 수)
        e = torch.bmm(f_A, f_B.permute(0, 2, 1))
        # `beta`의 모양: (`배치 크기`, 시퀀스 A의 토큰 수, `embed_size`),
        # 여기서 시퀀스 B는 시퀀스 A의 각 토큰(`beta`의 축 1)과 소프트하게 정렬됩니다
        beta = torch.bmm(F.softmax(e, dim=-1), B)
        # `alpha`의 모양: (`배치 크기`, 시퀀스 B의 토큰 수, `embed_size`),
        # 여기서 시퀀스 A는 시퀀스 B의 각 토큰(`alpha`의 축 1)과 소프트하게 정렬됩니다
        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)
        return beta, alpha
</code></pre>
<h3 id="비교-comparing"><a class="header" href="#비교-comparing">비교 (Comparing)</a></h3>
<p>다음 단계에서는 한 시퀀스의 토큰을 그 토큰과 소프트하게 정렬된 다른 시퀀스와 비교합니다.
소프트 정렬에서는 한 시퀀스의 모든 토큰이(아마도 다른 어텐션 가중치로) 다른 시퀀스의 토큰과 비교된다는 점에 유의하십시오.
쉬운 시연을 위해, :numref:<code>fig_nli_attention</code>은 토큰을 정렬된 토큰과 <em>하드</em> 방식으로 짝을 짓습니다.
예를 들어, 참석 단계에서 전제의 "need"와 "sleep"이 모두 가설의 "tired"와 정렬된 것으로 결정되면, "tired--need sleep" 쌍이 비교됩니다.</p>
<p>비교 단계에서는 한 시퀀스의 토큰과 다른 시퀀스의 정렬된 토큰의 연결(연산자 $[]$, $\cdot$])을 함수 $g$(MLP)에 공급합니다:</p>
<p>$$\mathbf{v}_{A,i} = g([]\mathbf{a}_i, \boldsymbol{\beta}<em>i]), i = 1, \ldots, m<br />
\mathbf{v}</em>{B,j} = g([]\mathbf{b}_j, \boldsymbol{\alpha}_j]), j = 1, \ldots, n.$$</p>
<p>:eqlabel:<code>eq_nli_v_ab</code></p>
<p>:eqref:<code>eq_nli_v_ab</code>에서 $\mathbf{v}<em>{A,i}$는 전제의 토큰 $i$와 토큰 $i$와 소프트하게 정렬된 모든 가설 토큰 간의 비교입니다.
반면 $\mathbf{v}</em>{B,j}$는 가설의 토큰 $j$와 토큰 $j$와 소프트하게 정렬된 모든 전제 토큰 간의 비교입니다.
다음 <code>Compare</code> 클래스는 이러한 비교 단계를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class Compare(nn.Block):
    def __init__(self, num_hiddens, **kwargs):
        super(Compare, self).__init__(**kwargs)
        self.g = mlp(num_hiddens=num_hiddens, flatten=False)

    def forward(self, A, B, beta, alpha):
        V_A = self.g(np.concatenate([A, beta], axis=2))
        V_B = self.g(np.concatenate([B, alpha], axis=2))
        return V_A, V_B
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class Compare(nn.Module):
    def __init__(self, num_inputs, num_hiddens, **kwargs):
        super(Compare, self).__init__(**kwargs)
        self.g = mlp(num_inputs, num_hiddens, flatten=False)

    def forward(self, A, B, beta, alpha):
        V_A = self.g(torch.cat([A, beta], dim=2))
        V_B = self.g(torch.cat([B, alpha], dim=2))
        return V_A, V_B
</code></pre>
<h3 id="집계-aggregating"><a class="header" href="#집계-aggregating">집계 (Aggregating)</a></h3>
<p>두 세트의 비교 벡터 $\mathbf{v}<em>{A,i}$ ($i = 1, \ldots, m$)와 $\mathbf{v}</em>{B,j}$ ($j = 1, \ldots, n$)를 가지고,
마지막 단계에서는 논리적 관계를 추론하기 위해 이러한 정보를 집계합니다.
두 세트를 합산하는 것으로 시작합니다:</p>
<p>$$
\mathbf{v}<em>A = \sum</em>{i=1}^{m} \mathbf{v}<em>{A,i}, \quad \mathbf{v}<em>B = \sum</em>{j=1}^{n}\mathbf{v}</em>{B,j}.
$$</p>
<p>다음으로 두 요약 결과의 연결을 함수 $h$(MLP)에 공급하여 논리적 관계의 분류 결과를 얻습니다:</p>
<p>$$
\hat{\mathbf{y}} = h([]\mathbf{v}_A, \mathbf{v}_B]).
$$</p>
<p>집계 단계는 다음 <code>Aggregate</code> 클래스에 정의되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class Aggregate(nn.Block):
    def __init__(self, num_hiddens, num_outputs, **kwargs):
        super(Aggregate, self).__init__(**kwargs)
        self.h = mlp(num_hiddens=num_hiddens, flatten=True)
        self.h.add(nn.Dense(num_outputs))

    def forward(self, V_A, V_B):
        # 두 비교 벡터 세트를 합산합니다
        V_A = V_A.sum(axis=1)
        V_B = V_B.sum(axis=1)
        # 두 요약 결과의 연결을 MLP에 공급합니다
        Y_hat = self.h(np.concatenate([V_A, V_B], axis=1))
        return Y_hat
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class Aggregate(nn.Module):
    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):
        super(Aggregate, self).__init__(**kwargs)
        self.h = mlp(num_inputs, num_hiddens, flatten=True)
        self.linear = nn.Linear(num_hiddens, num_outputs)

    def forward(self, V_A, V_B):
        # 두 비교 벡터 세트를 합산합니다
        V_A = V_A.sum(dim=1)
        V_B = V_B.sum(dim=1)
        # 두 요약 결과의 연결을 MLP에 공급합니다
        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))
        return Y_hat
</code></pre>
<h3 id="종합하기-putting-it-all-together-6"><a class="header" href="#종합하기-putting-it-all-together-6">종합하기 (Putting It All Together)</a></h3>
<p>참석, 비교, 집계 단계를 종합하여,
우리는 이 세 단계를 공동으로 훈련하기 위한 분해 가능한 어텐션 모델을 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class DecomposableAttention(nn.Block):
    def __init__(self, vocab, embed_size, num_hiddens, **kwargs):
        super(DecomposableAttention, self).__init__(**kwargs)
        self.embedding = nn.Embedding(len(vocab), embed_size)
        self.attend = Attend(num_hiddens)
        self.compare = Compare(num_hiddens)
        # 3가지 가능한 출력: 함의, 모순, 중립
        self.aggregate = Aggregate(num_hiddens, 3)

    def forward(self, X):
        premises, hypotheses = X
        A = self.embedding(premises)
        B = self.embedding(hypotheses)
        beta, alpha = self.attend(A, B)
        V_A, V_B = self.compare(A, B, beta, alpha)
        Y_hat = self.aggregate(V_A, V_B)
        return Y_hat
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class DecomposableAttention(nn.Module):
    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,
                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):
        super(DecomposableAttention, self).__init__(**kwargs)
        self.embedding = nn.Embedding(len(vocab), embed_size)
        self.attend = Attend(num_inputs_attend, num_hiddens)
        self.compare = Compare(num_inputs_compare, num_hiddens)
        # 3가지 가능한 출력: 함의, 모순, 중립
        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)

    def forward(self, X):
        premises, hypotheses = X
        A = self.embedding(premises)
        B = self.embedding(hypotheses)
        beta, alpha = self.attend(A, B)
        V_A, V_B = self.compare(A, B, beta, alpha)
        Y_hat = self.aggregate(V_A, V_B)
        return Y_hat
</code></pre>
<h2 id="모델-훈련-및-평가-training-and-evaluating-the-model-2"><a class="header" href="#모델-훈련-및-평가-training-and-evaluating-the-model-2">모델 훈련 및 평가 (Training and Evaluating the Model)</a></h2>
<p>이제 정의된 분해 가능한 어텐션 모델을 SNLI 데이터셋에서 훈련하고 평가할 것입니다.
데이터셋 읽기로 시작합니다.</p>
<h3 id="데이터셋-읽기-reading-the-dataset-13"><a class="header" href="#데이터셋-읽기-reading-the-dataset-13">데이터셋 읽기 (Reading the dataset)</a></h3>
<p>:numref:<code>sec_natural-language-inference-and-dataset</code>에 정의된 함수를 사용하여 SNLI 데이터셋을 다운로드하고 읽습니다. 배치 크기와 시퀀스 길이는 각각 $256$과 $50$으로 설정됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
batch_size, num_steps = 256, 50
train_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps)
</code></pre>
<h3 id="모델-생성-creating-the-model"><a class="header" href="#모델-생성-creating-the-model">모델 생성 (Creating the Model)</a></h3>
<p>우리는 입력 토큰을 나타내기 위해 사전 훈련된 100차원 GloVe 임베딩을 사용합니다.
따라서 :eqref:<code>eq_nli_e</code>의 벡터 $\mathbf{a}_i$와 $\mathbf{b}_j$의 차원을 100으로 미리 정의합니다.
:eqref:<code>eq_nli_e</code>의 함수 $f$와 :eqref:<code>eq_nli_v_ab</code>의 함수 $g$의 출력 차원은 200으로 설정됩니다.
그런 다음 모델 인스턴스를 생성하고, 파라미터를 초기화하고,
GloVe 임베딩을 로드하여 입력 토큰의 벡터를 초기화합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()
net = DecomposableAttention(vocab, embed_size, num_hiddens)
net.initialize(init.Xavier(), ctx=devices)
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
embeds = glove_embedding[vocab.idx_to_token]
net.embedding.weight.set_data(embeds)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()
net = DecomposableAttention(vocab, embed_size, num_hiddens)
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
embeds = glove_embedding[vocab.idx_to_token]
net.embedding.weight.data.copy_(embeds);
</code></pre>
<h3 id="모델-훈련-및-평가-training-and-evaluating-the-model-3"><a class="header" href="#모델-훈련-및-평가-training-and-evaluating-the-model-3">모델 훈련 및 평가 (Training and Evaluating the Model)</a></h3>
<p>텍스트 시퀀스(또는 이미지)와 같은 단일 입력을 받는 :numref:<code>sec_multi_gpu</code>의 <code>split_batch</code> 함수와 달리,
우리는 미니배치에서 전제와 가설과 같은 다중 입력을 받기 위해 <code>split_batch_multi_inputs</code> 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def split_batch_multi_inputs(X, y, devices):
    """다중 입력 `X`와 `y`를 여러 장치로 분할합니다."""
    X = list(zip(*[gluon.utils.split_and_load(
        feature, devices, even_split=False) for feature in X]))
    return (X, gluon.utils.split_and_load(y, devices, even_split=False))
</code></pre>
<p>이제 SNLI 데이터셋에서 모델을 훈련하고 평가할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
lr, num_epochs = 0.001, 4
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices,
               split_batch_multi_inputs)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
lr, num_epochs = 0.001, 4
trainer = torch.optim.Adam(net.parameters(), lr=lr)
loss = nn.CrossEntropyLoss(reduction="none")
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<h3 id="모델-사용-using-the-model"><a class="header" href="#모델-사용-using-the-model">모델 사용 (Using the Model)</a></h3>
<p>마지막으로, 전제와 가설 쌍 간의 논리적 관계를 출력하는 예측 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def predict_snli(net, vocab, premise, hypothesis):
    """전제와 가설 간의 논리적 관계를 예측합니다."""
    premise = np.array(vocab[premise], ctx=d2l.try_gpu())
    hypothesis = np.array(vocab[hypothesis], ctx=d2l.try_gpu())
    label = np.argmax(net([premise.reshape((1, -1)),
                           hypothesis.reshape((1, -1))]), axis=1)
    return 'entailment' if label == 0 else 'contradiction' if label == 1 \
            else 'neutral'
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def predict_snli(net, vocab, premise, hypothesis):
    """전제와 가설 간의 논리적 관계를 예측합니다."""
    net.eval()
    premise = torch.tensor(vocab[premise], device=d2l.try_gpu())
    hypothesis = torch.tensor(vocab[hypothesis], device=d2l.try_gpu())
    label = torch.argmax(net([premise.reshape((1, -1)),
                           hypothesis.reshape((1, -1))]), dim=1)
    return 'entailment' if label == 0 else 'contradiction' if label == 1 \
            else 'neutral'
</code></pre>
<p>훈련된 모델을 사용하여 샘플 문장 쌍에 대한 자연어 추론 결과를 얻을 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
predict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'bad', '.'])
</code></pre>
<h2 id="요약-summary-97"><a class="header" href="#요약-summary-97">요약 (Summary)</a></h2>
<ul>
<li>분해 가능한 어텐션 모델은 전제와 가설 간의 논리적 관계를 예측하기 위한 세 단계(참석, 비교, 집계)로 구성됩니다.</li>
<li>어텐션 메커니즘을 사용하면 한 텍스트 시퀀스의 토큰을 다른 시퀀스의 모든 토큰에 정렬할 수 있으며, 그 반대의 경우도 마찬가지입니다. 이러한 정렬은 가중 평균을 사용하는 소프트 정렬이며, 이상적으로는 정렬될 토큰에 큰 가중치가 연관됩니다.</li>
<li>분해 트릭은 어텐션 가중치를 계산할 때 이차 복잡도보다 바람직한 선형 복잡도를 제공합니다.</li>
<li>자연어 추론과 같은 다운스트림 자연어 처리 작업을 위한 입력 표현으로 사전 훈련된 단어 벡터를 사용할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-112"><a class="header" href="#연습-문제-exercises-112">연습 문제 (Exercises)</a></h2>
<ol>
<li>다른 하이퍼파라미터 조합으로 모델을 훈련해 보십시오. 테스트 세트에서 더 나은 정확도를 얻을 수 있습니까?</li>
<li>자연어 추론을 위한 분해 가능한 어텐션 모델의 주요 단점은 무엇입니까?</li>
<li>임의의 문장 쌍에 대해 의미적 유사성 수준(예: 0과 1 사이의 연속 값)을 얻고 싶다고 가정해 봅시다. 데이터셋을 어떻게 수집하고 레이블을 지정해야 합니까? 어텐션 메커니즘을 사용하여 모델을 설계할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/395">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1530">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="시퀀스-수준-및-토큰-수준-응용을-위한-bert-미세-조정-fine-tuning-bert-for-sequence-level-and-token-level-applications"><a class="header" href="#시퀀스-수준-및-토큰-수준-응용을-위한-bert-미세-조정-fine-tuning-bert-for-sequence-level-and-token-level-applications">시퀀스 수준 및 토큰 수준 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></h1>
<p>:label:<code>sec_finetuning-bert</code></p>
<p>이 장의 이전 섹션들에서, 우리는 RNN, CNN, 어텐션 및 MLP를 기반으로 한 다양한 자연어 처리 응용 모델을 설계했습니다. 이러한 모델은 공간이나 시간 제약이 있을 때 유용하지만, 모든 자연어 처리 작업에 대해 특정 모델을 만드는 것은 실제로 불가능합니다. :numref:<code>sec_bert</code>에서 우리는 광범위한 자연어 처리 작업을 위해 최소한의 아키텍처 변경만 요구하는 사전 훈련 모델인 BERT를 소개했습니다. 한편으로, 제안 당시 BERT는 다양한 자연어 처리 작업에서 최첨단 기술을 향상시켰습니다. 다른 한편으로, :numref:<code>sec_bert-pretraining</code>에서 언급했듯이 원래 BERT 모델의 두 버전은 1억 1천만 개와 3억 4천만 개의 파라미터를 가지고 있습니다. 따라서 계산 리소스가 충분할 때, 우리는 다운스트림 자연어 처리 응용 프로그램을 위해 BERT를 미세 조정하는 것을 고려할 수 있습니다.</p>
<p>다음에서, 우리는 자연어 처리 응용 프로그램의 하위 집합을 시퀀스 수준과 토큰 수준으로 일반화합니다. 시퀀스 수준에서는 텍스트 입력의 BERT 표현을 단일 텍스트 분류와 텍스트 쌍 분류 또는 회귀에서 출력 레이블로 변환하는 방법을 소개합니다. 토큰 수준에서는 텍스트 태깅 및 질문 응답과 같은 새로운 응용 프로그램을 간략하게 소개하고, BERT가 어떻게 그들의 입력을 표현하고 출력 레이블로 변환되는지 설명합니다. 미세 조정 중에, 서로 다른 응용 프로그램에서 BERT가 요구하는 "최소한의 아키텍처 변경"은 추가적인 완전 연결 레이어입니다. 다운스트림 응용 프로그램의 지도 학습 중에, 추가 레이어의 파라미터는 처음부터 학습되는 반면 사전 훈련된 BERT 모델의 모든 파라미터는 미세 조정됩니다.</p>
<h2 id="단일-텍스트-분류-single-text-classification"><a class="header" href="#단일-텍스트-분류-single-text-classification">단일 텍스트 분류 (Single Text Classification)</a></h2>
<p><em>단일 텍스트 분류</em>는 단일 텍스트 시퀀스를 입력으로 받아 그 분류 결과를 출력합니다. 이 장에서 공부한 감정 분석 외에도, 언어적 용납 가능성 코퍼스(Corpus of Linguistic Acceptability, CoLA)도 주어진 문장이 문법적으로 용납 가능한지 여부를 판단하는 단일 텍스트 분류 데이터셋입니다 :cite:<code>Warstadt.Singh.Bowman.2019</code>. 예를 들어, "I should study."는 용납 가능하지만 "I should studying."은 그렇지 않습니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/bert-one-seq.svg" alt="감정 분석 및 언어적 용납 가능성 테스트와 같은 단일 텍스트 분류 응용 프로그램을 위한 BERT 미세 조정. 입력 단일 텍스트에 6개의 토큰이 있다고 가정합니다." />
:label:<code>fig_bert-one-seq</code></p>
<p>:numref:<code>sec_bert</code>는 BERT의 입력 표현을 설명합니다. BERT 입력 시퀀스는 단일 텍스트와 텍스트 쌍을 명확하게 나타내며, 여기서 특수 분류 토큰 “<cls>”는 시퀀스 분류에 사용되고 특수 분류 토큰 “<sep>”는 단일 텍스트의 끝을 표시하거나 텍스트 쌍을 구분합니다. :numref:<code>fig_bert-one-seq</code>에 표시된 것처럼, 단일 텍스트 분류 응용 프로그램에서 특수 분류 토큰 “<cls>”의 BERT 표현은 전체 입력 텍스트 시퀀스의 정보를 인코딩합니다. 입력 단일 텍스트의 표현으로서, 이는 완전 연결(밀집) 레이어로 구성된 작은 MLP에 공급되어 모든 이산 레이블 값의 분포를 출력합니다.</p>
<h2 id="텍스트-쌍-분류-또는-회귀-text-pair-classification-or-regression"><a class="header" href="#텍스트-쌍-분류-또는-회귀-text-pair-classification-or-regression">텍스트 쌍 분류 또는 회귀 (Text Pair Classification or Regression)</a></h2>
<p>우리는 또한 이 장에서 자연어 추론을 살펴보았습니다. 이는 텍스트 쌍을 분류하는 응용 프로그램 유형인 <em>텍스트 쌍 분류</em>에 속합니다.</p>
<p>텍스트 쌍을 입력으로 받지만 연속적인 값을 출력하는 *의미적 텍스트 유사성(semantic textual similarity)*은 인기 있는 <em>텍스트 쌍 회귀</em> 작업입니다. 이 작업은 문장의 의미적 유사성을 측정합니다. 예를 들어, 의미적 텍스트 유사성 벤치마크 데이터셋에서 한 쌍의 문장의 유사성 점수는 0(의미 중첩 없음)에서 5(의미 동일)까지의 서순 척도입니다 :cite:<code>Cer.Diab.Agirre.ea.2017</code>. 목표는 이러한 점수를 예측하는 것입니다. 의미적 텍스트 유사성 벤치마크 데이터셋의 예(문장 1, 문장 2, 유사성 점수)는 다음과 같습니다:</p>
<ul>
<li>"A plane is taking off.", "An air plane is taking off.", 5.000;</li>
<li>"A woman is eating something.", "A woman is eating meat.", 3.000;</li>
<li>"A woman is dancing.", "A man is talking.", 0.000.</li>
</ul>
<p><img src="chapter_natural-language-processing-applications/../img/bert-two-seqs.svg" alt="자연어 추론 및 의미적 텍스트 유사성과 같은 텍스트 쌍 분류 또는 회귀 응용 프로그램을 위한 BERT 미세 조정. 입력 텍스트 쌍에 2개와 3개의 토큰이 있다고 가정합니다." />
:label:<code>fig_bert-two-seqs</code></p>
<p>:numref:<code>fig_bert-one-seq</code>의 단일 텍스트 분류와 비교할 때, :numref:<code>fig_bert-two-seqs</code>의 텍스트 쌍 분류를 위한 BERT 미세 조정은 입력 표현에서 다릅니다. 의미적 텍스트 유사성과 같은 텍스트 쌍 회귀 작업의 경우, 연속적인 레이블 값을 출력하고 평균 제곱 손실을 사용하는 것과 같은 사소한 변경이 적용될 수 있습니다: 이는 회귀에서 흔한 일입니다.</p>
<h2 id="텍스트-태깅-text-tagging"><a class="header" href="#텍스트-태깅-text-tagging">텍스트 태깅 (Text Tagging)</a></h2>
<p>이제 각 토큰에 레이블이 할당되는 <em>텍스트 태깅</em>과 같은 토큰 수준 작업을 고려해 봅시다. 텍스트 태깅 작업 중에서, *품사 태깅(part-of-speech tagging)*은 문장에서 단어의 역할에 따라 각 단어에 품사 태그(예: 형용사 및 한정사)를 할당합니다. 예를 들어, Penn Treebank II 태그 세트에 따르면 문장 "John Smith 's car is new"는 "NNP(고유 명사, 단수) NNP POS(소유격 어미) NN(명사, 단수 또는 질량) VB(동사, 기본형) JJ(형용사)"로 태깅되어야 합니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/bert-tagging.svg" alt="품사 태깅과 같은 텍스트 태깅 응용 프로그램을 위한 BERT 미세 조정. 입력 단일 텍스트에 6개의 토큰이 있다고 가정합니다." />
:label:<code>fig_bert-tagging</code></p>
<p>텍스트 태깅 응용 프로그램을 위한 BERT 미세 조정은 :numref:<code>fig_bert-tagging</code>에 설명되어 있습니다. :numref:<code>fig_bert-one-seq</code>와의 유일한 차이점은 텍스트 태깅에서 입력 텍스트의 <em>모든 토큰</em>의 BERT 표현이 동일한 추가 완전 연결 레이어에 공급되어 품사 태그와 같은 토큰의 레이블을 출력한다는 점입니다.</p>
<h2 id="질문-응답-question-answering"><a class="header" href="#질문-응답-question-answering">질문 응답 (Question Answering)</a></h2>
<p>또 다른 토큰 수준 응용 프로그램으로서, *질문 응답(question answering)*은 독해 능력을 반영합니다. 예를 들어, 스탠포드 질문 응답 데이터셋(SQuAD v1.1)은 읽기 지문과 질문으로 구성되며, 모든 질문에 대한 답은 질문이 다루는 지문의 텍스트 세그먼트(텍스트 스팬, text span)입니다 :cite:<code>Rajpurkar.Zhang.Lopyrev.ea.2016</code>. 설명하자면, 지문 "Some experts report that a mask's efficacy is inconclusive. However, mask makers insist that their products, such as N95 respirator masks, can guard against the virus."와 질문 "Who say that N95 respirator masks can guard against the virus?"를 고려해 보십시오. 답은 지문의 텍스트 스팬인 "mask makers"여야 합니다. 따라서 SQuAD v1.1의 목표는 질문과 지문 쌍이 주어졌을 때 지문에서 텍스트 스팬의 시작과 끝을 예측하는 것입니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/bert-qa.svg" alt="질문 응답을 위한 BERT 미세 조정. 입력 텍스트 쌍에 2개와 3개의 토큰이 있다고 가정합니다." />
:label:<code>fig_bert-qa</code></p>
<p>질문 응답을 위해 BERT를 미세 조정하기 위해, 질문과 지문은 BERT 입력의 첫 번째와 두 번째 텍스트 시퀀스로 각각 패킹됩니다. 텍스트 스팬 시작 위치를 예측하기 위해, 동일한 추가 완전 연결 레이어가 위치 $i$의 지문 토큰의 BERT 표현을 스칼라 점수 $s_i$로 변환합니다. 모든 지문 토큰의 이러한 점수는 소프트맥스 연산에 의해 확률 분포로 추가 변환되어, 지문의 각 토큰 위치 $i$에 텍스트 스팬의 시작일 확률 $p_i$가 할당됩니다. 텍스트 스팬 끝을 예측하는 것도 위와 동일하지만, 추가 완전 연결 레이어의 파라미터는 시작 예측을 위한 파라미터와 독립적입니다. 끝을 예측할 때, 위치 $i$의 지문 토큰은 동일한 완전 연결 레이어에 의해 스칼라 점수 $e_i$로 변환됩니다. :numref:<code>fig_bert-qa</code>는 질문 응답을 위한 BERT 미세 조정을 묘사합니다.</p>
<p>질문 응답의 경우, 지도 학습의 훈련 목표는 실제 시작 및 끝 위치의 로그 우도를 최대화하는 것만큼 간단합니다. 스팬을 예측할 때, 우리는 위치 $i$에서 위치 $j$ ($i \leq j$)까지의 유효한 스팬에 대해 점수 $s_i + e_j$를 계산하고 가장 높은 점수를 가진 스팬을 출력할 수 있습니다.</p>
<h2 id="요약-summary-98"><a class="header" href="#요약-summary-98">요약 (Summary)</a></h2>
<ul>
<li>BERT는 단일 텍스트 분류(예: 감정 분석 및 언어적 용납 가능성 테스트), 텍스트 쌍 분류 또는 회귀(예: 자연어 추론 및 의미적 텍스트 유사성), 텍스트 태깅(예: 품사 태깅), 질문 응답과 같은 시퀀스 수준 및 토큰 수준 자연어 처리 응용 프로그램을 위해 최소한의 아키텍처 변경(추가 완전 연결 레이어)만 요구합니다.</li>
<li>다운스트림 응용 프로그램의 지도 학습 중에, 추가 레이어의 파라미터는 처음부터 학습되는 반면 사전 훈련된 BERT 모델의 모든 파라미터는 미세 조정됩니다.</li>
</ul>
<h2 id="연습-문제-exercises-113"><a class="header" href="#연습-문제-exercises-113">연습 문제 (Exercises)</a></h2>
<ol>
<li>뉴스 기사를 위한 검색 엔진 알고리즘을 설계해 봅시다. 시스템이 쿼리(예: "코로나바이러스 발생 중 석유 산업")를 받으면 쿼리와 가장 관련성이 높은 뉴스 기사의 순위 목록을 반환해야 합니다. 뉴스 기사 풀이 거대하고 쿼리 수가 많다고 가정해 봅시다. 문제를 단순화하기 위해 각 쿼리에 대해 가장 관련성이 높은 기사가 라벨링되었다고 가정합니다. 알고리즘 설계에 음성 샘플링(:numref:<code>subsec_negative-sampling</code> 참조)과 BERT를 어떻게 적용할 수 있습니까?</li>
<li>언어 모델 훈련에 BERT를 어떻게 활용할 수 있습니까?</li>
<li>기계 번역에 BERT를 활용할 수 있습니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/396">토론</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="자연어-추론-bert-미세-조정-natural-language-inference-fine-tuning-bert"><a class="header" href="#자연어-추론-bert-미세-조정-natural-language-inference-fine-tuning-bert">자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></h1>
<p>:label:<code>sec_natural-language-inference-bert</code></p>
<p>이 장의 이전 섹션들에서,
우리는 SNLI 데이터셋(:numref:<code>sec_natural-language-inference-and-dataset</code>에 설명됨)의
자연어 추론 작업을 위한
어텐션 기반 아키텍처(:numref:<code>sec_natural-language-inference-attention</code>)를 설계했습니다.
이제 BERT를 미세 조정하여 이 작업을 다시 살펴보겠습니다.
:numref:<code>sec_finetuning-bert</code>에서 논의했듯이,
자연어 추론은 시퀀스 수준의 텍스트 쌍 분류 문제이며,
BERT를 미세 조정하는 데는 :numref:<code>fig_nlp-map-nli-bert</code>에 설명된 대로
추가적인 MLP 기반 아키텍처만 필요합니다.</p>
<p><img src="chapter_natural-language-processing-applications/../img/nlp-map-nli-bert.svg" alt="이 섹션에서는 자연어 추론을 위해 사전 훈련된 BERT를 MLP 기반 아키텍처에 공급합니다." />
:label:<code>fig_nlp-map-nli-bert</code></p>
<p>이 섹션에서는
사전 훈련된 BERT의 작은 버전을 다운로드한 다음,
SNLI 데이터셋에서 자연어 추론을 위해
미세 조정할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
import json
import multiprocessing
from mxnet import gluon, np, npx
from mxnet.gluon import nn
import os

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import json
import multiprocessing
import torch
from torch import nn
import os
</code></pre>
<h2 id="사전-훈련된-bert-로드-loading-pretrained-bert"><a class="header" href="#사전-훈련된-bert-로드-loading-pretrained-bert">[<strong>사전 훈련된 BERT 로드 (Loading Pretrained BERT)</strong>]</a></h2>
<p>우리는 :numref:<code>sec_bert-dataset</code>과 :numref:<code>sec_bert-pretraining</code>에서
WikiText-2 데이터셋에 대해 BERT를 사전 훈련하는 방법을 설명했습니다
(원래 BERT 모델은 훨씬 더 큰 코퍼스에서 사전 훈련되었음에 유의하십시오).
:numref:<code>sec_bert-pretraining</code>에서 논의했듯이,
원래 BERT 모델에는 수억 개의 파라미터가 있습니다.
다음에서,
우리는 두 가지 버전의 사전 훈련된 BERT를 제공합니다:
"bert.base"는 원래 BERT 기본 모델만큼 커서 미세 조정에 많은 계산 리소스가 필요하고,
"bert.small"은 시연을 용이하게 하기 위한 작은 버전입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.zip',
                             '7b3820b35da691042e5d34c0971ac3edbd80d3f4')
d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.zip',
                              'a4e718a47137ccd1809c9107ab4f5edd317bae2c')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',
                             '225d66f04cae318b841a13d32af3acc165f253ac')
d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',
                              'c72329e68a732bef0452e4b96a1c341c8910f81f')
</code></pre>
<p>두 사전 훈련된 BERT 모델 모두 어휘 집합을 정의하는 "vocab.json" 파일과
사전 훈련된 파라미터의 "pretrained.params" 파일을 포함합니다.
우리는 [<strong>사전 훈련된 BERT 파라미터를 로드</strong>]하기 위해 다음 <code>load_pretrained_model</code> 함수를 구현합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,
                          num_heads, num_blks, dropout, max_len, devices):
    data_dir = d2l.download_extract(pretrained_model)
    # 미리 정의된 어휘를 로드하기 위해 빈 어휘 정의
    vocab = d2l.Vocab()
    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))
    vocab.token_to_idx = {token: idx for idx, token in enumerate(
        vocab.idx_to_token)}
    bert = d2l.BERTModel(len(vocab), num_hiddens, ffn_num_hiddens, num_heads, 
                         num_blks, dropout, max_len)
    # 사전 훈련된 BERT 파라미터 로드
    bert.load_parameters(os.path.join(data_dir, 'pretrained.params'),
                         ctx=devices)
    return bert, vocab
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,
                          num_heads, num_blks, dropout, max_len, devices):
    data_dir = d2l.download_extract(pretrained_model)
    # 미리 정의된 어휘를 로드하기 위해 빈 어휘 정의
    vocab = d2l.Vocab()
    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))
    vocab.token_to_idx = {token: idx for idx, token in enumerate(
        vocab.idx_to_token)}
    bert = d2l.BERTModel(
        len(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=4,
        num_blks=2, dropout=0.2, max_len=max_len)
    # 사전 훈련된 BERT 파라미터 로드
    bert.load_state_dict(torch.load(os.path.join(data_dir,
                                                 'pretrained.params')))
    return bert, vocab
</code></pre>
<p>대부분의 기계에서 시연을 용이하게 하기 위해,
이 섹션에서는 사전 훈련된 BERT의 작은 버전("bert.small")을 로드하고 미세 조정할 것입니다.
연습 문제에서는 훨씬 더 큰 "bert.base"를 미세 조정하여 테스트 정확도를 크게 향상시키는 방법을 보여줄 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
devices = d2l.try_all_gpus()
bert, vocab = load_pretrained_model(
    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,
    num_blks=2, dropout=0.1, max_len=512, devices=devices)
</code></pre>
<h2 id="bert-미세-조정을-위한-데이터셋-the-dataset-for-fine-tuning-bert"><a class="header" href="#bert-미세-조정을-위한-데이터셋-the-dataset-for-fine-tuning-bert">[<strong>BERT 미세 조정을 위한 데이터셋 (The Dataset for Fine-Tuning BERT)</strong>]</a></h2>
<p>SNLI 데이터셋의 다운스트림 작업 자연어 추론을 위해,
사용자 정의 데이터셋 클래스 <code>SNLIBERTDataset</code>을 정의합니다.
각 예제에서,
전제와 가설은 텍스트 시퀀스 쌍을 형성하고
:numref:<code>fig_bert-two-seqs</code>에 묘사된 대로 하나의 BERT 입력 시퀀스로 압축됩니다.
:numref:<code>subsec_bert_input_rep</code>를 상기하면, 세그먼트 ID는
BERT 입력 시퀀스에서 전제와 가설을 구별하는 데 사용됩니다.
BERT 입력 시퀀스의 미리 정의된 최대 길이(<code>max_len</code>)로,
입력 텍스트 쌍 중 더 긴 것의 마지막 토큰은
<code>max_len</code>이 충족될 때까지 계속 제거됩니다.
BERT 미세 조정을 위한 SNLI 데이터셋 생성을 가속화하기 위해,
우리는 4개의 작업자 프로세스를 사용하여 훈련 또는 테스트 예제를 병렬로 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class SNLIBERTDataset(gluon.data.Dataset):
    def __init__(self, dataset, max_len, vocab=None):
        all_premise_hypothesis_tokens = [[
            p_tokens, h_tokens] for p_tokens, h_tokens in zip(
            *[d2l.tokenize([s.lower() for s in sentences])
              for sentences in dataset[:2]])]
        
        self.labels = np.array(dataset[2])
        self.vocab = vocab
        self.max_len = max_len
        (self.all_token_ids, self.all_segments,
         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)
        print('read ' + str(len(self.all_token_ids)) + ' examples')

    def _preprocess(self, all_premise_hypothesis_tokens):
        pool = multiprocessing.Pool(4)  # 4개의 작업자 프로세스 사용
        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)
        all_token_ids = [
            token_ids for token_ids, segments, valid_len in out]
        all_segments = [segments for token_ids, segments, valid_len in out]
        valid_lens = [valid_len for token_ids, segments, valid_len in out]
        return (np.array(all_token_ids, dtype='int32'),
                np.array(all_segments, dtype='int32'), 
                np.array(valid_lens))

    def _mp_worker(self, premise_hypothesis_tokens):
        p_tokens, h_tokens = premise_hypothesis_tokens
        self._truncate_pair_of_tokens(p_tokens, h_tokens)
        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)
        token_ids = self.vocab[tokens] + [self.vocab['&lt;pad&gt;']] \
                             * (self.max_len - len(tokens))
        segments = segments + [0] * (self.max_len - len(segments))
        valid_len = len(tokens)
        return token_ids, segments, valid_len

    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):
        # BERT 입력을 위해 '&lt;CLS&gt;', '&lt;SEP&gt;', '&lt;SEP&gt;' 토큰을 위한 슬롯 예약
        while len(p_tokens) + len(h_tokens) &gt; self.max_len - 3:
            if len(p_tokens) &gt; len(h_tokens):
                p_tokens.pop()
            else:
                h_tokens.pop()

    def __getitem__(self, idx):
        return (self.all_token_ids[idx], self.all_segments[idx],
                self.valid_lens[idx]), self.labels[idx]

    def __len__(self):
        return len(self.all_token_ids)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class SNLIBERTDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, max_len, vocab=None):
        all_premise_hypothesis_tokens = [[
            p_tokens, h_tokens] for p_tokens, h_tokens in zip(
            *[d2l.tokenize([s.lower() for s in sentences])
              for sentences in dataset[:2]])]
        
        self.labels = torch.tensor(dataset[2])
        self.vocab = vocab
        self.max_len = max_len
        (self.all_token_ids, self.all_segments,
         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)
        print('read ' + str(len(self.all_token_ids)) + ' examples')

    def _preprocess(self, all_premise_hypothesis_tokens):
        pool = multiprocessing.Pool(4)  # 4개의 작업자 프로세스 사용
        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)
        all_token_ids = [
            token_ids for token_ids, segments, valid_len in out]
        all_segments = [segments for token_ids, segments, valid_len in out]
        valid_lens = [valid_len for token_ids, segments, valid_len in out]
        return (torch.tensor(all_token_ids, dtype=torch.long),
                torch.tensor(all_segments, dtype=torch.long), 
                torch.tensor(valid_lens))

    def _mp_worker(self, premise_hypothesis_tokens):
        p_tokens, h_tokens = premise_hypothesis_tokens
        self._truncate_pair_of_tokens(p_tokens, h_tokens)
        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)
        token_ids = self.vocab[tokens] + [self.vocab['&lt;pad&gt;']] \
                             * (self.max_len - len(tokens))
        segments = segments + [0] * (self.max_len - len(segments))
        valid_len = len(tokens)
        return token_ids, segments, valid_len

    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):
        # BERT 입력을 위해 '&lt;CLS&gt;', '&lt;SEP&gt;', '&lt;SEP&gt;' 토큰을 위한 슬롯 예약
        while len(p_tokens) + len(h_tokens) &gt; self.max_len - 3:
            if len(p_tokens) &gt; len(h_tokens):
                p_tokens.pop()
            else:
                h_tokens.pop()

    def __getitem__(self, idx):
        return (self.all_token_ids[idx], self.all_segments[idx],
                self.valid_lens[idx]), self.labels[idx]

    def __len__(self):
        return len(self.all_token_ids)
</code></pre>
<p>SNLI 데이터셋을 다운로드한 후,
<code>SNLIBERTDataset</code> 클래스를 인스턴스화하여 [<strong>훈련 및 테스트 예제를 생성</strong>]합니다.
이러한 예제들은 자연어 추론의 훈련 및 테스트 중에 미니배치로 읽힐 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 메모리 부족 오류가 발생하면 `batch_size`를 줄이십시오. 원래 BERT 모델에서 `max_len` = 512입니다
batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()
data_dir = d2l.download_extract('SNLI')
train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)
test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)
train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
                                   num_workers=num_workers)
test_iter = gluon.data.DataLoader(test_set, batch_size,
                                  num_workers=num_workers)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 메모리 부족 오류가 발생하면 `batch_size`를 줄이십시오. 원래 BERT 모델에서 `max_len` = 512입니다
batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()
data_dir = d2l.download_extract('SNLI')
train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)
test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)
train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,
                                   num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(test_set, batch_size,
                                  num_workers=num_workers)
</code></pre>
<h2 id="bert-미세-조정-fine-tuning-bert-1"><a class="header" href="#bert-미세-조정-fine-tuning-bert-1">BERT 미세 조정 (Fine-Tuning BERT)</a></h2>
<p>:numref:<code>fig_bert-two-seqs</code>가 나타내듯이,
자연어 추론을 위한 BERT 미세 조정에는
두 개의 완전 연결 레이어로 구성된 추가 MLP만 필요합니다
(다음 <code>BERTClassifier</code> 클래스의 <code>self.hidden</code> 및 <code>self.output</code> 참조).
[<strong>이 MLP는 전제와 가설 모두의 정보를 인코딩하는
특수 “&lt;cls&gt;” 토큰의 BERT 표현을
자연어 추론의 세 가지 출력(함의, 모순, 중립)으로 변환합니다</strong>].</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class BERTClassifier(nn.Block):
    def __init__(self, bert):
        super(BERTClassifier, self).__init__()
        self.encoder = bert.encoder
        self.hidden = bert.hidden
        self.output = nn.Dense(3)

    def forward(self, inputs):
        tokens_X, segments_X, valid_lens_x = inputs
        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)
        return self.output(self.hidden(encoded_X[:, 0, :]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class BERTClassifier(nn.Module):
    def __init__(self, bert):
        super(BERTClassifier, self).__init__()
        self.encoder = bert.encoder
        self.hidden = bert.hidden
        self.output = nn.LazyLinear(3)

    def forward(self, inputs):
        tokens_X, segments_X, valid_lens_x = inputs
        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)
        return self.output(self.hidden(encoded_X[:, 0, :]))
</code></pre>
<p>다음에서,
사전 훈련된 BERT 모델 <code>bert</code>는 다운스트림 응용 프로그램을 위해
<code>BERTClassifier</code> 인스턴스 <code>net</code>에 공급됩니다.
BERT 미세 조정의 일반적인 구현에서,
추가 MLP의 출력 레이어 파라미터(<code>net.output</code>)만 처음부터 학습됩니다.
사전 훈련된 BERT 인코더의 모든 파라미터(<code>net.encoder</code>)와 추가 MLP의 은닉층(<code>net.hidden</code>)은 미세 조정됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net = BERTClassifier(bert)
net.output.initialize(ctx=devices)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net = BERTClassifier(bert)
</code></pre>
<p>:numref:<code>sec_bert</code>에서
<code>MaskLM</code> 클래스와 <code>NextSentencePred</code> 클래스 모두
사용된 MLP에 파라미터가 있음을 상기하십시오.
이 파라미터들은 사전 훈련된 BERT 모델 <code>bert</code>의 파라미터의 일부이므로 <code>net</code>의 파라미터의 일부입니다.
그러나 이러한 파라미터는
사전 훈련 중에 마스킹된 언어 모델링 손실과
다음 문장 예측 손실을 계산하기 위한 것일 뿐입니다.
이 두 손실 함수는 다운스트림 응용 프로그램의 미세 조정과 관련이 없으므로,
BERT가 미세 조정될 때 <code>MaskLM</code> 및 <code>NextSentencePred</code>에서 사용된 MLP의 파라미터는 업데이트되지 않습니다(staled).</p>
<p>오래된 기울기를 가진 파라미터를 허용하기 위해,
<code>d2l.train_batch_ch13</code>의 <code>step</code> 함수에서 <code>ignore_stale_grad=True</code> 플래그가 설정됩니다.
우리는 이 함수를 사용하여 SNLI의 훈련 세트(<code>train_iter</code>)와 테스트 세트(<code>test_iter</code>)를 사용하여 모델 <code>net</code>을 훈련하고 평가합니다.
제한된 계산 리소스로 인해, [<strong>훈련</strong>] 및 테스트 정확도는
더 향상될 수 있습니다. 이에 대한 논의는 연습 문제로 남겨둡니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
lr, num_epochs = 1e-4, 5
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices,
               d2l.split_batch_multi_inputs)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
lr, num_epochs = 1e-4, 5
trainer = torch.optim.Adam(net.parameters(), lr=lr)
loss = nn.CrossEntropyLoss(reduction='none')
net(next(iter(train_iter))[0])
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<h2 id="요약-summary-99"><a class="header" href="#요약-summary-99">요약 (Summary)</a></h2>
<ul>
<li>SNLI 데이터셋의 자연어 추론과 같은 다운스트림 응용 프로그램을 위해 사전 훈련된 BERT 모델을 미세 조정할 수 있습니다.</li>
<li>미세 조정 중에, BERT 모델은 다운스트림 응용 프로그램을 위한 모델의 일부가 됩니다. 사전 훈련 손실과만 관련된 파라미터는 미세 조정 중에 업데이트되지 않습니다.</li>
</ul>
<h2 id="연습-문제-exercises-114"><a class="header" href="#연습-문제-exercises-114">연습 문제 (Exercises)</a></h2>
<ol>
<li>계산 리소스가 허용하는 경우 원래 BERT 기본 모델만큼 큰 훨씬 더 큰 사전 훈련된 BERT 모델을 미세 조정하십시오. <code>load_pretrained_model</code> 함수의 인수를 다음과 같이 설정하십시오: 'bert.small'을 'bert.base'로 교체하고, <code>num_hiddens=256</code>, <code>ffn_num_hiddens=512</code>, <code>num_heads=4</code>, <code>num_blks=2</code>의 값을 각각 768, 3072, 12, 12로 늘립니다. 미세 조정 에포크를 늘리고(가능하면 다른 하이퍼파라미터를 조정하여), 0.86보다 높은 테스트 정확도를 얻을 수 있습니까?</li>
<li>길이 비율에 따라 시퀀스 쌍을 어떻게 자를 수 있습니까? 이 쌍 자르기 방법과 <code>SNLIBERTDataset</code> 클래스에서 사용된 방법을 비교하십시오. 장단점은 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/397">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1526">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="강화-학습-reinforcement-learning-1"><a class="header" href="#강화-학습-reinforcement-learning-1">강화 학습 (Reinforcement Learning)</a></h1>
<p>:label:<code>chap_reinforcement_learning</code></p>
<p><strong>Pratik Chaudhari</strong> (<em>University of Pennsylvania and Amazon</em>), <strong>Rasool Fakoor</strong> (<em>Amazon</em>), 및 <strong>Kavosh Asadi</strong> (<em>Amazon</em>)</p>
<p>강화 학습(Reinforcement Learning, RL)은 순차적으로 결정을 내리는 머신러닝 시스템을 구축할 수 있게 해주는 기술 모음입니다. 예를 들어, 온라인 소매업체에서 구매한 새 옷이 들어 있는 패키지가 일련의 결정, 즉 소매업체가 집에서 가장 가까운 창고에서 옷을 찾고, 옷을 상자에 넣고, 육로 또는 항공으로 상자를 운송하고, 도시 내 집으로 배달한 후 문앞에 도착합니다. 그 과정에서 패키지 배송에 영향을 미치는 많은 변수가 있습니다. 예를 들어 창고에 옷이 있었는지 여부, 상자를 운송하는 데 걸린 시간, 일일 배달 트럭이 출발하기 전에 도시에 도착했는지 여부 등입니다. 핵심 아이디어는 각 단계에서 우리가 종종 통제하지 않는 이러한 변수가 미래의 전체 사건 순서에 영향을 미친다는 것입니다. 예를 들어 창고에서 상자를 포장하는 데 지연이 발생하면 소매업체는 적시 배송을 보장하기 위해 지상 대신 항공으로 패키지를 보내야 할 수도 있습니다. 강화 학습 방법을 사용하면 순차적 의사 결정 문제의 각 단계에서 적절한 조치를 취하여 결국 어떤 효용, 예를 들어 패키지의 적시 배송을 극대화할 수 있습니다.</p>
<p>이러한 순차적 의사 결정 문제는 수많은 다른 곳에서도 볼 수 있습니다. 예를 들어 <a href="https://en.wikipedia.org/wiki/Go_(game)">바둑(Go)</a>을 두는 동안 현재의 움직임이 다음 움직임을 결정하고 상대방의 움직임은 제어할 수 없는 변수입니다... 일련의 움직임이 결국 승패를 결정합니다; Netflix가 현재 추천하는 영화는 당신이 무엇을 볼지 결정하고, 당신이 그 영화를 좋아할지 여부는 Netflix에 알려지지 않았으며, 결국 일련의 영화 추천은 당신이 Netflix에 얼마나 만족하는지 결정합니다. 오늘날 이러한 문제에 대한 효과적인 솔루션을 개발하기 위해 강화 학습이 사용되고 있습니다 :cite:<code>mnih2013playing,Silver.Huang.Maddison.ea.2016</code>. 강화 학습과 표준 딥러닝의 주요 차이점은 표준 딥러닝에서는 하나의 테스트 데이터에 대한 훈련된 모델의 예측이 미래의 테스트 데이터에 대한 예측에 영향을 미치지 않는다는 것입니다; 강화 학습에서는 미래의 순간(RL에서는 결정을 행동(action)이라고도 함)에서의 결정이 과거에 내린 결정에 의해 영향을 받습니다.</p>
<p>이 장에서는 강화 학습의 기초를 개발하고 널리 사용되는 강화 학습 방법을 구현하는 실무 경험을 얻을 것입니다. 먼저 그러한 순차적 의사 결정 문제를 생각할 수 있게 해주는 마르코프 결정 과정(Markov Decision Process, MDP)이라는 개념을 개발할 것입니다. 가치 반복(Value Iteration)이라는 알고리즘은 MDP의 통제되지 않는 변수(RL에서는 이러한 제어된 변수를 환경이라고 함)가 일반적으로 어떻게 행동하는지 알고 있다는 가정 하에 강화 학습 문제를 해결하는 첫 번째 통찰력이 될 것입니다. 가치 반복의 더 일반적인 버전인 Q-러닝(Q-Learning)이라는 알고리즘을 사용하면 환경에 대한 완전한 지식이 없더라도 적절한 조치를 취할 수 있습니다. 그런 다음 전문가의 행동을 모방하여 강화 학습 문제에 딥 네트워크를 사용하는 방법을 연구할 것입니다. 마지막으로, 딥 네트워크를 사용하여 알려지지 않은 환경에서 행동을 취하는 강화 학습 방법을 개발할 것입니다. 이러한 기술은 오늘날 다양한 실제 응용 분야에서 사용되는 더 발전된 RL 알고리즘의 기초를 형성하며, 그 중 일부는 장에서 지적할 것입니다.</p>
<p><img src="chapter_reinforcement-learning/../img/RL_main.png" alt="강화 학습 구조" />
:width:<code>400px</code>
:label:<code>fig_rl_big</code></p>
<pre><code class="language-toc">:maxdepth: 2

mdp
value-iter
qlearning
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="마르코프-결정-과정-markov-decision-process-mdp"><a class="header" href="#마르코프-결정-과정-markov-decision-process-mdp">마르코프 결정 과정 (Markov Decision Process, MDP)</a></h1>
<p>:label:<code>sec_mdp</code>
이 섹션에서는 마르코프 결정 과정(MDP)을 사용하여 강화 학습 문제를 공식화하는 방법에 대해 논의하고 MDP의 다양한 구성 요소를 자세히 설명합니다.</p>
<h2 id="mdp의-정의"><a class="header" href="#mdp의-정의">MDP의 정의</a></h2>
<p>마르코프 결정 과정(MDP) :cite:<code>BellmanMDP</code>은 시스템에 다양한 행동이 적용될 때 시스템의 상태가 어떻게 진화하는지에 대한 모델입니다. 몇 가지 다른 수량들이 함께 모여 MDP를 형성합니다.</p>
<p><img src="chapter_reinforcement-learning/../img/mdp.png" alt="간단한 그리드 월드 내비게이션 작업. 로봇은 목표 위치(녹색 집으로 표시됨)로 가는 길을 찾아야 할 뿐만 아니라 함정 위치(빨간색 십자가로 표시됨)를 피해야 합니다." />
:width:<code>250px</code>
:label:<code>fig_mdp</code></p>
<ul>
<li>$\mathcal{S}$를 MDP의 상태 집합이라고 합시다. 구체적인 예로 :numref:<code>fig_mdp</code>를 참조하십시오. 로봇이 그리드 월드를 탐색하고 있습니다. 이 경우, $\mathcal{S}$는 주어진 타임스텝에서 로봇이 있을 수 있는 위치 집합에 해당합니다.</li>
<li>$\mathcal{A}$를 로봇이 각 상태에서 취할 수 있는 행동 집합이라고 합시다. 예: "앞으로 이동", "오른쪽으로 회전", "왼쪽으로 회전", "같은 위치에 머무르기" 등. 행동은 로봇의 현재 상태를 $\mathcal{S}$ 내의 다른 상태로 변경할 수 있습니다.</li>
<li>로봇이 <em>정확히</em> 어떻게 움직이는지 모르고 대략적으로만 알 수도 있습니다. 강화 학습에서는 이 상황을 다음과 같이 모델링합니다: 로봇이 "앞으로 이동" 행동을 취하면 현재 상태에 머무를 확률이 작고, "왼쪽으로 회전"할 확률도 작을 수 있습니다. 수학적으로 이것은 "전이 함수(transition function)" $T: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0,1]$를 정의하는 것과 같습니다. 로봇이 상태 $s$에 있고 행동 $a$를 취했을 때 상태 $s'$에 도달할 조건부 확률을 사용하여 $T(s, a, s') = P(s' \mid s, a)$로 정의합니다. 전이 함수는 확률 분포이므로 모든 $s \in \mathcal{S}$ 및 $a \in \mathcal{A}$에 대해 $\sum_{s' \in \mathcal{S}} T(s, a, s') = 1$입니다. 즉, 로봇이 행동을 취하면 어떤 상태로든 가야 합니다.</li>
<li>이제 "보상(reward)" $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ 개념을 사용하여 어떤 행동이 유용하고 어떤 행동이 유용하지 않은지에 대한 개념을 구성합니다. 로봇이 상태 $s$에서 행동 $a$를 취하면 보상 $r(s,a)$를 받는다고 말합니다. 보상 $r(s, a)$가 크면 상태 $s$에서 행동 $a$를 취하는 것이 로봇의 목표(예: 녹색 집으로 이동)를 달성하는 데 더 유용함을 나타냅니다. 보상 $r(s, a)$가 작으면 행동 $a$는 이 목표를 달성하는 데 덜 유용합니다. 보상은 목표를 염두에 두고 사용자(강화 학습 알고리즘을 만드는 사람)가 설계한다는 점에 유의하는 것이 중요합니다.</li>
</ul>
<h2 id="반환값-및-할인율-return-and-discount-factor"><a class="header" href="#반환값-및-할인율-return-and-discount-factor">반환값 및 할인율 (Return and Discount Factor)</a></h2>
<p>위의 다양한 구성 요소가 함께 마르코프 결정 과정(MDP)을 형성합니다.
$$\textrm{MDP}: (\mathcal{S}, \mathcal{A}, T, r).$$</p>
<p>이제 로봇이 특정 상태 $s_0 \in \mathcal{S}$에서 시작하여 행동을 계속 취하여 궤적을 만드는 상황을 고려해 봅시다.
$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, a_2, r_2, \ldots).$$</p>
<p>각 타임스텝 $t$에서 로봇은 상태 $s_t$에 있고 행동 $a_t$를 취하며 그 결과 보상 $r_t = r(s_t, a_t)$를 얻습니다. 궤적의 *반환값(return)*은 로봇이 해당 궤적을 따라 얻은 총 보상입니다.
$$R(\tau) = r_0 + r_1 + r_2 + \cdots.$$</p>
<p>강화 학습의 목표는 가장 큰 <em>반환값</em>을 갖는 궤적을 찾는 것입니다.</p>
<p>로봇이 목표 위치에 도달하지 못하고 그리드 월드를 계속 여행하는 상황을 생각해 봅시다. 이 경우 궤적의 상태 및 행동 시퀀스는 무한히 길 수 있으며, 그러한 무한히 긴 궤적의 <em>반환값</em>은 무한대가 될 것입니다. 강화 학습 공식을 그러한 궤적에 대해서도 의미 있게 유지하기 위해 할인율(discount factor) $\gamma &lt; 1$의 개념을 도입합니다. 할인된 <em>반환값</em>은 다음과 같이 씁니다.
$$R(\tau) = r_0 + \gamma r_1 + \gamma^2 r_2 + \cdots = \sum_{t=0}^\infty \gamma^t r_t.$$</p>
<p>$\gamma$가 매우 작으면 먼 미래(예: $t = 1000$)에 로봇이 얻는 보상은 $\gamma^{1000}$만큼 크게 할인됩니다. 이것은 로봇이 목표, 즉 그리드 월드 예제(:numref:<code>fig_mdp</code> 참조)에서 녹색 집으로 가는 목표를 달성하는 짧은 궤적을 선택하도록 장려합니다. 할인율이 큰 값(예: $\gamma = 0.99$)이면 로봇이 <em>탐험</em>하고 목표 위치로 가는 최상의 궤적을 찾도록 장려됩니다.</p>
<h2 id="마르코프-가정에-대한-논의"><a class="header" href="#마르코프-가정에-대한-논의">마르코프 가정에 대한 논의</a></h2>
<p>상태 $s_t$가 위와 같이 위치이지만 행동 $a_t$가 "앞으로 이동"과 같은 추상적인 명령이 아니라 로봇이 바퀴에 적용하는 가속도인 새로운 로봇을 생각해 봅시다. 이 로봇이 상태 $s_t$에서 0이 아닌 속도를 갖는다면, 다음 위치 $s_{t+1}$은 과거 위치 $s_t$, 가속도 $a_t$, 그리고 $s_t - s_{t-1}$에 비례하는 시간 $t$에서의 로봇 속도의 함수입니다. 이것은 다음을 의미합니다.</p>
<p>$$s_{t+1} = \textrm{some function}(s_t, a_t, s_{t-1});$$</p>
<p>우리 경우 "어떤 함수"는 뉴턴의 운동 법칙일 것입니다. 이것은 단순히 $s_t$와 $a_t$에만 의존하는 전이 함수와는 매우 다릅니다.</p>
<p>마르코프 시스템은 다음 상태 $s_{t+1}$이 현재 상태 $s_t$와 현재 상태에서 취한 행동 $a_t$의 함수일 뿐인 모든 시스템입니다. 마르코프 시스템에서 다음 상태는 과거에 취한 행동이나 과거에 로봇이 있었던 상태에 의존하지 않습니다. 예를 들어, 위에서 가속도를 행동으로 하는 새로운 로봇은 다음 위치 $s_{t+1}$이 속도를 통해 이전 상태 $s_{t-1}$에 의존하기 때문에 마르코프적이지 않습니다. 시스템의 마르코프적 특성이 제한적인 가정인 것처럼 보일 수 있지만 그렇지 않습니다. 마르코프 결정 과정은 여전히 매우 큰 클래스의 실제 시스템을 모델링할 수 있습니다. 예를 들어, 새로운 로봇의 경우 상태 $s_t$를 튜플 $(\textrm{location}, \textrm{velocity})$로 선택하면 다음 상태 $(\textrm{location}<em>{t+1}, \textrm{velocity}</em>{t+1})$이 현재 상태 $(\textrm{location}_t, \textrm{velocity}_t)$와 현재 상태에서의 행동 $a_t$에만 의존하기 때문에 시스템은 마르코프적입니다.</p>
<h2 id="요약-2"><a class="header" href="#요약-2">요약</a></h2>
<p>강화 학습 문제는 일반적으로 마르코프 결정 과정을 사용하여 모델링됩니다. 마르코프 결정 과정(MDP)은 네 가지 엔터티 $(\mathcal{S}, \mathcal{A}, T, r)$의 튜플로 정의됩니다. 여기서 $\mathcal{S}$는 상태 공간, $\mathcal{A}$는 행동 공간, $T$는 MDP의 전이 확률을 인코딩하는 전이 함수, $r$은 특정 상태에서 행동을 취하여 얻은 즉각적인 보상입니다.</p>
<h2 id="연습-문제-7"><a class="header" href="#연습-문제-7">연습 문제</a></h2>
<ol>
<li><a href="https://www.gymlibrary.dev/environments/classic_control/mountain_car/">MountainCar</a> 문제를 모델링하기 위해 MDP를 설계하고 싶다고 가정해 봅시다.
<ol>
<li>상태 집합은 무엇입니까?</li>
<li>행동 집합은 무엇입니까?</li>
<li>가능한 보상 함수는 무엇입니까?</li>
</ol>
</li>
<li><a href="https://www.gymlibrary.dev/environments/atari/pong/">Pong 게임</a>과 같은 Atari 게임을 위한 MDP를 어떻게 설계하시겠습니까?</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12084">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%%tab all

%matplotlib inline
import numpy as np
import random
from d2l import torch as d2l

seed = 0  # 난수 생성기 시드
gamma = 0.95  # 할인율
num_iters = 10  # 반복 횟수
random.seed(seed)  # 결과 재현성을 보장하기 위해 랜덤 시드 설정
np.random.seed(seed)

# 이제 환경 설정
env_info = d2l.make_env('FrozenLake-v1', seed=seed)
</code></pre>
<h1 id="가치-반복-value-iteration"><a class="header" href="#가치-반복-value-iteration">가치 반복 (Value Iteration)</a></h1>
<p>:label:<code>sec_value_iter</code></p>
<p>이 섹션에서는 동적 프로그래밍을 사용하여 마르코프 결정 과정(MDP)에 대한 최적의 정책을 찾는 고전적인 알고리즘인 가치 반복(Value Iteration)에 대해 논의합니다. 가치 반복은 벨만 최적 방정식(Bellman optimality equation)을 반복적으로 적용하여 상태 가치 함수(state-value function)를 업데이트하고, 결과적으로 최적의 정책을 도출합니다.</p>
<p>FrozenLake 환경에서 로봇은 "위"($\uparrow$), "아래"($\rightarrow$), "왼쪽"($\leftarrow$), "오른쪽"($\rightarrow$) 행동으로 $4 \times 4$ 그리드(상태) 위를 이동합니다. 환경에는 로봇에게 알려지지 않은 여러 구멍(H) 셀과 얼어붙은(F) 셀, 목표 셀(G)이 포함되어 있습니다. 문제를 간단하게 유지하기 위해 로봇이 신뢰할 수 있는 행동을 한다고 가정합니다. 즉, 모든 $s \in \mathcal{S}, a \in \mathcal{A}$에 대해 $P(s' \mid s, a) = 1$입니다. 로봇이 목표에 도달하면 시도가 종료되고 행동에 관계없이 $1$의 보상을 받습니다. 다른 상태에서의 보상은 모든 행동에 대해 $0$입니다. 로봇의 목표는 주어진 시작 위치(S)($s_0$)에서 목표 위치(G)에 도달하여 <em>반환값</em>을 최대화하는 정책을 학습하는 것입니다.</p>
<p>다음 함수는 가치 반복을 구현합니다. 여기서 <code>env_info</code>는 MDP 및 환경 관련 정보를 포함하고 <code>gamma</code>는 할인율입니다.</p>
<pre><code class="language-{.python .input}">%%tab all

def value_iteration(env_info, gamma, num_iters):
    env_desc = env_info['desc']  # 각 항목이 의미하는 바를 보여주는 2D 배열
    prob_idx = env_info['trans_prob_idx']
    nextstate_idx = env_info['nextstate_idx']
    reward_idx = env_info['reward_idx']
    num_states = env_info['num_states']
    num_actions = env_info['num_actions']
    mdp = env_info['mdp']

    V  = np.zeros((num_iters + 1, num_states))
    Q  = np.zeros((num_iters + 1, num_states, num_actions))
    pi = np.zeros((num_iters + 1, num_states))

    for k in range(1, num_iters + 1):
        for s in range(num_states):
            for a in range(num_actions):
                # \sum_{s'} p(s'\mid s,a) [r + \gamma v_k(s')] 계산
                for pxrds in mdp[(s,a)]:
                    # mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]
                    pr = pxrds[prob_idx]  # p(s'\mid s,a)
                    nextstate = pxrds[nextstate_idx]  # 다음 상태
                    reward = pxrds[reward_idx]  # 보상
                    Q[k,s,a] += pr * (reward + gamma * V[k - 1, nextstate])
            # 최대 가치와 최대 행동 기록
            V[k,s] = np.max(Q[k,s,:])
            pi[k,s] = np.argmax(Q[k,s,:])
    d2l.show_value_function_progress(env_desc, V[:-1], pi[:-1])

value_iteration(env_info=env_info, gamma=gamma, num_iters=num_iters)
</code></pre>
<p>위의 그림은 정책(화살표는 행동을 나타냄)과 가치 함수(색상 변화는 가치 함수가 어두운 색상으로 표시된 초기 값에서 밝은 색상으로 표시된 최적 값으로 시간이 지남에 따라 어떻게 변하는지 보여줌)를 보여줍니다. 보시다시피 가치 반복은 10번의 반복 후에 최적 가치 함수를 찾고 H 셀이 아닌 한 어떤 상태에서 시작하든 목표 상태(G)에 도달할 수 있습니다. 구현의 또 다른 흥미로운 측면은 최적 가치 함수를 찾는 것 외에도 이 가치 함수에 해당하는 최적 정책 $\pi^*$도 자동으로 찾았다는 것입니다.</p>
<h2 id="요약-3"><a class="header" href="#요약-3">요약</a></h2>
<p>가치 반복 알고리즘의 주요 아이디어는 동적 프로그래밍 원칙을 사용하여 주어진 상태에서 얻은 최적 평균 반환값을 찾는 것입니다. 가치 반복 알고리즘을 구현하려면 마르코프 결정 과정(MDP), 예를 들어 전이 및 보상 함수를 완전히 알아야 합니다.</p>
<h2 id="연습-문제-8"><a class="header" href="#연습-문제-8">연습 문제</a></h2>
<ol>
<li>그리드 크기를 $8 \times 8$로 늘려보십시오. $4 \times 4$ 그리드와 비교하여 최적 가치 함수를 찾는 데 몇 번의 반복이 걸립니까?</li>
<li>가치 반복 알고리즘의 계산 복잡도는 얼마입니까?</li>
<li>$\gamma$(즉, 위 코드의 "gamma")가 $0$, $0.5$, $1$일 때 가치 반복 알고리즘을 다시 실행하고 결과를 분석하십시오.</li>
<li>$\gamma$ 값은 가치 반복이 수렴하는 데 걸리는 반복 횟수에 어떤 영향을 줍니까? $\gamma=1$일 때 어떻게 됩니까?</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12005">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%%tab all

%matplotlib inline
import numpy as np
import random
from d2l import torch as d2l

seed = 0  # 난수 생성기 시드
gamma = 0.95  # 할인율
num_iters = 256  # 반복 횟수
alpha   = 0.9  # 학습률
epsilon = 0.9  # 엡실론 탐욕적 알고리즘의 엡실론
random.seed(seed)  # 랜덤 시드 설정
np.random.seed(seed)

# 이제 환경 설정
env_info = d2l.make_env('FrozenLake-v1', seed=seed)
</code></pre>
<h1 id="q-러닝-q-learning"><a class="header" href="#q-러닝-q-learning">Q-러닝 (Q-Learning)</a></h1>
<p>:label:<code>sec_qlearning</code></p>
<p>이 섹션에서는 가장 인기 있고 널리 사용되는 강화 학습 알고리즘 중 하나인 Q-러닝(Q-Learning)을 소개합니다. Q-러닝은 모델 없이 동작하는(model-free) 강화 학습 알고리즘으로, 에이전트가 환경에 대한 사전 지식 없이도 최적의 정책을 학습할 수 있게 해줍니다.</p>
<p>FrozenLake 환경에서 로봇은 "위"($\uparrow$), "아래"($\rightarrow$), "왼쪽"($\leftarrow$), "오른쪽"($\rightarrow$) 행동으로 $4 \times 4$ 그리드(상태) 위를 이동합니다. 환경에는 로봇에게 알려지지 않은 여러 구멍(H) 셀과 얼어붙은(F) 셀, 목표 셀(G)이 포함되어 있습니다. 문제를 간단하게 유지하기 위해 로봇이 신뢰할 수 있는 행동을 한다고 가정합니다. 즉, 모든 $s \in \mathcal{S}, a \in \mathcal{A}$에 대해 $P(s' \mid s, a) = 1$입니다. 로봇이 목표에 도달하면 시도가 종료되고 행동에 관계없이 $1$의 보상을 받습니다. 다른 상태에서의 보상은 모든 행동에 대해 $0$입니다. 로봇의 목표는 주어진 시작 위치(S)($s_0$)에서 목표 위치(G)에 도달하여 <em>반환값</em>을 최대화하는 정책을 학습하는 것입니다.</p>
<p>먼저 $\epsilon$-탐욕적 방법을 다음과 같이 구현합니다.</p>
<pre><code class="language-{.python .input}">%%tab all

def e_greedy(env, Q, s, epsilon):
    if random.random() &lt; epsilon:
        return env.action_space.sample()

    else:
        return np.argmax(Q[s,:])

</code></pre>
<p>이제 Q-러닝을 구현할 준비가 되었습니다.</p>
<pre><code class="language-{.python .input}">%%tab all

def q_learning(env_info, gamma, num_iters, alpha, epsilon):
    env_desc = env_info['desc']  # 각 그리드 항목이 의미하는 바를 지정하는 2D 배열
    env = env_info['env']  # 각 그리드 항목이 의미하는 바를 지정하는 2D 배열
    num_states = env_info['num_states']
    num_actions = env_info['num_actions']

    Q  = np.zeros((num_states, num_actions))
    V  = np.zeros((num_iters + 1, num_states))
    pi = np.zeros((num_iters + 1, num_states))

    for k in range(1, num_iters + 1):
        # 환경 재설정
        state, done = env.reset(), False
        while not done:
            # 주어진 상태에 대한 행동을 선택하고 선택된 행동에 따라 환경에서 행동
            action = e_greedy(env, Q, state, epsilon)
            next_state, reward, done, _ = env.step(action)

            # Q-업데이트:
            y = reward + gamma * np.max(Q[next_state,:])
            Q[state, action] = Q[state, action] + alpha * (y - Q[state, action])

            # 다음 상태로 이동
            state = next_state
        # 시각화 목적으로만 최대 가치와 최대 행동 기록
        for s in range(num_states):
            V[k,s]  = np.max(Q[s,:])
            pi[k,s] = np.argmax(Q[s,:])
    d2l.show_Q_function_progress(env_desc, V[:-1], pi[:-1])

q_learning(env_info=env_info, gamma=gamma, num_iters=num_iters, alpha=alpha, epsilon=epsilon)

</code></pre>
<p>이 결과는 Q-러닝이 대략 250번의 반복 후에 이 문제에 대한 최적의 해결책을 찾을 수 있음을 보여줍니다. 그러나 이 결과를 가치 반복 알고리즘의 결과(:ref:<code>subsec_valueitercode</code> 참조)와 비교할 때, 가치 반복 알고리즘이 이 문제에 대한 최적의 해결책을 찾는 데 훨씬 더 적은 반복이 필요함을 알 수 있습니다. 이는 가치 반복 알고리즘이 전체 MDP에 접근할 수 있는 반면 Q-러닝은 그렇지 않기 때문에 발생합니다.</p>
<h2 id="요약-4"><a class="header" href="#요약-4">요약</a></h2>
<p>Q-러닝은 가장 기본적인 강화 학습 알고리즘 중 하나입니다. 그것은 최근 강화 학습의 성공, 특히 비디오 게임 플레이 학습의 진원지에 있었습니다 :cite:<code>mnih2013playing</code>. Q-러닝을 구현하기 위해 마르코프 결정 과정(MDP), 예를 들어 전이 및 보상 함수를 완전히 알 필요는 없습니다.</p>
<h2 id="연습-문제-9"><a class="header" href="#연습-문제-9">연습 문제</a></h2>
<ol>
<li>그리드 크기를 $8 \times 8$로 늘려보십시오. $4 \times 4$ 그리드와 비교하여 최적 가치 함수를 찾는 데 몇 번의 반복이 걸립니까?</li>
<li>$\gamma$(즉, 위 코드의 "gamma")가 $0$, $0.5$, $1$일 때 Q-러닝 알고리즘을 다시 실행하고 결과를 분석하십시오.</li>
<li>$\epsilon$(즉, 위 코드의 "epsilon")이 $0$, $0.5$, $1$일 때 Q-러닝 알고리즘을 다시 실행하고 결과를 분석하십시오.</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12103">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="가우스-프로세스-gaussian-processes"><a class="header" href="#가우스-프로세스-gaussian-processes">가우스 프로세스 (Gaussian Processes)</a></h1>
<p>:label:<code>chap_gp</code></p>
<p><strong>Andrew Gordon Wilson</strong> (<em>New York University and Amazon</em>)</p>
<p>가우스 프로세스(Gaussian processes, GPs)는 어디에나 있습니다. 여러분은 이미 깨닫지 못하는 사이에 많은 GP 예제를 접했습니다. 파라미터에 대한 가우스 분포를 가진 파라미터에 대해 선형인 모든 모델은 가우스 프로세스입니다. 이 클래스는 무작위 보행(random walks) 및 자기 회귀 프로세스(autoregressive processes)를 포함한 이산 모델뿐만 아니라 베이지안 선형 회귀 모델, 다항식, 푸리에 급수, 방사형 기저 함수(radial basis functions), 심지어 무한한 수의 은닉 유닛이 있는 신경망을 포함한 연속 모델까지 포괄합니다. "모든 것이 가우스 프로세스의 특수한 경우"라는 농담이 있습니다.</p>
<p>가우스 프로세스에 대해 배우는 것은 세 가지 이유로 중요합니다: (1) 모델링에 대한 <em>함수 공간(function space)</em> 관점을 제공하여 심층 신경망을 포함한 다양한 모델 클래스를 훨씬 더 이해하기 쉽게 만듭니다; (2) 능동 학습(active learning), 하이퍼파라미터 학습, auto-ML, 시공간 회귀를 포함하여 최첨단 기술인 놀라운 범위의 응용 분야가 있습니다; (3) 지난 몇 년 동안 알고리즘의 발전으로 가우스 프로세스가 점점 더 확장 가능하고 관련성이 높아져 <a href="https://gpytorch.ai">GPyTorch</a> :cite:<code>Gardner.Pleiss.Weinberger.Bindel.Wilson.2018</code>와 같은 프레임워크를 통해 딥러닝과 조화를 이루었습니다. 실제로 GP와 심층 신경망은 경쟁적인 접근 방식이 아니라 매우 상호 보완적이며 큰 효과를 위해 결합될 수 있습니다. 이러한 알고리즘의 발전은 가우스 프로세스와 관련이 있을 뿐만 아니라 딥러닝에 광범위하게 유용한 수치적 방법의 기초를 제공합니다.</p>
<p>이 장에서는 가우스 프로세스를 소개합니다. 입문용 노트북에서는 가우스 프로세스가 무엇이며 함수를 직접 모델링하는 방법에 대해 직관적으로 추론하는 것으로 시작합니다. 사전 확률(priors) 노트북에서는 가우스 프로세스 사전 확률을 지정하는 방법에 초점을 맞춥니다. 우리는 모델링에 대한 전통적인 가중치 공간(weight-space) 접근 방식을 함수 공간에 직접 연결하여 심층 신경망을 포함한 머신러닝 모델을 구성하고 이해하는 데 도움을 줄 것입니다. 그런 다음 가우스 프로세스의 일반화 속성을 제어하는 *커널(kernels)*이라고도 하는 널리 사용되는 공분산 함수를 소개합니다. 주어진 커널을 가진 GP는 함수에 대한 사전 확률을 정의합니다. 추론 노트북에서는 데이터를 사용하여 *사후 확률(posterior)*을 추론하여 예측을 수행하는 방법을 보여줍니다. 이 노트북에는 가우스 프로세스로 예측을 수행하기 위한 처음부터 작성된 코드와 GPyTorch에 대한 소개가 포함되어 있습니다. 다가오는 노트북에서는 가우스 프로세스 뒤에 있는 수치 해석을 소개할 것입니다. 이는 가우스 프로세스를 확장하는 데 유용할 뿐만 아니라 딥러닝을 위한 강력한 일반적인 기초이며, 딥러닝의 하이퍼파라미터 튜닝과 같은 고급 사용 사례이기도 합니다. 우리의 예제는 가우스 프로세스를 확장하고 딥러닝 기능 및 PyTorch와 밀접하게 통합된 GPyTorch를 사용할 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

gp-intro
gp-priors
gp-inference
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['pytorch'])
</code></pre>
<h1 id="가우시안-프로세스-소개-introduction-to-gaussian-processes"><a class="header" href="#가우시안-프로세스-소개-introduction-to-gaussian-processes">가우시안 프로세스 소개 (Introduction to Gaussian Processes)</a></h1>
<p>많은 경우, 머신러닝은 데이터에서 파라미터를 추정하는 것과 같습니다. 이러한 파라미터는 종종 수많은 신경망의 가중치와 같이 비교적 해석하기 어렵습니다. 대조적으로, 가우시안 프로세스는 데이터에 적합할 수 있는 함수의 고수준 속성을 직접 추론하는 메커니즘을 제공합니다. 예를 들어, 우리는 이러한 함수가 빠르게 변하는지, 주기적인지, 조건부 독립성을 포함하는지, 또는 평행 이동 불변성을 갖는지에 대한 감각을 가질 수 있습니다. 가우시안 프로세스를 사용하면 데이터에 적합할 수 있는 함수 값에 대한 가우시안 분포를 직접 지정하여 이러한 속성을 모델에 쉽게 통합할 수 있습니다.</p>
<p>가우시안 프로세스가 어떻게 작동하는지 몇 가지 예제로 시작하여 감을 잡아봅시다.</p>
<p>입력 $x$에 의해 인덱싱된 회귀 타겟(출력) $y$의 다음 데이터셋을 관찰한다고 가정해 봅시다. 예를 들어, 타겟은 이산화탄소 농도의 변화일 수 있고, 입력은 이러한 타겟이 기록된 시간일 수 있습니다. 데이터의 특징은 무엇입니까? 얼마나 빨리 변하는 것 같습니까? 정기적인 간격으로 수집된 데이터 포인트가 있습니까, 아니면 누락된 입력이 있습니까? 누락된 영역을 채우거나 $x=25$까지 예측하는 것을 어떻게 상상하십니까?</p>
<p><img src="chapter_gaussian-processes/../img/gp-observed-data.svg" alt="관찰된 데이터." /></p>
<p>가우시안 프로세스로 데이터를 피팅하기 위해, 우리는 어떤 유형의 함수가 합리적이라고 믿을 수 있는지에 대한 사전 분포를 지정하는 것으로 시작합니다. 여기에서는 가우시안 프로세스의 몇 가지 샘플 함수를 보여줍니다. 이 사전 분포가 합리적으로 보입니까? 여기서 우리는 데이터셋에 맞는 함수를 찾는 것이 아니라, 입력에 따라 얼마나 빨리 변하는지와 같은 솔루션의 합리적인 고수준 속성을 지정하는 것을 찾고 있습니다. 다음 노트북에서 사전 분포 및 추론에 대한 모든 플롯을 재현하기 위한 코드를 볼 것입니다.</p>
<p><img src="chapter_gaussian-processes/../img/gp-sample-prior-functions.svg" alt="우리 모델로 표현하고 싶은 샘플 사전 함수." /></p>
<p>데이터에 대한 조건을 설정하면, 이 사전 분포를 사용하여 데이터에 적합할 수 있는 함수에 대한 사후 분포를 추론할 수 있습니다. 여기서는 샘플 사후 함수를 보여줍니다.</p>
<p><img src="chapter_gaussian-processes/../img/gp-sample-posterior-functions.svg" alt="데이터를 관찰한 후의 샘플 사후 함수." /></p>
<p>우리는 이러한 각 함수가 데이터와 완전히 일치하며 각 관찰을 완벽하게 통과함을 알 수 있습니다. 이러한 사후 샘플을 사용하여 예측을 하려면, 사후 분포에서 가능한 모든 샘플 함수의 값을 평균하여 아래의 굵은 파란색 곡선을 만들 수 있습니다. 이 기댓값을 계산하기 위해 실제로 무한한 수의 샘플을 취할 필요는 없습니다. 나중에 보게 되겠지만, 닫힌 형식으로 기댓값을 계산할 수 있습니다.</p>
<p><img src="chapter_gaussian-processes/../img/gp-posterior-samples.svg" alt="사후 샘플과 점 예측에 사용할 수 있는 사후 평균(파란색)." /></p>
<p>우리는 예측에 대해 얼마나 확신해야 하는지 알기 위해 불확실성의 표현을 원할 수도 있습니다. 직관적으로, 샘플 사후 함수의 변동성이 더 큰 곳에서 더 많은 불확실성을 가져야 합니다. 이는 실제 함수가 취할 수 있는 가능한 값이 훨씬 더 많다는 것을 알려주기 때문입니다. 이러한 유형의 불확실성을 *인식적 불확실성(epistemic uncertainty)*이라고 하며, 이는 정보 부족과 관련된 <em>줄일 수 있는 불확실성</em>입니다. 더 많은 데이터를 획득함에 따라 관찰한 것과 일치하는 솔루션이 점점 줄어들 것이므로 이러한 유형의 불확실성은 사라집니다. 사후 평균과 마찬가지로 사후 분산(사후 분포에서 이러한 함수의 변동성)을 닫힌 형식으로 계산할 수 있습니다. 음영으로 평균의 양쪽에 사후 표준 편차의 두 배를 표시하여 모든 입력 $x$에 대해 함수의 실제 값을 포함할 확률이 95%인 *신용 구간(credible interval)*을 만듭니다.</p>
<p><img src="chapter_gaussian-processes/../img/gp-posterior-samples-95.svg" alt="95% 신용 집합을 포함한 사후 샘플." /></p>
<p>사후 샘플을 제거하고 데이터, 사후 평균, 95% 신용 집합만 시각화하면 플롯이 좀 더 깔끔해 보입니다. 불확실성이 데이터에서 멀어질수록 어떻게 커지는지 주목하십시오. 이는 인식적 불확실성의 속성입니다.</p>
<p><img src="chapter_gaussian-processes/../img/gp-point-predictions.svg" alt="점 예측 및 신용 집합." /></p>
<p>데이터를 피팅하는 데 사용한 가우시안 프로세스의 속성은 *커널(kernel)*이라고도 하는 <em>공분산 함수</em>에 의해 강력하게 제어됩니다. 우리가 사용한 공분산 함수는 <em>RBF(Radial Basis Function) 커널</em>이라고 하며 다음과 같은 형태를 갖습니다.
$$ k_{\textrm{RBF}}(x,x') = \textrm{Cov}(f(x),f(x')) = a^2 \exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right) $$</p>
<p>이 커널의 <em>하이퍼파라미터</em>는 해석 가능합니다. <em>진폭(amplitude)</em> 파라미터 $a$는 함수가 변하는 수직 스케일을 제어하고, <em>길이 척도(length-scale)</em> 파라미터 $\ell$은 함수의 변화율(구불구불함)을 제어합니다. $a$가 클수록 함수 값이 커지고, $\ell$이 클수록 더 천천히 변하는 함수를 의미합니다. $a$와 $\ell$을 변경할 때 샘플 사전 및 사후 함수에 어떤 일이 발생하는지 봅시다.</p>
<p><em>길이 척도</em>는 GP의 예측과 불확실성에 특히 두드러진 영향을 미칩니다.
$||x-x'|| = \ell$에서, 함수 값 쌍 간의 공분산은 $a^2\exp(-0.5)$입니다.
$\ell$보다 먼 거리에서는 함수 값의 상관관계가 거의 없어집니다. 이는 $x_*$ 지점에서 예측을 하려는 경우 $||x-x'||&gt;\ell$인 입력 $x$를 가진 함수 값은 예측에 큰 영향을 미치지 않음을 의미합니다.</p>
<p>길이 척도를 변경하면 샘플 사전 및 사후 함수와 신용 집합에 어떤 영향을 미치는지 봅시다. 위의 피팅은 길이 척도 $2$를 사용합니다. 이제 $\ell = 0.1, 0.5, 2, 5, 10$을 고려해 봅시다. 길이 척도 $0.1$은 우리가 고려하는 입력 도메인 범위인 $25$에 비해 매우 작습니다. 예를 들어 $x=5$와 $x=10$에서의 함수 값은 그러한 길이 척도에서 본질적으로 상관관계가 없습니다. 반면, 길이 척도 $10$의 경우 이러한 입력에서의 함수 값은 높은 상관관계를 갖습니다. 다음 그림에서 수직 스케일이 변경됨에 유의하십시오.</p>
<p><img src="chapter_gaussian-processes/../img/gp-priorpoint1.svg" alt="priorpoint1" />
<img src="chapter_gaussian-processes/../img/gp-postpoint1.svg" alt="postpoint1" /></p>
<p><img src="chapter_gaussian-processes/../img/gp-priorpoint5.svg" alt="priorpoint5" />
<img src="chapter_gaussian-processes/../img/gp-postpoint5.svg" alt="postpoint5" /></p>
<p><img src="chapter_gaussian-processes/../img/gp-prior2.svg" alt="prior2" />
<img src="chapter_gaussian-processes/../img/gp-post2.svg" alt="post2" /></p>
<p><img src="chapter_gaussian-processes/../img/gp-prior5.svg" alt="prior5" />
<img src="chapter_gaussian-processes/../img/gp-post5.svg" alt="post5" /></p>
<p>길이 척도가 증가함에 따라 함수의 '구불구불함'이 감소하고 불확실성이 감소하는 것을 알 수 있습니다. 길이 척도가 작으면 데이터 포인트가 함수 값에 대해 덜 유익해지기 때문에 데이터에서 멀어질수록 불확실성이 빠르게 증가합니다.</p>
<p>이제 진폭 파라미터를 변경하고 길이 척도를 $2$로 고정해 봅시다. 수직 스케일은 사전 샘플에 대해 고정되어 있고 사후 샘플에 대해 변하므로 함수의 증가하는 스케일과 데이터에 대한 피팅을 명확하게 볼 수 있습니다.</p>
<p><img src="chapter_gaussian-processes/../img/gp-priorap1.svg" alt="priorap1" />
<img src="chapter_gaussian-processes/../img/gp-postapoint1.svg" alt="postapoint1" /></p>
<p><img src="chapter_gaussian-processes/../img/gp-priora2.svg" alt="priora2" />
<img src="chapter_gaussian-processes/../img/gp-posta2.svg" alt="posta2" /></p>
<p><img src="chapter_gaussian-processes/../img/gp-priora8.svg" alt="priora8" />
<img src="chapter_gaussian-processes/../img/gp-posta8.svg" alt="posta8" /></p>
<p>진폭 파라미터는 함수의 스케일에 영향을 미치지만 변화율에는 영향을 미치지 않음을 알 수 있습니다. 이 시점에서 우리는 또한 절차의 일반화 성능이 이러한 하이퍼파라미터에 대한 합리적인 값을 갖는 데 달려 있다는 느낌을 받습니다. $\ell=2$ 및 $a=1$의 값은 합리적인 피팅을 제공하는 것처럼 보였지만 다른 값 중 일부는 그렇지 않았습니다. 다행히도 *주변 우도(marginal likelihood)*라고 하는 것을 사용하여 이러한 하이퍼파라미터를 지정하는 강력하고 자동화된 방법이 있으며, 이는 추론에 대한 노트북에서 다시 다룰 것입니다.</p>
<p>그렇다면 GP란 실제로 무엇입니까? 시작하면서 말했듯이, GP는 단순히 입력 $x_1,\dots,x_n$에 의해 인덱싱된 함수 값 모음 $f(x_1),\dots,f(x_n)$이 결합 다변량 가우시안 분포를 갖는다고 말합니다. 이 분포의 평균 벡터 $\mu$는 일반적으로 상수 또는 0으로 간주되는 <em>평균 함수</em>에 의해 주어집니다. 이 분포의 공분산 행렬은 입력 $x$의 모든 쌍에서 평가된 <em>커널</em>에 의해 주어집니다.</p>
<p>$$\begin{bmatrix}f(x) \f(x_1) \ \vdots \ f(x_n) \end{bmatrix}\sim \mathcal{N}\left(\mu, \begin{bmatrix}k(x,x) &amp; k(x, x_1) &amp; \dots &amp; k(x,x_n) \ k(x_1,x) &amp; k(x_1,x_1) &amp; \dots &amp; k(x_1,x_n) \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ k(x_n, x) &amp; k(x_n, x_1) &amp; \dots &amp; k(x_n,x_n) \end{bmatrix}\right)$$
:eqlabel:<code>eq_gp_prior</code></p>
<p>방정식 :eqref:<code>eq_gp_prior</code>는 GP 사전 분포를 지정합니다. 우리가 관찰한 함수 값인 $f(x_1), \dots, f(x_n)$이 주어졌을 때 임의의 $x$에 대한 $f(x)$의 조건부 분포를 계산할 수 있습니다. 이 조건부 분포를 *사후 분포(posterior)*라고 하며 예측에 사용하는 것입니다.</p>
<p>특히,</p>
<p>$$f(x) | f(x_1), \dots, f(x_n) \sim \mathcal{N}(m,s^2)$$</p>
<p>여기서</p>
<p>$$m = k(x,x_{1:n}) k(x_{1:n},x_{1:n})^{-1} f(x_{1:n})$$</p>
<p>$$s^2 = k(x,x) - k(x,x_{1:n})k(x_{1:n},x_{1:n})^{-1}k(x,x_{1:n})$$</p>
<p>여기서 $k(x,x_{1:n})$은 $i=1,\dots,n$에 대해 $k(x,x_{i})$를 평가하여 형성된 $1 \times n$ 벡터이고 $k(x_{1:n},x_{1:n})$은 $i,j = 1,\dots,n$에 대해 $k(x_i,x_j)$를 평가하여 형성된 $n \times n$ 행렬입니다. $m$은 임의의 $x$에 대해 점 예측기로 사용할 수 있는 것이고, $s^2$는 불확실성에 사용하는 것입니다. $f(x)$가 구간에 있을 확률이 95%인 구간을 만들려면 $m \pm 2s$를 사용합니다. 위의 모든 그림에 대한 예측 평균과 불확실성은 이 방정식을 사용하여 생성되었습니다. 관찰된 데이터 포인트는 $f(x_1), \dots, f(x_n)$으로 주어졌고 예측을 위해 세분화된 $x$ 포인트 세트를 선택했습니다.</p>
<p>단일 데이터 포인트 $f(x_1)$을 관찰하고 어떤 $x$에서 $f(x)$의 값을 결정하고 싶다고 가정해 봅시다. $f(x)$가 가우시안 프로세스로 설명되므로 $(f(x), f(x_1))$에 대한 결합 분포가 가우시안임을 알 수 있습니다.</p>
<p>$$
\begin{bmatrix}
f(x) \
f(x_1) \
\end{bmatrix}
\sim
\mathcal{N}\left(\mu,
\begin{bmatrix}
k(x,x) &amp; k(x, x_1) \
k(x_1,x) &amp; k(x_1,x_1)
\end{bmatrix}
\right)
$$</p>
<p>대각선이 아닌 표현식 $k(x,x_1) = k(x_1,x)$는 함수 값이 얼마나 상관관계가 있는지, 즉 $f(x_1)$에서 $f(x)$가 얼마나 강력하게 결정되는지 알려줍니다. $x$와 $x_1$ 사이의 거리 $||x-x_1||$에 비해 큰 길이 척도를 사용하면 함수 값이 높은 상관관계를 갖는다는 것을 이미 보았습니다. 함수 공간과 $f(x_1), f(x)$에 대한 결합 분포 모두에서 $f(x_1)$로부터 $f(x)$를 결정하는 과정을 시각화할 수 있습니다. 먼저 $k(x,x_1) = 0.9$이고 $k(x,x)=1$인 $x$를 고려해 봅시다. 이는 $f(x)$의 값이 $f(x_1)$의 값과 적당히 상관관계가 있음을 의미합니다. 결합 분포에서 등확률 등고선은 비교적 좁은 타원이 될 것입니다.</p>
<p>$f(x_1) = 1.2$를 관찰한다고 가정해 봅시다.
이 $f(x_1)$ 값에 대한 조건을 설정하기 위해, 밀도 플롯에서 $1.2$에 수평선을 그릴 수 있으며, $f(x)$의 값이 대부분 $[0.64,1.52]$로 제한됨을 볼 수 있습니다. 우리는 또한 이 플롯을 함수 공간에 그려서 관찰된 포인트 $f(x_1)$을 주황색으로, 평균 값 $1.08$에 대한 $f(x)$의 가우시안 프로세스 예측 분포의 1 표준 편차를 파란색으로 표시했습니다.</p>
<p><img src="https://user-images.githubusercontent.com/6753639/206867364-b4707db5-0c2e-4ae4-a412-8292bca4d08d.svg" alt="k(x,x_1) = 0.9인 f(x_1)과 f(x)에 대한 이변량 가우시안 밀도의 등확률 등고선." />
<img src="https://user-images.githubusercontent.com/6753639/206867367-3815720c-93c8-4b4b-80e7-296db1d3553b.svg" alt="k(x,x_1) = 0.9인 f(x)에서의 가우시안 프로세스 예측 분포(함수 공간)." /></p>
<p>이제 더 강한 상관관계 $k(x,x_1) = 0.95$가 있다고 가정해 봅시다.
이제 타원이 더 좁아졌고 $f(x)$의 값은 $f(x_1)$에 의해 더 강력하게 결정됩니다. $1.2$에 수평선을 그리면 $f(x)$의 등고선이 대부분 $[0.83, 1.45]$ 내의 값을 지원함을 알 수 있습니다. 다시 함수 공간에 플롯을 표시하며, 평균 예측 값 $1.14$에 대한 1 표준 편차를 보여줍니다.</p>
<p><img src="https://user-images.githubusercontent.com/6753639/206867797-20e42783-31de-4c50-8103-e9441ba6d0a9.svg" alt="k(x,x_1) = 0.95인 f(x_1)과 f(x)에 대한 이변량 가우시안 밀도의 등확률 등고선." />
<img src="https://user-images.githubusercontent.com/6753639/206867800-d9fc7add-649d-492c-8848-cab07c8fb83e.svg" alt="k(x,x_1) = 0.95인 f(x)에서의 가우시안 프로세스 예측 분포(함수 공간)." /></p>
<p>이제 상관관계가 더 강하기 때문에 가우시안 프로세스의 사후 평균 예측자가 $1.2$에 더 가깝다는 것을 알 수 있습니다. 또한 불확실성(오차 막대)이 다소 감소했음을 알 수 있습니다. 이러한 함수 값 간의 강한 상관관계에도 불구하고 단일 데이터 포인트만 관찰했기 때문에 불확실성은 여전히 꽤 큽니다!</p>
<p>이 절차는 우리가 관찰한 포인트 수에 관계없이 모든 $x$에 대해 $f(x)$에 대한 사후 분포를 제공할 수 있습니다. $f(x_1), f(x_2)$를 관찰한다고 가정해 봅시다. 이제 함수 공간에서 특정 $x=x'$에 대한 $f(x)$의 사후 분포를 시각화합니다. $f(x)$에 대한 정확한 분포는 위 방정식에 의해 주어집니다. $f(x)$는 가우시안 분포이며, 평균은</p>
<p>$$m = k(x,x_{1:3}) k(x_{1:3},x_{1:3})^{-1} f(x_{1:3})$$</p>
<p>분산은</p>
<p>$$s^2 = k(x,x) - k(x,x_{1:3})k(x_{1:3},x_{1:3})^{-1}k(x,x_{1:3})$$</p>
<p>이 소개 노트북에서는 <em>노이즈 없는</em> 관찰을 고려했습니다. 보게 되겠지만 관찰 노이즈를 포함하는 것은 쉽습니다. 데이터가 잠재적인 노이즈 없는 함수 $f(x)$에 분산 $\sigma^2$을 갖는 iid 가우시안 노이즈 $\epsilon(x) \sim \mathcal{N}(0,\sigma^2)$를 더한 것으로 생성된다고 가정하면, 공분산 함수는 단순히 $k(x_i,x_j) \to k(x_i,x_j) + \delta_{ij}\sigma^2$가 됩니다. 여기서 $\delta_{ij}$는 $i=j$이면 1이고 그렇지 않으면 0입니다.</p>
<p>우리는 가우시안 프로세스를 사용하여 솔루션에 대한 사전 및 사후 분포를 지정하는 방법과 커널 함수가 이러한 솔루션의 속성에 미치는 영향에 대한 직관을 얻기 시작했습니다. 다음 노트북에서는 가우시안 프로세스 사전 분포를 지정하는 방법을 정확하게 보여주고, 다양한 커널 함수를 소개 및 유도한 다음, 커널 하이퍼파라미터를 자동으로 학습하고 예측을 위해 가우시안 프로세스 사후 분포를 형성하는 메커니즘을 살펴볼 것입니다. "함수에 대한 분포"와 같은 개념에 익숙해지는 데는 시간과 연습이 필요하지만, GP 예측 방정식을 찾는 실제 메커니즘은 실제로 매우 간단하여 이러한 개념에 대한 직관적인 이해를 형성하기 위해 연습하기 쉽습니다.</p>
<h2 id="요약-summary-100"><a class="header" href="#요약-summary-100">요약 (Summary)</a></h2>
<p>일반적인 머신러닝에서는 일부 자유 파라미터(신경망 및 가중치 등)를 사용하여 함수를 지정하고 해석 가능하지 않을 수 있는 이러한 파라미터를 추정하는 데 중점을 둡니다. 가우시안 프로세스를 사용하면 대신 함수에 대한 분포를 직접 추론하여 솔루션의 고수준 속성을 추론할 수 있습니다. 이러한 속성은 종종 몇 가지 고도로 해석 가능한 하이퍼파라미터를 갖는 공분산 함수(커널)에 의해 제어됩니다. 이러한 하이퍼파라미터에는 함수가 얼마나 빠르게(얼마나 구불구불하게) 변하는지 제어하는 <em>길이 척도</em>가 포함됩니다. 또 다른 하이퍼파라미터는 함수가 변하는 수직 스케일을 제어하는 진폭입니다.
데이터에 적합할 수 있는 많은 다른 함수를 나타내고 이를 모두 예측 분포로 결합하는 것은 베이지안 방법의 독특한 특징입니다. 데이터에서 멀리 떨어진 가능한 솔루션 간에 더 많은 변동성이 있기 때문에 불확실성은 직관적으로 데이터에서 멀어질수록 커집니다.</p>
<p>가우시안 프로세스는 가능한 모든 함수 값에 대해 다변량 정규(가우시안) 분포를 지정하여 함수에 대한 분포를 나타냅니다. 가우시안 분포를 쉽게 조작하여 다른 값들의 집합 값을 기반으로 한 함수 값의 분포를 찾을 수 있습니다. 즉, 포인트 세트를 관찰하면 이러한 포인트에 대한 조건을 설정하고 다른 입력에서 함수 값이 어떤 모습일지에 대한 분포를 추론할 수 있습니다. 이러한 포인트 간의 상관관계를 모델링하는 방법은 공분산 함수에 의해 결정되며 가우시안 프로세스의 일반화 속성을 정의하는 것입니다. 가우시안 프로세스에 익숙해지는 데는 시간이 걸리지만 작업하기 쉽고 많은 응용 분야가 있으며 신경망과 같은 다른 모델 클래스를 이해하고 개발하는 데 도움이 됩니다.</p>
<h2 id="연습-문제-exercises-115"><a class="header" href="#연습-문제-exercises-115">연습 문제 (Exercises)</a></h2>
<ol>
<li>인식적 불확실성과 관찰 불확실성의 차이점은 무엇입니까?</li>
<li>변화율과 진폭 외에 함수의 어떤 다른 속성을 고려하고 싶을 수 있으며, 그러한 속성을 가진 함수의 실제 예는 무엇입니까?</li>
<li>우리가 고려한 RBF 공분산 함수는 관찰 간의 공분산(및 상관관계)이 입력 공간(시간, 공간 위치 등)에서의 거리에 따라 감소한다고 말합니다. 이것이 합리적인 가정입니까? 그 이유는 무엇입니까?</li>
<li>두 가우시안 변수의 합은 가우시안입니까? 두 가우시안 변수의 곱은 가우시안입니까? (a,b)가 결합 가우시안 분포를 갖는 경우, a|b (b가 주어졌을 때 a)는 가우시안입니까? 가우시안입니까?</li>
<li>$f(x_1) = 1.2$에서 데이터 포인트를 관찰하는 연습을 반복하되, 이제 추가로 $f(x_2) = 1.4$를 관찰한다고 가정해 봅시다. $k(x,x_1) = 0.9$ 및 $k(x,x_2) = 0.8$이라고 합시다. $f(x_1)$만 관찰했을 때보다 $f(x)$의 값에 대해 더 확신하게 됩니까, 덜 확신하게 됩니까? 지금 $f(x)$ 값에 대한 평균과 95% 신용 집합은 무엇입니까?</li>
<li>관찰 노이즈 추정치를 늘리면 실제 함수의 길이 척도 추정치가 증가할 것이라고 생각하십니까, 아니면 감소할 것이라고 생각하십니까?</li>
<li>데이터에서 멀어짐에 따라 예측 분포의 불확실성이 어느 지점까지 증가했다가 증가를 멈춘다고 가정해 봅시다. 왜 그런 일이 발생할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12115">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="가우시안-프로세스-사전-분포-gaussian-process-priors"><a class="header" href="#가우시안-프로세스-사전-분포-gaussian-process-priors">가우시안 프로세스 사전 분포 (Gaussian Process Priors)</a></h1>
<p>가우시안 프로세스(GP)를 이해하는 것은 모델 구성 및 일반화에 대해 추론하고, 능동 학습 및 딥러닝의 하이퍼파라미터 튜닝을 포함한 다양한 응용 분야에서 최첨단 성능을 달성하는 데 중요합니다. GP는 어디에나 있으며, 그것이 무엇이며 어떻게 사용할 수 있는지 아는 것은 우리에게 이익이 됩니다.</p>
<p>이 섹션에서는 함수에 대한 가우시안 프로세스 *사전 분포(priors)*를 소개합니다. 다음 노트북에서는 이러한 사전 분포를 사용하여 *사후 추론(posterior inference)*을 수행하고 예측하는 방법을 보여줍니다. 다음 섹션은 "요약된 GP"로 볼 수 있으며, 가우시안 프로세스를 실제로 적용하는 데 필요한 내용을 빠르게 제공합니다.</p>
<pre><code class="language-{.python .input}">from d2l import torch as d2l
import numpy as np
from scipy.spatial import distance_matrix

d2l.set_figsize()
</code></pre>
<h2 id="정의-definition"><a class="header" href="#정의-definition">정의 (Definition)</a></h2>
<p>가우시안 프로세스는 <em>임의의 유한한 수가 결합 가우시안 분포를 갖는 확률 변수의 모음</em>으로 정의됩니다. 함수 $f(x)$가 <em>평균 함수</em> $m(x)$와 <em>공분산 함수</em> 또는 <em>커널</em> $k(x,x')$를 갖는 가우시안 프로세스, $f(x) \sim \mathcal{GP}(m, k)$라면, 임의의 입력 포인트 모음 $x$(시간, 공간 위치, 이미지 픽셀 등)에서 조회된 함수 값 모음은 평균 벡터 $\mu$와 공분산 행렬 $K$를 갖는 결합 다변량 가우시안 분포를 갖습니다: $f(x_1),\dots,f(x_n) \sim \mathcal{N}(\mu, K)$, 여기서 $\mu_i = E[f(x_i)] = m(x_i)$이고 $K_{ij} = \textrm{Cov}(f(x_i),f(x_j)) = k(x_i,x_j)$입니다.</p>
<p>이 정의는 추상적이고 접근하기 어려운 것처럼 보일 수 있지만, 가우시안 프로세스는 사실 매우 단순한 객체입니다. 어떤 함수든</p>
<p>$$f(x) = w^{\top} \phi(x) = \langle w, \phi(x) \rangle,$$:eqlabel:<code>eq_gp-function</code></p>
<p>$w$가 가우시안(정규) 분포에서 추출되고, $\phi$가 기저 함수 벡터(예: $\phi(x) = (1, x, x^2, ..., x^d)^{\top}$)라면,
가우시안 프로세스입니다. 게다가, 어떤 가우시안 프로세스 f(x)라도 방정식 :eqref:<code>eq_gp-function</code>의 형태로 표현될 수 있습니다. 가우시안 프로세스에 익숙해지기 위해 몇 가지 구체적인 예를 고려해 봅시다. 그 후에는 그것이 얼마나 간단하고 유용한지 알게 될 것입니다.</p>
<h2 id="간단한-가우시안-프로세스-a-simple-gaussian-process"><a class="header" href="#간단한-가우시안-프로세스-a-simple-gaussian-process">간단한 가우시안 프로세스 (A Simple Gaussian Process)</a></h2>
<p>$f(x) = w_0 + w_1 x$이고 $w_0, w_1 \sim \mathcal{N}(0,1)$이며 $w_0, w_1, x$가 모두 1차원이라고 가정해 봅시다. 이 함수를 내적 $f(x) = (w_0, w_1)(1, x)^{\top}$으로 동등하게 쓸 수 있습니다. 위의 :eqref:<code>eq_gp-function</code>에서 $w = (w_0, w_1)^{\top}$이고 $\phi(x) = (1,x)^{\top}$입니다.</p>
<p>임의의 $x$에 대해, $f(x)$는 두 가우시안 확률 변수의 합입니다. 가우시안은 덧셈에 대해 닫혀 있으므로 $f(x)$도 임의의 $x$에 대해 가우시안 확률 변수입니다. 사실, 우리는 특정 $x$에 대해 $f(x)$가 $\mathcal{N}(0,1+x^2)$임을 계산할 수 있습니다. 마찬가지로, 임의의 입력 모음 $x_1,\dots,x_n$에 대한 임의의 함수 값 모음 $(f(x_1),\dots,f(x_n))$의 결합 분포는 다변량 가우시안 분포입니다. 따라서 $f(x)$는 가우시안 프로세스입니다.</p>
<p>요컨대, $f(x)$는 <em>무작위 함수(random function)</em> 또는 *함수에 대한 분포(distribution over functions)*입니다. 우리는 $w_0, w_1$에 대한 값을 반복적으로 샘플링하고 해당하는 함수 $f(x)$(기울기와 절편이 다른 직선임)를 시각화하여 이 분포에 대한 통찰력을 얻을 수 있습니다. 다음과 같습니다:</p>
<pre><code class="language-{.python .input}">def lin_func(x, n_sample):
    preds = np.zeros((n_sample, x.shape[0]))
    for ii in range(n_sample):
        w = np.random.normal(0, 1, 2)
        y = w[0] + w[1] * x
        preds[ii, :] = y
    return preds

x_points = np.linspace(-5, 5, 50)
outs = lin_func(x_points, 10)
lw_bd = -2 * np.sqrt((1 + x_points ** 2))
up_bd = 2 * np.sqrt((1 + x_points ** 2))

d2l.plt.fill_between(x_points, lw_bd, up_bd, alpha=0.25)
d2l.plt.plot(x_points, np.zeros(len(x_points)), linewidth=4, color='black')
d2l.plt.plot(x_points, outs.T)
d2l.plt.xlabel("x", fontsize=20)
d2l.plt.ylabel("f(x)", fontsize=20)
d2l.plt.show()
</code></pre>
<p>만약 $w_0$와 $w_1$이 대신 $\mathcal{N}(0,\alpha^2)$에서 추출된다면, $\alpha$를 변경하는 것이 함수에 대한 분포에 어떤 영향을 미칠 것이라고 상상하십니까?</p>
<h2 id="가중치-공간에서-함수-공간으로-from-weight-space-to-function-space"><a class="header" href="#가중치-공간에서-함수-공간으로-from-weight-space-to-function-space">가중치 공간에서 함수 공간으로 (From Weight Space to Function Space)</a></h2>
<p>위의 플롯에서 우리는 모델의 파라미터에 대한 분포가 함수에 대한 분포를 유도하는 방법을 보았습니다. 우리는 종종 모델링하려는 함수에 대한 아이디어(부드러운지, 주기적인지, 빠르게 변하는지 등)를 가지고 있지만, 대체로 해석하기 어려운 파라미터에 대해 추론하는 것은 비교적 지루합니다. 다행히도 가우시안 프로세스는 함수에 대해 <em>직접</em> 추론할 수 있는 쉬운 메커니즘을 제공합니다. 가우시안 분포는 처음 두 모멘트인 평균과 공분산 행렬에 의해 완전히 정의되므로, 가우시안 프로세스는 확장하여 평균 함수와 공분산 함수에 의해 정의됩니다.</p>
<p>위의 예에서 평균 함수는</p>
<p>$$m(x) = E[f(x)] = E[w_0 + w_1x] = E[w_0] + E[w_1]x = 0+0 = 0.$$</p>
<p>마찬가지로 공분산 함수는</p>
<p>$$k(x,x') = \textrm{Cov}(f(x),f(x')) = E[f(x)f(x')]-E[f(x)]E[f(x')] = E[w_0^2 + w_0w_1x' + w_1w_0x + w_1^2xx'] = 1 + xx'.$$</p>
<p>이제 함수에 대한 분포를 직접 지정하고 샘플링할 수 있으며, 파라미터에 대한 분포에서 샘플링할 필요가 없습니다. 예를 들어 $f(x)$에서 추출하기 위해, 쿼리하려는 $x$ 모음과 관련된 다변량 가우시안 분포를 형성하고 그로부터 직접 샘플링할 수 있습니다. 우리는 이 공식이 얼마나 유리한지 보기 시작할 것입니다.</p>
<p>먼저, 위의 단순 직선 모델에 대한 본질적으로 동일한 유도가 $w \sim \mathcal{N}(u,S)$인 $f(x) = w^{\top} \phi(x)$ 형식의 <em>모든</em> 모델에 대한 평균 및 공분산 함수를 찾는 데 적용될 수 있음을 주목합니다. 이 경우 평균 함수 $m(x) = u^{\top}\phi(x)$이고 공분산 함수 $k(x,x') = \phi(x)^{\top}S\phi(x')$입니다. $\phi(x)$는 임의의 비선형 기저 함수의 벡터를 나타낼 수 있으므로, 우리는 심지어 <em>무한한</em> 수의 파라미터를 가진 모델을 포함하여 매우 일반적인 모델 클래스를 고려하고 있습니다.</p>
<h2 id="방사형-기저-함수-rbf-커널-the-radial-basis-function-rbf-kernel"><a class="header" href="#방사형-기저-함수-rbf-커널-the-radial-basis-function-rbf-kernel">방사형 기저 함수 (RBF) 커널 (The Radial Basis Function (RBF) Kernel)</a></h2>
<p><em>방사형 기저 함수(Radial Basis Function, RBF)</em> 커널은 가우시안 프로세스 및 커널 머신 전반에서 가장 인기 있는 공분산 함수입니다.
이 커널은 $k_{\textrm{RBF}}(x,x') = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right)$ 형식을 가지며, 여기서 $a$는 진폭 파라미터이고 $\ell$은 <em>길이 척도</em> 하이퍼파라미터입니다.</p>
<p>가중치 공간에서 시작하여 이 커널을 유도해 봅시다. 다음 함수를 고려하십시오.</p>
<p>$$f(x) = \sum_{i=1}^J w_i \phi_i(x), w_i  \sim \mathcal{N}\left(0,\frac{\sigma^2}{J}\right), \phi_i(x) = \exp\left(-\frac{(x-c_i)^2}{2\ell^2 }\right).$$</p>
<p>$f(x)$는 다음 그림과 같이 점 $c_i$를 중심으로 하는 너비 $\ell$인 방사형 기저 함수의 합입니다.</p>
<p>우리는 $f(x)$가 $w^{\top} \phi(x)$ 형식을 가짐을 알 수 있습니다. 여기서 $w = (w_1,\dots,w_J)^{\top}$이고 $\phi(x)$는 각 방사형 기저 함수를 포함하는 벡터입니다. 이 가우시안 프로세스의 공분산 함수는 다음과 같습니다.</p>
<p>$$k(x,x') = \frac{\sigma^2}{J} \sum_{i=1}^{J} \phi_i(x)\phi_i(x').$$</p>
<p>이제 파라미터(및 기저 함수)의 수를 무한대로 가져갈 때 어떤 일이 발생하는지 고려해 봅시다. $c_J = \log J$, $c_1 = -<code>log J</code>, $c_{i+1}-c_{i} = \Delta c = 2\frac{\log J}{J}$로 두고 $J \to \infty$로 합니다. 공분산 함수는 리만 합이 됩니다.</p>
<p>$$k(x,x') = \lim_{J \to \infty} \frac{\sigma^2}{J} \sum_{i=1}^{J} \phi_i(x)\phi_i(x') = \int_{c_0}^{c_\infty} \phi_c(x)\phi_c(x') dc.$$</p>
<p>$c_0 = -<code>infty</code> 및 $c_\infty = <code>infty</code>로 설정하여 무한히 많은 기저 함수를 전체 실수 라인에 걸쳐 펼치고, 각각 $\Delta c \to 0$만큼 떨어져 있게 합니다.</p>
<p>$$k(x,x') = \int_{-\infty}^{\infty} \exp(-\frac{(x-c)^2}{2\ell^2}) \exp(-\frac{(x'-c)^2}{2\ell^2 }) dc = \sqrt{\pi}\ell \sigma^2 \exp(-\frac{(x-x')^2}{2(\sqrt{2} \ell)^2}) \propto k_{\textrm{RBF}}(x,x').$$</p>
<p>우리가 여기서 무엇을 했는지 잠시 흡수할 가치가 있습니다. 함수 공간 표현으로 이동함으로써, 유한한 양의 계산을 사용하여 <em>무한한</em> 수의 파라미터를 가진 모델을 표현하는 방법을 유도했습니다. RBF 커널이 있는 가우시안 프로세스는 *보편적 근사자(universal approximator)*이며, 어떤 연속 함수든 임의의 정밀도로 표현할 수 있습니다. 위의 유도에서 그 이유를 직관적으로 알 수 있습니다. 각 방사형 기저 함수를 $\ell \to 0$으로 취하여 점 질량으로 축소하고 각 점 질량에 원하는 높이를 줄 수 있습니다.</p>
<p>따라서 RBF 커널이 있는 가우시안 프로세스는 무한한 수의 파라미터를 가진 모델이며 유한한 신경망보다 훨씬 더 많은 유연성을 갖습니다. 아마도 <em>과도하게 파라미터화된(overparametrized)</em> 신경망에 대한 소동은 잘못된 것일 수 있습니다. 보게 되겠지만, RBF 커널이 있는 GP는 과대적합되지 않으며 실제로 작은 데이터셋에서 특히 강력한 일반화 성능을 제공합니다. 게다가, 무작위 레이블로 이미지를 완벽하게 피팅할 수 있지만 구조화된 문제에 대해 여전히 잘 일반화하는 능력과 같은 :cite:<code>zhang2021understanding</code>의 예제는 가우시안 프로세스를 사용하여 완벽하게 재현될 수 있습니다 :cite:<code>wilson2020bayesian</code>. 신경망은 우리가 생각하는 것만큼 뚜렷하지 않습니다.</p>
<p>우리는 함수에 대한 분포에서 직접 샘플링하여 RBF 커널이 있는 가우시안 프로세스 및 <em>길이 척도</em>와 같은 하이퍼파라미터에 대한 추가 직관을 구축할 수 있습니다. 이전과 마찬가지로 이것은 간단한 절차를 포함합니다.</p>
<ol>
<li>GP를 쿼리할 입력 $x$ 포인트를 선택합니다: $x_1,\dots,x_n$.</li>
<li>$i = 1,\dots,n$에 대해 $m(x_i)$를 평가하고 $i,j = 1,\dots,n$에 대해 $k(x_i,x_j)$를 평가하여 각각 평균 벡터 및 공분산 행렬 $\mu$와 $K$를 형성합니다. 여기서 $(f(x_1),\dots,f(x_n)) \sim \mathcal{N}(\mu, K)$입니다.</li>
<li>이 다변량 가우시안 분포에서 샘플링하여 샘플 함수 값을 얻습니다.</li>
<li>해당 포인트에서 쿼리된 더 많은 샘플 함수를 시각화하기 위해 더 많이 샘플링합니다.</li>
</ol>
<p>아래 그림에서 이 과정을 보여줍니다.</p>
<pre><code class="language-{.python .input}">def rbfkernel(x1, x2, ls=4.):  #@save
    dist = distance_matrix(np.expand_dims(x1, 1), np.expand_dims(x2, 1))
    return np.exp(-(1. / ls / 2) * (dist ** 2))

x_points = np.linspace(0, 5, 50)
meanvec = np.zeros(len(x_points))
covmat = rbfkernel(x_points,x_points, 1)

prior_samples= np.random.multivariate_normal(meanvec, covmat, size=5);
d2l.plt.plot(x_points, prior_samples.T, alpha=0.5)
d2l.plt.show()
</code></pre>
<h2 id="신경망-커널-the-neural-network-kernel"><a class="header" href="#신경망-커널-the-neural-network-kernel">신경망 커널 (The Neural Network Kernel)</a></h2>
<p>머신러닝에서 가우시안 프로세스에 대한 연구는 신경망 연구에 의해 촉발되었습니다. Radford Neal은 점점 더 큰 베이지안 신경망을 추구했으며, 궁극적으로 1994년(가장 악명 높은 NeurIPS 거절 중 하나였기 때문에 1996년에 발표됨)에 무한한 수의 은닉 유닛을 가진 그러한 네트워크가 특정 커널 함수를 가진 가우시안 프로세스가 됨을 보여주었습니다 :cite:<code>neal1996bayesian</code>. 신경망의 일반화 속성을 조사하는 데 사용되는 신경 접선 커널(neural tangent kernel)과 같은 아이디어와 함께 이 유도에 대한 관심이 다시 떠올랐습니다 :cite:<code>matthews2018gaussian</code> :cite:<code>novak2018bayesian</code>. 우리는 다음과 같이 신경망 커널을 유도할 수 있습니다.</p>
<p>하나의 은닉층이 있는 신경망 함수 $f(x)$를 고려하십시오.</p>
<p>$$f(x) = b + \sum_{i=1}^{J} v_i h(x; u_i).$$</p>
<p>$b$는 편향, $v_i$는 은닉에서 출력으로의 가중치, $h$는 제한된 은닉 유닛 전달 함수, $u_i$는 입력에서 은닉으로의 가중치, $J$는 은닉 유닛 수입니다. $b$와 $v_i$가 각각 0 평균과 분산 $\sigma_b^2$ 및 $\sigma_v^2/J$를 갖는 독립적이라고 하고, $u_i$가 독립적이고 동일한 분포를 갖는다고 합시다. 그런 다음 중심 극한 정리를 사용하여 함수 값 모음 $f(x_1),\dots,f(x_n)$이 결합 다변량 가우시안 분포를 가짐을 보일 수 있습니다.</p>
<p>해당 가우시안 프로세스의 평균 및 공분산 함수는 다음과 같습니다.</p>
<p>$$m(x) = E[f(x)] = 0$$</p>
<p>$$k(x,x') = \textrm{cov}[f(x),f(x')] = E[f(x)f(x')] = \sigma_b^2 + \frac{1}{J} \sum_{i=1}^{J} \sigma_v^2 E[h_i(x; u_i)h_i(x'; u_i)]$$</p>
<p>어떤 경우에는 본질적으로 이 공분산 함수를 닫힌 형식으로 평가할 수 있습니다. $h(x; u) = \textrm{erf}(u_0 + \sum_{j=1}^{P} u_j x_j)$라고 합시다. 여기서 $\textrm{erf}(z) = \frac{2}{\sqrt{\pi}} \int_{0}^{z} e^{-t^2} dt$이고 $u \sim \mathcal{N}(0,\Sigma)$입니다. 그러면 $k(x,x') = \frac{2}{\pi} \textrm{sin}(\frac{2 \tilde{x}^{\top} \Sigma \tilde{x}'}{\sqrt{(1 + 2 \tilde{x}^{\top} \Sigma \tilde{x})(1 + 2 \tilde{x}'^{\top} \Sigma \tilde{x}')}})$입니다.</p>
<p>RBF 커널은 *정상적(stationary)*입니다. 즉, *평행 이동 불변(translation invariant)*이며 따라서 $\tau = x-x'$의 함수로 쓸 수 있습니다. 직관적으로 정상성은 변화율과 같은 함수의 고수준 속성이 입력 공간에서 이동할 때 변하지 않음을 의미합니다. 그러나 신경망 커널은 *비정상적(non-stationary)*입니다. 아래에서 우리는 이 커널을 가진 가우시안 프로세스의 샘플 함수를 보여줍니다. 우리는 함수가 원점 근처에서 질적으로 다르게 보임을 알 수 있습니다.</p>
<h2 id="요약-summary-101"><a class="header" href="#요약-summary-101">요약 (Summary)</a></h2>
<p>베이지안 추론을 수행하는 첫 번째 단계는 사전 분포를 지정하는 것입니다. 가우시안 프로세스를 사용하여 함수에 대한 전체 사전 분포를 지정할 수 있습니다. 모델링에 대한 전통적인 "가중치 공간" 관점에서 시작하여 모델의 함수적 형태에서 시작하고 파라미터에 대한 분포를 도입하여 함수에 대한 사전 분포를 유도할 수 있습니다. 대안으로 커널에 의해 제어되는 속성을 사용하여 함수 공간에서 직접 사전 분포를 지정할 수 있습니다. 함수 공간 접근 방식에는 많은 장점이 있습니다. 우리는 실제로 무한한 수의 파라미터에 해당하지만 유한한 양의 계산을 사용하는 모델을 구축할 수 있습니다! 또한 이러한 모델은 유연성이 크지만 어떤 유형의 함수가 선험적으로 가능성이 높은지에 대해 강력한 가정을 하므로 작은 데이터셋에서 비교적 좋은 일반화로 이어집니다.</p>
<p>함수 공간에서 모델의 가정은 직관적으로 평활도 및 주기성과 같은 함수의 고수준 속성을 인코딩하는 커널에 의해 제어됩니다. 많은 커널은 정상적이며, 이는 평행 이동 불변임을 의미합니다. 정상 커널을 가진 가우시안 프로세스에서 추출한 함수는 입력 공간의 어디를 보든 대략 동일한 고수준 속성(예: 변화율)을 갖습니다.</p>
<p>가우시안 프로세스는 파라미터에 대한 가우시안 사전 분포가 있는 한 다항식, 푸리에 급수 등을 포함하여 이미 익숙한 모델의 많은 예를 포함하는 비교적 일반적인 모델 클래스입니다. 또한 파라미터에 대한 가우시안 분포가 없더라도 무한한 수의 파라미터를 가진 신경망을 포함합니다. Radford Neal이 발견한 이 연결은 머신러닝 연구자들이 신경망에서 벗어나 가우시안 프로세스로 이동하도록 촉발했습니다.</p>
<h2 id="연습-문제-exercises-116"><a class="header" href="#연습-문제-exercises-116">연습 문제 (Exercises)</a></h2>
<ol>
<li>
<p>Ornstein-Uhlenbeck(OU) 커널 $k_{\textrm{OU}}(x,x') = \exp\left(-\frac{1}{2\ell}||x - x'|\right)$을 사용하여 GP에서 샘플 사전 함수를 그립니다. 길이 척도 $\ell$을 동일하게 고정하면 이러한 함수가 RBF 커널을 가진 GP의 샘플 함수와 어떻게 다르게 보입니까?</p>
</li>
<li>
<p>RBF 커널의 <em>진폭</em> $a^2$를 변경하면 함수에 대한 분포에 어떤 영향을 줍니까?</p>
</li>
<li>
<p>$u(x) = f(x) + 2g(x)$를 형성한다고 가정해 봅시다. 여기서 $f(x) \sim \mathcal{GP}(m_1,k_1)$이고 $g(x) \sim \mathcal{GP}(m_2,k_2)$입니다. $u(x)$는 가우시안 프로세스입니까? 그렇다면 평균과 공분산 함수는 무엇입니까?</p>
</li>
<li>
<p>$g(x) = a(x)f(x)$를 형성한다고 가정해 봅시다. 여기서 $f(x) \sim \mathcal{GP}(0,k)$이고 $a(x) = x^2$입니다. $g(x)$는 가우시안 프로세스입니까? 그렇다면 평균과 공분산 함수는 무엇입니까? $a(x)$의 효과는 무엇입니까? $g(x)$에서 추출한 샘플 함수는 어떻게 생겼습니까?</p>
</li>
<li>
<p>$u(x) = f(x)g(x)$를 형성한다고 가정해 봅시다. 여기서 $f(x) \sim \mathcal{GP}(m_1,k_1)$이고 $g(x) \sim \mathcal{GP}(m_2,k_2)$입니다. $u(x)$는 가우시안 프로세스입니까? 그렇다면 평균과 공분산 함수는 무엇입니까?</p>
</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12116">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(["pytorch"])
#required_libs("gpytorch")
</code></pre>
<h1 id="가우시안-프로세스-추론-gaussian-process-inference"><a class="header" href="#가우시안-프로세스-추론-gaussian-process-inference">가우시안 프로세스 추론 (Gaussian Process Inference)</a></h1>
<p>이 섹션에서는 이전 섹션에서 소개한 GP 사전 분포를 사용하여 사후 추론을 수행하고 예측하는 방법을 보여줄 것입니다. 우리는 *닫힌 형식(closed form)*으로 추론을 수행할 수 있는 회귀로 시작할 것입니다. 이것은 가우시안 프로세스를 실제로 빠르게 시작하고 실행하기 위한 "요약된 GP" 섹션입니다. 처음부터 모든 기본 연산을 코딩한 다음, 최신 가우시안 프로세스 작업과 심층 신경망과의 통합을 훨씬 더 편리하게 만들어 줄 <a href="https://gpytorch.ai/">GPyTorch</a>를 소개할 것입니다. 우리는 다음 섹션에서 이러한 고급 주제를 깊이 있게 고려할 것입니다. 해당 섹션에서는 분류, 포인트 프로세스 또는 비가우시안 우도와 같이 근사 추론이 필요한 설정도 고려할 것입니다.</p>
<h2 id="회귀에-대한-사후-추론-posterior-inference-for-regression"><a class="header" href="#회귀에-대한-사후-추론-posterior-inference-for-regression">회귀에 대한 사후 추론 (Posterior Inference for Regression)</a></h2>
<p><em>관찰(observation)</em> 모델은 우리가 학습하려는 함수 $f(x)$를 관찰 $y(x)$와 관련시키며, 둘 다 일부 입력 $x$에 의해 인덱싱됩니다. 분류에서 $x$는 이미지의 픽셀이 될 수 있고 $y$는 관련 클래스 레이블이 될 수 있습니다. 회귀에서 $y$는 일반적으로 지표면 온도, 해수면, $CO_2$ 농도 등과 같은 연속적인 출력을 나타냅니다.</p>
<p>회귀에서는 종종 출력이 잠재적인 노이즈 없는 함수 $f(x)$에 i.i.d. 가우시안 노이즈 $\epsilon(x)$를 더한 것으로 가정합니다.</p>
<p>$$y(x) = f(x) + \epsilon(x),$$
:eqlabel:<code>eq_gp-regression</code></p>
<p>여기서 $\epsilon(x) \sim \mathcal{N}(0,\sigma^2)$입니다. $\mathbf{y} = y(X) = (y(x_1),\dots,y(x_n))^{\top}$를 훈련 관찰 벡터라고 하고, $\textbf{f} = (f(x_1),\dots,f(x_n))^{\top}$를 훈련 입력 $X = {x_1, \dots, x_n}$에서 쿼리된 잠재 노이즈 없는 함수 값의 벡터라고 합시다.</p>
<p>우리는 $f(x) \sim \mathcal{GP}(m,k)$라고 가정할 것입니다. 이는 함수 값 $\textbf{f}$의 모음이 평균 벡터 $\mu_i = m(x_i)$와 공분산 행렬 $K_{ij} = k(x_i,x_j)$를 갖는 결합 다변량 가우시안 분포를 갖는다는 것을 의미합니다. RBF 커널 $k(x_i,x_j) = a^2 \exp\left(-\frac{1}{2\ell^2}||x_i-x_j||^2\right)$는 공분산 함수의 표준 선택이 될 것입니다. 표기법의 단순성을 위해 평균 함수 $m(x)=0$이라고 가정하겠습니다. 우리의 유도는 나중에 쉽게 일반화될 수 있습니다.</p>
<p>입력 세트 $$X_* = x_{<em>1},x_{<em>2},\dots,x_{<em>m}$$에서 예측을 하고 싶다고 가정해 봅시다. 그런 다음 $p(\mathbf{f}_</em> | \mathbf{y}, X)$를 찾고 싶습니다. 회귀 설정에서는 $\mathbf{f}_</em> = f(X_</em>)$와 $\mathbf{y}$에 대한 결합 분포를 찾은 후 가우시안 항등식을 사용하여 이 분포를 편리하게 찾을 수 있습니다.</p>
<p>훈련 입력 $X$에서 방정식 :eqref:<code>eq_gp-regression</code>을 평가하면 $\mathbf{y} = \mathbf{f} + \mathbf{\epsilon}$이 됩니다. 가우시안 프로세스의 정의(지난 섹션 참조)에 의해 $\mathbf{f} \sim \mathcal{N}(0,K(X,X))$이며, 여기서 $K(X,X)$는 가능한 모든 입력 쌍 $x_i, x_j \in X$에서 공분산 함수(일명 <em>커널</em>)를 평가하여 형성된 $n \times n$ 행렬입니다. $\mathbf{\epsilon}$은 단순히 $\mathcal{N}(0,\sigma^2)$의 iid 샘플로 구성된 벡터이므로 분포 $\mathcal{N}(0,\sigma^2I)$를 갖습니다. 따라서 $\mathbf{y}$는 두 개의 독립적인 다변량 가우시안 변수의 합이므로 분포 $\mathcal{N}(0, K(X,X) + \sigma^2I)$를 갖습니다. 또한 $\textrm{cov}(\mathbf{f}<em>*, \mathbf{y}) = \textrm{cov}(\mathbf{y},\mathbf{f}</em><em>)^{\top} = K(X_</em>,X)$임을 보일 수 있습니다. 여기서 $K(X_*,X)$는 테스트 및 훈련 입력의 모든 쌍에서 커널을 평가하여 형성된 $m \times n$ 행렬입니다.</p>
<p>$$
\begin{bmatrix}
\mathbf{y} \
\mathbf{f}_*
\end{bmatrix}</p>
<p>\sim</p>
<p>\mathcal{N}</p>
<p>\left(0,</p>
<p>\mathbf{A} = \begin{bmatrix}
K(X,X)+\sigma^2I &amp; K(X,X_<em>)
\K(X_</em>,X) &amp; K(X_<em>,X_</em>)
\end{bmatrix}</p>
<p>\right)
$$</p>
<p>그런 다음 표준 가우시안 항등식을 사용하여 결합 분포에서 조건부 분포를 찾을 수 있습니다(예: Bishop Chapter 2 참조).
$\mathbf{f}<em>* | \mathbf{y}, X, X</em>* \sim \mathcal{N}(m_<em>,S_</em>)$, 여기서 $m_* = K(X_<em>,X)[K(X,X)+\sigma^2I]^{-1}\textbf{y}$이고 $S = K(X_</em>,X_<em>) - K(X_</em>,X)[K(X,X)+\sigma^2I]^{-1}K(X,X_*)$입니다.</p>
<p>일반적으로 우리는 전체 예측 공분산 행렬 $S$를 사용할 필요가 없으며, 대신 각 예측에 대한 불확실성으로 $S$의 대각선을 사용합니다. 종종 이러한 이유로 테스트 포인트 모음이 아닌 단일 테스트 포인트 $x_*$에 대한 예측 분포를 씁니다.</p>
<p>커널 행렬에는 위의 RBF 커널의 진폭 $a$와 길이 척도 $\ell$과 같이 추정하고자 하는 파라미터 $\theta$가 있습니다. 이러한 목적으로 우리는 <em>주변 우도(marginal likelihood)</em> $p(\textbf{y} | \theta, X)$를 사용합니다. 이는 $\textbf{y},\textbf{f}_*$에 대한 결합 분포를 찾기 위해 주변 분포를 계산할 때 이미 유도했습니다. 보게 되겠지만, 주변 우도는 모델 적합성 및 모델 복잡성 항으로 구분되며 하이퍼파라미터 학습을 위한 오컴의 면도날 개념을 자동으로 인코딩합니다. 자세한 논의는 MacKay Ch. 28 :cite:<code>mackay2003information</code> 및 Rasmussen and Williams Ch. 5 :cite:<code>rasmussen2006gaussian</code>를 참조하십시오.</p>
<pre><code class="language-{.python .input}">from d2l import torch as d2l
import numpy as np
from scipy.spatial import distance_matrix
from scipy import optimize
import matplotlib.pyplot as plt
import math
import torch
import gpytorch
import os

d2l.set_figsize()
</code></pre>
<h2 id="gp-회귀에서-예측-및-커널-하이퍼파라미터-학습을-위한-방정식-equations-for-making-predictions-and-learning-kernel-hyperparameters-in-gp-regression"><a class="header" href="#gp-회귀에서-예측-및-커널-하이퍼파라미터-학습을-위한-방정식-equations-for-making-predictions-and-learning-kernel-hyperparameters-in-gp-regression">GP 회귀에서 예측 및 커널 하이퍼파라미터 학습을 위한 방정식 (Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression)</a></h2>
<p>여기서는 가우시안 프로세스 회귀에서 하이퍼파라미터를 학습하고 예측하는 데 사용할 방정식을 나열합니다. 다시 말하지만, 입력 $X = {x_1,\dots,x_n}$으로 인덱싱된 회귀 타겟 벡터 $\textbf{y}$를 가정하고 테스트 입력 $x_<em>$에서 예측을 하려고 합니다. 분산 $\sigma^2$를 갖는 i.i.d. 가법적 0 평균 가우시안 노이즈를 가정합니다. 우리는 잠재 노이즈 없는 함수에 대해 평균 함수 $m$과 커널 함수 $k$를 갖는 가우시안 프로세스 사전 분포 $f(x) \sim \mathcal{GP}(m,k)$를 사용합니다. 커널 자체에는 학습하려는 파라미터 $\theta$가 있습니다. 예를 들어 RBF 커널 $k(x_i,x_j) = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right)$를 사용하는 경우 $\theta = {a^2, \ell^2}$를 학습하려고 합니다. $K(X,X)$를 $n$개의 훈련 입력의 가능한 모든 쌍에 대해 커널을 평가하는 것에 해당하는 $n \times n$ 행렬이라고 합시다. $K(x_</em>,X)$를 $i=1,\dots,n$에 대해 $k(x_*, x_i)$를 평가하여 형성된 $1 \times n$ 벡터라고 합시다. $\mu$를 모든 훈련 포인트 $x$에서 평균 함수 $m(x)$를 평가하여 형성된 평균 벡터라고 합시다.</p>
<p>일반적으로 가우시안 프로세스 작업에서는 두 단계 절차를 따릅니다.</p>
<ol>
<li>이러한 하이퍼파라미터에 대한 주변 우도를 최대화하여 커널 하이퍼파라미터 $\hat{\theta}$를 학습합니다.</li>
<li>예측 평균을 점 예측기로 사용하고 예측 표준 편차의 2배를 사용하여 이러한 학습된 하이퍼파라미터 $\hat{\theta}$에 대한 95% 신용 집합을 형성합니다.</li>
</ol>
<p>로그 주변 우도는 단순히 로그 가우시안 밀도이며 다음과 같은 형식을 갖습니다.
$$\log p(\textbf{y} | \theta, X) = -\frac{1}{2}\textbf{y}^{\top}[K_{\theta}(X,X) + \sigma^2I]^{-1}\textbf{y} - \frac{1}{2}\log|K_{\theta}(X,X)| + c$$</p>
<p>예측 분포는 다음과 같은 형식을 갖습니다.
$$p(y_* | x_<em>, \textbf{y}, \theta) = \mathcal{N}(a_</em>,v_<em>)$$
$$a_</em> = k_{\theta}(x_<em>,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}(\textbf{y}-\mu) + \mu$$
$$v_</em> = k_{\theta}(x_<em>,x_</em>) - K_{\theta}(x_<em>,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}k_{\theta}(X,x_</em>)$$</p>
<h2 id="학습-및-예측을-위한-방정식-해석-interpreting-equations-for-learning-and-predictions"><a class="header" href="#학습-및-예측을-위한-방정식-해석-interpreting-equations-for-learning-and-predictions">학습 및 예측을 위한 방정식 해석 (Interpreting Equations for Learning and Predictions)</a></h2>
<p>가우시안 프로세스에 대한 예측 분포에 대해 주목해야 할 몇 가지 요점이 있습니다.</p>
<ul>
<li>
<p>모델 클래스의 유연성에도 불구하고 GP 회귀에 대해 <em>닫힌 형식</em>으로 <em>정확한</em> 베이지안 추론을 수행할 수 있습니다. 커널 하이퍼파라미터를 학습하는 것 외에는 <em>훈련</em>이 없습니다. 예측을 위해 사용하려는 방정식을 정확히 적을 수 있습니다. 가우시안 프로세스는 이러한 측면에서 비교적 예외적이며, 이는 편리함, 다재다능함 및 지속적인 인기에 크게 기여했습니다.</p>
</li>
<li>
<p>예측 평균 $a_<em>$는 훈련 타겟 $\textbf{y}$의 선형 결합이며, 커널 $k_{\theta}(x_</em>,X)[K_{\theta}(X,X)+\sigma^2I]^{-1}$에 의해 가중치가 부여됩니다. 보게 되겠지만 커널(및 그 하이퍼파라미터)은 따라서 모델의 일반화 속성에서 중요한 역할을 합니다.</p>
</li>
<li>
<p>예측 평균은 타겟 값 $\textbf{y}$에 명시적으로 의존하지만 예측 분산은 그렇지 않습니다. 대신 예측 불확실성은 커널 함수에 의해 제어되는 대로 테스트 입력 $x_*$가 타겟 위치 $X$에서 멀어짐에 따라 증가합니다. 그러나 불확실성은 데이터에서 학습된 커널 하이퍼파라미터 $\theta$를 통해 타겟 $\textbf{y}$의 값에 암시적으로 의존합니다.</p>
</li>
<li>
<p>주변 우도는 모델 적합성 및 모델 복잡성(로그 행렬식) 항으로 구분됩니다. 주변 우도는 데이터와 여전히 일치하는 가장 단순한 피팅을 제공하는 하이퍼파라미터를 선택하는 경향이 있습니다.</p>
</li>
<li>
<p>주요 계산 병목 현상은 선형 시스템을 해결하고 $n$개의 훈련 포인트에 대해 $n \times n$ 대칭 양의 정부호 행렬 $K(X,X)$에 대한 로그 행렬식을 계산하는 데서 발생합니다. 순진하게 이러한 연산은 각각 $\mathcal{O}(n^3)$ 계산과 커널(공분산) 행렬의 각 항목에 대한 $\mathcal{O}(n^2)$ 저장소를 초래하며, 종종 촐레스키 분해로 시작합니다. 역사적으로 이러한 병목 현상은 GP를 약 10,000개 미만의 훈련 포인트가 있는 문제로 제한했으며 거의 10년 동안 부정확했던 "느리다"는 평판을 GP에 주었습니다. 고급 주제에서는 수백만 개의 포인트가 있는 문제로 GP를 확장하는 방법에 대해 논의할 것입니다.</p>
</li>
<li>
<p>널리 사용되는 커널 함수 선택의 경우, $K(X,X)$는 종종 특이(singular)에 가까워 촐레스키 분해 또는 선형 시스템을 해결하기 위한 기타 연산을 수행할 때 수치적 문제를 일으킬 수 있습니다. 다행히도 회귀에서는 종종 $K_{\theta}(X,X)+\sigma^2I$로 작업하므로 노이즈 분산 $\sigma^2$이 $K(X,X)$의 대각선에 추가되어 조건이 크게 개선됩니다. 노이즈 분산이 작거나 노이즈 없는 회귀를 수행하는 경우 조건을 개선하기 위해 대각선에 $10^{-6}$ 정도의 소량의 "지터(jitter)"를 추가하는 것이 일반적입니다.</p>
</li>
</ul>
<h2 id="처음부터-작업한-예제-worked-example-from-scratch"><a class="header" href="#처음부터-작업한-예제-worked-example-from-scratch">처음부터 작업한 예제 (Worked Example from Scratch)</a></h2>
<p>회귀 데이터를 생성한 다음, 처음부터 모든 단계를 구현하여 GP로 데이터를 피팅해 봅시다.
$\epsilon \sim \mathcal{N}(0,\sigma^2)$인 $$y(x) = \sin(x) + \frac{1}{2}\sin(4x) + \epsilon$$에서 데이터를 샘플링할 것입니다. 우리가 찾고자 하는 노이즈 없는 함수는 $f(x) = \sin(x) + \frac{1}{2}\sin(4x)$입니다. 노이즈 표준 편차 $\sigma = 0.25$를 사용하여 시작하겠습니다.</p>
<pre><code class="language-{.python .input}">def data_maker1(x, sig):
    return np.sin(x) + 0.5 * np.sin(4 * x) + np.random.randn(x.shape[0]) * sig

sig = 0.25
train_x, test_x = np.linspace(0, 5, 50), np.linspace(0, 5, 500)
train_y, test_y = data_maker1(train_x, sig=sig), data_maker1(test_x, sig=0.)

d2l.plt.scatter(train_x, train_y)
d2l.plt.plot(test_x, test_y)
d2l.plt.xlabel("x", fontsize=20)
d2l.plt.ylabel("Observations y", fontsize=20)
d2l.plt.show()
</code></pre>
<p>여기서 우리는 원으로 표시된 노이즈가 있는 관찰과 파란색으로 표시된 우리가 찾고자 하는 노이즈 없는 함수를 봅니다.</p>
<p>이제 잠재 노이즈 없는 함수에 대한 GP 사전 분포 $f(x)\sim \mathcal{GP}(m,k)$를 지정해 봅시다. 평균 함수 $m(x) = 0$과 RBF 공분산 함수(커널)를 사용할 것입니다.
$$k(x_i,x_j) = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right).$$</p>
<pre><code class="language-{.python .input}">mean = np.zeros(test_x.shape[0])
cov = d2l.rbfkernel(test_x, test_x, ls=0.2)
</code></pre>
<p>우리는 길이 척도 0.2로 시작했습니다. 데이터를 피팅하기 전에 합리적인 사전 분포를 지정했는지 고려하는 것이 중요합니다. 이 사전 분포의 샘플 함수 몇 가지와 95% 신용 집합(실제 함수가 이 영역 내에 있을 확률이 95%라고 믿습니다)을 시각화해 봅시다.</p>
<pre><code class="language-{.python .input}">prior_samples = np.random.multivariate_normal(mean=mean, cov=cov, size=5)
d2l.plt.plot(test_x, prior_samples.T, color='black', alpha=0.5)
d2l.plt.plot(test_x, mean, linewidth=2.)
d2l.plt.fill_between(test_x, mean - 2 * np.diag(cov), mean + 2 * np.diag(cov), 
                 alpha=0.25)
d2l.plt.show()
</code></pre>
<p>이 샘플들이 합리적으로 보입니까? 함수의 고수준 속성이 우리가 모델링하려는 데이터 유형과 일치합니까?</p>
<p>이제 임의의 테스트 포인트 $x_*$에서 사후 예측 분포의 평균과 분산을 형성해 봅시다.</p>
<p>$$
\bar{f}<em>{*} = K(x, x</em>*)^T (K(x, x) + \sigma^2 I)^{-1}y
$$</p>
<p>$$
V(f_{<em>}) = K(x_</em>, x_<em>) - K(x, x_</em>)^T (K(x, x) + \sigma^2 I)^{-1}K(x, x_*)
$$</p>
<p>예측을 하기 전에 커널 하이퍼파라미터 $\theta$와 노이즈 분산 $\sigma^2$을 학습해야 합니다. 사전 함수가 우리가 피팅하는 데이터에 비해 너무 빠르게 변하는 것처럼 보였으므로 길이 척도를 0.75로 초기화해 봅시다. 또한 노이즈 표준 편차 $\sigma$를 0.75로 추측할 것입니다.</p>
<p>이러한 파라미터를 학습하기 위해, 이 파라미터에 대한 주변 우도를 최대화할 것입니다.</p>
<p>$$
\log p(y | X) = \log \int p(y | f, X)p(f | X)df
$$
$$
\log p(y | X) = -\frac{1}{2}y^T(K(x, x) + \sigma^2 I)^{-1}y - \frac{1}{2}\log |K(x, x) + \sigma^2 I| - \frac{n}{2}\log 2\pi
$$</p>
<p>아마도 우리의 사전 함수가 너무 빠르게 변했을 것입니다. 길이 척도를 0.4로 추측해 봅시다. 또한 노이즈 표준 편차를 0.75로 추측할 것입니다. 이들은 단순히 하이퍼파라미터 초기화입니다. 우리는 주변 우도에서 이러한 파라미터를 학습할 것입니다.</p>
<pre><code class="language-{.python .input}">ell_est = 0.4
post_sig_est = 0.5

def neg_MLL(pars):
    K = d2l.rbfkernel(train_x, train_x, ls=pars[0])
    kernel_term = -0.5 * train_y @ \
        np.linalg.inv(K + pars[1] ** 2 * np.eye(train_x.shape[0])) @ train_y
    logdet = -0.5 * np.log(np.linalg.det(K + pars[1] ** 2 * \
                                         np.eye(train_x.shape[0])))
    const = -train_x.shape[0] / 2. * np.log(2 * np.pi)
    
    return -(kernel_term + logdet + const)


learned_hypers = optimize.minimize(neg_MLL, x0=np.array([ell_est,post_sig_est]), 
                                   bounds=((0.01, 10.), (0.01, 10.)))
ell = learned_hypers.x[0]
post_sig_est = learned_hypers.x[1]
</code></pre>
<p>이 경우 우리는 길이 척도 0.299와 노이즈 표준 편차 0.24를 학습합니다. 학습된 노이즈가 실제 노이즈에 매우 가깝다는 점에 유의하십시오. 이는 우리 GP가 이 문제에 매우 잘 지정되었음을 나타내는 데 도움이 됩니다.</p>
<p>일반적으로 커널을 선택하고 하이퍼파라미터를 초기화하는 데 신중한 생각을 기울이는 것이 중요합니다. 주변 우도 최적화는 초기화에 비교적 견고할 수 있지만 나쁜 초기화에 면역이 되지는 않습니다. 다양한 초기화로 위의 스크립트를 실행해 보고 어떤 결과를 얻는지 확인해 보십시오.</p>
<p>이제 학습된 하이퍼파라미터로 예측을 해봅시다.</p>
<pre><code class="language-{.python .input}">K_x_xstar = d2l.rbfkernel(train_x, test_x, ls=ell)
K_x_x = d2l.rbfkernel(train_x, train_x, ls=ell)
K_xstar_xstar = d2l.rbfkernel(test_x, test_x, ls=ell)

post_mean = K_x_xstar.T @ np.linalg.inv((K_x_x + \
                post_sig_est ** 2 * np.eye(train_x.shape[0]))) @ train_y
post_cov = K_xstar_xstar - K_x_xstar.T @ np.linalg.inv((K_x_x + \
                post_sig_est ** 2 * np.eye(train_x.shape[0]))) @ K_x_xstar

lw_bd = post_mean - 2 * np.sqrt(np.diag(post_cov))
up_bd = post_mean + 2 * np.sqrt(np.diag(post_cov))

d2l.plt.scatter(train_x, train_y)
d2l.plt.plot(test_x, test_y, linewidth=2.)
d2l.plt.plot(test_x, post_mean, linewidth=2.)
d2l.plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25)
d2l.plt.legend(['Observed Data', 'True Function', 'Predictive Mean', '95% Set on True Func'])
d2l.plt.show()
</code></pre>
<p>주황색의 사후 평균이 실제 노이즈 없는 함수와 거의 완벽하게 일치하는 것을 볼 수 있습니다! 우리가 보여주는 95% 신용 집합은 데이터 포인트가 아니라 잠재 <em>노이즈 없는</em>(실제) 함수에 대한 것입니다. 이 신용 집합이 실제 함수를 완전히 포함하고 있으며 지나치게 넓거나 좁아 보이지 않음을 알 수 있습니다. 우리는 그것이 데이터 포인트를 포함하기를 원하지도 기대하지도 않습니다. 관찰에 대한 신용 집합을 갖고 싶다면 다음을 계산해야 합니다.</p>
<pre><code class="language-{.python .input}">lw_bd_observed = post_mean - 2 * np.sqrt(np.diag(post_cov) + post_sig_est ** 2)
up_bd_observed = post_mean + 2 * np.sqrt(np.diag(post_cov) + post_sig_est ** 2)
</code></pre>
<p>두 가지 불확실성 소스가 있습니다. <em>줄일 수 있는</em> 불확실성을 나타내는 <em>인식적</em> 불확실성과 <em>우발적(aleatoric)</em> 또는 <em>줄일 수 없는</em> 불확실성입니다. 여기서 <em>인식적</em> 불확실성은 노이즈 없는 함수의 실제 값에 대한 불확실성을 나타냅니다. 데이터에서 멀어질수록 데이터와 일치하는 다양한 함수 값이 존재하므로 이 불확실성은 데이터 포인트에서 멀어질수록 커져야 합니다. 점점 더 많은 데이터를 관찰함에 따라 실제 함수에 대한 우리의 믿음은 더 확신을 갖게 되고 인식적 불확실성은 사라집니다. 이 경우 <em>우발적</em> 불확실성은 관찰 노이즈입니다. 데이터가 이 노이즈와 함께 우리에게 주어지며 줄일 수 없기 때문입니다.</p>
<p>데이터의 <em>인식적</em> 불확실성은 잠재 노이즈 없는 함수의 분산 np.diag(post_cov)에 의해 포착됩니다. <em>우발적</em> 불확실성은 노이즈 분산 post_sig_est**2에 의해 포착됩니다.</p>
<p>불행히도 사람들은 불확실성을 표현하는 방법에 대해 종종 부주의하여, 많은 논문이 완전히 정의되지 않은 오차 막대를 보여주거나, 인식적 불확실성 또는 우발적 불확실성 또는 둘 다를 시각화하고 있는지에 대한 명확한 감각이 없으며, 노이즈 분산을 노이즈 표준 편차와 혼동하고, 표준 편차를 표준 오차와 혼동하고, 신뢰 구간을 신용 집합과 혼동하는 등의 일이 발생합니다. 불확실성이 무엇을 나타내는지 정확하지 않으면 본질적으로 무의미합니다.</p>
<p>우리의 불확실성이 무엇을 나타내는지에 세심한 주의를 기울이는 정신으로, 노이즈 없는 함수에 대한 분산 추정치의 <em>제곱근</em>의 <em>두 배</em>를 취하고 있다는 점에 유의하는 것이 중요합니다. 예측 분포가 가우시안이므로 이 수량을 통해 실제 함수를 포함할 확률이 95%인 구간에 대한 우리의 믿음을 나타내는 95% 신용 집합을 형성할 수 있습니다. 노이즈 <em>분산</em>은 완전히 다른 스케일에 있으며 훨씬 덜 해석 가능합니다.</p>
<p>마지막으로 20개의 사후 샘플을 살펴봅시다. 이 샘플들은 사후적으로 우리 데이터에 적합할 수 있다고 믿는 함수의 유형을 알려줍니다.</p>
<pre><code class="language-{.python .input}">post_samples = np.random.multivariate_normal(post_mean, post_cov, size=20)
d2l.plt.scatter(train_x, train_y)
d2l.plt.plot(test_x, test_y, linewidth=2.)
d2l.plt.plot(test_x, post_mean, linewidth=2.)
d2l.plt.plot(test_x, post_samples.T, color='gray', alpha=0.25)
d2l.plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25)
plt.legend(['Observed Data', 'True Function', 'Predictive Mean', 'Posterior Samples'])
d2l.plt.show()
</code></pre>
<p>기본 회귀 응용 프로그램에서는 사후 예측 평균과 표준 편차를 각각 점 예측기 및 불확실성 지표로 사용하는 것이 가장 일반적입니다. 몬테카를로 획득 함수를 사용한 베이지안 최적화 또는 모델 기반 RL을 위한 가우시안 프로세스와 같은 고급 응용 프로그램에서는 종종 사후 샘플을 취해야 합니다. 그러나 기본 응용 프로그램에서 엄격하게 요구되지 않더라도 이러한 샘플은 데이터에 대한 적합성에 대한 더 많은 직관을 제공하며 시각화에 포함하는 데 종종 유용합니다.</p>
<h2 id="gpytorch로-쉽게-만들기-making-life-easy-with-gpytorch"><a class="header" href="#gpytorch로-쉽게-만들기-making-life-easy-with-gpytorch">GPyTorch로 쉽게 만들기 (Making Life Easy with GPyTorch)</a></h2>
<p>우리가 보았듯이 기본 가우시안 프로세스 회귀를 처음부터 완전히 구현하는 것은 실제로 꽤 쉽습니다. 그러나 다양한 커널 선택을 탐색하거나, 근사 추론(분류에도 필요함)을 고려하거나, GP를 신경망과 결합하거나, 심지어 약 10,000개 이상의 포인트가 있는 데이터셋을 갖게 되면 처음부터 구현하는 것은 다루기 힘들고 번거로워집니다. SKI(KISS-GP라고도 함)와 같은 확장 가능한 GP 추론을 위한 가장 효과적인 방법 중 일부는 수백 줄의 코드로 고급 수치 선형 대수 루틴을 구현해야 할 수 있습니다.</p>
<p>이러한 경우 <em>GPyTorch</em> 라이브러리는 우리의 삶을 훨씬 쉽게 만들어 줄 것입니다. 우리는 가우시안 프로세스 수치 및 고급 방법에 대한 향후 노트북에서 GPyTorch에 대해 더 논의할 것입니다. GPyTorch 라이브러리에는 <a href="https://github.com/cornellius-gp/gpytorch/tree/master/examples">많은 예제</a>가 포함되어 있습니다. 패키지에 대한 감을 잡기 위해 <a href="https://github.com/cornellius-gp/gpytorch/blob/master/examples/01_Exact_GPs/Simple_GP_Regression.ipynb">간단한 회귀 예제</a>를 살펴보며 GPyTorch를 사용하여 위의 결과를 재현하도록 어떻게 조정할 수 있는지 보여줄 것입니다. 이것은 단순히 위의 기본 회귀를 재현하기 위해 많은 코드처럼 보일 수 있으며, 어떤 의미에서는 그렇습니다. 그러나 잠재적으로 수천 줄의 새 코드를 작성하는 대신 아래 코드에서 몇 줄만 변경하여 다양한 커널, 확장 가능한 추론 기술 및 근사 추론을 즉시 사용할 수 있습니다.</p>
<pre><code class="language-{.python .input}"># 먼저 데이터를 PyTorch에서 사용할 수 있도록 텐서로 변환해 봅시다
train_x = torch.tensor(train_x)
train_y = torch.tensor(train_y)
test_y = torch.tensor(test_y)

# 우리는 0 평균과 RBF 커널을 사용하여 정확한 GP 추론을 사용하고 있습니다
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ZeroMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel())
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)
</code></pre>
<p>이 코드 블록은 데이터를 GPyTorch에 맞는 형식으로 넣고, 정확한 추론을 사용하고 있음을 지정하며, 사용하려는 평균 함수(0)와 커널 함수(RBF)를 지정합니다. 예를 들어 gpytorch.kernels.matern_kernel() 또는 gpyotrch.kernels.spectral_mixture_kernel()을 호출하여 다른 커널을 매우 쉽게 사용할 수 있습니다. 지금까지 우리는 근사를 하지 않고 예측 분포를 추론할 수 있는 정확한 추론에 대해서만 논의했습니다.
가우시안 프로세스의 경우 가우시안 우도가 있을 때만 정확한 추론을 수행할 수 있습니다. 더 구체적으로 말하자면, 관찰이 가우시안 프로세스로 표현되는 노이즈 없는 함수와 가우시안 노이즈로 생성된다고 가정할 때입니다.
향후 노트북에서는 이러한 가정을 할 수 없는 분류와 같은 다른 설정을 고려할 것입니다.</p>
<pre><code class="language-{.python .input}"># 가우시안 우도 초기화
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x, train_y, likelihood)
training_iter = 50
# 최적 모델 하이퍼파라미터 찾기
model.train()
likelihood.train()
# adam 최적화 도구 사용, GaussianLikelihood 파라미터 포함
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  
# 손실을 음의 로그 GP 주변 우도로 설정
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
</code></pre>
<p>여기서 우리는 사용하려는 우도(가우시안), 커널 하이퍼파라미터 훈련에 사용할 목적 함수(여기서는 주변 우도), 그리고 해당 목적 함수를 최적화하는 데 사용할 절차(이 경우 Adam)를 명시적으로 지정합니다. "확률적" 최적화 도구인 Adam을 사용하고 있지만, 이 경우에는 전체 배치 Adam입니다. 주변 우도는 데이터 인스턴스에 대해 인수분해되지 않으므로 데이터의 "미니배치"에 대한 최적화 도구를 사용할 수 없으며 수렴이 보장되지 않습니다. L-BFGS와 같은 다른 최적화 도구도 GPyTorch에서 지원됩니다. 표준 딥러닝과 달리 주변 우도를 최적화하는 작업을 잘 수행하는 것은 좋은 일반화와 강력하게 일치하며, 엄청나게 비싸지 않다고 가정할 때 L-BFGS와 같은 강력한 최적화 도구로 기울게 합니다.</p>
<pre><code class="language-{.python .input}">for i in range(training_iter):
    # 이전 반복의 기울기 0으로 설정
    optimizer.zero_grad()
    # 모델의 출력
    output = model(train_x)
    # 손실 계산 및 역전파 기울기
    loss = -mll(output, train_y)
    loss.backward()
    if i % 10 == 0:
        print(f'Iter {i+1:d}/{training_iter:d} - Loss: {loss.item():.3f} '
              f'squared lengthscale: '
              f'{model.covar_module.base_kernel.lengthscale.item():.3f} '
              f'noise variance: {model.likelihood.noise.item():.3f}')
    optimizer.step()
</code></pre>
<p>여기서 실제로 최적화 절차를 실행하고 10번의 반복마다 손실 값을 출력합니다.</p>
<pre><code class="language-{.python .input}"># 평가(예측 사후) 모드로 전환
test_x = torch.tensor(test_x)
model.eval()
likelihood.eval()
observed_pred = likelihood(model(test_x)) 
</code></pre>
<p>위의 코드 블록을 사용하면 테스트 입력에 대한 예측을 할 수 있습니다.</p>
<pre><code class="language-{.python .input}">with torch.no_grad():
    # 플롯 초기화
    f, ax = d2l.plt.subplots(1, 1, figsize=(4, 3))
    # 95% 신용 집합에 대한 상한 및 하한 가져오기 (이 경우 관찰 공간에서)
    lower, upper = observed_pred.confidence_region()
    ax.scatter(train_x.numpy(), train_y.numpy())
    ax.plot(test_x.numpy(), test_y.numpy(), linewidth=2.)
    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), linewidth=2.)
    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.25)
    ax.set_ylim([-1.5, 1.5])
    ax.legend(['True Function', 'Predictive Mean', 'Observed Data',
               '95% Credible Set'])
</code></pre>
<p>마지막으로 피팅을 플로팅합니다.</p>
<p>피팅이 사실상 동일함을 알 수 있습니다. 주목해야 할 몇 가지 사항: GPyTorch는 <em>제곱된</em> 길이 척도와 관찰 노이즈로 작업합니다. 예를 들어, 처음부터 작성한 코드에서 학습된 노이즈 표준 편차는 약 0.283입니다. GPyTorch가 찾은 노이즈 분산은 $0.81 \approx 0.283^2$입니다. GPyTorch 플롯에서는 잠재 함수 공간이 아닌 <em>관찰 공간</em>에 신용 집합을 표시하여 실제로 관찰된 데이터 포인트를 덮고 있음을 보여줍니다.</p>
<h2 id="요약-summary-102"><a class="header" href="#요약-summary-102">요약 (Summary)</a></h2>
<p>가우시안 프로세스 사전 분포를 데이터와 결합하여 사후 분포를 형성하고 이를 사용하여 예측을 할 수 있습니다. 또한 가우시안 프로세스의 변화율과 같은 속성을 제어하는 커널 하이퍼파라미터의 자동 학습에 유용한 주변 우도를 형성할 수 있습니다. 회귀에 대한 사후 분포를 형성하고 커널 하이퍼파라미터를 학습하는 메커니즘은 간단하며 약 12줄의 코드를 포함합니다. 이 노트북은 실제로 가우시안 프로세스를 빠르게 "시작하고 실행"하려는 독자에게 좋은 참고 자료입니다. 또한 GPyTorch 라이브러리를 소개했습니다. 기본 회귀를 위한 GPyTorch 코드는 비교적 길지만, 다른 커널 함수나 확장 가능한 추론 또는 분류를 위한 비가우시안 우드와 같이 향후 노트북에서 논의할 고급 기능을 위해 사소하게 수정할 수 있습니다.</p>
<h2 id="연습-문제-exercises-117"><a class="header" href="#연습-문제-exercises-117">연습 문제 (Exercises)</a></h2>
<ol>
<li>우리는 커널 하이퍼파라미터 <em>학습</em>의 중요성과 하이퍼파라미터 및 커널이 가우시안 프로세스의 일반화 속성에 미치는 영향을 강조했습니다. 하이퍼를 학습하는 단계를 건너뛰고 대신 다양한 길이 척도와 노이즈 분산을 추측하고 예측에 미치는 영향을 확인해 보십시오. 큰 길이 척도를 사용하면 어떻게 됩니까? 작은 길이 척도는요? 큰 노이즈 분산은요? 작은 노이즈 분산은요?</li>
<li>우리는 주변 우도가 볼록 목적 함수가 아니지만 길이 척도 및 노이즈 분산과 같은 하이퍼파라미터를 GP 회귀에서 안정적으로 추정할 수 있다고 말했습니다. 이것은 일반적으로 사실입니다. 실제로 주변 우도는 경험적 자기상관 함수("covariograms")를 피팅하는 것을 포함하는 공간 통계의 기존 접근 방식보다 길이 척도 하이퍼파라미터를 학습하는 데 <em>훨씬</em> 더 좋습니다. 틀림없이, 적어도 확장 가능한 추론에 대한 최근 작업 이전에 가우시안 프로세스 연구에 대한 머신러닝의 가장 큰 기여는 하이퍼파라미터 학습을 위한 주변 우도의 도입이었습니다.</li>
</ol>
<p><em>그러나</em> 이러한 파라미터의 다른 쌍조차도 많은 데이터셋에 대해 해석 가능하게 다른 타당한 설명을 제공하여 목적 함수에서 국소 최적값을 초래합니다. 큰 길이 척도를 사용하면 기본 실제 함수가 천천히 변한다고 가정합니다. 관찰된 데이터가 실제로 상당히 변하는 경우, 큰 길이 척도를 가질 수 있는 유일한 방법은 큰 노이즈 분산을 갖는 것입니다. 반면에 작은 길이 척도를 사용하면 피팅이 데이터의 변동에 매우 민감하여 노이즈(우발적 불확실성)로 변동을 설명할 여지가 거의 없습니다.</p>
<p>이러한 국소 최적값을 찾을 수 있는지 확인해 보십시오. 큰 노이즈가 있는 매우 큰 길이 척도와 작은 노이즈가 있는 작은 길이 척도로 초기화하십시오. 다른 솔루션으로 수렴합니까?</p>
<ol start="3">
<li>
<p>우리는 베이지안 방법의 근본적인 장점이 <em>인식적</em> 불확실성을 자연스럽게 표현하는 데 있다고 말했습니다. 위의 예에서는 인식적 불확실성의 효과를 완전히 볼 수 없습니다. 대신 <code>test_x = np.linspace(0, 10, 1000)</code>으로 예측해 보십시오. 예측이 데이터를 넘어서 이동함에 따라 95% 신용 집합에 어떤 일이 발생합니까? 해당 구간에서 실제 함수를 덮습니까? 해당 영역에서 우발적 불확실성만 시각화하면 어떻게 됩니까?</p>
</li>
<li>
<p>위의 예제를 실행하되, 대신 10,000, 20,000 및 40,000개의 훈련 포인트로 실행하고 런타임을 측정해 보십시오. 훈련 시간은 어떻게 확장됩니까? 대안으로 런타임은 테스트 포인트 수에 따라 어떻게 확장됩니까? 예측 평균과 예측 분산에 대해 다릅니까? 훈련 및 테스트 시간 복잡도를 이론적으로 해결하고 다른 수의 포인트로 위 코드를 실행하여 이 질문에 답하십시오.</p>
</li>
<li>
<p>Matern 커널과 같은 다른 공분산 함수를 사용하여 GPyTorch 예제를 실행해 보십시오. 결과는 어떻게 변합니까? GPyTorch 라이브러리에서 찾을 수 있는 스펙트럼 혼합 커널은 어떻습니까? 일부는 다른 것보다 주변 우도를 훈련하기가 더 쉽습니까? 장거리 대 단거리 예측에 더 가치 있는 것이 있습니까?</p>
</li>
<li>
<p>GPyTorch 예제에서는 관찰 노이즈를 포함한 예측 분포를 플로팅한 반면, "처음부터" 예제에서는 인식적 불확실성만 포함했습니다. 이번에는 인식적 불확실성만 플로팅하여 GPyTorch 예제를 다시 실행하고 처음부터 결과와 비교하십시오. 예측 분포가 이제 동일하게 보입니까? (그래야 합니다.)</p>
</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12117">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="하이퍼파라미터-최적화-hyperparameter-optimization"><a class="header" href="#하이퍼파라미터-최적화-hyperparameter-optimization">하이퍼파라미터 최적화 (Hyperparameter Optimization)</a></h1>
<p>:label:<code>chap_hyperopt</code></p>
<p><strong>Aaron Klein</strong> (<em>Amazon</em>), <strong>Matthias Seeger</strong> (<em>Amazon</em>), 및 <strong>Cedric Archambeau</strong> (<em>Amazon</em>)</p>
<p>모든 머신러닝 모델의 성능은 하이퍼파라미터에 따라 달라집니다.
하이퍼파라미터는 학습 알고리즘이나 기본 통계 모델의 구조를 제어합니다.
그러나 실제로 하이퍼파라미터를 선택하는 일반적인 방법은 없습니다.
대신 하이퍼파라미터는 종종 시행착오 방식으로 설정되거나 때로는 실무자에 의해 기본값으로 남겨져 최적이 아닌 일반화로 이어집니다.</p>
<p>하이퍼파라미터 최적화는 이 문제를 최적화 문제로 캐스팅하여 체계적인 접근 방식을 제공합니다: 좋은 하이퍼파라미터 세트는 (적어도) 검증 오차를 최소화해야 합니다.
머신러닝에서 발생하는 대부분의 다른 최적화 문제와 비교할 때, 하이퍼파라미터 최적화는 중첩된 문제이며, 각 반복마다 머신러닝 모델을 훈련하고 검증해야 합니다.</p>
<p>이 장에서는 먼저 하이퍼파라미터 최적화의 기초를 소개합니다.
또한 원래 목적 함수의 평가하기 저렴한 프록시를 활용하여 하이퍼파라미터 최적화의 전반적인 효율성을 향상시키는 몇 가지 최근 발전 사항을 제시할 것입니다.
이 장의 끝부분에서는 최첨단 하이퍼파라미터 최적화 기술을 적용하여 자신의 머신러닝 알고리즘의 하이퍼파라미터를 최적화할 수 있어야 합니다.</p>
<pre><code class="language-toc">:maxdepth: 2

hyperopt-intro
hyperopt-api
rs-async.md
sh-intro
sh-async
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(["pytorch"])
</code></pre>
<h1 id="하이퍼파라미터-최적화란-무엇인가-what-is-hyperparameter-optimization"><a class="header" href="#하이퍼파라미터-최적화란-무엇인가-what-is-hyperparameter-optimization">하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></h1>
<p>:label:<code>sec_what_is_hpo</code></p>
<p>이전 장에서 보았듯이 심층 신경망에는 훈련 중에 학습되는 많은 수의 파라미터 또는 가중치가 있습니다. 이 외에도 모든 신경망에는 사용자가 구성해야 하는 추가적인 <em>하이퍼파라미터</em>가 있습니다. 예를 들어, 확률적 경사 하강법이 훈련 손실의 국소 최적값으로 수렴하도록 하려면(:numref:<code>chap_optimization</code> 참조), 학습률과 배치 크기를 조정해야 합니다. 훈련 데이터셋에 대한 과대적합을 피하기 위해, 가중치 감소(:numref:<code>sec_weight_decay</code> 참조)나 드롭아웃(:numref:<code>sec_dropout</code> 참조)과 같은 정규화 파라미터를 설정해야 할 수도 있습니다. 레이어 수와 레이어당 유닛 또는 필터 수(즉, 유효 가중치 수)를 설정하여 모델의 용량과 귀납적 편향을 정의할 수 있습니다.</p>
<p>불행히도 훈련 손실을 최소화하여 이러한 하이퍼파라미터를 단순히 조정할 수는 없습니다. 그렇게 하면 훈련 데이터에 과대적합되기 때문입니다. 예를 들어 드롭아웃이나 가중치 감소와 같은 정규화 파라미터를 0으로 설정하면 훈련 손실은 작아지지만 일반화 성능은 저하될 수 있습니다.</p>
<p><img src="chapter_hyperparameter-optimization/../img/ml_workflow.svg" alt="다양한 하이퍼파라미터로 모델을 여러 번 훈련하는 것으로 구성된 머신러닝의 일반적인 워크플로우." />
:label:<code>ml_workflow</code></p>
<p>다른 형태의 자동화 없이는 하이퍼파라미터를 시행착오 방식으로 수동으로 설정해야 하며, 이는 머신러닝 워크플로우에서 시간이 많이 걸리고 어려운 부분입니다. 예를 들어 CIFAR-10에서 ResNet(:numref:<code>sec_resnet</code> 참조)을 훈련하는 것을 고려해 보십시오. Amazon Elastic Cloud Compute (EC2) <code>g4dn.xlarge</code> 인스턴스에서 2시간 이상 걸립니다. 10개의 하이퍼파라미터 구성을 순서대로 시도하는 것만으로도 대략 하루가 걸립니다. 설상가상으로 하이퍼파라미터는 일반적으로 아키텍처와 데이터셋 간에 직접 전이되지 않으며 :cite:<code>feurer-arxiv22,wistuba-ml18,bardenet-icml13a</code>, 모든 새로운 작업에 대해 다시 최적화해야 합니다. 또한 대부분의 하이퍼파라미터에 대한 경험 법칙이 없으며 합리적인 값을 찾으려면 전문가 지식이 필요합니다.</p>
<p><em>하이퍼파라미터 최적화(Hyperparameter Optimization, HPO)</em> 알고리즘은 이 문제를 전역 최적화 문제로 프레임화하여 원칙적이고 자동화된 방식으로 해결하도록 설계되었습니다 :cite:<code>feurer-automlbook18a</code>. 기본 목표는 보류된 검증 데이터셋에 대한 오류이지만 원칙적으로 다른 비즈니스 메트릭일 수 있습니다. 훈련 시간, 추론 시간 또는 모델 복잡성과 같은 보조 목표와 결합되거나 제한될 수 있습니다.</p>
<p>최근 하이퍼파라미터 최적화는 <em>신경망 아키텍처 검색(Neural Architecture Search, NAS)</em> :cite:<code>elsken-arxiv18a,wistuba-arxiv19</code>으로 확장되었습니다. 여기서 목표는 완전히 새로운 신경망 아키텍처를 찾는 것입니다. 고전적인 HPO에 비해 NAS는 계산 측면에서 훨씬 더 비싸고 실제로 실행 가능하려면 추가적인 노력이 필요합니다. HPO와 NAS는 모두 전체 ML 파이프라인을 자동화하는 것을 목표로 하는 AutoML :cite:<code>hutter-book19a</code>의 하위 분야로 간주될 수 있습니다.</p>
<p>이 섹션에서는 HPO를 소개하고 :numref:<code>sec_softmax_concise</code>에서 소개된 로지스틱 회귀 예제의 최적 하이퍼파라미터를 자동으로 찾는 방법을 보여줍니다.</p>
<h2 id="최적화-문제-the-optimization-problem"><a class="header" href="#최적화-문제-the-optimization-problem">최적화 문제 (The Optimization Problem)</a></h2>
<p>:label:<code>sec_definition_hpo</code></p>
<p>간단한 장난감 문제로 시작하겠습니다. Fashion MNIST 데이터셋에 대한 검증 오류를 최소화하기 위해 :numref:<code>sec_softmax_concise</code>의 다중 클래스 로지스틱 회귀 모델 <code>SoftmaxRegression</code>의 학습률을 검색합니다. 배치 크기나 에포크 수와 같은 다른 하이퍼파라미터도 튜닝할 가치가 있지만 간단히 하기 위해 학습률에만 집중합니다.</p>
<pre><code class="language-{.python .input}">%%tab pytorch
from d2l import torch as d2l
import numpy as np
import torch
from torch import nn
from scipy import stats
</code></pre>
<p>HPO를 실행하기 전에 먼저 목적 함수와 구성 공간이라는 두 가지 요소를 정의해야 합니다.</p>
<h3 id="목적-함수-the-objective-function"><a class="header" href="#목적-함수-the-objective-function">목적 함수 (The Objective Function)</a></h3>
<p>학습 알고리즘의 성능은 하이퍼파라미터 공간 $\mathbf{x} \in \mathcal{X}$에서 검증 손실로 매핑하는 함수 $f: \mathcal{X} \rightarrow \mathbb{R}$로 볼 수 있습니다. $f(\mathbf{x})$를 평가할 때마다 머신러닝 모델을 훈련하고 검증해야 하는데, 대규모 데이터셋에서 훈련된 심층 신경망의 경우 시간과 계산이 많이 소요될 수 있습니다. 기준 $f(\mathbf{x})$가 주어지면 우리의 목표는 $\mathbf{x}<em>{\star} \in \mathrm{argmin}</em>{\mathbf{x} \in \mathcal{X}} f(\mathbf{x})$를 찾는 것입니다.</p>
<p>$\mathbf{x}$에 대한 $f$의 기울기를 계산하는 간단한 방법은 없습니다. 전체 훈련 과정을 통해 기울기를 전파해야 하기 때문입니다. 근사적인 "하이퍼그라디언트(hypergradients)"로 HPO를 구동하려는 최근 연구 :cite:<code>maclaurin-icml15,franceschi-icml17a</code>가 있지만, 기존 접근 방식 중 어느 것도 아직 최첨단 기술과 경쟁력이 없으므로 여기서는 논의하지 않겠습니다. 또한 $f$를 평가하는 계산 부담으로 인해 HPO 알고리즘은 가능한 한 적은 샘플로 전역 최적값에 접근해야 합니다.</p>
<p>신경망의 훈련은 확률적입니다(예: 가중치가 무작위로 초기화되고 미니배치가 무작위로 샘플링됨). 따라서 관찰은 노이즈가 있습니다: $y \sim f(\mathbf{x}) + \epsilon$, 여기서 우리는 보통 $\epsilon \sim N(0, \sigma)$ 관찰 노이즈가 가우시안 분포를 따른다고 가정합니다.</p>
<p>이러한 모든 과제에 직면하여 우리는 일반적으로 전역 최적값을 정확하게 맞추는 대신 성능이 좋은 하이퍼파라미터 구성의 작은 집합을 빠르게 식별하려고 합니다. 그러나 대부분의 신경망 모델의 큰 계산 요구로 인해 이것조차 며칠 또는 몇 주가 걸릴 수 있습니다. 우리는 :numref:<code>sec_mf_hpo</code>에서 검색을 분산시키거나 목적 함수의 평가 비용이 더 저렴한 근사를 사용하여 최적화 프로세스를 가속화하는 방법을 탐구할 것입니다.</p>
<p>모델의 검증 오류를 계산하는 메서드로 시작합니다.</p>
<pre><code class="language-{.python .input  n=8}">%%tab pytorch
class HPOTrainer(d2l.Trainer):  #@save
    def validation_error(self):
        self.model.eval()
        accuracy = 0
        val_batch_idx = 0
        for batch in self.val_dataloader:
            with torch.no_grad():
                x, y = self.prepare_batch(batch)
                y_hat = self.model(x)
                accuracy += self.model.accuracy(y_hat, y)
            val_batch_idx += 1
        return 1 -  accuracy / val_batch_idx
</code></pre>
<p><code>learning_rate</code>로 구성된 하이퍼파라미터 구성 <code>config</code>에 대해 검증 오류를 최적화합니다. 각 평가에 대해 <code>max_epochs</code> 동안 모델을 훈련한 다음 검증 오류를 계산하고 반환합니다.</p>
<pre><code class="language-{.python .input  n=5}">%%tab pytorch
def hpo_objective_softmax_classification(config, max_epochs=8):
    learning_rate = config["learning_rate"]
    trainer = d2l.HPOTrainer(max_epochs=max_epochs)
    data = d2l.FashionMNIST(batch_size=16)
    model = d2l.SoftmaxRegression(num_outputs=10, lr=learning_rate)
    trainer.fit(model=model, data=data)
    return d2l.numpy(trainer.validation_error())
</code></pre>
<h3 id="구성-공간-the-configuration-space"><a class="header" href="#구성-공간-the-configuration-space">구성 공간 (The Configuration Space)</a></h3>
<p>:label:<code>sec_intro_config_spaces</code></p>
<p>목적 함수 $f(\mathbf{x})$와 함께 최적화할 실행 가능 집합 $\mathbf{x} \in \mathcal{X}$도 정의해야 하며, 이를 <em>구성 공간(configuration space)</em> 또는 *검색 공간(search space)*이라고 합니다. 로지스틱 회귀 예제의 경우 다음을 사용합니다.</p>
<pre><code class="language-{.python .input  n=6}">config_space = {"learning_rate": stats.loguniform(1e-4, 1)}
</code></pre>
<p>여기서는 SciPy의 <code>loguniform</code> 객체를 사용합니다. 이는 로그 공간에서 -4와 -1 사이의 균일 분포를 나타냅니다. 이 객체를 사용하여 이 분포에서 확률 변수를 샘플링할 수 있습니다.</p>
<p>각 하이퍼파라미터에는 <code>learning_rate</code>의 <code>float</code>와 같은 데이터 유형과 닫힌 경계 범위(즉, 하한 및 상한)가 있습니다. 우리는 일반적으로 각 하이퍼파라미터에 샘플링할 사전 분포(예: 균일 또는 로그 균일)를 할당합니다. <code>learning_rate</code>와 같은 일부 양수 파라미터는 최적값이 몇 자리 수만큼 다를 수 있으므로 로그 스케일로 가장 잘 표현되는 반면, 모멘텀과 같은 다른 파라미터는 선형 스케일로 제공됩니다.</p>
<p>아래에서는 유형 및 표준 범위를 포함하여 다층 퍼셉트론의 일반적인 하이퍼파라미터로 구성된 구성 공간의 간단한 예를 보여줍니다.</p>
<p>: 다층 퍼셉트론의 구성 공간 예시
:label:<code>tab_example_configspace</code></p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: center">이름 (Name)</th><th style="text-align: center">유형 (Type)</th><th style="text-align: center">하이퍼파라미터 범위 (Hyperparameter Ranges)</th><th style="text-align: center">로그 스케일 (log-scale)</th></tr></thead><tbody>
<tr><td style="text-align: center">학습률 (learning rate)</td><td style="text-align: center">실수 (float)</td><td style="text-align: center">$[10^{-6},10^{-1}]$</td><td style="text-align: center">예 (yes)</td></tr>
<tr><td style="text-align: center">배치 크기 (batch size)</td><td style="text-align: center">정수 (integer)</td><td style="text-align: center">$[8,256]$</td><td style="text-align: center">예 (yes)</td></tr>
<tr><td style="text-align: center">모멘텀 (momentum)</td><td style="text-align: center">실수 (float)</td><td style="text-align: center">$[0,0.99]$</td><td style="text-align: center">아니오 (no)</td></tr>
<tr><td style="text-align: center">활성화 함수 (activation function)</td><td style="text-align: center">범주형 (categorical)</td><td style="text-align: center">{$\textrm{tanh}, \textrm{relu}$}</td><td style="text-align: center">-</td></tr>
<tr><td style="text-align: center">유닛 수 (number of units)</td><td style="text-align: center">정수 (integer)</td><td style="text-align: center">$[32, 1024]$</td><td style="text-align: center">예 (yes)</td></tr>
<tr><td style="text-align: center">레이어 수 (number of layers)</td><td style="text-align: center">정수 (integer)</td><td style="text-align: center">$[1, 6]$</td><td style="text-align: center">아니오 (no)</td></tr>
</tbody></table>
</div>
<p>일반적으로 구성 공간 $\mathcal{X}$의 구조는 복잡할 수 있으며 $\mathbb{R}^d$와 상당히 다를 수 있습니다. 실제로 일부 하이퍼파라미터는 다른 하이퍼파라미터의 값에 따라 달라질 수 있습니다. 예를 들어 다층 퍼셉트론의 레이어 수와 각 레이어의 유닛 수를 조정하려고 한다고 가정해 봅시다. $l\textrm{-번째}$ 레이어의 유닛 수는 네트워크에 적어도 $l+1$개의 레이어가 있는 경우에만 관련이 있습니다. 이러한 고급 HPO 문제는 이 장의 범위를 벗어납니다. 관심 있는 독자는 :cite:<code>hutter-lion11a,jenatton-icml17a,baptista-icml18a</code>를 참조하십시오.</p>
<p>구성 공간은 하이퍼파라미터 최적화에서 중요한 역할을 합니다. 어떤 알고리즘도 구성 공간에 포함되지 않은 것을 찾을 수 없기 때문입니다. 반면에 범위가 너무 크면 성능이 좋은 구성을 찾는 데 드는 계산 예산이 실행 불가능할 수 있습니다.</p>
<h2 id="무작위-검색-random-search"><a class="header" href="#무작위-검색-random-search">무작위 검색 (Random Search)</a></h2>
<p>:label:<code>sec_rs</code></p>
<p><em>무작위 검색</em>은 우리가 고려할 첫 번째 하이퍼파라미터 최적화 알고리즘입니다. 무작위 검색의 주요 아이디어는 미리 정의된 예산(예: 최대 반복 횟수)이 소진될 때까지 구성 공간에서 독립적으로 샘플링하고 관찰된 최적의 구성을 반환하는 것입니다. 모든 평가는 병렬로 독립적으로 실행될 수 있지만(:numref:<code>sec_rs_async</code> 참조), 여기서는 간단히 순차 루프를 사용합니다.</p>
<pre><code class="language-{.python .input  n=7}">errors, values = [], []
num_iterations = 5

for i in range(num_iterations):
    learning_rate = config_space["learning_rate"].rvs()
    print(f"Trial {i}: learning_rate = {learning_rate}")
    y = hpo_objective_softmax_classification({"learning_rate": learning_rate})
    print(f"    validation_error = {y}")
    values.append(learning_rate)
    errors.append(y)
</code></pre>
<p>그러면 최적의 학습률은 단순히 검증 오류가 가장 낮은 학습률입니다.</p>
<pre><code class="language-{.python .input  n=7}">best_idx = np.argmin(errors)
print(f"optimal learning rate = {values[best_idx]}")
</code></pre>
<p>단순성과 일반성으로 인해 무작위 검색은 가장 자주 사용되는 HPO 알고리즘 중 하나입니다. 정교한 구현이 필요하지 않으며 각 하이퍼파라미터에 대한 확률 분포를 정의할 수 있는 한 모든 구성 공간에 적용할 수 있습니다.</p>
<p>불행히도 무작위 검색에는 몇 가지 단점도 있습니다. 첫째, 지금까지 수집한 이전 관찰을 기반으로 샘플링 분포를 조정하지 않습니다. 따라서 성능이 좋은 구성보다 성능이 나쁜 구성을 샘플링할 가능성이 동일합니다. 둘째, 일부 구성은 초기에 나쁜 성능을 보여 이전에 본 구성보다 성능이 뛰어날 가능성이 낮음에도 불구하고 모든 구성에 동일한 리소스가 소비됩니다.</p>
<p>다음 섹션에서는 모델을 사용하여 검색을 안내함으로써 무작위 검색의 단점을 극복하는 더 샘플 효율적인 하이퍼파라미터 최적화 알고리즘을 살펴볼 것입니다. 또한 성능이 좋지 않은 구성의 평가 프로세스를 자동으로 중지하여 최적화 프로세스를 가속화하는 알고리즘도 살펴볼 것입니다.</p>
<h2 id="요약-summary-103"><a class="header" href="#요약-summary-103">요약 (Summary)</a></h2>
<p>이 섹션에서는 하이퍼파라미터 최적화(HPO)를 소개하고 구성 공간과 목적 함수를 정의하여 이를 전역 최적화로 표현하는 방법을 설명했습니다. 또한 첫 번째 HPO 알고리즘인 무작위 검색을 구현하고 간단한 소프트맥스 분류 문제에 적용했습니다.</p>
<p>무작위 검색은 매우 간단하지만 고정된 하이퍼파라미터 세트만 평가하는 그리드 검색보다 더 나은 대안입니다. 무작위 검색은 차원의 저주를 어느 정도 완화하며 :cite:<code>bellman-science66</code>, 기준이 하이퍼파라미터의 작은 하위 집합에 가장 강하게 의존하는 경우 그리드 검색보다 훨씬 효율적일 수 있습니다.</p>
<h2 id="연습-문제-exercises-118"><a class="header" href="#연습-문제-exercises-118">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 장에서는 분리된 훈련 세트에서 훈련한 후 모델의 검증 오류를 최적화합니다. 간단히 하기 위해 우리 코드는 <code>FashionMNIST.val</code> 주변의 로더에 매핑되는 <code>Trainer.val_dataloader</code>를 사용합니다.
<ol>
<li>이것이 우리가 훈련을 위해 원본 FashionMNIST 훈련 세트(60,000개 예제)를 사용하고 검증을 위해 원본 <em>테스트 세트</em>(10,000개 예제)를 사용한다는 의미임을 코드(코드를 살펴봄으로써)를 통해 확신하십시오.</li>
<li>이 관행이 문제가 될 수 있는 이유는 무엇입니까? 힌트: :numref:<code>sec_generalization_basics</code>를 다시 읽어보십시오. 특히 <em>모델 선택</em>에 대해 읽어보십시오.</li>
<li>대신 무엇을 했어야 합니까?</li>
</ol>
</li>
<li>위에서 경사 하강법에 의한 하이퍼파라미터 최적화는 수행하기 매우 어렵다고 말했습니다. 배치 크기가 256인 FashionMNIST 데이터셋(:numref:<code>sec_mlp-implementation</code>)에서 2레이어 퍼셉트론을 훈련하는 것과 같은 작은 문제를 생각해 보십시오. 훈련 1에포크 후 검증 메트릭을 최소화하기 위해 SGD의 학습률을 튜닝하고 싶습니다.
<ol>
<li>이 목적을 위해 검증 <em>오류</em>를 사용할 수 없는 이유는 무엇입니까? 검증 세트에서 어떤 메트릭을 사용하시겠습니까?</li>
<li>1에포크 훈련 후 검증 메트릭의 계산 그래프를 (대략적으로) 스케치하십시오. 초기 가중치와 하이퍼파라미터(예: 학습률)가 이 그래프의 입력 노드라고 가정할 수 있습니다. 힌트: :numref:<code>sec_backprop</code>에서 계산 그래프에 대해 다시 읽어보십시오.</li>
<li>이 그래프에서 순방향 패스 동안 저장해야 하는 부동 소수점 값의 수를 대략적으로 추정하십시오. 힌트: FashionMNIST에는 60,000개의 케이스가 있습니다. 필요한 메모리가 각 레이어 이후의 활성화에 의해 지배된다고 가정하고 :numref:<code>sec_mlp-implementation</code>에서 레이어 너비를 찾아보십시오.</li>
<li>필요한 엄청난 양의 계산 및 저장 공간 외에도 기울기 기반 하이퍼파라미터 최적화가 겪게 될 다른 문제는 무엇입니까? 힌트: :numref:<code>sec_numerical_stability</code>에서 기울기 소실 및 폭발에 대해 다시 읽어보십시오.</li>
<li><em>고급</em>: 기울기 기반 HPO에 대한 우아한(아직 다소 비실용적인) 접근 방식에 대해서는 :cite:<code>maclaurin-icml15</code>를 읽어보십시오.</li>
</ol>
</li>
<li>그리드 검색은 또 다른 HPO 기준선으로, 각 하이퍼파라미터에 대해 등간격 그리드를 정의한 다음 구성을 제안하기 위해 (조합) 데카르트 곱을 반복합니다.
<ol>
<li>위에서 우리는 기준이 하이퍼파라미터의 작은 하위 집합에 가장 강하게 의존하는 경우 상당한 수의 하이퍼파라미터에 대한 HPO에 대해 무작위 검색이 그리드 검색보다 훨씬 효율적일 수 있다고 말했습니다. 그 이유는 무엇입니까? 힌트: :cite:<code>bergstra2011algorithms</code>를 읽어보십시오.</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12090">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input  n=1}">%load_ext d2lbook.tab
tab.interact_select(["pytorch"])
</code></pre>
<h1 id="하이퍼파라미터-최적화-api-hyperparameter-optimization-api"><a class="header" href="#하이퍼파라미터-최적화-api-hyperparameter-optimization-api">하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></h1>
<p>:label:<code>sec_api_hpo</code></p>
<p>방법론을 살펴보기 전에, 먼저 다양한 HPO 알고리즘을 효율적으로 구현할 수 있는 기본 코드 구조에 대해 논의하겠습니다. 일반적으로 여기서 고려되는 모든 HPO 알고리즘은 *검색(searching)*과 *스케줄링(scheduling)*이라는 두 가지 의사 결정 프리미티브를 구현해야 합니다. 첫째, 새로운 하이퍼파라미터 구성을 샘플링해야 하는데, 여기에는 종종 구성 공간에 대한 일종의 검색이 포함됩니다. 둘째, 각 구성에 대해 HPO 알고리즘은 평가를 스케줄링하고 얼마나 많은 리소스를 할당할지 결정해야 합니다. 구성 평가를 시작하면 이를 *시험(trial)*이라고 합니다. 이러한 결정을 <code>HPOSearcher</code>와 <code>HPOScheduler</code>라는 두 클래스에 매핑합니다. 그 위에 최적화 프로세스를 실행하는 <code>HPOTuner</code> 클래스도 제공합니다.</p>
<p>스케줄러와 검색자의 개념은 Syne Tune :cite:<code>salinas-automl22</code>, Ray Tune :cite:<code>liaw-arxiv18</code> 또는 Optuna :cite:<code>akiba-sigkdd19</code>와 같은 인기 있는 HPO 라이브러리에도 구현되어 있습니다.</p>
<pre><code class="language-{.python .input  n=2}">%%tab pytorch
import time
from d2l import torch as d2l
from scipy import stats
</code></pre>
<h2 id="검색자-searcher"><a class="header" href="#검색자-searcher">검색자 (Searcher)</a></h2>
<p>아래에서는 <code>sample_configuration</code> 함수를 통해 새로운 후보 구성을 제공하는 검색자의 기본 클래스를 정의합니다. 이 함수를 구현하는 간단한 방법은 :numref:<code>sec_what_is_hpo</code>에서 무작위 검색을 위해 했던 것처럼 구성을 균일하게 무작위로 샘플링하는 것입니다. 베이지안 최적화와 같은 더 정교한 알고리즘은 이전 시험의 성능을 기반으로 이러한 결정을 내립니다. 결과적으로 이러한 알고리즘은 시간이 지남에 따라 더 유망한 후보를 샘플링할 수 있습니다. <code>update</code> 함수를 추가하여 이전 시험의 기록을 업데이트하며, 이는 샘플링 분포를 개선하는 데 활용될 수 있습니다.</p>
<pre><code class="language-{.python .input  n=3}">%%tab pytorch
class HPOSearcher(d2l.HyperParameters):  #@save
    def sample_configuration() -&gt; dict:
        raise NotImplementedError

    def update(self, config: dict, error: float, additional_info=None):
        pass
</code></pre>
<p>다음 코드는 이전 섹션의 무작위 검색 최적화 도구를 이 API에서 구현하는 방법을 보여줍니다. 약간의 확장으로 사용자가 <code>initial_config</code>를 통해 평가할 첫 번째 구성을 지정할 수 있도록 하고, 후속 구성은 무작위로 추출됩니다.</p>
<pre><code class="language-{.python .input  n=4}">%%tab pytorch
class RandomSearcher(HPOSearcher):  #@save
    def __init__(self, config_space: dict, initial_config=None):
        self.save_hyperparameters()

    def sample_configuration(self) -&gt; dict:
        if self.initial_config is not None:
            result = self.initial_config
            self.initial_config = None
        else:
            result = {
                name: domain.rvs()
                for name, domain in self.config_space.items()
            }
        return result
</code></pre>
<h2 id="스케줄러-scheduler"><a class="header" href="#스케줄러-scheduler">스케줄러 (Scheduler)</a></h2>
<p>새로운 시험을 위한 구성을 샘플링하는 것 외에도 시험을 언제 얼마나 오래 실행할지 결정해야 합니다. 실제로 이러한 모든 결정은 <code>HPOScheduler</code>에 의해 수행되며, 이는 새로운 구성의 선택을 <code>HPOSearcher</code>에 위임합니다. <code>suggest</code> 메서드는 훈련을 위한 리소스가 사용 가능해질 때마다 호출됩니다. 검색자의 <code>sample_configuration</code>을 호출하는 것 외에도 <code>max_epochs</code>(즉, 모델을 훈련할 기간)와 같은 파라미터를 결정할 수도 있습니다. <code>update</code> 메서드는 시험이 새로운 관찰을 반환할 때마다 호출됩니다.</p>
<pre><code class="language-{.python .input  n=5}">%%tab pytorch
class HPOScheduler(d2l.HyperParameters):  #@save
    def suggest(self) -&gt; dict:
        raise NotImplementedError
    
    def update(self, config: dict, error: float, info=None):
        raise NotImplementedError
</code></pre>
<p>무작위 검색뿐만 아니라 다른 HPO 알고리즘을 구현하기 위해, 새로운 리소스가 사용 가능해질 때마다 새로운 구성을 스케줄링하는 기본 스케줄러만 있으면 됩니다.</p>
<pre><code class="language-{.python .input  n=6}">%%tab pytorch
class BasicScheduler(HPOScheduler):  #@save
    def __init__(self, searcher: HPOSearcher):
        self.save_hyperparameters()

    def suggest(self) -&gt; dict:
        return self.searcher.sample_configuration()

    def update(self, config: dict, error: float, info=None):
        self.searcher.update(config, error, additional_info=info)
</code></pre>
<h2 id="튜너-tuner"><a class="header" href="#튜너-tuner">튜너 (Tuner)</a></h2>
<p>마지막으로 스케줄러/검색자를 실행하고 결과의 부기(book-keeping)를 수행하는 구성 요소가 필요합니다. 다음 코드는 훈련 작업을 차례로 평가하는 HPO 시험의 순차적 실행을 구현하며 기본 예제로 사용됩니다. 나중에 더 확장 가능한 분산 HPO 사례를 위해 <em>Syne Tune</em>을 사용할 것입니다.</p>
<pre><code class="language-{.python .input  n=7}">%%tab pytorch
class HPOTuner(d2l.HyperParameters):  #@save
    def __init__(self, scheduler: HPOScheduler, objective: callable):
        self.save_hyperparameters()
        # 플로팅을 위한 결과 부기
        self.incumbent = None
        self.incumbent_error = None
        self.incumbent_trajectory = []
        self.cumulative_runtime = []
        self.current_runtime = 0
        self.records = []

    def run(self, number_of_trials):
        for i in range(number_of_trials):
            start_time = time.time()
            config = self.scheduler.suggest()
            print(f"Trial {i}: config = {config}")
            error = self.objective(**config)
            error = float(d2l.numpy(error.cpu()))
            self.scheduler.update(config, error)
            runtime = time.time() - start_time
            self.bookkeeping(config, error, runtime)
            print(f"    error = {error}, runtime = {runtime}")
</code></pre>
<h2 id="hpo-알고리즘의-성능-부기-bookkeeping-the-performance-of-hpo-algorithms"><a class="header" href="#hpo-알고리즘의-성능-부기-bookkeeping-the-performance-of-hpo-algorithms">HPO 알고리즘의 성능 부기 (Bookkeeping the Performance of HPO Algorithms)</a></h2>
<p>어떤 HPO 알고리즘이든 우리는 주로 주어진 벽시계 시간(wall-clock time) 이후의 최고 성능 구성(<em>incumbent</em>라고 함)과 검증 오류에 관심이 있습니다. 이것이 반복당 <code>runtime</code>을 추적하는 이유이며, 여기에는 평가를 실행하는 시간(<code>objective</code> 호출)과 결정을 내리는 시간(<code>scheduler.suggest</code> 호출)이 모두 포함됩니다. 뒤이어 <code>scheduler</code>(및 <code>searcher</code>) 측면에서 정의된 HPO 알고리즘의 <em>any-time 성능</em>을 시각화하기 위해 <code>cumulative_runtime</code> 대 <code>incumbent_trajectory</code>를 플롯할 것입니다. 이를 통해 최적화 도구가 찾은 구성이 얼마나 잘 작동하는지뿐만 아니라 최적화 도구가 얼마나 빨리 찾을 수 있는지도 정량화할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=8}">%%tab pytorch
@d2l.add_to_class(HPOTuner)  #@save
def bookkeeping(self, config: dict, error: float, runtime: float):
    self.records.append({"config": config, "error": error, "runtime": runtime})
    # 마지막 하이퍼파라미터 구성이 incumbent보다 성능이 좋은지 확인
    if self.incumbent is None or self.incumbent_error &gt; error:
        self.incumbent = config
        self.incumbent_error = error
    # 현재 관찰된 최고 성능을 최적화 궤적에 추가
    self.incumbent_trajectory.append(self.incumbent_error)
    # 런타임 업데이트
    self.current_runtime += runtime
    self.cumulative_runtime.append(self.current_runtime)
</code></pre>
<h2 id="예제-합성곱-신경망의-하이퍼파라미터-최적화"><a class="header" href="#예제-합성곱-신경망의-하이퍼파라미터-최적화">예제: 합성곱 신경망의 하이퍼파라미터 최적화</a></h2>
<p>이제 무작위 검색의 새로운 구현을 사용하여 :numref:<code>sec_lenet</code>의 <code>LeNet</code> 합성곱 신경망의 <em>배치 크기</em>와 <em>학습률</em>을 최적화합니다. 목적 함수를 정의하는 것으로 시작하며, 이번에도 검증 오류가 될 것입니다.</p>
<pre><code class="language-{.python .input  n=9}">%%tab pytorch
def hpo_objective_lenet(learning_rate, batch_size, max_epochs=10):  #@save
    model = d2l.LeNet(lr=learning_rate, num_classes=10)
    trainer = d2l.HPOTrainer(max_epochs=max_epochs, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=batch_size)
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
    trainer.fit(model=model, data=data)
    validation_error = trainer.validation_error()
    return validation_error
</code></pre>
<p>구성 공간도 정의해야 합니다. 또한 평가할 첫 번째 구성은 :numref:<code>sec_lenet</code>에서 사용된 기본 설정입니다.</p>
<pre><code class="language-{.python .input  n=10}">config_space = {
    "learning_rate": stats.loguniform(1e-2, 1),
    "batch_size": stats.randint(32, 256),
}
initial_config = {
    "learning_rate": 0.1,
    "batch_size": 128,
}
</code></pre>
<p>이제 무작위 검색을 시작할 수 있습니다.</p>
<pre><code class="language-{.python .input}">searcher = RandomSearcher(config_space, initial_config=initial_config)
scheduler = BasicScheduler(searcher=searcher)
tuner = HPOTuner(scheduler=scheduler, objective=hpo_objective_lenet)
tuner.run(number_of_trials=5)
</code></pre>
<p>아래에서는 무작위 검색의 any-time 성능을 얻기 위해 incumbent의 최적화 궤적을 그립니다.</p>
<pre><code class="language-{.python .input  n=11}">board = d2l.ProgressBoard(xlabel="time", ylabel="error")
for time_stamp, error in zip(
    tuner.cumulative_runtime, tuner.incumbent_trajectory
):
    board.draw(time_stamp, error, "random search", every_n=1)
</code></pre>
<h2 id="hpo-알고리즘-비교"><a class="header" href="#hpo-알고리즘-비교">HPO 알고리즘 비교</a></h2>
<p>훈련 알고리즘이나 모델 아키텍처와 마찬가지로, 서로 다른 HPO 알고리즘을 가장 잘 비교하는 방법을 이해하는 것이 중요합니다. 각 HPO 실행은 무작위 가중치 초기화나 미니배치 순서와 같은 훈련 과정의 무작위 효과와 무작위 검색의 무작위 샘플링과 같은 HPO 알고리즘 자체의 본질적인 무작위성이라는 두 가지 주요 무작위성 소스에 따라 달라집니다. 따라서 다른 알고리즘을 비교할 때 각 실험을 여러 번 실행하고 난수 생성기의 서로 다른 시드를 기반으로 한 알고리즘의 여러 반복 모집단에 대한 평균 또는 중앙값과 같은 통계를 보고하는 것이 중요합니다.</p>
<p>이를 설명하기 위해 피드 포워드 신경망의 하이퍼파라미터 튜닝에서 무작위 검색(:numref:<code>sec_rs</code> 참조)과 베이지안 최적화 :cite:<code>snoek-nips12</code>를 비교합니다. 각 알고리즘은 다른 무작위 시드로 $50$번 평가되었습니다. 실선은 이 $50$번의 반복에 걸친 incumbent의 평균 성능을 나타내고 점선은 표준 편차를 나타냅니다. 무작위 검색과 베이지안 최적화는 ~1000초까지 거의 동일하게 수행되지만, 베이지안 최적화는 과거 관찰을 사용하여 더 나은 구성을 식별할 수 있으므로 그 이후에는 무작위 검색보다 빠르게 성능이 뛰어납니다.</p>
<p><img src="chapter_hyperparameter-optimization/../img/example_anytime_performance.svg" alt="두 알고리즘 A와 B를 비교하기 위한 예시 any-time 성능 플롯." />
:label:<code>example_anytime_performance</code></p>
<h2 id="요약-5"><a class="header" href="#요약-5">요약</a></h2>
<p>이 섹션에서는 이 장에서 살펴볼 다양한 HPO 알고리즘을 구현할 수 있는 간단하면서도 유연한 인터페이스를 제시했습니다. 유사한 인터페이스는 인기 있는 오픈 소스 HPO 프레임워크에서 찾을 수 있습니다. 또한 HPO 알고리즘을 비교하는 방법과 주의해야 할 잠재적인 함정을 살펴보았습니다.</p>
<h2 id="연습-문제-10"><a class="header" href="#연습-문제-10">연습 문제</a></h2>
<ol>
<li>이 연습의 목표는 약간 더 도전적인 HPO 문제에 대한 목적 함수를 구현하고 더 현실적인 실험을 실행하는 것입니다. :numref:<code>sec_dropout</code>에서 구현된 두 은닉층 MLP <code>DropoutMLP</code>를 사용할 것입니다.
<ol>
<li>모델의 모든 하이퍼파라미터와 <code>batch_size</code>에 의존해야 하는 목적 함수를 코딩하십시오. <code>max_epochs=50</code>을 사용하십시오. 여기서는 GPU가 도움이 되지 않으므로 <code>num_gpus=0</code>입니다. 힌트: <code>hpo_objective_lenet</code>을 수정하십시오.</li>
<li><code>num_hiddens_1</code>, <code>num_hiddens_2</code>는 $[8, 1024]$의 정수이고, 드롭아웃 값은 $[0, 0.95]$에 있으며, <code>batch_size</code>는 $[16, 384]$에 있는 합리적인 검색 공간을 선택하십시오. <code>scipy.stats</code>의 합리적인 분포를 사용하여 <code>config_space</code>에 대한 코드를 제공하십시오.</li>
<li><code>number_of_trials=20</code>으로 이 예제에서 무작위 검색을 실행하고 결과를 플로팅하십시오. :numref:<code>sec_dropout</code>의 기본 구성인 <code>initial_config = {'num_hiddens_1': 256, 'num_hiddens_2': 256, 'dropout_1': 0.5, 'dropout_2': 0.5, 'lr': 0.1, 'batch_size': 256}</code>을 먼저 평가해야 합니다.</li>
</ol>
</li>
<li>이 연습에서는 과거 데이터를 기반으로 결정을 내리는 새로운 검색자(<code>HPOSearcher</code>의 서브클래스)를 구현할 것입니다. 파라미터 <code>probab_local</code>, <code>num_init_random</code>에 따라 달라집니다. <code>sample_configuration</code> 메서드는 다음과 같이 작동합니다. 처음 <code>num_init_random</code> 호출의 경우 <code>RandomSearcher.sample_configuration</code>과 동일하게 수행합니다. 그렇지 않으면 확률 <code>1 - probab_local</code>로 <code>RandomSearcher.sample_configuration</code>과 동일하게 수행합니다. 그렇지 않으면 지금까지 가장 작은 검증 오류를 달성한 구성을 선택하고 하이퍼파라미터 중 하나를 무작위로 선택한 다음 <code>RandomSearcher.sample_configuration</code>과 같이 값을 무작위로 샘플링하지만 다른 모든 값은 그대로 둡니다. 이 하나의 하이퍼파라미터를 제외하고 지금까지 가장 좋은 구성과 동일한 이 구성을 반환합니다.
<ol>
<li>이 새로운 <code>LocalSearcher</code>를 코딩하십시오. 힌트: 검색자는 구성 시 인수로 <code>config_space</code>가 필요합니다. <code>RandomSearcher</code> 유형의 멤버를 자유롭게 사용하십시오. <code>update</code> 메서드도 구현해야 합니다.</li>
<li>이전 연습의 실험을 다시 실행하되 <code>RandomSearcher</code> 대신 새 검색자를 사용하십시오. <code>probab_local</code>, <code>num_init_random</code>에 대해 다른 값으로 실험해 보십시오. 그러나 다른 HPO 방법을 적절하게 비교하려면 실험을 여러 번 반복하고 이상적으로는 여러 벤치마크 작업을 고려해야 한다는 점에 유의하십시오.</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12092">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(["pytorch"])
#required_libs("syne-tune[gpsearchers]==0.3.2")
</code></pre>
<h1 id="비동기-무작위-검색-asynchronous-random-search"><a class="header" href="#비동기-무작위-검색-asynchronous-random-search">비동기 무작위 검색 (Asynchronous Random Search)</a></h1>
<p>:label:<code>sec_rs_async</code></p>
<p>이전 :numref:<code>sec_api_hpo</code>에서 보았듯이 하이퍼파라미터 구성의 평가 비용이 많이 들기 때문에 무작위 검색이 좋은 하이퍼파라미터 구성을 반환하기까지 몇 시간 또는 며칠을 기다려야 할 수 있습니다. 실제로 우리는 종종 동일한 머신에 있는 여러 GPU 또는 단일 GPU가 있는 여러 머신과 같은 리소스 풀에 액세스할 수 있습니다. 이는 다음과 같은 질문을 제기합니다: <em>무작위 검색을 어떻게 효율적으로 분산할 수 있을까?</em></p>
<p>일반적으로 동기식 병렬 하이퍼파라미터 최적화와 비동기식 병렬 하이퍼파라미터 최적화를 구별합니다(:numref:<code>distributed_scheduling</code> 참조). 동기식 설정에서는 다음 배치를 시작하기 전에 동시에 실행 중인 모든 시험이 끝날 때까지 기다립니다. 심층 신경망의 필터 수 또는 레이어 수와 같은 하이퍼파라미터를 포함하는 구성 공간을 고려해 보십시오. 더 많은 레이어나 필터를 포함하는 하이퍼파라미터 구성은 당연히 완료하는 데 더 많은 시간이 걸리며, 동일한 배치의 다른 모든 시험은 최적화 프로세스를 계속하기 전에 동기화 지점(:numref:<code>distributed_scheduling</code>의 회색 영역)에서 기다려야 합니다.</p>
<p>비동기 설정에서는 리소스가 사용 가능해지는 즉시 새로운 시험을 스케줄링합니다. 동기화 오버헤드를 피할 수 있으므로 리소스를 최적으로 활용할 수 있습니다. 무작위 검색의 경우, 각각의 새로운 하이퍼파라미터 구성은 다른 모든 구성과 독립적으로, 특히 이전 평가의 관찰을 활용하지 않고 선택됩니다. 이는 무작위 검색을 비동기식으로 쉽게 병렬화할 수 있음을 의미합니다. 이전 관찰을 기반으로 결정을 내리는 더 정교한 방법의 경우 이는 간단하지 않습니다(:numref:<code>sec_sh_async</code> 참조). 순차적 설정보다 더 많은 리소스에 액세스해야 하지만 비동기 무작위 검색은 선형 속도 향상을 보여줍니다. 즉, $K$개의 시험을 병렬로 실행할 수 있으면 특정 성능에 $K$배 더 빨리 도달합니다.</p>
<p><img src="chapter_hyperparameter-optimization/../img/distributed_scheduling.svg" alt="하이퍼파라미터 최적화 프로세스를 동기식 또는 비동기식으로 분산합니다. 순차적 설정에 비해 전체 계산을 일정하게 유지하면서 전체 벽시계 시간을 줄일 수 있습니다. 동기식 스케줄링은 낙오자가 있는 경우 작업자가 유휴 상태가 될 수 있습니다." />
:label:<code>distributed_scheduling</code></p>
<p>이 노트북에서는 동일한 머신의 여러 파이썬 프로세스에서 시험이 실행되는 비동기 무작위 검색을 살펴볼 것입니다. 분산 작업 스케줄링 및 실행은 처음부터 구현하기 어렵습니다. 비동기 HPO를 위한 간단한 인터페이스를 제공하는 <em>Syne Tune</em> :cite:<code>salinas-automl22</code>을 사용할 것입니다. Syne Tune은 다양한 실행 백엔드로 실행되도록 설계되었으며, 관심 있는 독자는 분산 HPO에 대해 자세히 알아보기 위해 간단한 API를 연구하도록 초대됩니다.</p>
<pre><code class="language-{.python .input}">from d2l import torch as d2l
import logging
logging.basicConfig(level=logging.INFO)
from syne_tune.config_space import loguniform, randint
from syne_tune.backend.python_backend import PythonBackend
from syne_tune.optimizer.baselines import RandomSearch
from syne_tune import Tuner, StoppingCriterion
from syne_tune.experiments import load_experiment
</code></pre>
<h2 id="목적-함수"><a class="header" href="#목적-함수">목적 함수</a></h2>
<p>먼저 <code>report</code> 콜백을 통해 성능을 Syne Tune에 반환하도록 새로운 목적 함수를 정의해야 합니다.</p>
<pre><code class="language-{.python .input  n=34}">def hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):
    from d2l import torch as d2l    
    from syne_tune import Reporter

    model = d2l.LeNet(lr=learning_rate, num_classes=10)
    trainer = d2l.HPOTrainer(max_epochs=1, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=batch_size)
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
    report = Reporter() 
    for epoch in range(1, max_epochs + 1):
        if epoch == 1:
            # Trainer 상태 초기화
            trainer.fit(model=model, data=data) 
        else:
            trainer.fit_epoch()
        validation_error = d2l.numpy(trainer.validation_error().cpu())
        report(epoch=epoch, validation_error=float(validation_error))
</code></pre>
<p>Syne Tune의 <code>PythonBackend</code>는 함수 정의 내에서 종속성을 가져와야 한다는 점에 유의하십시오.</p>
<h2 id="비동기-스케줄러"><a class="header" href="#비동기-스케줄러">비동기 스케줄러</a></h2>
<p>먼저 시험을 동시에 평가하는 작업자 수를 정의합니다. 또한 전체 벽시계 시간에 대한 상한을 정의하여 무작위 검색을 실행할 기간을 지정해야 합니다.</p>
<pre><code class="language-{.python .input  n=37}">n_workers = 2  # 사용 가능한 GPU 수보다 작거나 같아야 함

max_wallclock_time = 12 * 60  # 12분
</code></pre>
<p>다음으로 최적화하려는 메트릭과 이 메트릭을 최소화할지 최대화할지 명시합니다. 즉, <code>metric</code>은 <code>report</code> 콜백에 전달된 인수 이름과 일치해야 합니다.</p>
<pre><code class="language-{.python .input  n=38}">mode = "min"
metric = "validation_error"
</code></pre>
<p>이전 예제의 구성 공간을 사용합니다. Syne Tune에서 이 딕셔너리는 훈련 스크립트에 상수 속성을 전달하는 데에도 사용할 수 있습니다. <code>max_epochs</code>를 전달하기 위해 이 기능을 활용합니다. 또한 <code>initial_config</code>에 평가할 첫 번째 구성을 지정합니다.</p>
<pre><code class="language-{.python .input  n=39}">config_space = {
    "learning_rate": loguniform(1e-2, 1),
    "batch_size": randint(32, 256),
    "max_epochs": 10,
}
initial_config = {
    "learning_rate": 0.1,
    "batch_size": 128,
}
</code></pre>
<p>다음으로 작업 실행을 위한 백엔드를 지정해야 합니다. 여기서는 병렬 작업이 하위 프로세스로 실행되는 로컬 머신의 배포만 고려합니다. 그러나 대규모 HPO의 경우 각 시험이 전체 인스턴스를 소비하는 클러스터 또는 클라우드 환경에서도 실행할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=40}">trial_backend = PythonBackend(
    tune_function=hpo_objective_lenet_synetune,
    config_space=config_space,
)
</code></pre>
<p>이제 :numref:<code>sec_api_hpo</code>의 <code>BasicScheduler</code>와 동작이 유사한 비동기 무작위 검색을 위한 스케줄러를 만들 수 있습니다.</p>
<pre><code class="language-{.python .input  n=41}">scheduler = RandomSearch(
    config_space,
    metric=metric,
    mode=mode,
    points_to_evaluate=[initial_config],
)
</code></pre>
<p>Syne Tune은 또한 <code>Tuner</code>를 제공합니다. 여기서 주요 실험 루프와 부기가 중앙 집중화되고 스케줄러와 백엔드 간의 상호 작용이 중재됩니다.</p>
<pre><code class="language-{.python .input  n=42}">stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)

tuner = Tuner(
    trial_backend=trial_backend,
    scheduler=scheduler, 
    stop_criterion=stop_criterion,
    n_workers=n_workers,
    print_update_interval=int(max_wallclock_time * 0.6),
)
</code></pre>
<p>분산 HPO 실험을 실행해 봅시다. 중지 기준에 따르면 약 12분 동안 실행됩니다.</p>
<pre><code class="language-{.python .input  n=43}">tuner.run()
</code></pre>
<p>평가된 모든 하이퍼파라미터 구성의 로그는 추가 분석을 위해 저장됩니다. 튜닝 작업 중 언제든지 지금까지 얻은 결과를 쉽게 가져와서 incumbent 궤적을 그릴 수 있습니다.</p>
<pre><code class="language-{.python .input  n=46}">d2l.set_figsize()
tuning_experiment = load_experiment(tuner.name)
tuning_experiment.plot()
</code></pre>
<h2 id="비동기-최적화-프로세스-시각화"><a class="header" href="#비동기-최적화-프로세스-시각화">비동기 최적화 프로세스 시각화</a></h2>
<p>아래에서는 비동기 최적화 프로세스 중에 모든 시험의 학습 곡선(플롯의 각 색상은 시험을 나타냄)이 어떻게 진화하는지 시각화합니다. 어느 시점에서든 작업자 수만큼의 시험이 동시에 실행됩니다. 시험이 끝나면 다른 시험이 끝나기를 기다리지 않고 즉시 다음 시험을 시작합니다. 비동기 스케줄링으로 작업자의 유휴 시간이 최소화됩니다.</p>
<pre><code class="language-{.python .input  n=45}">d2l.set_figsize([6, 2.5])
results = tuning_experiment.results

for trial_id in results.trial_id.unique():
    df = results[results["trial_id"] == trial_id]
    d2l.plt.plot(
        df["st_tuner_time"],
        df["validation_error"],
        marker="o"
    )
    
d2l.plt.xlabel("wall-clock time")
d2l.plt.ylabel("objective function")
</code></pre>
<h2 id="요약-6"><a class="header" href="#요약-6">요약</a></h2>
<p>병렬 리소스에 시험을 분산하여 무작위 검색의 대기 시간을 상당히 줄일 수 있습니다. 일반적으로 동기식 스케줄링과 비동기식 스케줄링을 구별합니다. 동기식 스케줄링은 이전 배치가 완료되면 새로운 하이퍼파라미터 구성 배치를 샘플링하는 것을 의미합니다. 다른 시험보다 완료하는 데 시간이 더 오래 걸리는 시험인 낙오자가 있는 경우 작업자는 동기화 지점에서 기다려야 합니다. 비동기식 스케줄링은 리소스가 사용 가능해지는 즉시 새로운 하이퍼파라미터 구성을 평가하므로 모든 작업자가 어느 시점에서든 바쁘게 움직이도록 보장합니다. 무작위 검색은 비동기식으로 분산하기 쉽고 실제 알고리즘의 변경이 필요하지 않지만 다른 방법은 약간의 추가 수정이 필요합니다.</p>
<h2 id="연습-문제-11"><a class="header" href="#연습-문제-11">연습 문제</a></h2>
<ol>
<li>:numref:<code>sec_dropout</code>에서 구현되고 :numref:<code>sec_api_hpo</code>의 연습 문제 1에서 사용된 <code>DropoutMLP</code> 모델을 고려하십시오.
<ol>
<li>Syne Tune과 함께 사용할 목적 함수 <code>hpo_objective_dropoutmlp_synetune</code>을 구현하십시오. 함수가 매 에포크 후 검증 오류를 보고하는지 확인하십시오.</li>
<li>:numref:<code>sec_api_hpo</code>의 연습 문제 1 설정을 사용하여 무작위 검색과 베이지안 최적화를 비교하십시오. SageMaker를 사용하는 경우 실험을 병렬로 실행하기 위해 Syne Tune의 벤치마킹 기능을 자유롭게 사용하십시오. 힌트: 베이지안 최적화는 <code>syne_tune.optimizer.baselines.BayesianOptimization</code>으로 제공됩니다.</li>
<li>이 연습을 위해서는 최소 4개의 CPU 코어가 있는 인스턴스에서 실행해야 합니다. 위에서 사용된 방법 중 하나(무작위 검색, 베이지안 최적화)에 대해 <code>n_workers=1</code>, <code>n_workers=2</code>, <code>n_workers=4</code>로 실험을 실행하고 결과(incumbent 궤적)를 비교하십시오. 적어도 무작위 검색의 경우 작업자 수에 따른 선형 확장을 관찰해야 합니다. 힌트: 강력한 결과를 얻으려면 각각 여러 번 반복하여 평균을 내야 할 수 있습니다.</li>
</ol>
</li>
<li><em>고급</em>. 이 연습의 목표는 Syne Tune에서 새로운 스케줄러를 구현하는 것입니다.
<ol>
<li><a href="https://github.com/d2l-ai/d2l-en/blob/master/INFO.md#installation-for-developers">d2lbook</a> 및 <a href="https://syne-tune.readthedocs.io/en/latest/getting_started.html">syne-tune</a> 소스를 모두 포함하는 가상 환경을 만듭니다.</li>
<li>:numref:<code>sec_api_hpo</code>의 연습 문제 2에 있는 <code>LocalSearcher</code>를 Syne Tune의 새 검색자로 구현하십시오. 힌트: <a href="https://syne-tune.readthedocs.io/en/latest/tutorials/developer/README.html">이 튜토리얼</a>을 읽어보십시오. 대안으로 <a href="https://syne-tune.readthedocs.io/en/latest/examples.html#launch-hpo-experiment-with-home-made-scheduler">이 예제</a>를 따를 수 있습니다.</li>
<li><code>DropoutMLP</code> 벤치마크에서 새로운 <code>LocalSearcher</code>와 <code>RandomSearch</code>를 비교하십시오.</li>
</ol>
</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12093">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(["pytorch"])
</code></pre>
<h1 id="다중-충실도-하이퍼파라미터-최적화-multi-fidelity-hyperparameter-optimization"><a class="header" href="#다중-충실도-하이퍼파라미터-최적화-multi-fidelity-hyperparameter-optimization">다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></h1>
<p>:label:<code>sec_mf_hpo</code></p>
<p>신경망 훈련은 중간 규모의 데이터셋에서도 비용이 많이 들 수 있습니다. 구성 공간(:numref:<code>sec_intro_config_spaces</code> 참조)에 따라, 하이퍼파라미터 최적화는 잘 작동하는 하이퍼파라미터 구성을 찾기 위해 수십 번에서 수백 번의 함수 평가를 필요로 합니다. :numref:<code>sec_rs_async</code>에서 보았듯이, 병렬 리소스를 활용하여 HPO의 전체 벽시계 시간(wall-clock time)을 크게 단축할 수 있지만, 이는 필요한 총 계산량을 줄이지는 못합니다.</p>
<p>이 섹션에서는 하이퍼파라미터 구성 평가를 어떻게 가속화할 수 있는지 보여줄 것입니다. 랜덤 서치와 같은 방법은 각 하이퍼파라미터 평가에 동일한 양의 리소스(예: 에폭 수, 훈련 데이터 포인트 수)를 할당합니다. :numref:<code>img_samples_lc</code>는 신경망 하이퍼파라미터 구성 집합의 학습 곡선을 보여줍니다.</p>
<p><img src="chapter_hyperparameter-optimization/../img/learning_curves.svg" alt="여러 하이퍼파라미터 구성의 에폭에 따른 검증 오차(학습 곡선). 좋은 구성과 나쁜 구성을 비교적 빠르게 구별할 수 있다는 것을 알 수 있습니다." />
:label:<code>img_samples_lc</code></p>
<p>많은 나쁜 구성들이 훈련 초기에 이미 성능이 좋지 않음을 알 수 있습니다. <em>다중 충실도(multi-fidelity)</em> HPO는 이 관찰을 활용합니다. 이는 저충실도 평가(예: 적은 수의 에폭에 대한 훈련)를 사용하여 나쁜 구성을 조기에 식별하고 성능이 좋은 것으로 보이는 구성에만 더 많은 리소스를 할당하려고 시도합니다.</p>
<p>이 섹션에서는 가장 인기 있는 다중 충실도 HPO 알고리즘 중 하나인 Successive Halving(:cite:<code>jamieson-aistats16,karnin-icml13</code>)을 살펴볼 것입니다.</p>
<h2 id="successive-halving"><a class="header" href="#successive-halving">Successive Halving</a></h2>
<p>Successive Halving(SH) 알고리즘은 성능이 낮은 구성을 리소스 $R$의 고정된 집합에서 조기에 중단하는 아이디어에 기반합니다. 리소스 $r$은 에폭 수, 훈련 데이터 포인트 수 또는 하위 샘플링된 이미지 해상도일 수 있습니다.</p>
<p>입력으로 $n$개의 하이퍼파라미터 구성 집합과 최소 리소스 $r_{min}$ 및 최대 리소스 $r_{max}$를 받습니다. $r_{min}$에서 시작하여 모든 구성을 평가합니다. 그런 다음 $1/\eta$ 비율의 최상의 구성을 유지하고 다음 스테이지로 이동하여 이 구성들을 더 많은 리소스로 평가합니다. 이 과정은 구성이 하나만 남을 때까지 반복됩니다.</p>
<p>공식적으로, $\eta$를 축소 인자(reduction factor)라고 합시다. 스테이지 $i$에서 우리는 $n_i$개의 구성을 각각 $r_i$ 리소스를 사용하여 평가합니다. 다음 스테이지 $i+1$로 넘어가기 위해 상위 $n_{i+1} = \lfloor n_i / \eta \rfloor$개의 구성을 선택하고 리소스를 $r_{i+1} = r_i \cdot \eta$로 늘립니다.</p>
<p>리소스 당 평가되는 구성의 수 $n_i r_i$는 모든 스테이지에서 거의 일정하게 유지됩니다. $\eta=2$라고 가정하면, 매 스테이지마다 구성의 절반을 탈락시키고 리소스를 두 배로 늘립니다. 따라서 전체 비용은 모든 $n$개 구성을 최대 리소스 $r_{max}$로 평가하는 비용보다 훨씬 작습니다. 구체적으로 SH의 총 리소스 비용은 대략 $n \cdot r_{min} \cdot \log_\eta(r_{max}/r_{min})$입니다.</p>
<h2 id="구현-implementation-3"><a class="header" href="#구현-implementation-3">구현 (Implementation)</a></h2>
<p>이제 Successive Halving을 구현해 보겠습니다. 리소스로 에폭 수를 사용합니다.</p>
<pre><code class="language-{.python .input}">#@tab pytorch
import numpy as np
from d2l import torch as d2l

class SuccessiveHalving(d2l.HPOScheduler):
    def __init__(self, searcher, eta, r_min, r_max):
        self.searcher = searcher
        self.eta = eta
        self.r_min = r_min
        self.r_max = r_max
        self.s_max = int(np.log(r_max / r_min) / np.log(eta))
        self.stage = 0
        self.configs = [self.searcher.sample_config() for _ in range(self.get_n_i(0))]
        self.observed_error = []

    def get_n_i(self, stage):
        return int(len(self.configs) * (self.eta ** -stage))

    def get_r_i(self, stage):
        return self.r_min * (self.eta ** stage)

    def suggest(self):
        if len(self.configs) == 0:
            return None
        config = self.configs.pop(0)
        config['epochs'] = self.get_r_i(self.stage)
        return config

    def update(self, config, error):
        self.observed_error.append((config, error))
        if len(self.configs) == 0:  # 스테이지 종료
            self.stage += 1
            if self.stage &lt;= self.s_max:
                # 오차를 기준으로 정렬하고 상위 1/eta 유지
                self.observed_error.sort(key=lambda x: x[1])
                n_next = self.get_n_i(self.stage)
                self.configs = [x[0] for x in self.observed_error[:n_next]]
                self.observed_error = []
</code></pre>
<h2 id="요약-summary-104"><a class="header" href="#요약-summary-104">요약 (Summary)</a></h2>
<ul>
<li>다중 충실도 HPO는 저충실도 평가를 사용하여 나쁜 하이퍼파라미터 구성을 조기에 식별함으로써 검색 프로세스를 가속화합니다.</li>
<li>Successive Halving은 반복적으로 최상의 구성 집합을 유지하고 리소스를 늘리는 간단하지만 강력한 알고리즘입니다.</li>
<li>에폭 수를 리소스로 사용함으로써 훈련 프로세스를 조기에 중단할 수 있어 계산 효율성을 높일 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-119"><a class="header" href="#연습-문제-exercises-119">연습 문제 (Exercises)</a></h2>
<ol>
<li>SH의 축소 인자 $\eta$가 성능에 어떤 영향을 미치는지 논의해 보십시오. $\eta$가 매우 크거나 작을 때의 장단점은 무엇입니까?</li>
<li>SH의 한 가지 잠재적인 단점은 초기 스테이지($r_{min}$)에서 성능이 좋지 않지만 나중에($r_{max}$) 성능이 좋아질 구성을 탈락시킬 수 있다는 것입니다. 이를 "나쁜 초기 단계(bad start)" 문제라고 합니다. 이를 완화할 수 있는 방법이 있을까요?</li>
<li>Successive Halving을 Hyperband(:cite:<code>li-jmlr17</code>)로 확장하는 방법을 찾아보십시오. Hyperband는 SH의 어떤 한계를 해결합니까?</li>
</ol>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12104">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(["pytorch"])
#required_libs("syne-tune[gpsearchers]==0.3.2")
</code></pre>
<h1 id="비동기-연속-반감-asynchronous-successive-halving"><a class="header" href="#비동기-연속-반감-asynchronous-successive-halving">비동기 연속 반감 (Asynchronous Successive Halving)</a></h1>
<p>:label:<code>sec_sh_async</code></p>
<p>:numref:<code>sec_rs_async</code>에서 보았듯이 여러 인스턴스 또는 단일 인스턴스의 여러 CPU/GPU에 하이퍼파라미터 구성 평가를 분산하여 HPO를 가속화할 수 있습니다. 그러나 무작위 검색에 비해 분산 설정에서 연속 반감(SH)을 비동기적으로 실행하는 것은 간단하지 않습니다. 다음에 실행할 구성을 결정하기 전에 먼저 현재 렁 수준에서 모든 관찰을 수집해야 합니다. 이를 위해서는 각 렁 수준에서 작업자를 동기화해야 합니다. 예를 들어 가장 낮은 렁 수준 $r_{\mathrm{min}}$의 경우, 그중 $\frac{1}{\eta}$를 다음 렁 수준으로 승격시키기 전에 먼저 모든 $N = \eta^K$ 구성을 평가해야 합니다.</p>
<p>어떤 분산 시스템에서든 동기화는 일반적으로 작업자의 유휴 시간을 의미합니다. 첫째, 하이퍼파라미터 구성 전반에 걸쳐 훈련 시간에 큰 차이가 종종 관찰됩니다. 예를 들어 레이어당 필터 수가 하이퍼파라미터라고 가정하면 필터가 적은 네트워크는 필터가 많은 네트워크보다 훈련이 더 빨리 끝나므로 낙오자로 인해 유휴 작업자 시간이 발생합니다. 또한 렁 수준의 슬롯 수가 항상 작업자 수의 배수는 아니며, 이 경우 일부 작업자는 전체 배치 동안 유휴 상태일 수도 있습니다.</p>
<p>그림 :numref:<code>synchronous_sh</code>는 두 작업자가 있는 4개의 서로 다른 시험에 대해 $\eta=2$인 동기식 SH의 스케줄링을 보여줍니다. Trial-0과 Trial-1을 1 에포크 동안 평가하는 것으로 시작하고 완료되면 즉시 다음 두 시험을 계속합니다. 다른 시험보다 상당히 많은 시간이 걸리는 Trial-2가 완료될 때까지 기다려야 상위 두 시험, 즉 Trial-0과 Trial-3을 다음 렁 수준으로 승격시킬 수 있습니다. 이로 인해 Worker-1에 유휴 시간이 발생합니다. 그런 다음 렁 1을 계속합니다. 여기에서도 Trial-3이 Trial-0보다 오래 걸리므로 Worker-0의 추가 유휴 시간이 발생합니다. 렁 2에 도달하면 가장 좋은 시험인 Trial-0만 남게 되어 한 명의 작업자만 점유합니다. 그 시간 동안 Worker-1이 유휴 상태가 되는 것을 피하기 위해 대부분의 SH 구현은 이미 다음 라운드를 계속하고 첫 번째 렁에서 새로운 시험(예: Trial-4)을 평가하기 시작합니다.</p>
<p><img src="chapter_hyperparameter-optimization/../img/sync_sh.svg" alt="두 작업자가 있는 동기식 연속 반감." />
:label:<code>synchronous_sh</code></p>
<p>비동기 연속 반감(ASHA) :cite:<code>li-arxiv18</code>은 SH를 비동기 병렬 시나리오에 적응시킵니다. ASHA의 주요 아이디어는 현재 렁 수준에서 최소 $\eta$개의 관찰을 수집하는 즉시 구성을 다음 렁 수준으로 승격시키는 것입니다. 이 결정 규칙은 차선책으로 승격될 수 있습니다. 구성이 다음 렁 수준으로 승격될 수 있지만, 나중에 돌이켜보면 같은 렁 수준의 다른 대부분의 구성과 비교하여 유리하지 않을 수 있습니다. 반면에 우리는 이런 식으로 모든 동기화 지점을 제거합니다. 실제로 이러한 차선책 초기 승격은 성능에 미미한 영향을 미칩니다. 하이퍼파라미터 구성의 순위가 렁 수준 전체에서 상당히 일관될 뿐만 아니라 렁이 시간이 지남에 따라 커지고 이 수준의 메트릭 값 분포를 점점 더 잘 반영하기 때문입니다. 작업자가 비어 있지만 승격할 수 있는 구성이 없는 경우 $r = r_{\mathrm{min}}$, 즉 첫 번째 렁 수준에서 새 구성을 시작합니다.</p>
<p>:numref:<code>asha</code>는 ASHA에 대한 동일한 구성의 스케줄링을 보여줍니다. Trial-1이 완료되면 두 시험(즉, Trial-0과 Trial-1)의 결과를 수집하고 그중 더 나은 것(Trial-0)을 즉시 다음 렁 수준으로 승격시킵니다. Trial-0이 렁 1에서 완료된 후에는 추가 승격을 지원하기에는 그곳에 시험이 너무 적습니다. 따라서 렁 0을 계속하고 Trial-3을 평가합니다. Trial-3이 완료되면 Trial-2는 아직 보류 중입니다. 이 시점에서 우리는 렁 0에서 3개의 시험을 평가했고 렁 1에서 하나의 시험을 이미 평가했습니다. Trial-3이 렁 0에서 Trial-0보다 성능이 나쁘고 $\eta=2$이므로 아직 새로운 시험을 승격시킬 수 없으며 Worker-1은 대신 처음부터 Trial-4를 시작합니다. 그러나 Trial-2가 완료되고 Trial-3보다 점수가 나쁘면 후자는 렁 1로 승격됩니다. 그 후 렁 1에서 2개의 평가를 수집했으므로 이제 Trial-0을 렁 2로 승격시킬 수 있습니다. 동시에 Worker-1은 렁 0에서 새로운 시험(즉, Trial-5)을 계속 평가합니다.</p>
<p><img src="chapter_hyperparameter-optimization/../img/asha.svg" alt="두 작업자가 있는 비동기 연속 반감(ASHA)." />
:label:<code>asha</code></p>
<pre><code class="language-{.python .input}">from d2l import torch as d2l
import logging
logging.basicConfig(level=logging.INFO)
import matplotlib.pyplot as plt
from syne_tune.config_space import loguniform, randint
from syne_tune.backend.python_backend import PythonBackend
from syne_tune.optimizer.baselines import ASHA
from syne_tune import Tuner, StoppingCriterion
from syne_tune.experiments import load_experiment
</code></pre>
<h2 id="목적-함수-1"><a class="header" href="#목적-함수-1">목적 함수</a></h2>
<p>우리는 :numref:<code>sec_rs_async</code>와 동일한 목적 함수로 <em>Syne Tune</em>을 사용할 것입니다.</p>
<pre><code class="language-{.python .input  n=54}">def hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):
    from d2l import torch as d2l
    from syne_tune import Reporter

    model = d2l.LeNet(lr=learning_rate, num_classes=10)
    trainer = d2l.HPOTrainer(max_epochs=1, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=batch_size)
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
    report = Reporter()
    for epoch in range(1, max_epochs + 1):
        if epoch == 1:
            # Trainer 상태 초기화
            trainer.fit(model=model, data=data)
        else:
            trainer.fit_epoch()
        validation_error = d2l.numpy(trainer.validation_error().cpu())
        report(epoch=epoch, validation_error=float(validation_error))
</code></pre>
<p>이전과 동일한 구성 공간을 사용합니다.</p>
<pre><code class="language-{.python .input  n=55}">min_number_of_epochs = 2
max_number_of_epochs = 10
eta = 2

config_space = {
    "learning_rate": loguniform(1e-2, 1),
    "batch_size": randint(32, 256),
    "max_epochs": max_number_of_epochs,
}
initial_config = {
    "learning_rate": 0.1,
    "batch_size": 128,
}
</code></pre>
<h2 id="비동기-스케줄러-1"><a class="header" href="#비동기-스케줄러-1">비동기 스케줄러</a></h2>
<p>먼저 시험을 동시에 평가하는 작업자 수를 정의합니다. 또한 전체 벽시계 시간에 대한 상한을 정의하여 무작위 검색을 실행할 기간을 지정해야 합니다.</p>
<pre><code class="language-{.python .input  n=56}">n_workers = 2  # 사용 가능한 GPU 수보다 작거나 같아야 함
max_wallclock_time = 12 * 60  # 12분
</code></pre>
<p>ASHA를 실행하는 코드는 비동기 무작위 검색에 대해 했던 것의 간단한 변형입니다.</p>
<pre><code class="language-{.python .input  n=56}">mode = "min"
metric = "validation_error"
resource_attr = "epoch"

scheduler = ASHA(
    config_space,
    metric=metric,
    mode=mode,
    points_to_evaluate=[initial_config],
    max_resource_attr="max_epochs",
    resource_attr=resource_attr,
    grace_period=min_number_of_epochs,
    reduction_factor=eta,
)
</code></pre>
<p>여기서 <code>metric</code>과 <code>resource_attr</code>은 <code>report</code> 콜백과 함께 사용되는 키 이름을 지정하고 <code>max_resource_attr</code>은 목적 함수에 대한 어떤 입력이 $r_{\mathrm{max}}$에 해당하는지 나타냅니다. 또한 <code>grace_period</code>는 $r_{\mathrm{min}}$을 제공하고 <code>reduction_factor</code>는 $\eta$입니다. 이전과 같이 Syne Tune을 실행할 수 있습니다(약 12분 소요).</p>
<pre><code class="language-{.python .input  n=57}">trial_backend = PythonBackend(
    tune_function=hpo_objective_lenet_synetune,
    config_space=config_space,
)

stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)
tuner = Tuner(
    trial_backend=trial_backend,
    scheduler=scheduler,
    stop_criterion=stop_criterion,
    n_workers=n_workers,
    print_update_interval=int(max_wallclock_time * 0.6),
)
tuner.run()
</code></pre>
<p>우리는 실적이 저조한 시험을 조기에 중지하는 ASHA 변형을 실행하고 있다는 점에 유의하십시오. 이는 각 훈련 작업이 고정된 <code>max_epochs</code>로 시작되는 :numref:<code>sec_mf_hpo_sh</code>의 구현과 다릅니다. 후자의 경우 전체 10 에포크에 도달하는 성능이 좋은 시험은 먼저 1, 그 다음 2, 4, 8 에포크를 훈련해야 하며 매번 처음부터 시작해야 합니다. 이러한 유형의 일시 중지 및 재개 스케줄링은 각 에포크 후 훈련 상태를 체크포인팅하여 효율적으로 구현할 수 있지만 여기서는 이 추가 복잡성을 피합니다. 실험이 완료된 후 결과를 검색하고 플로팅할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=59}">d2l.set_figsize()
e = load_experiment(tuner.name)
e.plot()
</code></pre>
<h2 id="최적화-프로세스-시각화"><a class="header" href="#최적화-프로세스-시각화">최적화 프로세스 시각화</a></h2>
<p>다시 한 번 모든 시험의 학습 곡선(플롯의 각 색상은 시험을 나타냄)을 시각화합니다. 이를 :numref:<code>sec_rs_async</code>의 비동기 무작위 검색과 비교해 보십시오. :numref:<code>sec_mf_hpo</code>의 연속 반감에서 보았듯이 대부분의 시험은 1 또는 2 에포크($r_{\mathrm{min}}$ 또는 $\eta * r_{\mathrm{min}}$)에서 중지됩니다. 그러나 에포크당 다른 시간이 필요하기 때문에 시험이 같은 지점에서 중지되지는 않습니다. ASHA 대신 표준 연속 반감을 실행했다면 구성을 다음 렁 수준으로 승격시키기 전에 작업자를 동기화해야 했을 것입니다.</p>
<pre><code class="language-{.python .input  n=60}">d2l.set_figsize([6, 2.5])
results = e.results
for trial_id in results.trial_id.unique():
    df = results[results["trial_id"] == trial_id]
    d2l.plt.plot(
        df["st_tuner_time"],
        df["validation_error"],
        marker="o"
    )
d2l.plt.xlabel("wall-clock time")
d2l.plt.ylabel("objective function")
</code></pre>
<h2 id="요약-7"><a class="header" href="#요약-7">요약</a></h2>
<p>무작위 검색에 비해 연속 반감은 비동기 분산 설정에서 실행하기가 그리 간단하지 않습니다. 동기화 지점을 피하기 위해 구성을 가능한 한 빨리 다음 렁 수준으로 승격시키며, 이는 일부 잘못된 구성을 승격시키는 것을 의미하더라도 마찬가지입니다. 실제로 이것은 대개 큰 해가 되지 않으며, 비동기식 대 동기식 스케줄링의 이점은 일반적으로 차선책 의사 결정의 손실보다 훨씬 큽니다.</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/12101">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="생성적-적대-신경망-generative-adversarial-networks"><a class="header" href="#생성적-적대-신경망-generative-adversarial-networks">생성적 적대 신경망 (Generative Adversarial Networks)</a></h1>
<p>:label:<code>chap_gans</code></p>
<pre><code class="language-toc">:maxdepth: 2

gan
dcgan
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="생성적-적대-신경망-generative-adversarial-networks-1"><a class="header" href="#생성적-적대-신경망-generative-adversarial-networks-1">생성적 적대 신경망 (Generative Adversarial Networks)</a></h1>
<p>:label:<code>sec_basic_gan</code></p>
<p>이 책의 대부분에서 우리는 예측을 하는 방법에 대해 이야기했습니다. 어떤 형태로든 심층 신경망을 사용하여 데이터 예제에서 레이블로의 매핑을 학습했습니다. 이러한 종류의 학습을 판별 학습(discriminative learning)이라고 합니다. 즉, 고양이 사진과 개 사진을 구별할 수 있기를 원합니다. 분류기와 회귀 분석기는 모두 판별 학습의 예입니다. 그리고 역전파로 훈련된 신경망은 크고 복잡한 데이터셋에 대한 판별 학습에 대해 우리가 알고 있던 모든 것을 뒤집어 놓았습니다. 고해상도 이미지의 분류 정확도는 불과 5-6년 만에 쓸모없는 수준에서 인간 수준(몇 가지 주의 사항이 있음)으로 올라갔습니다. 심층 신경망이 놀랍도록 잘 수행하는 다른 모든 판별 작업에 대한 또 다른 이야기는 생략하겠습니다.</p>
<p>그러나 머신러닝에는 단순히 판별 작업을 해결하는 것 이상의 것이 있습니다. 예를 들어, 레이블 없이 대규모 데이터셋이 주어졌을 때, 이 데이터의 특성을 간결하게 포착하는 모델을 학습하고 싶을 수 있습니다. 그러한 모델이 주어지면 훈련 데이터의 분포와 유사한 합성 데이터 예제를 샘플링할 수 있습니다. 예를 들어, 얼굴 사진의 대규모 코퍼스가 주어졌을 때, 우리는 동일한 데이터셋에서 나온 것처럼 그럴듯해 보이는 새로운 사실적인 이미지를 생성할 수 있기를 원할 수 있습니다. 이러한 종류의 학습을 생성 모델링(generative modeling)이라고 합니다.</p>
<p>최근까지 우리는 새로운 사실적인 이미지를 합성할 수 있는 방법이 없었습니다. 그러나 판별 학습을 위한 심층 신경망의 성공은 새로운 가능성을 열었습니다. 지난 3년 동안의 큰 트렌드 중 하나는 일반적으로 지도 학습 문제로 생각하지 않는 문제의 어려움을 극복하기 위해 판별 심층 신경망을 적용하는 것이었습니다. 순환 신경망 언어 모델은 훈련된 후 생성 모델로 작동할 수 있는 판별 네트워크(다음 문자를 예측하도록 훈련됨)를 사용하는 한 예입니다.</p>
<p>2014년에 획기적인 논문이 생성적 적대 신경망(GAN) :cite:<code>Goodfellow.Pouget-Abadie.Mirza.ea.2014</code>을 소개했습니다. 이는 판별 모델의 힘을 활용하여 좋은 생성 모델을 얻는 영리한 새로운 방법입니다. GAN의 핵심은 가짜 데이터와 실제 데이터를 구별할 수 없다면 데이터 생성기가 좋다는 아이디어에 의존합니다. 통계학에서는 이것을 두 표본 검정(two-sample test)이라고 합니다. 즉, 데이터셋 $X={x_1,\ldots, x_n}$과 $X'={x'_1,\ldots, x'_n}$이 동일한 분포에서 추출되었는지 여부에 대한 질문에 답하는 검정입니다. 대부분의 통계 논문과 GAN의 주요 차이점은 후자가 이 아이디어를 건설적인 방식으로 사용한다는 것입니다. 즉, 단순히 "이봐, 이 두 데이터셋은 같은 분포에서 나온 것 같지 않아"라고 말하도록 모델을 훈련시키는 대신, <a href="https://en.wikipedia.org/wiki/Two-sample_hypothesis_testing">두 표본 검정</a>을 사용하여 생성 모델에 훈련 신호를 제공합니다. 이를 통해 실제 데이터와 유사한 것을 생성할 때까지 데이터 생성기를 개선할 수 있습니다. 적어도 분류기가 최첨단 심층 신경망이라 하더라도 분류기를 속여야 합니다.</p>
<p><img src="chapter_generative-adversarial-networks/../img/gan.svg" alt="생성적 적대 신경망" />
:label:<code>fig_gan</code></p>
<p>GAN 아키텍처는 :numref:<code>fig_gan</code>에 설명되어 있습니다.
보시다시피 GAN 아키텍처에는 두 가지 요소가 있습니다. 우선, 실제와 똑같이 보이는 데이터를 생성할 잠재력이 있는 장치(예: 딥 네트워크이지만 실제로는 게임 렌더링 엔진과 같은 무엇이든 될 수 있음)가 필요합니다. 이미지를 다루는 경우 이미지를 생성해야 합니다. 음성을 다루는 경우 오디오 시퀀스를 생성해야 하는 식입니다. 우리는 이것을 생성기(generator) 네트워크라고 부릅니다. 두 번째 구성 요소는 판별기(discriminator) 네트워크입니다. 가짜 데이터와 실제 데이터를 서로 구별하려고 시도합니다. 두 네트워크는 서로 경쟁합니다. 생성기 네트워크는 판별기 네트워크를 속이려고 시도합니다. 그 시점에서 판별기 네트워크는 새로운 가짜 데이터에 적응합니다. 이 정보는 차례로 생성기 네트워크를 개선하는 데 사용되는 식입니다.</p>
<p>판별기는 입력 $x$가 실제(실제 데이터에서)인지 가짜(생성기에서)인지 구별하는 이진 분류기입니다. 일반적으로 판별기는 입력 $\mathbf x$에 대해 스칼라 예측 $o\in\mathbb R$을 출력합니다. 예를 들어 은닉 크기가 1인 완전 연결 레이어를 사용한 다음 시그모이드 함수를 적용하여 예측 확률 $D(\mathbf x) = 1/(1+e^{-o})$를 얻습니다. 실제 데이터에 대한 레이블 $y$를 $1$, 가짜 데이터에 대한 레이블을 $0$이라고 가정합니다. 우리는 크로스 엔트로피 손실을 최소화하도록 판별기를 훈련합니다. 즉,</p>
<p>$$ \min_D { - y \log D(\mathbf x) - (1-y)\log(1-D(\mathbf x)) },
$$</p>
<p>생성기의 경우, 먼저 무작위 소스(예: 정규 분포 $\mathbf z \sim \mathcal N (0, 1)$)에서 일부 파라미터 $\mathbf z\in\mathbb R^d$를 추출합니다. 우리는 종종 $\mathbf z$를 잠재 변수(latent variable)라고 부릅니다.
그런 다음 함수를 적용하여 $\mathbf x'=G(\mathbf z)$를 생성합니다. 생성기의 목표는 판별기를 속여서 $\mathbf x'=G(\mathbf z)$를 실제 데이터로 분류하게 하는 것입니다. 즉, $D( G(\mathbf z)) \approx 1$이기를 원합니다.
즉, 주어진 판별기 $D$에 대해, $y=0$일 때 크로스 엔트로피 손실을 최대화하도록 생성기 $G$의 파라미터를 업데이트합니다. 즉,</p>
<p>$$ \max_G { - (1-y) \log(1-D(G(\mathbf z))) } = \max_G { - \log(1-D(G(\mathbf z))) }.
$$</p>
<p>생성기가 완벽하게 작동하면 $D(\mathbf x')\approx 1$이므로 위의 손실은 0에 가까워지며, 이는 판별기가 좋은 진전을 이루기에 너무 작은 기울기를 초래합니다. 따라서 일반적으로 다음 손실을 최소화합니다:</p>
<p>$$ \min_G { - y \log(D(G(\mathbf z))) } = \min_G { - \log(D(G(\mathbf z))) },
$$</p>
<p>이는 $\mathbf x'=G(\mathbf z)$를 판별기에 공급하지만 레이블 $y=1$을 주는 것입니다.</p>
<p>요약하자면, $D$와 $G$는 포괄적인 목적 함수를 가지고 "미니맥스(minimax)" 게임을 하고 있습니다:</p>
<p>$$\min_D \max_G { -E_{x \sim \textrm{Data}} \log D(\mathbf x) - E_{z \sim \textrm{Noise}} \log(1 - D(G(\mathbf z))) }.
$$</p>
<p>GAN 애플리케이션의 대부분은 이미지 맥락에 있습니다. 시연 목적으로, 우리는 먼저 훨씬 더 간단한 분포를 피팅하는 데 만족할 것입니다. 우리는 GAN을 사용하여 가우시안 파라미터에 대한 세계에서 가장 비효율적인 추정기를 구축하면 어떻게 되는지 설명할 것입니다. 시작해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<h2 id="실제-데이터-생성-generate-some-real-data"><a class="header" href="#실제-데이터-생성-generate-some-real-data">"실제" 데이터 생성 (Generate Some "Real" Data)</a></h2>
<p>이것은 세계에서 가장 형편없는 예가 될 것이므로, 단순히 가우시안에서 추출한 데이터를 생성합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet, pytorch
X = d2l.normal(0.0, 1, (1000, 2))
A = d2l.tensor([[1, 2], [-0.1, 0.5]])
b = d2l.tensor([1, 2])
data = d2l.matmul(X, A) + b
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
X = d2l.normal((1000, 2), 0.0, 1)
A = d2l.tensor([[1, 2], [-0.1, 0.5]])
b = d2l.tensor([1, 2], tf.float32)
data = d2l.matmul(X, A) + b
</code></pre>
<p>무엇을 얻었는지 봅시다. 이것은 평균 $b$와 공분산 행렬 $A^TA$를 사용하여 다소 임의적인 방식으로 이동된 가우시안이어야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet, pytorch
d2l.set_figsize()
d2l.plt.scatter(d2l.numpy(data[:100, 0]), d2l.numpy(data[:100, 1]));
print(f'The covariance matrix is\n{d2l.matmul(A.T, A)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
d2l.set_figsize()
d2l.plt.scatter(d2l.numpy(data[:100, 0]), d2l.numpy(data[:100, 1]));
print(f'The covariance matrix is\n{tf.matmul(A, A, transpose_a=True)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab all
batch_size = 8
data_iter = d2l.load_array((data,), batch_size)
</code></pre>
<h2 id="생성기-generator"><a class="header" href="#생성기-generator">생성기 (Generator)</a></h2>
<p>우리 생성기 네트워크는 가능한 가장 간단한 네트워크인 단일 레이어 선형 모델이 될 것입니다. 이는 우리가 가우시안 데이터 생성기로 해당 선형 네트워크를 구동할 것이기 때문입니다. 따라서 말 그대로 상황을 완벽하게 조작하기 위해 파라미터만 학습하면 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net_G = nn.Sequential()
net_G.add(nn.Dense(2))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net_G = nn.Sequential(nn.Linear(2, 2))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
net_G = tf.keras.layers.Dense(2)
</code></pre>
<h2 id="판별기-discriminator"><a class="header" href="#판별기-discriminator">판별기 (Discriminator)</a></h2>
<p>판별기의 경우 조금 더 구별력 있게 할 것입니다. 상황을 좀 더 흥미롭게 만들기 위해 3개의 레이어가 있는 MLP를 사용할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
net_D = nn.Sequential()
net_D.add(nn.Dense(5, activation='tanh'),
          nn.Dense(3, activation='tanh'),
          nn.Dense(1))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
net_D = nn.Sequential(
    nn.Linear(2, 5), nn.Tanh(),
    nn.Linear(5, 3), nn.Tanh(),
    nn.Linear(3, 1))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
net_D = tf.keras.models.Sequential([
    tf.keras.layers.Dense(5, activation="tanh", input_shape=(2,)),
    tf.keras.layers.Dense(3, activation="tanh"),
    tf.keras.layers.Dense(1)
])
</code></pre>
<h2 id="훈련-training-29"><a class="header" href="#훈련-training-29">훈련 (Training)</a></h2>
<p>먼저 판별기를 업데이트하는 함수를 정의합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def update_D(X, Z, net_D, net_G, loss, trainer_D):
    """판별기 업데이트."""
    batch_size = X.shape[0]
    ones = np.ones((batch_size,), ctx=X.ctx)
    zeros = np.zeros((batch_size,), ctx=X.ctx)
    with autograd.record():
        real_Y = net_D(X)
        fake_X = net_G(Z)
        # `net_G`에 대한 기울기를 계산할 필요가 없으므로, 기울기 계산에서 분리합니다.
        fake_Y = net_D(fake_X.detach())
        loss_D = (loss(real_Y, ones) + loss(fake_Y, zeros)) / 2
    loss_D.backward()
    trainer_D.step(batch_size)
    return float(loss_D.sum())
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def update_D(X, Z, net_D, net_G, loss, trainer_D):
    """판별기 업데이트."""
    batch_size = X.shape[0]
    ones = torch.ones((batch_size,), device=X.device)
    zeros = torch.zeros((batch_size,), device=X.device)
    trainer_D.zero_grad()
    real_Y = net_D(X)
    fake_X = net_G(Z)
    # `net_G`에 대한 기울기를 계산할 필요가 없으므로, 기울기 계산에서 분리합니다.
    fake_Y = net_D(fake_X.detach())
    loss_D = (loss(real_Y, ones.reshape(real_Y.shape)) +
              loss(fake_Y, zeros.reshape(fake_Y.shape))) / 2
    loss_D.backward()
    trainer_D.step()
    return loss_D
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
#@save
def update_D(X, Z, net_D, net_G, loss, optimizer_D):
    """판별기 업데이트."""
    batch_size = X.shape[0]
    ones = tf.ones((batch_size,)) # 실제 데이터에 해당하는 레이블
    zeros = tf.zeros((batch_size,)) # 가짜 데이터에 해당하는 레이블
    # `net_G`에 대한 기울기를 계산할 필요가 없으므로 GradientTape 외부에 있습니다
    fake_X = net_G(Z)
    with tf.GradientTape() as tape:
        real_Y = net_D(X)
        fake_Y = net_D(fake_X)
        # PyTorch의 BCEWithLogitsLoss와 일치시키기 위해 손실에 batch_size를 곱합니다
        loss_D = (loss(ones, tf.squeeze(real_Y)) + loss(
            zeros, tf.squeeze(fake_Y))) * batch_size / 2
    grads_D = tape.gradient(loss_D, net_D.trainable_variables)
    optimizer_D.apply_gradients(zip(grads_D, net_D.trainable_variables))
    return loss_D
</code></pre>
<p>생성기도 비슷하게 업데이트됩니다. 여기서는 크로스 엔트로피 손실을 재사용하지만 가짜 데이터의 레이블을 $0$에서 $1$로 변경합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
def update_G(Z, net_D, net_G, loss, trainer_G):
    """생성기 업데이트."""
    batch_size = Z.shape[0]
    ones = np.ones((batch_size,), ctx=Z.ctx)
    with autograd.record():
        # 계산을 절약하기 위해 `update_D`의 `fake_X`를 재사용할 수 있습니다
        fake_X = net_G(Z)
        # `net_D`가 변경되었으므로 `fake_Y` 재계산이 필요합니다
        fake_Y = net_D(fake_X)
        loss_G = loss(fake_Y, ones)
    loss_G.backward()
    trainer_G.step(batch_size)
    return float(loss_G.sum())
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
def update_G(Z, net_D, net_G, loss, trainer_G):
    """생성기 업데이트."""
    batch_size = Z.shape[0]
    ones = torch.ones((batch_size,), device=Z.device)
    trainer_G.zero_grad()
    # 계산을 절약하기 위해 `update_D`의 `fake_X`를 재사용할 수 있습니다
    fake_X = net_G(Z)
    # `net_D`가 변경되었으므로 `fake_Y` 재계산이 필요합니다
    fake_Y = net_D(fake_X)
    loss_G = loss(fake_Y, ones.reshape(fake_Y.shape))
    loss_G.backward()
    trainer_G.step()
    return loss_G
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
#@save
def update_G(Z, net_D, net_G, loss, optimizer_G):
    """생성기 업데이트."""
    batch_size = Z.shape[0]
    ones = tf.ones((batch_size,))
    with tf.GradientTape() as tape:
        # 계산을 절약하기 위해 `update_D`의 `fake_X`를 재사용할 수 있습니다
        fake_X = net_G(Z)
        # `net_D`가 변경되었으므로 `fake_Y` 재계산이 필요합니다
        fake_Y = net_D(fake_X)
        # PyTorch의 BCEWithLogits 손실과 일치시키기 위해 손실에 batch_size를 곱합니다
        loss_G = loss(ones, tf.squeeze(fake_Y)) * batch_size
    grads_G = tape.gradient(loss_G, net_G.trainable_variables)
    optimizer_G.apply_gradients(zip(grads_G, net_G.trainable_variables))
    return loss_G
</code></pre>
<p>판별기와 생성기 모두 크로스 엔트로피 손실을 사용하여 이진 로지스틱 회귀를 수행합니다. 훈련 과정을 원활하게 하기 위해 Adam을 사용합니다. 각 반복에서 먼저 판별기를 업데이트한 다음 생성기를 업데이트합니다. 손실과 생성된 예제를 모두 시각화합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):
    loss = gluon.loss.SigmoidBCELoss()
    net_D.initialize(init=init.Normal(0.02), force_reinit=True)
    net_G.initialize(init=init.Normal(0.02), force_reinit=True)
    trainer_D = gluon.Trainer(net_D.collect_params(),
                              'adam', {'learning_rate': lr_D})
    trainer_G = gluon.Trainer(net_G.collect_params(),
                              'adam', {'learning_rate': lr_G})
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
                            legend=['discriminator', 'generator'])
    animator.fig.subplots_adjust(hspace=0.3)
    for epoch in range(num_epochs):
        # 한 에포크 훈련
        timer = d2l.Timer()
        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples
        for X in data_iter:
            batch_size = X.shape[0]
            Z = np.random.normal(0, 1, size=(batch_size, latent_dim))
            metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),
                       update_G(Z, net_D, net_G, loss, trainer_G),
                       batch_size)
        # 생성된 예제 시각화
        Z = np.random.normal(0, 1, size=(100, latent_dim))
        fake_X = net_G(Z).asnumpy()
        animator.axes[1].cla()
        animator.axes[1].scatter(data[:, 0], data[:, 1])
        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])
        animator.axes[1].legend(['real', 'generated'])
        # 손실 표시
        loss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]
        animator.add(epoch + 1, (loss_D, loss_G))
    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, ' 
          f'{metric[2] / timer.stop():.1f} examples/sec')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):
    loss = nn.BCEWithLogitsLoss(reduction='sum')
    for w in net_D.parameters():
        nn.init.normal_(w, 0, 0.02)
    for w in net_G.parameters():
        nn.init.normal_(w, 0, 0.02)
    trainer_D = torch.optim.Adam(net_D.parameters(), lr=lr_D)
    trainer_G = torch.optim.Adam(net_G.parameters(), lr=lr_G)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
                            legend=['discriminator', 'generator'])
    animator.fig.subplots_adjust(hspace=0.3)
    for epoch in range(num_epochs):
        # 한 에포크 훈련
        timer = d2l.Timer()
        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples
        for (X,) in data_iter:
            batch_size = X.shape[0]
            Z = torch.normal(0, 1, size=(batch_size, latent_dim))
            metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),
                       update_G(Z, net_D, net_G, loss, trainer_G),
                       batch_size)
        # 생성된 예제 시각화
        Z = torch.normal(0, 1, size=(100, latent_dim))
        fake_X = net_G(Z).detach().numpy()
        animator.axes[1].cla()
        animator.axes[1].scatter(data[:, 0], data[:, 1])
        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])
        animator.axes[1].legend(['real', 'generated'])
        # 손실 표시
        loss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]
        animator.add(epoch + 1, (loss_D, loss_G))
    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, ' 
          f'{metric[2] / timer.stop():.1f} examples/sec')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):
    loss = tf.keras.losses.BinaryCrossentropy(
        from_logits=True, reduction=tf.keras.losses.Reduction.SUM)
    for w in net_D.trainable_variables:
        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))
    for w in net_G.trainable_variables:
        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))
    optimizer_D = tf.keras.optimizers.Adam(learning_rate=lr_D)
    optimizer_G = tf.keras.optimizers.Adam(learning_rate=lr_G)
    animator = d2l.Animator(
        xlabel="epoch", ylabel="loss", xlim=[1, num_epochs], nrows=2,
        figsize=(5, 5), legend=["discriminator", "generator"])
    animator.fig.subplots_adjust(hspace=0.3)
    for epoch in range(num_epochs):
        # 한 에포크 훈련
        timer = d2l.Timer()
        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples
        for (X,) in data_iter:
            batch_size = X.shape[0]
            Z = tf.random.normal(
                mean=0, stddev=1, shape=(batch_size, latent_dim))
            metric.add(update_D(X, Z, net_D, net_G, loss, optimizer_D),
                       update_G(Z, net_D, net_G, loss, optimizer_G),
                       batch_size)
        # 생성된 예제 시각화
        Z = tf.random.normal(mean=0, stddev=1, shape=(100, latent_dim))
        fake_X = net_G(Z)
        animator.axes[1].cla()
        animator.axes[1].scatter(data[:, 0], data[:, 1])
        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])
        animator.axes[1].legend(["real", "generated"])

        # 손실 표시
        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]
        animator.add(epoch + 1, (loss_D, loss_G))

    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, ' 
          f'{metric[2] / timer.stop():.1f} examples/sec')
</code></pre>
<p>이제 가우시안 분포를 피팅하기 위해 하이퍼파라미터를 지정합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
lr_D, lr_G, latent_dim, num_epochs = 0.05, 0.005, 2, 20
train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,
      latent_dim, d2l.numpy(data[:100]))
</code></pre>
<h2 id="요약-summary-105"><a class="header" href="#요약-summary-105">요약 (Summary)</a></h2>
<ul>
<li>생성적 적대 신경망(GAN)은 두 개의 심층 네트워크, 생성기와 판별기로 구성됩니다.</li>
<li>생성기는 크로스 엔트로피 손실을 최대화하여, 즉 $\max \log(D(\mathbf{x'}))$, 판별기를 속이기 위해 실제 이미지에 최대한 가까운 이미지를 생성합니다.</li>
<li>판별기는 크로스 엔트로피 손실을 최소화하여, 즉 $\min - y \log D(\mathbf{x}) - (1-y)\log(1-D(\mathbf{x}))$, 실제 이미지와 생성된 이미지를 구별하려고 합니다.</li>
</ul>
<h2 id="연습-문제-exercises-120"><a class="header" href="#연습-문제-exercises-120">연습 문제 (Exercises)</a></h2>
<ul>
<li>생성기가 이기는 평형, 즉 판별기가 유한한 샘플에서 두 분포를 구별할 수 없게 되는 평형이 존재합니까?</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/408">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1082">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="심층-합성곱-생성적-적대-신경망-deep-convolutional-generative-adversarial-networks"><a class="header" href="#심층-합성곱-생성적-적대-신경망-deep-convolutional-generative-adversarial-networks">심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></h1>
<p>:label:<code>sec_dcgan</code></p>
<p>:numref:<code>sec_basic_gan</code>에서 우리는 GAN이 작동하는 기본 아이디어를 소개했습니다. 우리는 GAN이 균일 분포나 정규 분포와 같이 간단하고 샘플링하기 쉬운 분포에서 샘플을 추출하고, 이를 일부 데이터셋의 분포와 일치하는 것처럼 보이는 샘플로 변환할 수 있음을 보여주었습니다. 2D 가우시안 분포를 일치시키는 예제는 요점을 전달했지만, 특별히 흥미롭지는 않았습니다.</p>
<p>이 섹션에서는 GAN을 사용하여 사실적인 이미지를 생성하는 방법을 보여줄 것입니다. 우리는 :citet:<code>Radford.Metz.Chintala.2015</code>에 소개된 심층 합성곱 GAN(DCGAN)을 기반으로 모델을 만들 것입니다. 우리는 판별적 컴퓨터 비전 문제에 매우 성공적인 것으로 입증된 합성곱 아키텍처를 차용하고, GAN을 통해 이를 활용하여 사실적인 이미지를 생성하는 방법을 보여줄 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn
from d2l import mxnet as d2l

npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch
import torchvision
from torch import nn
import warnings
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<h2 id="포켓몬-데이터셋-the-pokemon-dataset"><a class="header" href="#포켓몬-데이터셋-the-pokemon-dataset">포켓몬 데이터셋 (The Pokemon Dataset)</a></h2>
<p>우리가 사용할 데이터셋은 <a href="https://pokemondb.net/sprites">pokemondb</a>에서 얻은 포켓몬 스프라이트 모음입니다. 먼저 이 데이터셋을 다운로드하고 추출하여 로드합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
#@save
d2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',
                           'c065c0e2593b8b161a2d7873e42418bf6a21106c')

data_dir = d2l.download_extract('pokemon')
pokemon = gluon.data.vision.datasets.ImageFolderDataset(data_dir)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
#@save
d2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',
                           'c065c0e2593b8b161a2d7873e42418bf6a21106c')

data_dir = d2l.download_extract('pokemon')
pokemon = torchvision.datasets.ImageFolder(data_dir)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
#@save
d2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',
                           'c065c0e2593b8b161a2d7873e42418bf6a21106c')

data_dir = d2l.download_extract('pokemon')
batch_size = 256
pokemon = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir, batch_size=batch_size, image_size=(64, 64))
</code></pre>
<p>우리는 각 이미지의 크기를 $64\times 64$로 조정합니다. <code>ToTensor</code> 변환은 픽셀 값을 $[0, 1]$로 투영하는 반면, 생성기는 tanh 함수를 사용하여 $[-1, 1]$의 출력을 얻습니다. 따라서 값 범위를 일치시키기 위해 $0.5$ 평균과 $0.5$ 표준 편차로 데이터를 정규화합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
batch_size = 256
transformer = gluon.data.vision.transforms.Compose([
    gluon.data.vision.transforms.Resize(64),
    gluon.data.vision.transforms.ToTensor(),
    gluon.data.vision.transforms.Normalize(0.5, 0.5)
])
data_iter = gluon.data.DataLoader(
    pokemon.transform_first(transformer), batch_size=batch_size,
    shuffle=True, num_workers=d2l.get_dataloader_workers())
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
batch_size = 256
transformer = torchvision.transforms.Compose([
    torchvision.transforms.Resize((64, 64)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(0.5, 0.5)
])
pokemon.transform = transformer
data_iter = torch.utils.data.DataLoader(
    pokemon, batch_size=batch_size,
    shuffle=True, num_workers=d2l.get_dataloader_workers())
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def transform_func(X):
    X = X / 255.
    X = (X - 0.5) / (0.5)
    return X

# TF&gt;=2.4의 경우 `num_parallel_calls = tf.data.AUTOTUNE` 사용
data_iter = pokemon.map(lambda x, y: (transform_func(x), y),
                        num_parallel_calls=tf.data.experimental.AUTOTUNE)
data_iter = data_iter.cache().shuffle(buffer_size=1000).prefetch(
    buffer_size=tf.data.experimental.AUTOTUNE)
</code></pre>
<p>처음 20개의 이미지를 시각화해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
d2l.set_figsize((4, 4))
for X, y in data_iter:
    imgs = X[:20,:,:,:].transpose(0, 2, 3, 1)/2+0.5
    d2l.show_images(imgs, num_rows=4, num_cols=5)
    break
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
warnings.filterwarnings('ignore')
d2l.set_figsize((4, 4))
for X, y in data_iter:
    imgs = X[:20,:,:,:].permute(0, 2, 3, 1)/2+0.5
    d2l.show_images(imgs, num_rows=4, num_cols=5)
    break
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
d2l.set_figsize(figsize=(4, 4))
for X, y in data_iter.take(1):
    imgs = X[:20, :, :, :] / 2 + 0.5
    d2l.show_images(imgs, num_rows=4, num_cols=5)
</code></pre>
<h2 id="생성기-the-generator"><a class="header" href="#생성기-the-generator">생성기 (The Generator)</a></h2>
<p>생성기는 길이 $d$ 벡터인 노이즈 변수 $\mathbf z\in\mathbb R^d$를 너비와 높이가 $64\times 64$인 RGB 이미지로 매핑해야 합니다. :numref:<code>sec_fcn</code>에서 우리는 전치 합성곱 레이어(:numref:<code>sec_transposed_conv</code> 참조)를 사용하여 입력 크기를 확대하는 완전 합성곱 신경망을 소개했습니다. 생성기의 기본 블록에는 전치 합성곱 레이어와 배치 정규화 및 ReLU 활성화가 포함됩니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class G_block(nn.Block):
    def __init__(self, channels, kernel_size=4,
                 strides=2, padding=1, **kwargs):
        super(G_block, self).__init__(**kwargs)
        self.conv2d_trans = nn.Conv2DTranspose(
            channels, kernel_size, strides, padding, use_bias=False)
        self.batch_norm = nn.BatchNorm()
        self.activation = nn.Activation('relu')

    def forward(self, X):
        return self.activation(self.batch_norm(self.conv2d_trans(X)))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class G_block(nn.Module):
    def __init__(self, out_channels, in_channels=3, kernel_size=4, strides=2,
                 padding=1, **kwargs):
        super(G_block, self).__init__(**kwargs)
        self.conv2d_trans = nn.ConvTranspose2d(in_channels, out_channels,
                                kernel_size, strides, padding, bias=False)
        self.batch_norm = nn.BatchNorm2d(out_channels)
        self.activation = nn.ReLU()

    def forward(self, X):
        return self.activation(self.batch_norm(self.conv2d_trans(X)))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
class G_block(tf.keras.layers.Layer):
    def __init__(self, out_channels, kernel_size=4, strides=2, padding="same",
                 **kwargs):
        super().__init__(**kwargs)
        self.conv2d_trans = tf.keras.layers.Conv2DTranspose(
            out_channels, kernel_size, strides, padding, use_bias=False)
        self.batch_norm = tf.keras.layers.BatchNormalization()
        self.activation = tf.keras.layers.ReLU()

    def call(self, X):
        return self.activation(self.batch_norm(self.conv2d_trans(X)))
</code></pre>
<p>기본적으로 전치 합성곱 레이어는 $k_h = k_w = 4$ 커널, $s_h = s_w = 2$ 스트라이드, $p_h = p_w = 1$ 패딩을 사용합니다. 입력 모양이 $n_h^{'} \times n_w^{'} = 16 \times 16$인 경우, 생성기 블록은 입력의 너비와 높이를 두 배로 늘립니다.</p>
<p>$$
\begin{aligned}
n_h^{'} \times n_w^{'} &amp;= [(n_h k_h - (n_h-1)(k_h-s_h)- 2p_h] \times [(n_w k_w - (n_w-1)(k_w-s_w)- 2p_w]<br />
&amp;= [(k_h + s_h (n_h-1)- 2p_h] \times [(k_w + s_w (n_w-1)- 2p_w]<br />
&amp;= [(4 + 2 \times (16-1)- 2 \times 1] \times [(4 + 2 \times (16-1)- 2 \times 1]<br />
&amp;= 32 \times 32 .
\end{aligned}
$$</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.zeros((2, 3, 16, 16))
g_blk = G_block(20)
g_blk.initialize()
g_blk(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.zeros((2, 3, 16, 16))
g_blk = G_block(20)
g_blk(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.zeros((2, 16, 16, 3))  # 채널 마지막 규칙
g_blk = G_block(20)
g_blk(x).shape
</code></pre>
<p>전치 합성곱 레이어를 $4\times 4$ 커널, $1\times 1$ 스트라이드, 0 패딩으로 변경하면, 입력 크기가 $1 \times 1$일 때 출력의 너비와 높이가 각각 3씩 증가합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.zeros((2, 3, 1, 1))
g_blk = G_block(20, strides=1, padding=0)
g_blk.initialize()
g_blk(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.zeros((2, 3, 1, 1))
g_blk = G_block(20, strides=1, padding=0)
g_blk(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.zeros((2, 1, 1, 3))
# `padding="valid"`는 패딩 없음에 해당합니다
g_blk = G_block(20, strides=1, padding="valid")
g_blk(x).shape
</code></pre>
<p>생성기는 입력의 너비와 높이를 1에서 32로 늘리는 4개의 기본 블록으로 구성됩니다. 동시에, 먼저 잠재 변수를 $64\times 8$ 채널로 투영한 다음 매번 채널을 반으로 줄입니다. 마지막으로, 전치 합성곱 레이어를 사용하여 출력을 생성합니다. 원하는 $64\times 64$ 모양과 일치하도록 너비와 높이를 두 배로 늘리고 채널 크기를 $3$으로 줄입니다. tanh 활성화 함수를 적용하여 출력 값을 $(-1, 1)$ 범위로 투영합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
n_G = 64
net_G = nn.Sequential()
net_G.add(G_block(n_G*8, strides=1, padding=0),  # 출력: (64 * 8, 4, 4)
          G_block(n_G*4),  # 출력: (64 * 4, 8, 8)
          G_block(n_G*2),  # 출력: (64 * 2, 16, 16)
          G_block(n_G),    # 출력: (64, 32, 32)
          nn.Conv2DTranspose(
              3, kernel_size=4, strides=2, padding=1, use_bias=False,
              activation='tanh'))  # 출력: (3, 64, 64)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
n_G = 64
net_G = nn.Sequential(
    G_block(in_channels=100, out_channels=n_G*8,
            strides=1, padding=0),                  # 출력: (64 * 8, 4, 4)
    G_block(in_channels=n_G*8, out_channels=n_G*4), # 출력: (64 * 4, 8, 8)
    G_block(in_channels=n_G*4, out_channels=n_G*2), # 출력: (64 * 2, 16, 16)
    G_block(in_channels=n_G*2, out_channels=n_G),   # 출력: (64, 32, 32)
    nn.ConvTranspose2d(in_channels=n_G, out_channels=3,
                       kernel_size=4, stride=2, padding=1, bias=False),
    nn.Tanh())  # 출력: (3, 64, 64)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
n_G = 64
net_G = tf.keras.Sequential([
    # 출력: (4, 4, 64 * 8)
    G_block(out_channels=n_G*8, strides=1, padding="valid"),
    G_block(out_channels=n_G*4), # 출력: (8, 8, 64 * 4)
    G_block(out_channels=n_G*2), # 출력: (16, 16, 64 * 2)
    G_block(out_channels=n_G), # 출력: (32, 32, 64)
    # 출력: (64, 64, 3)
    tf.keras.layers.Conv2DTranspose(
        3, kernel_size=4, strides=2, padding="same", use_bias=False,
        activation="tanh")
])
</code></pre>
<p>100차원 잠재 변수를 생성하여 생성기의 출력 모양을 확인합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.zeros((1, 100, 1, 1))
net_G.initialize()
net_G(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.zeros((1, 100, 1, 1))
net_G(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.zeros((1, 1, 1, 100))
net_G(x).shape
</code></pre>
<h2 id="판별기-discriminator-1"><a class="header" href="#판별기-discriminator-1">판별기 (Discriminator)</a></h2>
<p>판별기는 Leaky ReLU를 활성화 함수로 사용한다는 점을 제외하면 일반적인 합성곱 신경망입니다. $\alpha \in[0, 1]$이 주어지면 정의는 다음과 같습니다.</p>
<p>$$\textrm{leaky ReLU}(x) = \begin{cases}x &amp; \textrm{if}\ x &gt; 0\ \alpha x &amp;\textrm{otherwise}\end{cases}.$$</p>
<p>보시다시피 $\alpha=0$이면 일반 ReLU이고, $\alpha=1$이면 항등 함수입니다. $\alpha \in (0, 1)$인 경우, Leaky ReLU는 음수 입력에 대해 0이 아닌 출력을 제공하는 비선형 함수입니다. 이는 뉴런이 항상 음수 값을 출력하여 ReLU의 기울기가 0이 되어 진행할 수 없는 "죽어가는 ReLU(dying ReLU)" 문제를 해결하는 것을 목표로 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet,pytorch
alphas = [0, .2, .4, .6, .8, 1]
x = d2l.arange(-2, 1, 0.1)
Y = [d2l.numpy(nn.LeakyReLU(alpha)(x)) for alpha in alphas]
d2l.plot(d2l.numpy(x), Y, 'x', 'y', alphas)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
alphas = [0, .2, .4, .6, .8, 1]
x = tf.range(-2, 1, 0.1)
Y = [tf.keras.layers.LeakyReLU(alpha)(x).numpy() for alpha in alphas]
d2l.plot(x.numpy(), Y, 'x', 'y', alphas)
</code></pre>
<p>판별기의 기본 블록은 합성곱 레이어와 배치 정규화 레이어, 그리고 Leaky ReLU 활성화가 뒤따르는 구조입니다. 합성곱 레이어의 하이퍼파라미터는 생성기 블록의 전치 합성곱 레이어와 유사합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
class D_block(nn.Block):
    def __init__(self, channels, kernel_size=4, strides=2,
                 padding=1, alpha=0.2, **kwargs):
        super(D_block, self).__init__(**kwargs)
        self.conv2d = nn.Conv2D(
            channels, kernel_size, strides, padding, use_bias=False)
        self.batch_norm = nn.BatchNorm()
        self.activation = nn.LeakyReLU(alpha)

    def forward(self, X):
        return self.activation(self.batch_norm(self.conv2d(X)))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
class D_block(nn.Module):
    def __init__(self, out_channels, in_channels=3, kernel_size=4, strides=2,
                padding=1, alpha=0.2, **kwargs):
        super(D_block, self).__init__(**kwargs)
        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size,
                                strides, padding, bias=False)
        self.batch_norm = nn.BatchNorm2d(out_channels)
        self.activation = nn.LeakyReLU(alpha, inplace=True)

    def forward(self, X):
        return self.activation(self.batch_norm(self.conv2d(X)))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
class D_block(tf.keras.layers.Layer):
    def __init__(self, out_channels, kernel_size=4, strides=2, padding="same",
                 alpha=0.2, **kwargs):
        super().__init__(**kwargs)
        self.conv2d = tf.keras.layers.Conv2D(out_channels, kernel_size,
                                             strides, padding, use_bias=False)
        self.batch_norm = tf.keras.layers.BatchNormalization()
        self.activation = tf.keras.layers.LeakyReLU(alpha)

    def call(self, X):
        return self.activation(self.batch_norm(self.conv2d(X)))
</code></pre>
<p>기본 설정이 있는 기본 블록은 :numref:<code>sec_padding</code>에서 시연한 것처럼 입력의 너비와 높이를 반으로 줄입니다. 예를 들어, 입력 모양 $n_h = n_w = 16$, 커널 모양 $k_h = k_w = 4$, 스트라이드 모양 $s_h = s_w = 2$, 패딩 모양 $p_h = p_w = 1$이 주어지면 출력 모양은 다음과 같습니다:</p>
<p>$$
\begin{aligned}
n_h^{'} \times n_w^{'} &amp;= \lfloor(n_h-k_h+2p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+2p_w+s_w)/s_w\rfloor\
&amp;= \lfloor(16-4+2\times 1+2)/2\rfloor \times \lfloor(16-4+2\times 1+2)/2\rfloor\
&amp;= 8 \times 8 .
\end{aligned}
$$</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.zeros((2, 3, 16, 16))
d_blk = D_block(20)
d_blk.initialize()
d_blk(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.zeros((2, 3, 16, 16))
d_blk = D_block(20)
d_blk(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.zeros((2, 16, 16, 3))
d_blk = D_block(20)
d_blk(x).shape
</code></pre>
<p>판별기는 생성기의 거울입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
n_D = 64
net_D = nn.Sequential()
net_D.add(D_block(n_D),   # 출력: (64, 32, 32)
          D_block(n_D*2),  # 출력: (64 * 2, 16, 16)
          D_block(n_D*4),  # 출력: (64 * 4, 8, 8)
          D_block(n_D*8),  # 출력: (64 * 8, 4, 4)
          nn.Conv2D(1, kernel_size=4, use_bias=False))  # 출력: (1, 1, 1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
n_D = 64
net_D = nn.Sequential(
    D_block(n_D),  # 출력: (64, 32, 32)
    D_block(in_channels=n_D, out_channels=n_D*2),  # 출력: (64 * 2, 16, 16)
    D_block(in_channels=n_D*2, out_channels=n_D*4),  # 출력: (64 * 4, 8, 8)
    D_block(in_channels=n_D*4, out_channels=n_D*8),  # 출력: (64 * 8, 4, 4)
    nn.Conv2d(in_channels=n_D*8, out_channels=1,
              kernel_size=4, bias=False))  # 출력: (1, 1, 1)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
n_D = 64
net_D = tf.keras.Sequential([
    D_block(n_D), # 출력: (32, 32, 64)
    D_block(out_channels=n_D*2), # 출력: (16, 16, 64 * 2)
    D_block(out_channels=n_D*4), # 출력: (8, 8, 64 * 4)
    D_block(out_channels=n_D*8), # 출력: (4, 4, 64 * 64)
    # 출력: (1, 1, 1)
    tf.keras.layers.Conv2D(1, kernel_size=4, use_bias=False)
])
</code></pre>
<p>마지막 레이어로 출력 채널이 $1$인 합성곱 레이어를 사용하여 단일 예측 값을 얻습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.zeros((1, 3, 64, 64))
net_D.initialize()
net_D(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.zeros((1, 3, 64, 64))
net_D(x).shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.zeros((1, 64, 64, 3))
net_D(x).shape
</code></pre>
<h2 id="훈련-training-30"><a class="header" href="#훈련-training-30">훈련 (Training)</a></h2>
<p>:numref:<code>sec_basic_gan</code>의 기본 GAN과 비교하여, 생성기와 판별기가 서로 비슷하기 때문에 두 모델에 대해 동일한 학습률을 사용합니다. 또한 Adam(:numref:<code>sec_adam</code>)의 $\beta_1$을 $0.9$에서 $0.5$로 변경합니다. 이는 생성기와 판별기가 서로 싸우기 때문에 급격하게 변하는 기울기를 처리하기 위해 과거 기울기의 지수 가중 이동 평균인 운동량의 부드러움을 줄입니다. 또한 무작위로 생성된 노이즈 <code>Z</code>는 4D 텐서이며 계산 속도를 높이기 위해 GPU를 사용하고 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,
          device=d2l.try_gpu()):
    loss = gluon.loss.SigmoidBCELoss()
    net_D.initialize(init=init.Normal(0.02), force_reinit=True, ctx=device)
    net_G.initialize(init=init.Normal(0.02), force_reinit=True, ctx=device)
    trainer_hp = {'learning_rate': lr, 'beta1': 0.5}
    trainer_D = gluon.Trainer(net_D.collect_params(), 'adam', trainer_hp)
    trainer_G = gluon.Trainer(net_G.collect_params(), 'adam', trainer_hp)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
                            legend=['discriminator', 'generator'])
    animator.fig.subplots_adjust(hspace=0.3)
    for epoch in range(1, num_epochs + 1):
        # 한 에포크 훈련
        timer = d2l.Timer()
        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples
        for X, _ in data_iter:
            batch_size = X.shape[0]
            Z = np.random.normal(0, 1, size=(batch_size, latent_dim, 1, 1))
            X, Z = X.as_in_ctx(device), Z.as_in_ctx(device),
            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),
                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),
                       batch_size)
        # 생성된 예제 표시
        Z = np.random.normal(0, 1, size=(21, latent_dim, 1, 1), ctx=device)
        # 합성 데이터를 N(0, 1)로 정규화
        fake_x = net_G(Z).transpose(0, 2, 3, 1) / 2 + 0.5
        imgs = np.concatenate(
            [np.concatenate([fake_x[i * 7 + j] for j in range(7)], axis=1)
             for i in range(len(fake_x)//7)], axis=0)
        animator.axes[1].cla()
        animator.axes[1].imshow(imgs.asnumpy())
        # 손실 표시
        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]
        animator.add(epoch, (loss_D, loss_G))
    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, ' 
          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,
          device=d2l.try_gpu()):
    loss = nn.BCEWithLogitsLoss(reduction='sum')
    for w in net_D.parameters():
        nn.init.normal_(w, 0, 0.02)
    for w in net_G.parameters():
        nn.init.normal_(w, 0, 0.02)
    net_D, net_G = net_D.to(device), net_G.to(device)
    trainer_hp = {'lr': lr, 'betas': [0.5,0.999]}
    trainer_D = torch.optim.Adam(net_D.parameters(), **trainer_hp)
    trainer_G = torch.optim.Adam(net_G.parameters(), **trainer_hp)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
                            legend=['discriminator', 'generator'])
    animator.fig.subplots_adjust(hspace=0.3)
    for epoch in range(1, num_epochs + 1):
        # 한 에포크 훈련
        timer = d2l.Timer()
        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples
        for X, _ in data_iter:
            batch_size = X.shape[0]
            Z = torch.normal(0, 1, size=(batch_size, latent_dim, 1, 1))
            X, Z = X.to(device), Z.to(device)
            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),
                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),
                       batch_size)
        # 생성된 예제 표시
        Z = torch.normal(0, 1, size=(21, latent_dim, 1, 1), device=device)
        # 합성 데이터를 N(0, 1)로 정규화
        fake_x = net_G(Z).permute(0, 2, 3, 1) / 2 + 0.5
        imgs = torch.cat(
            [torch.cat([
                fake_x[i * 7 + j].cpu().detach() for j in range(7)], dim=1)
             for i in range(len(fake_x)//7)], dim=0)
        animator.axes[1].cla()
        animator.axes[1].imshow(imgs)
        # 손실 표시
        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]
        animator.add(epoch, (loss_D, loss_G))
    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, ' 
          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,
          device=d2l.try_gpu()):
    loss = tf.keras.losses.BinaryCrossentropy(
        from_logits=True, reduction=tf.keras.losses.Reduction.SUM)

    for w in net_D.trainable_variables:
        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))
    for w in net_G.trainable_variables:
        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))

    optimizer_hp = {"lr": lr, "beta_1": 0.5, "beta_2": 0.999}
    optimizer_D = tf.keras.optimizers.Adam(**optimizer_hp)
    optimizer_G = tf.keras.optimizers.Adam(**optimizer_hp)

    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
                            legend=['discriminator', 'generator'])
    animator.fig.subplots_adjust(hspace=0.3)

    for epoch in range(1, num_epochs + 1):
        # 한 에포크 훈련
        timer = d2l.Timer()
        metric = d2l.Accumulator(3) # loss_D, loss_G, num_examples
        for X, _ in data_iter:
            batch_size = X.shape[0]
            Z = tf.random.normal(mean=0, stddev=1,
                                 shape=(batch_size, 1, 1, latent_dim))
            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, optimizer_D),
                       d2l.update_G(Z, net_D, net_G, loss, optimizer_G),
                       batch_size)

        # 생성된 예제 표시
        Z = tf.random.normal(mean=0, stddev=1, shape=(21, 1, 1, latent_dim))
        # 합성 데이터를 N(0, 1)로 정규화
        fake_x = net_G(Z) / 2 + 0.5
        imgs = tf.concat([tf.concat([fake_x[i * 7 + j] for j in range(7)],
                                    axis=1)
                          for i in range(len(fake_x) // 7)], axis=0)
        animator.axes[1].cla()
        animator.axes[1].imshow(imgs)
        # 손실 표시
        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]
        animator.add(epoch, (loss_D, loss_G))
    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, ' 
          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device._device_name)}')
</code></pre>
<p>시연을 위해 적은 수의 에포크로 모델을 훈련합니다. 더 나은 성능을 위해 <code>num_epochs</code> 변수를 더 큰 숫자로 설정할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet, pytorch
latent_dim, lr, num_epochs = 100, 0.005, 20
train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
latent_dim, lr, num_epochs = 100, 0.0005, 40
train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)
</code></pre>
<h2 id="요약-summary-106"><a class="header" href="#요약-summary-106">요약 (Summary)</a></h2>
<ul>
<li>DCGAN 아키텍처에는 판별기를 위한 4개의 합성곱 레이어와 생성기를 위한 4개의 "분수 스트라이드(fractionally-strided)" 합성곱 레이어가 있습니다.</li>
<li>판별기는 배치 정규화(입력 레이어 제외)와 Leaky ReLU 활성화가 있는 4레이어 스트라이드 합성곱입니다.</li>
<li>Leaky ReLU는 음수 입력에 대해 0이 아닌 출력을 제공하는 비선형 함수입니다. 이는 "죽어가는 ReLU" 문제를 해결하고 아키텍처를 통해 기울기가 더 쉽게 흐르도록 돕습니다.</li>
</ul>
<h2 id="연습-문제-exercises-121"><a class="header" href="#연습-문제-exercises-121">연습 문제 (Exercises)</a></h2>
<ol>
<li>Leaky ReLU 대신 표준 ReLU 활성화를 사용하면 어떻게 됩니까?</li>
<li>Fashion-MNIST에 DCGAN을 적용하고 어떤 범주가 잘 작동하고 어떤 범주가 그렇지 않은지 확인하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/409">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1083">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="추천-시스템-recommender-systems"><a class="header" href="#추천-시스템-recommender-systems">추천 시스템 (Recommender Systems)</a></h1>
<p>:label:<code>chap_recsys</code></p>
<p><strong>Shuai Zhang</strong> (<em>Amazon</em>), <strong>Aston Zhang</strong> (<em>Amazon</em>), 및 <strong>Yi Tay</strong> (<em>Google</em>)</p>
<p>추천 시스템은 산업계에서 널리 채택되고 있으며 우리 일상 생활 어디에나 있습니다. 이러한 시스템은 온라인 쇼핑 사이트(예: amazon.com), 음악/영화 서비스 사이트(예: Netflix 및 Spotify), 모바일 애플리케이션 스토어(예: IOS 앱 스토어 및 구글 플레이), 온라인 광고 등 수많은 분야에서 활용되고 있습니다.</p>
<p>추천 시스템의 주요 목표는 사용자가 볼 영화, 읽을 텍스트 또는 구매할 제품과 같은 관련 항목을 발견하도록 도와 즐거운 사용자 경험을 제공하는 것입니다. 또한 추천 시스템은 온라인 소매업체가 증분 수익을 창출하기 위해 구현하는 가장 강력한 머신러닝 시스템 중 하나입니다. 추천 시스템은 사전 검색의 노력을 줄이고 사용자가 검색하지 않은 제안으로 사용자를 놀라게 함으로써 검색 엔진을 대체합니다. 많은 기업들이 더 효과적인 추천 시스템의 도움으로 경쟁사보다 앞서 나갈 수 있었습니다. 따라서 추천 시스템은 우리의 일상 생활뿐만 아니라 일부 산업에서 매우 필수적입니다.</p>
<p>이 장에서는 추천 시스템의 기초와 발전 사항을 다루고, 다양한 데이터 소스를 사용할 수 있는 추천 시스템을 구축하기 위한 몇 가지 일반적인 기본 기술과 구현을 살펴볼 것입니다. 구체적으로, 사용자가 잠재적 항목에 부여할 수 있는 평점을 예측하는 방법, 항목의 추천 목록을 생성하는 방법, 풍부한 특성으로부터 클릭률을 예측하는 방법을 배울 것입니다. 이러한 작업은 실제 응용 프로그램에서 흔히 볼 수 있습니다. 이 장을 공부함으로써, 고전적인 방법뿐만 아니라 더 발전된 딥러닝 기반 모델을 사용하여 실제 추천 문제를 해결하는 것과 관련된 실무 경험을 얻게 될 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

recsys-intro
movielens
mf
autorec
ranking
neumf
seqrec
ctr
fm
deepfm
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="추천-시스템-개요-overview-of-recommender-systems"><a class="header" href="#추천-시스템-개요-overview-of-recommender-systems">추천 시스템 개요 (Overview of Recommender Systems)</a></h1>
<p>지난 10년 동안 인터넷은 대규모 온라인 서비스를 위한 플랫폼으로 진화하여 우리가 의사 소통하고, 뉴스를 읽고, 제품을 구매하고, 영화를 보는 방식을 근본적으로 변화시켰습니다. 한편, 온라인에서 제공되는 전례 없는 수의 항목(우리는 영화, 뉴스, 책, 제품을 *항목(item)*이라고 부릅니다)으로 인해 우리가 선호하는 항목을 발견하는 데 도움이 될 수 있는 시스템이 필요하게 되었습니다. 따라서 추천 시스템은 개인화된 서비스를 촉진하고 개별 사용자에게 맞춤형 경험을 제공할 수 있는 강력한 정보 필터링 도구입니다. 요컨대, 추천 시스템은 선택을 관리 가능하게 만들기 위해 사용 가능한 풍부한 데이터를 활용하는 데 중추적인 역할을 합니다. 오늘날 추천 시스템은 Amazon, Netflix, YouTube와 같은 수많은 온라인 서비스 제공 업체의 핵심입니다. :numref:<code>subsec_recommender_systems</code>에서 Amazon이 추천하는 딥러닝 책의 예를 상기해 보십시오. 추천 시스템을 도입함으로써 얻을 수 있는 이점은 두 가지입니다: 한편으로는 사용자가 항목을 찾는 노력을 크게 줄이고 정보 과부하 문제를 완화할 수 있습니다. 다른 한편으로는 온라인 서비스 제공 업체에 비즈니스 가치를 더하고 중요한 수익원이 될 수 있습니다. 이 장에서는 추천 시스템 분야의 기본 개념, 고전적인 모델, 딥러닝을 이용한 최신 발전 사항을 구현된 예제와 함께 소개할 것입니다.</p>
<p><img src="chapter_recommender-systems/../img/rec-intro.svg" alt="추천 프로세스 그림" /></p>
<h2 id="협업-필터링-collaborative-filtering"><a class="header" href="#협업-필터링-collaborative-filtering">협업 필터링 (Collaborative Filtering)</a></h2>
<p>우리는 추천 시스템의 중요한 개념인 협업 필터링(Collaborative Filtering, CF)으로 여정을 시작합니다. 이 용어는 Tapestry 시스템 :cite:<code>Goldberg.Nichols.Oki.ea.1992</code>에서 처음 만들어졌으며, "뉴스그룹에 게시된 많은 양의 이메일과 메시지를 처리하기 위해 사람들이 서로 협력하여 필터링 프로세스를 수행하는 것"을 의미했습니다. 이 용어는 더 많은 의미로 풍성해졌습니다. 넓은 의미에서 이것은 여러 사용자, 에이전트 및 데이터 소스 간의 협업과 관련된 기술을 사용하여 정보 또는 패턴을 필터링하는 프로세스입니다. CF는 많은 형태를 가지고 있으며 CF가 등장한 이후 수많은 CF 방법이 제안되었습니다.</p>
<p>전반적으로 CF 기술은 메모리 기반 CF, 모델 기반 CF 및 하이브리드 :cite:<code>Su.Khoshgoftaar.2009</code>로 분류할 수 있습니다. 대표적인 메모리 기반 CF 기술은 사용자 기반 CF 및 아이템 기반 CF와 같은 최근접 이웃 기반 CF입니다 :cite:<code>Sarwar.Karypis.Konstan.ea.2001</code>. 행렬 분해와 같은 잠재 요인 모델은 모델 기반 CF의 예입니다. 메모리 기반 CF는 공통 항목을 기반으로 유사성 값을 계산하기 때문에 희소하고 대규모인 데이터를 처리하는 데 한계가 있습니다. 모델 기반 방법은 희소성과 확장성을 처리하는 더 나은 기능으로 인해 더 인기가 있습니다. 많은 모델 기반 CF 접근 방식은 신경망으로 확장될 수 있어 딥러닝의 계산 가속화와 함께 더 유연하고 확장 가능한 모델로 이어질 수 있습니다 :cite:<code>Zhang.Yao.Sun.ea.2019</code>. 일반적으로 CF는 사용자-항목 상호 작용 데이터만 사용하여 예측 및 추천을 수행합니다. CF 외에도 콘텐츠 기반 및 문맥 기반 추천 시스템은 항목/사용자의 콘텐츠 설명과 타임스탬프 및 위치와 같은 상황별 신호를 통합하는 데 유용합니다. 분명히, 사용 가능한 입력 데이터가 다를 때 모델 유형/구조를 조정해야 할 수도 있습니다.</p>
<h2 id="명시적-피드백과-암시적-피드백-explicit-feedback-and-implicit-feedback"><a class="header" href="#명시적-피드백과-암시적-피드백-explicit-feedback-and-implicit-feedback">명시적 피드백과 암시적 피드백 (Explicit Feedback and Implicit Feedback)</a></h2>
<p>사용자의 선호도를 학습하기 위해 시스템은 사용자로부터 피드백을 수집해야 합니다. 피드백은 명시적이거나 암시적일 수 있습니다 :cite:<code>Hu.Koren.Volinsky.2008</code>. 예를 들어, <a href="https://www.imdb.com/">IMDb</a>는 영화에 대해 별 1개에서 10개까지의 별점을 수집합니다. YouTube는 사용자가 선호도를 표시할 수 있도록 좋아요 및 싫어요 버튼을 제공합니다. 명시적 피드백을 수집하려면 사용자가 적극적으로 자신의 관심을 나타내야 함이 분명합니다. 그럼에도 불구하고 많은 사용자가 제품을 평가하는 것을 꺼려할 수 있으므로 명시적 피드백을 항상 쉽게 얻을 수 있는 것은 아닙니다. 상대적으로 말하자면, 암시적 피드백은 주로 사용자 클릭과 같은 암시적 행동을 모델링하는 것과 관련이 있으므로 종종 쉽게 얻을 수 있습니다. 따라서 많은 추천 시스템은 사용자 행동 관찰을 통해 사용자의 의견을 간접적으로 반영하는 암시적 피드백에 중점을 둡니다. 구매 내역, 검색 기록, 시청 및 마우스 움직임을 포함한 다양한 형태의 암시적 피드백이 있습니다. 예를 들어, 동일한 저자의 책을 많이 구매한 사용자는 아마도 그 저자를 좋아할 것입니다. 암시적 피드백은 본질적으로 노이즈가 많다는 점에 유의하십시오. 우리는 그들의 선호도와 진정한 동기를 <em>추측</em>할 수 있을 뿐입니다. 사용자가 영화를 봤다고 해서 반드시 그 영화에 대해 긍정적인 견해를 가지고 있다는 것을 의미하는 것은 아닙니다.</p>
<h2 id="추천-작업-recommendation-tasks"><a class="header" href="#추천-작업-recommendation-tasks">추천 작업 (Recommendation Tasks)</a></h2>
<p>지난 수십 년 동안 수많은 추천 작업이 조사되었습니다. 응용 도메인에 따라 영화 추천, 뉴스 추천, 관심 지점(POI) 추천 :cite:<code>Ye.Yin.Lee.ea.2011</code> 등이 있습니다. 피드백 및 입력 데이터의 유형에 따라 작업을 구분하는 것도 가능합니다. 예를 들어 평점 예측 작업은 명시적 평점을 예측하는 것을 목표로 합니다. Top-$n$ 추천(항목 순위 지정)은 암시적 피드백을 기반으로 각 사용자에게 개인적으로 모든 항목의 순위를 매깁니다. 타임스탬프 정보도 포함되어 있다면 시퀀스 인식 추천을 구축할 수 있습니다 :cite:<code>Quadrana.Cremonesi.Jannach.2018</code>. 또 다른 인기 있는 작업은 클릭률 예측이라고 하며, 이 또한 암시적 피드백을 기반으로 하지만 다양한 범주형 특성을 활용할 수 있습니다. 신규 사용자를 위한 추천 및 기존 사용자에게 신규 항목을 추천하는 것을 콜드 스타트 추천이라고 합니다 :cite:<code>Schein.Popescul.Ungar.ea.2002</code>.</p>
<h2 id="요약-summary-107"><a class="header" href="#요약-summary-107">요약 (Summary)</a></h2>
<ul>
<li>추천 시스템은 개별 사용자와 산업에 중요합니다. 협업 필터링은 추천의 핵심 개념입니다.</li>
<li>피드백에는 암시적 피드백과 명시적 피드백의 두 가지 유형이 있습니다. 지난 10년 동안 수많은 추천 작업이 탐구되었습니다.</li>
</ul>
<h2 id="연습-문제-exercises-122"><a class="header" href="#연습-문제-exercises-122">연습 문제 (Exercises)</a></h2>
<ol>
<li>추천 시스템이 여러분의 일상 생활에 어떤 영향을 미치는지 설명할 수 있습니까?</li>
<li>조사해 볼 만한 흥미로운 추천 작업은 무엇이라고 생각합니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/398">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="movielens-데이터셋-the-movielens-dataset"><a class="header" href="#movielens-데이터셋-the-movielens-dataset">MovieLens 데이터셋 (The MovieLens Dataset)</a></h1>
<p>추천 연구에 사용할 수 있는 데이터셋은 많이 있습니다. 그중에서도 <a href="https://movielens.org/">MovieLens</a> 데이터셋은 아마도 가장 인기 있는 데이터셋 중 하나일 것입니다. MovieLens는 비상업적 웹 기반 영화 추천 시스템입니다. 1997년에 만들어졌으며 연구 목적으로 영화 평점 데이터를 수집하기 위해 미네소타 대학교의 연구실인 GroupLens에서 운영합니다. MovieLens 데이터는 개인화된 추천 및 사회 심리학을 포함한 여러 연구 연구에 매우 중요했습니다.</p>
<h2 id="데이터-얻기-getting-the-data"><a class="header" href="#데이터-얻기-getting-the-data">데이터 얻기 (Getting the Data)</a></h2>
<p>MovieLens 데이터셋은 <a href="https://grouplens.org/datasets/movielens/">GroupLens</a> 웹사이트에서 호스팅됩니다. 여러 버전을 사용할 수 있습니다. 우리는 MovieLens 100K 데이터셋을 사용할 것입니다 :cite:<code>Herlocker.Konstan.Borchers.ea.1999</code>. 이 데이터셋은 1682개의 영화에 대해 943명의 사용자로부터 받은 1에서 5까지의 별점으로 구성된 100,000개의 평점으로 이루어져 있습니다. 각 사용자가 적어도 20개의 영화를 평가하도록 정리되었습니다. 사용자와 항목에 대한 나이, 성별, 장르와 같은 간단한 인구 통계 정보도 사용할 수 있습니다. <a href="http://files.grouplens.org/datasets/movielens/ml-100k.zip">ml-100k.zip</a>을 다운로드하고 csv 형식의 100,000개 평점을 모두 포함하는 <code>u.data</code> 파일을 추출할 수 있습니다. 폴더에는 다른 많은 파일이 있으며, 각 파일에 대한 자세한 설명은 데이터셋의 <a href="http://files.grouplens.org/datasets/movielens/ml-100k-README.txt">README</a> 파일에서 찾을 수 있습니다.</p>
<p>우선, 이 섹션의 실험을 실행하는 데 필요한 패키지를 가져옵니다.</p>
<pre><code class="language-{.python .input  n=1}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, np
import os
import pandas as pd
</code></pre>
<p>그런 다음 MovieLens 100k 데이터셋을 다운로드하고 상호 작용을 <code>DataFrame</code>으로 로드합니다.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
#@save
d2l.DATA_HUB['ml-100k'] = (
    'https://files.grouplens.org/datasets/movielens/ml-100k.zip',
    'cd4dcac4241c8a4ad7badc7ca635da8a69dddb83')

#@save
def read_data_ml100k():
    data_dir = d2l.download_extract('ml-100k')
    names = ['user_id', 'item_id', 'rating', 'timestamp']
    data = pd.read_csv(os.path.join(data_dir, 'u.data'), sep='\t',
                       names=names, engine='python')
    num_users = data.user_id.unique().shape[0]
    num_items = data.item_id.unique().shape[0]
    return data, num_users, num_items
</code></pre>
<h2 id="데이터셋-통계-statistics-of-the-dataset"><a class="header" href="#데이터셋-통계-statistics-of-the-dataset">데이터셋 통계 (Statistics of the Dataset)</a></h2>
<p>데이터를 로드하고 처음 5개 레코드를 수동으로 검사해 보겠습니다. 데이터 구조를 배우고 제대로 로드되었는지 확인하는 효과적인 방법입니다.</p>
<pre><code class="language-{.python .input  n=3}">#@tab mxnet
data, num_users, num_items = read_data_ml100k()
sparsity = 1 - len(data) / (num_users * num_items)
print(f'number of users: {num_users}, number of items: {num_items}')
print(f'matrix sparsity: {sparsity:f}')
print(data.head(5))
</code></pre>
<p>각 줄은 "user id" 1-943, "item id" 1-1682, "rating" 1-5 및 "timestamp"를 포함한 4개의 열로 구성되어 있음을 알 수 있습니다. $n 	imes m$ 크기의 상호 작용 행렬을 구성할 수 있습니다. 여기서 $n$과 $m$은 각각 사용자와 항목의 수입니다. 이 데이터셋은 기존 평점만 기록하므로 평점 행렬이라고도 할 수 있으며, 이 행렬의 값이 정확한 평점을 나타내는 경우 상호 작용 행렬과 평점 행렬을 혼용해서 사용할 것입니다. 사용자가 대다수의 영화를 평가하지 않았기 때문에 평점 행렬의 대부분의 값은 알 수 없습니다. 또한 이 데이터셋의 희소성을 보여줍니다. 희소성은 <code>1 - 0이 아닌 항목 수 / (사용자 수 * 항목 수)</code>로 정의됩니다. 분명히 상호 작용 행렬은 매우 희소합니다(즉, 희소성 = 93.695%). 실제 데이터셋은 더 큰 범위의 희소성을 겪을 수 있으며 이는 추천 시스템을 구축하는 데 있어 오랜 과제였습니다. 실행 가능한 솔루션은 희소성을 완화하기 위해 사용자/항목 특성과 같은 추가 부가 정보를 사용하는 것입니다.</p>
<p>그런 다음 서로 다른 평점 수의 분포를 그립니다. 예상대로 대부분의 평점이 3-4에 집중된 정규 분포로 보입니다.</p>
<pre><code class="language-{.python .input  n=4}">#@tab mxnet
d2l.plt.hist(data['rating'], bins=5, ec='black')
d2l.plt.xlabel('Rating')
d2l.plt.ylabel('Count')
d2l.plt.title('Distribution of Ratings in MovieLens 100K')
d2l.plt.show()
</code></pre>
<h2 id="데이터셋-분할-splitting-the-dataset"><a class="header" href="#데이터셋-분할-splitting-the-dataset">데이터셋 분할 (Splitting the dataset)</a></h2>
<p>데이터셋을 훈련 세트와 테스트 세트로 분할합니다. 다음 함수는 <code>random</code> 및 <code>seq-aware</code>를 포함한 두 가지 분할 모드를 제공합니다. <code>random</code> 모드에서 함수는 타임스탬프를 고려하지 않고 100k 상호 작용을 무작위로 분할하고 기본적으로 데이터의 90%를 훈련 샘플로 사용하고 나머지 10%를 테스트 샘플로 사용합니다. <code>seq-aware</code> 모드에서는 사용자가 가장 최근에 평가한 항목을 테스트용으로 남겨두고 사용자의 과거 상호 작용을 훈련 세트로 사용합니다. 사용자 과거 상호 작용은 타임스탬프를 기준으로 오래된 것부터 최신 순으로 정렬됩니다. 이 모드는 시퀀스 인식 추천 섹션에서 사용됩니다.</p>
<pre><code class="language-{.python .input  n=5}">#@tab mxnet
#@save
def split_data_ml100k(data, num_users, num_items,
                      split_mode='random', test_ratio=0.1):
    """데이터셋을 무작위 모드 또는 시퀀스 인식 모드로 분할합니다."""
    if split_mode == 'seq-aware':
        train_items, test_items, train_list = {}, {}, []
        for line in data.itertuples():
            u, i, rating, time = line[1], line[2], line[3], line[4]
            train_items.setdefault(u, []).append((u, i, rating, time))
            if u not in test_items or test_items[u][-1] &lt; time:
                test_items[u] = (i, rating, time)
        for u in range(1, num_users + 1):
            train_list.extend(sorted(train_items[u], key=lambda k: k[3]))
        test_data = [(key, *value) for key, value in test_items.items()]
        train_data = [item for item in train_list if item not in test_data]
        train_data = pd.DataFrame(train_data)
        test_data = pd.DataFrame(test_data)
    else:
        mask = [True if x == 1 else False for x in np.random.uniform(
            0, 1, (len(data))) &lt; 1 - test_ratio]
        neg_mask = [not x for x in mask]
        train_data, test_data = data[mask], data[neg_mask]
    return train_data, test_data
</code></pre>
<p>테스트 세트 외에도 검증 세트를 사용하는 것이 실제로는 좋은 관행이라는 점에 유의하십시오. 그러나 간결함을 위해 생략합니다. 이 경우 테스트 세트는 보류된 검증 세트로 간주될 수 있습니다.</p>
<h2 id="데이터-로드-loading-the-data"><a class="header" href="#데이터-로드-loading-the-data">데이터 로드 (Loading the data)</a></h2>
<p>데이터셋 분할 후 편의를 위해 훈련 세트와 테스트 세트를 리스트와 딕셔너리/행렬로 변환합니다. 다음 함수는 데이터프레임을 한 줄씩 읽고 사용자/항목의 인덱스를 0부터 열거합니다. 그런 다음 함수는 사용자, 항목, 평점 리스트와 상호 작용을 기록하는 딕셔너리/행렬을 반환합니다. 피드백 유형을 <code>explicit</code> 또는 <code>implicit</code>으로 지정할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=6}">#@tab mxnet
#@save
def load_data_ml100k(data, num_users, num_items, feedback='explicit'):
    users, items, scores = [], [], []
    inter = np.zeros((num_items, num_users)) if feedback == 'explicit' else {}
    for line in data.itertuples():
        user_index, item_index = int(line[1] - 1), int(line[2] - 1)
        score = int(line[3]) if feedback == 'explicit' else 1
        users.append(user_index)
        items.append(item_index)
        scores.append(score)
        if feedback == 'implicit':
            inter.setdefault(user_index, []).append(item_index)
        else:
            inter[item_index, user_index] = score
    return users, items, scores, inter
</code></pre>
<p>그 후 위의 단계들을 합치면 다음 섹션에서 사용될 것입니다. 결과는 <code>Dataset</code> 및 <code>DataLoader</code>로 래핑됩니다. 훈련 데이터에 대한 <code>DataLoader</code>의 <code>last_batch</code>는 <code>rollover</code> 모드(나머지 샘플은 다음 에포크로 롤오버됨)로 설정되고 순서는 섞입니다.</p>
<pre><code class="language-{.python .input  n=7}">#@tab mxnet
#@save
def split_and_load_ml100k(split_mode='seq-aware', feedback='explicit',
                          test_ratio=0.1, batch_size=256):
    data, num_users, num_items = read_data_ml100k()
    train_data, test_data = split_data_ml100k(
        data, num_users, num_items, split_mode, test_ratio)
    train_u, train_i, train_r, _ = load_data_ml100k(
        train_data, num_users, num_items, feedback)
    test_u, test_i, test_r, _ = load_data_ml100k(
        test_data, num_users, num_items, feedback)
    train_set = gluon.data.ArrayDataset(
        np.array(train_u), np.array(train_i), np.array(train_r))
    test_set = gluon.data.ArrayDataset(
        np.array(test_u), np.array(test_i), np.array(test_r))
    train_iter = gluon.data.DataLoader(
        train_set, shuffle=True, last_batch='rollover',
        batch_size=batch_size)
    test_iter = gluon.data.DataLoader(
        test_set, batch_size=batch_size)
    return num_users, num_items, train_iter, test_iter
</code></pre>
<h2 id="요약-summary-108"><a class="header" href="#요약-summary-108">요약 (Summary)</a></h2>
<ul>
<li>MovieLens 데이터셋은 추천 연구에 널리 사용됩니다. 공개적으로 사용 가능하며 무료로 사용할 수 있습니다.</li>
<li>이후 섹션에서 추가로 사용할 수 있도록 MovieLens 100k 데이터셋을 다운로드하고 전처리하는 함수를 정의합니다.</li>
</ul>
<h2 id="연습-문제-exercises-123"><a class="header" href="#연습-문제-exercises-123">연습 문제 (Exercises)</a></h2>
<ul>
<li>찾을 수 있는 다른 유사한 추천 데이터셋은 무엇입니까?</li>
<li>MovieLens에 대한 자세한 내용은 <a href="https://movielens.org/">https://movielens.org/</a> 사이트를 살펴보십시오.</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/399">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="행렬-분해-matrix-factorization"><a class="header" href="#행렬-분해-matrix-factorization">행렬 분해 (Matrix Factorization)</a></h1>
<p>행렬 분해 :cite:<code>Koren.Bell.Volinsky.2009</code>는 추천 시스템 문헌에서 잘 확립된 알고리즘입니다. 행렬 분해 모델의 첫 번째 버전은 Simon Funk가 유명한 <a href="https://sifter.org/%7Esimon/journal/20061211.html">블로그 게시물</a>에서 상호 작용 행렬을 분해하는 아이디어를 설명하면서 제안했습니다. 그 후 2006년에 개최된 Netflix 콘테스트로 인해 널리 알려지게 되었습니다. 당시 미디어 스트리밍 및 비디오 대여 회사인 Netflix는 추천 시스템 성능을 개선하기 위한 콘테스트를 발표했습니다. Netflix 기준선(즉, Cinematch)보다 10% 향상시킬 수 있는 최고의 팀이 100만 달러의 상금을 받게 됩니다. 따라서 이 콘테스트는 추천 시스템 연구 분야에 많은 관심을 끌었습니다. 그 후 BellKor, Pragmatic Theory, BigChaos의 연합 팀인 BellKor's Pragmatic Chaos 팀이 대상을 수상했습니다(지금은 이 알고리즘에 대해 걱정할 필요가 없습니다). 최종 점수는 앙상블 솔루션(즉, 많은 알고리즘의 조합)의 결과였지만 행렬 분해 알고리즘은 최종 혼합에서 중요한 역할을 했습니다. Netflix 대상 솔루션의 기술 보고서 :cite:<code>Toscher.Jahrer.Bell.2009</code>는 채택된 모델에 대한 자세한 소개를 제공합니다. 이 섹션에서는 행렬 분해 모델의 세부 사항과 구현에 대해 자세히 알아볼 것입니다.</p>
<h2 id="행렬-분해-모델"><a class="header" href="#행렬-분해-모델">행렬 분해 모델</a></h2>
<p>행렬 분해는 협업 필터링 모델의 한 종류입니다. 구체적으로, 이 모델은 사용자-항목 상호 작용 행렬(예: 평점 행렬)을 두 개의 저랭크(lower-rank) 행렬의 곱으로 분해하여 사용자-항목 상호 작용의 저랭크 구조를 포착합니다.</p>
<p>$\mathbf{R} \in \mathbb{R}^{m \times n}$을 $m$명의 사용자와 $n$개의 항목이 있는 상호 작용 행렬이라고 하고, $\mathbf{R}$의 값은 명시적 평점을 나타냅니다. 사용자-항목 상호 작용은 사용자 잠재 행렬 $\mathbf{P} \in \mathbb{R}^{m \times k}$와 항목 잠재 행렬 $\mathbf{Q} \in \mathbb{R}^{n \times k}$로 분해됩니다. 여기서 $k \ll m, n$은 잠재 요인 크기입니다. $\mathbf{p}_u$를 $\mathbf{P}$의 $u^\textrm{th}$ 행, $\mathbf{q}_i$를 $\mathbf{Q}$의 $i^\textrm{th}$ 행이라고 합시다. 주어진 항목 $i$에 대해, $\mathbf{q}_i$의 요소는 영화의 장르 및 언어와 같은 특성을 항목이 보유하는 정도를 측정합니다. 주어진 사용자 $u$에 대해, $\mathbf{p}_u$의 요소는 사용자가 항목의 해당 특성에 대해 갖는 관심의 정도를 측정합니다. 이러한 잠재 요인은 예시에서 언급한 것과 같은 명백한 차원을 측정할 수도 있고 완전히 해석 불가능할 수도 있습니다. 예측된 평점은 다음과 같이 추정할 수 있습니다.</p>
<p>$$ \hat{\mathbf{R}} = \mathbf{PQ}^\top $$</p>
<p>여기서 $\hat{\mathbf{R}}}\in \mathbb{R}^{m \times n}$은 $\mathbf{R}$과 모양이 같은 예측 평점 행렬입니다. 이 예측 규칙의 주요 문제 중 하나는 사용자/항목 편향을 모델링할 수 없다는 것입니다. 예를 들어, 어떤 사용자는 더 높은 평점을 주는 경향이 있거나 어떤 항목은 품질이 떨어져 항상 더 낮은 평점을 받습니다. 이러한 편향은 실제 응용 프로그램에서 흔히 발생합니다. 이러한 편향을 포착하기 위해 사용자별 및 항목별 편향 항이 도입됩니다. 구체적으로, 사용자 $u$가 항목 $i$에 대해 부여하는 예측 평점은 다음과 같이 계산됩니다.</p>
<p>$$
\hat{\mathbf{R}}}_{ui} = \mathbf{p}_u\mathbf{q}^\top_i + b_u + b_i
$$</p>
<p>그런 다음 예측 평점 점수와 실제 평점 점수 간의 평균 제곱 오차를 최소화하여 행렬 분해 모델을 훈련합니다. 목적 함수는 다음과 같이 정의됩니다.</p>
<p>$$
\underset{\mathbf{P}, \mathbf{Q}, b}{\mathrm{argmin}} \sum_{(u, i) \in \mathcal{K}} \| \mathbf{R}<em>{ui} -
\hat{\mathbf{R}}</em>{ui} \|^2 + \lambda (\| \mathbf{P} \| ^2_F + \| \mathbf{Q}
\|^2_F + b_u^2 + b_i^2 )
$$</p>
<p>여기서 $\lambda$는 정규화율을 나타냅니다. 정규화 항 $\lambda (\| \mathbf{P} \| ^2_F + \| \mathbf{Q}
\|^2_F + b_u^2 + b_i^2 )$는 파라미터의 크기에 페널티를 주어 과대적합을 피하는 데 사용됩니다. $\mathbf{R}<em>{ui}$가 알려진 $(u, i)$ 쌍은 집합
$\mathcal{K}=\{(u, i) \mid \mathbf{R}</em>{ui} \textrm{ is known}\}$에 저장됩니다. 모델 파라미터는 확률적 경사 하강법 및 Adam과 같은 최적화 알고리즘으로 학습할 수 있습니다.</p>
<p>행렬 분해 모델의 직관적인 그림은 아래와 같습니다.</p>
<p><img src="chapter_recommender-systems/../img/rec-mf.svg" alt="행렬 분해 모델 그림" /></p>
<p>이 섹션의 나머지 부분에서는 행렬 분해의 구현을 설명하고 MovieLens 데이터셋에서 모델을 훈련할 것입니다.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
import mxnet as mx
npx.set_np()
</code></pre>
<h2 id="모델-구현-model-implementation"><a class="header" href="#모델-구현-model-implementation">모델 구현 (Model Implementation)</a></h2>
<p>먼저 위에서 설명한 행렬 분해 모델을 구현합니다. 사용자 및 항목 잠재 요인은 <code>nn.Embedding</code>으로 생성할 수 있습니다. <code>input_dim</code>은 항목/사용자 수이고 <code>output_dim</code>은 잠재 요인 $k$의 차원입니다. <code>output_dim</code>을 1로 설정하여 <code>nn.Embedding</code>을 사용하여 사용자/항목 편향을 생성할 수도 있습니다. <code>forward</code> 함수에서 사용자 및 항목 ID는 임베딩을 조회하는 데 사용됩니다.</p>
<pre><code class="language-{.python .input  n=4}">#@tab mxnet
class MF(nn.Block):
    def __init__(self, num_factors, num_users, num_items, **kwargs):
        super(MF, self).__init__(**kwargs)
        self.P = nn.Embedding(input_dim=num_users, output_dim=num_factors)
        self.Q = nn.Embedding(input_dim=num_items, output_dim=num_factors)
        self.user_bias = nn.Embedding(num_users, 1)
        self.item_bias = nn.Embedding(num_items, 1)

    def forward(self, user_id, item_id):
        P_u = self.P(user_id)
        Q_i = self.Q(item_id)
        b_u = self.user_bias(user_id)
        b_i = self.item_bias(item_id)
        outputs = (P_u * Q_i).sum(axis=1) + np.squeeze(b_u) + np.squeeze(b_i)
        return outputs.flatten()
</code></pre>
<h2 id="평가-척도-evaluation-measures"><a class="header" href="#평가-척도-evaluation-measures">평가 척도 (Evaluation Measures)</a></h2>
<p>그런 다음 모델에 의해 예측된 평점 점수와 실제로 관찰된 평점(정답) 간의 차이를 측정하는 데 일반적으로 사용되는 RMSE(평균 제곱근 오차) 척도를 구현합니다 :cite:<code>Gunawardana.Shani.2015</code>. RMSE는 다음과 같이 정의됩니다.</p>
<p>$$
\textrm{RMSE} = \sqrt{\frac{1}{|\mathcal{T}|}\sum_{(u, i) \in \mathcal{T}}(\mathbf{R}<em>{ui} -\hat{\mathbf{R}}</em>{ui})^2}
$$</p>
<p>여기서 $\mathcal{T}$는 평가하려는 사용자 및 항목 쌍으로 구성된 집합입니다. $|\mathcal{T}|$는 이 집합의 크기입니다. <code>mx.metric</code>에서 제공하는 RMSE 함수를 사용할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=3}">#@tab mxnet
def evaluator(net, test_iter, devices):
    rmse = mx.metric.RMSE()  # RMSE 가져오기
    rmse_list = []
    for idx, (users, items, ratings) in enumerate(test_iter):
        u = gluon.utils.split_and_load(users, devices, even_split=False)
        i = gluon.utils.split_and_load(items, devices, even_split=False)
        r_ui = gluon.utils.split_and_load(ratings, devices, even_split=False)
        r_hat = [net(u, i) for u, i in zip(u, i)]
        rmse.update(labels=r_ui, preds=r_hat)
        rmse_list.append(rmse.get()[1])
    return float(np.mean(np.array(rmse_list)))
</code></pre>
<h2 id="모델-훈련-및-평가-training-and-evaluating-the-model-4"><a class="header" href="#모델-훈련-및-평가-training-and-evaluating-the-model-4">모델 훈련 및 평가 (Training and Evaluating the Model)</a></h2>
<p>훈련 함수에서는 가중치 감소가 있는 $\ell_2$ 손실을 채택합니다. 가중치 감소 메커니즘은 $\ell_2$ 정규화와 동일한 효과를 갖습니다.</p>
<pre><code class="language-{.python .input  n=4}">#@tab mxnet
#@save
def train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,
                        devices=d2l.try_all_gpus(), evaluator=None,
                        **kwargs):
    timer = d2l.Timer()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 2],
                            legend=['train loss', 'test RMSE'])
    for epoch in range(num_epochs):
        metric, l = d2l.Accumulator(3), 0.
        for i, values in enumerate(train_iter):
            timer.start()
            input_data = []
            values = values if isinstance(values, list) else [values]
            for v in values:
                input_data.append(gluon.utils.split_and_load(v, devices))
            train_feat = input_data[:-1] if len(values) &gt; 1 else input_data
            train_label = input_data[-1]
            with autograd.record():
                preds = [net(*t) for t in zip(*train_feat)]
                ls = [loss(p, s) for p, s in zip(preds, train_label)]
            [l.backward() for l in ls]
            l += sum([l.asnumpy() for l in ls]).mean() / len(devices)
            trainer.step(values[0].shape[0])
            metric.add(l, values[0].shape[0], values[0].size)
            timer.stop()
        if len(kwargs) &gt; 0:  # AutoRec 섹션에서 사용됨
            test_rmse = evaluator(net, test_iter, kwargs['inter_mat'],
                                  devices)
        else:
            test_rmse = evaluator(net, test_iter, devices)
        train_l = l / (i + 1)
        animator.add(epoch + 1, (train_l, test_rmse))
    print(f'train loss {metric[0] / metric[1]:.3f}, '
          f'test RMSE {test_rmse:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(devices)}')
</code></pre>
<p>마지막으로 모든 것을 합쳐 모델을 훈련해 봅시다. 여기서는 잠재 요인 차원을 30으로 설정합니다.</p>
<pre><code class="language-{.python .input  n=5}">#@tab mxnet
devices = d2l.try_all_gpus()
num_users, num_items, train_iter, test_iter = d2l.split_and_load_ml100k(
    test_ratio=0.1, batch_size=512)
net = MF(30, num_users, num_items)
net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))
lr, num_epochs, wd, optimizer = 0.002, 20, 1e-5, 'adam'
loss = gluon.loss.L2Loss()
trainer = gluon.Trainer(net.collect_params(), optimizer,
                        {"learning_rate": lr, 'wd': wd})
train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,
                    devices, evaluator)
</code></pre>
<p>아래에서는 훈련된 모델을 사용하여 사용자(ID 20)가 항목(ID 30)에 줄 수 있는 평점을 예측합니다.</p>
<pre><code class="language-{.python .input  n=6}">#@tab mxnet
scores = net(np.array([20], dtype='int', ctx=devices[0]),
             np.array([30], dtype='int', ctx=devices[0]))
scores
</code></pre>
<h2 id="요약-summary-109"><a class="header" href="#요약-summary-109">요약 (Summary)</a></h2>
<ul>
<li>행렬 분해 모델은 추천 시스템에서 널리 사용됩니다. 사용자가 항목에 부여할 수 있는 평점을 예측하는 데 사용할 수 있습니다.</li>
<li>추천 시스템을 위한 행렬 분해를 구현하고 훈련할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-124"><a class="header" href="#연습-문제-exercises-124">연습 문제 (Exercises)</a></h2>
<ul>
<li>잠재 요인의 크기를 변경해 보십시오. 잠재 요인의 크기가 모델 성능에 어떤 영향을 줍니까?</li>
<li>다른 최적화 도구, 학습률 및 가중치 감소율을 시도해 보십시오.</li>
<li>특정 영화에 대한 다른 사용자의 예측 평점 점수를 확인해 보십시오.</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/400">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="autorec-오토인코더를-사용한-평점-예측-autorec-rating-prediction-with-autoencoders"><a class="header" href="#autorec-오토인코더를-사용한-평점-예측-autorec-rating-prediction-with-autoencoders">AutoRec: 오토인코더를 사용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></h1>
<p>행렬 분해 모델이 평점 예측 작업에서 괜찮은 성능을 달성하지만, 본질적으로 선형 모델입니다. 따라서 이러한 모델은 사용자 선호도를 예측할 수 있는 복잡한 비선형 및 복잡한 관계를 포착할 수 없습니다. 이 섹션에서는 비선형 신경망 협업 필터링 모델인 AutoRec :cite:<code>Sedhain.Menon.Sanner.ea.2015</code>를 소개합니다. 이는 협업 필터링(CF)을 오토인코더 아키텍처와 동일시하고 명시적 피드백을 기반으로 CF에 비선형 변환을 통합하는 것을 목표로 합니다. 신경망은 모든 연속 함수를 근사할 수 있는 것으로 입증되어 행렬 분해의 한계를 해결하고 행렬 분해의 표현력을 풍부하게 하는 데 적합합니다.</p>
<p>한편으로, AutoRec은 입력 레이어, 은닉층 및 재구성(출력) 레이어로 구성된 오토인코더와 동일한 구조를 갖습니다. 오토인코더는 입력을 은닉(일반적으로 저차원) 표현으로 코딩하기 위해 입력을 출력으로 복사하는 법을 학습하는 신경망입니다. AutoRec에서는 사용자/항목을 저차원 공간에 명시적으로 임베딩하는 대신 상호 작용 행렬의 열/행을 입력으로 사용한 다음 출력 레이어에서 상호 작용 행렬을 재구성합니다.</p>
<p>반면에, AutoRec은 전통적인 오토인코더와 다릅니다. 은닉 표현을 학습하는 대신 AutoRec은 출력 레이어를 학습/재구성하는 데 중점을 둡니다. 부분적으로 관찰된 상호 작용 행렬을 입력으로 사용하여 완성된 평점 행렬을 재구성하는 것을 목표로 합니다. 그동안 추천 목적으로 입력의 누락된 항목이 재구성을 통해 출력 레이어에서 채워집니다.</p>
<p>AutoRec에는 사용자 기반과 항목 기반의 두 가지 변형이 있습니다. 간결함을 위해 여기서는 항목 기반 AutoRec만 소개합니다. 사용자 기반 AutoRec은 그에 따라 유도될 수 있습니다.</p>
<h2 id="모델-model-5"><a class="header" href="#모델-model-5">모델 (Model)</a></h2>
<p>$\mathbf{R}_{*i}$를 평점 행렬의 $i^\textrm{th}$ 열이라고 합시다. 여기서 알 수 없는 평점은 기본적으로 0으로 설정됩니다. 신경망 아키텍처는 다음과 같이 정의됩니다.</p>
<p>$$
h(\mathbf{R}<em>{*i}) = f(\mathbf{W} \cdot g(\mathbf{V} \mathbf{R}</em>{*i} + \mu) + b)
$$</p>
<p>여기서 $f(\cdot)$와 $g(\cdot)$은 활성화 함수를 나타내고, $\mathbf{W}$와 $\mathbf{V}$는 가중치 행렬, $\mu$와 $b$는 편향입니다. $h( \cdot )$는 AutoRec의 전체 네트워크를 나타냅니다. 출력 $h(\mathbf{R}_{*i})$는 평점 행렬의 $i^\textrm{th}$ 열의 재구성입니다.</p>
<p>다음 목적 함수는 재구성 오류를 최소화하는 것을 목표로 합니다.</p>
<p>$$
\underset{\mathbf{W},\mathbf{V},\mu, b}{\mathrm{argmin}} \sum_{i=1}^M{\parallel \mathbf{R}<em>{*i} - h(\mathbf{R}</em>{*i})\parallel_{\mathcal{O}}^2} +\lambda(\parallel \mathbf{W} \parallel_F^2 + \parallel \mathbf{V}\\parallel_F^2)
$$</p>
<p>여기서 $|| \cdot \|\_{\mathcal{O}}$는 관찰된 평점의 기여만 고려됨을 의미합니다. 즉, 관찰된 입력과 관련된 가중치만 역전파 중에 업데이트됩니다.</p>
<pre><code class="language-{.python .input  n=3}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
import mxnet as mx

npx.set_np()
</code></pre>
<h2 id="모델-구현-implementing-the-model"><a class="header" href="#모델-구현-implementing-the-model">모델 구현 (Implementing the Model)</a></h2>
<p>일반적인 오토인코더는 인코더와 디코더로 구성됩니다. 인코더는 입력을 은닉 표현으로 투영하고 디코더는 은닉층을 재구성 레이어로 매핑합니다. 우리는 이 관행을 따르고 완전 연결 레이어로 인코더와 디코더를 만듭니다. 인코더의 활성화는 기본적으로 <code>sigmoid</code>로 설정되고 디코더에는 활성화가 적용되지 않습니다. 과대적합을 줄이기 위해 인코딩 변환 후 드롭아웃이 포함됩니다. 관찰되지 않은 입력의 기울기는 관찰된 평점만 모델 학습 과정에 기여하도록 마스킹됩니다.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
class AutoRec(nn.Block):
    def __init__(self, num_hidden, num_users, dropout=0.05):
        super(AutoRec, self).__init__()
        self.encoder = nn.Dense(num_hidden, activation='sigmoid',
                                use_bias=True)
        self.decoder = nn.Dense(num_users, use_bias=True)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input):
        hidden = self.dropout(self.encoder(input))
        pred = self.decoder(hidden)
        if autograd.is_training():  # 훈련 중 기울기 마스킹
            return pred * np.sign(input)
        else:
            return pred
</code></pre>
<h2 id="평가기-재구현-reimplementing-the-evaluator"><a class="header" href="#평가기-재구현-reimplementing-the-evaluator">평가기 재구현 (Reimplementing the Evaluator)</a></h2>
<p>입력과 출력이 변경되었으므로 평가 함수를 재구현해야 하지만 여전히 RMSE를 정확도 척도로 사용합니다.</p>
<pre><code class="language-{.python .input  n=3}">#@tab mxnet
def evaluator(network, inter_matrix, test_data, devices):
    scores = []
    for values in inter_matrix:
        feat = gluon.utils.split_and_load(values, devices, even_split=False)
        scores.extend([network(i).asnumpy() for i in feat])
    recons = np.array([item for sublist in scores for item in sublist])
    # 테스트 RMSE 계산
    rmse = np.sqrt(np.sum(np.square(test_data - np.sign(test_data) * recons))
                   / np.sum(np.sign(test_data)))
    return float(rmse)
</code></pre>
<h2 id="모델-훈련-및-평가-training-and-evaluating-the-model-5"><a class="header" href="#모델-훈련-및-평가-training-and-evaluating-the-model-5">모델 훈련 및 평가 (Training and Evaluating the Model)</a></h2>
<p>이제 MovieLens 데이터셋에서 AutoRec을 훈련하고 평가해 봅시다. 테스트 RMSE가 행렬 분해 모델보다 낮다는 것을 분명히 알 수 있으며, 이는 평점 예측 작업에서 신경망의 효과를 확인해 줍니다.</p>
<pre><code class="language-{.python .input  n=4}">#@tab mxnet
devices = d2l.try_all_gpus()
# MovieLens 100K 데이터셋 로드
df, num_users, num_items = d2l.read_data_ml100k()
train_data, test_data = d2l.split_data_ml100k(df, num_users, num_items)
_, _, _, train_inter_mat = d2l.load_data_ml100k(train_data, num_users,
                                                num_items)
_, _, _, test_inter_mat = d2l.load_data_ml100k(test_data, num_users,
                                               num_items)
train_iter = gluon.data.DataLoader(train_inter_mat, shuffle=True,
                                   last_batch="rollover", batch_size=256,
                                   num_workers=d2l.get_dataloader_workers())
test_iter = gluon.data.DataLoader(np.array(train_inter_mat), shuffle=False,
                                  last_batch="keep", batch_size=1024,
                                  num_workers=d2l.get_dataloader_workers())
# 모델 초기화, 훈련 및 평가
net = AutoRec(500, num_users)
net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))
lr, num_epochs, wd, optimizer = 0.002, 25, 1e-5, 'adam'
loss = gluon.loss.L2Loss()
trainer = gluon.Trainer(net.collect_params(), optimizer,
                        {"learning_rate": lr, 'wd': wd})
d2l.train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,
                        devices, evaluator, inter_mat=test_inter_mat)
</code></pre>
<h2 id="요약-summary-110"><a class="header" href="#요약-summary-110">요약 (Summary)</a></h2>
<ul>
<li>비선형 레이어와 드롭아웃 정규화를 통합하면서 오토인코더로 행렬 분해 알고리즘을 구성할 수 있습니다.</li>
<li>MovieLens 100K 데이터셋에 대한 실험은 AutoRec이 행렬 분해보다 우수한 성능을 달성함을 보여줍니다.</li>
</ul>
<h2 id="연습-문제-exercises-125"><a class="header" href="#연습-문제-exercises-125">연습 문제 (Exercises)</a></h2>
<ul>
<li>AutoRec의 은닉 차원을 변경하여 모델 성능에 미치는 영향을 확인하십시오.</li>
<li>은닉층을 더 추가해 보십시오. 모델 성능을 향상시키는 데 도움이 됩니까?</li>
<li>디코더와 인코더 활성화 함수의 더 나은 조합을 찾을 수 있습니까?</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/401">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="추천-시스템을-위한-개인화-순위-personalized-ranking-for-recommender-systems"><a class="header" href="#추천-시스템을-위한-개인화-순위-personalized-ranking-for-recommender-systems">추천 시스템을 위한 개인화 순위 (Personalized Ranking for Recommender Systems)</a></h1>
<p>이전 섹션에서는 명시적 피드백만 고려되었으며 모델은 관찰된 평점으로 훈련 및 테스트되었습니다. 이러한 방법에는 두 가지 단점이 있습니다. 첫째, 대부분의 피드백은 실제 시나리오에서 명시적이지 않고 암시적이며, 명시적 피드백은 수집하는 데 비용이 더 많이 들 수 있습니다. 둘째, 사용자 관심사를 예측할 수 있는 관찰되지 않은 사용자-항목 쌍이 완전히 무시되어, 평점이 무작위가 아니라 사용자 선호도 때문에 누락된 경우에 이러한 방법이 부적합합니다. 관찰되지 않은 사용자-항목 쌍은 실제 부정적 피드백(사용자가 항목에 관심이 없음)과 누락된 값(사용자가 나중에 항목과 상호 작용할 수 있음)의 혼합입니다. 행렬 분해 및 AutoRec에서는 관찰되지 않은 쌍을 단순히 무시합니다. 분명히 이러한 모델은 관찰된 쌍과 관찰되지 않은 쌍을 구별할 수 없으며 일반적으로 개인화 순위 작업에 적합하지 않습니다.</p>
<p>이를 위해 암시적 피드백에서 순위가 매겨진 추천 목록을 생성하는 것을 목표로 하는 추천 모델 클래스가 인기를 얻었습니다. 일반적으로 개인화 순위 모델은 포인트와이즈(pointwise), 페어와이즈(pairwise) 또는 리스트와이즈(listwise) 접근 방식으로 최적화할 수 있습니다. 포인트와이즈 접근 방식은 한 번에 단일 상호 작용을 고려하고 분류기 또는 회귀 분석기를 훈련하여 개별 선호도를 예측합니다. 행렬 분해 및 AutoRec은 포인트와이즈 목표로 최적화됩니다. 페어와이즈 접근 방식은 각 사용자에 대해 항목 쌍을 고려하고 해당 쌍에 대한 최적의 순서를 근사하는 것을 목표로 합니다. 일반적으로 페어와이즈 접근 방식은 상대적 순서를 예측하는 것이 순위의 본질을 상기시키기 때문에 순위 작업에 더 적합합니다. 리스트와이즈 접근 방식은 전체 항목 목록의 순서를 근사합니다. 예를 들어 Normalized Discounted Cumulative Gain (<a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">NDCG</a>)과 같은 순위 척도를 직접 최적화합니다. 그러나 리스트와이즈 접근 방식은 포인트와이즈 또는 페어와이즈 접근 방식보다 더 복잡하고 계산 집약적입니다. 이 섹션에서는 두 가지 페어와이즈 목표/손실인 베이지안 개인화 순위 손실과 힌지 손실 및 그 구현을 소개합니다.</p>
<h2 id="베이지안-개인화-순위-손실-및-구현-bayesian-personalized-ranking-loss-and-its-implementation"><a class="header" href="#베이지안-개인화-순위-손실-및-구현-bayesian-personalized-ranking-loss-and-its-implementation">베이지안 개인화 순위 손실 및 구현 (Bayesian Personalized Ranking Loss and its Implementation)</a></h2>
<p>베이지안 개인화 순위(BPR) :cite:<code>Rendle.Freudenthaler.Gantner.ea.2009</code>는 최대 사후 추정기(maximum posterior estimator)에서 파생된 페어와이즈 개인화 순위 손실입니다. 많은 기존 추천 모델에서 널리 사용되었습니다. BPR의 훈련 데이터는 긍정적 쌍과 부정적 쌍(누락된 값)으로 구성됩니다. 사용자가 관찰되지 않은 다른 모든 항목보다 긍정적인 항목을 선호한다고 가정합니다.</p>
<p>공식적으로 훈련 데이터는 $(u, i, j)$ 형식의 튜플로 구성되며, 이는 사용자 $u$가 항목 $j$보다 항목 $i$를 선호함을 나타냅니다. 사후 확률을 최대화하는 것을 목표로 하는 BPR의 베이지안 공식은 다음과 같습니다.</p>
<p>$$
p(Θ t &gt;_u )  r c a b c p(&gt;_u t t Θ) p(Θ)
$$</p>
<p>여기서 $Θ$는 임의의 추천 모델의 파라미터를 나타내고, $&gt;_u$는 사용자 $u$에 대한 모든 항목의 원하는 개인화된 전체 순위를 나타냅니다. 최대 사후 추정기를 공식화하여 개인화 순위 작업에 대한 일반적인 최적화 기준을 도출할 수 있습니다.</p>
<p>$$
\beginredived
\textbf{BPR-OPT} : &amp; \ln p(Θ t &gt;<em>u) \ \b \c \ln p(&gt;<em>u t t Θ) p(Θ) \ &amp;= \ln \product</em>{(u, i, j \in D)} \sigma(\hty}</em>{ui} - \hty}<em>{uj}) p(Θ) \ &amp;= \sum</em>{(u, i, j \in D)} \ln \sigma(\hty}<em>{ui} - \hty}</em>{uj}) + \ln p(Θ) \ &amp;= \sum_{(u, i, j \in D)} \ln \sigma(\hty}<em>{ui} - \hty}</em>{uj}) - \lambda_Θ ||Θ ||^2
endredived
$$</p>
<p>여기서 $D \def = {(u, i, j) | i \in I^+_u ^ j \in I \I I^+_u }$는 훈련 세트이며, $I^+<em>u$는 사용자 $u$가 좋아한 항목을 나타내고, $I$는 모든 항목을 나타내며, $I \I I^+<em>u$는 사용자가 좋아한 항목을 제외한 다른 모든 항목을 나타냅니다. $\hty}</em>{ui}$와 $\hty}</em>{uj}$는 각각 사용자 $u$가 항목 $i$와 $j$에 대해 예측한 점수입니다. 사전 $p(Θ)$는 0 평균과 분산-공분산 행렬 $Σ}_Θ$를 갖는 정규 분포입니다. 여기서 $Σ}_Θ = \lambda_Θ I$로 둡니다.</p>
<p><img src="chapter_recommender-systems/../img/rec-ranking.svg" alt="베이지안 개인화 순위 그림" />
우리는 기본 클래스 <code>mxnet.gluon.loss.Loss</code>를 구현하고 <code>forward</code> 메서드를 재정의하여 베이지안 개인화 순위 손실을 구성할 것입니다. Loss 클래스와 np 모듈을 가져오는 것으로 시작합니다.</p>
<pre><code class="language-{.python .input  n=5}">#@tab mxnet
from mxnet import gluon, np, npx
npx.set_np()
</code></pre>
<p>BPR 손실의 구현은 다음과 같습니다.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
#@save
class BPRLoss(gluon.loss.Loss):
    def __init__(self, weight=None, batch_axis=0, **kwargs):
        super(BPRLoss, self).__init__(weight=None, batch_axis=0, **kwargs)

    def forward(self, positive, negative):
        distances = positive - negative
        loss = - np.sum(np.log(npx.sigmoid(distances)), 0, keepdims=True)
        return loss
</code></pre>
<h2 id="힌지-손실-및-구현-hinge-loss-and-its-implementation"><a class="header" href="#힌지-손실-및-구현-hinge-loss-and-its-implementation">힌지 손실 및 구현 (Hinge Loss and its Implementation)</a></h2>
<p>순위를 위한 힌지 손실은 SVM과 같은 분류기에서 종종 사용되는 gluon 라이브러리 내에서 제공되는 <a href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HingeLoss">힌지 손실</a>과 다른 형태를 가집니다. 추천 시스템의 순위에 사용되는 손실은 다음과 같은 형태를 가집니다.</p>
<p>$$
\sum_{(u, i, j \in D)} \max( m - \hty}<em>{ui} + \hty}</em>{uj}, 0)
$$</p>
<p>여기서 $m$은 안전 마진 크기입니다. 긍정적인 항목에서 부정적인 항목을 밀어내는 것을 목표로 합니다. BPR과 유사하게 절대 출력이 아닌 긍정적인 샘플과 부정적인 샘플 간의 관련 거리를 최적화하는 것을 목표로 하므로 추천 시스템에 적합합니다.</p>
<pre><code class="language-{.python .input  n=3}">#@tab mxnet
#@save
class HingeLossbRec(gluon.loss.Loss):
    def __init__(self, weight=None, batch_axis=0, **kwargs):
        super(HingeLossbRec, self).__init__(weight=None, batch_axis=0,
                                            **kwargs)

    def forward(self, positive, negative, margin=1):
        distances = positive - negative
        loss = np.sum(np.maximum(- distances + margin, 0))
        return loss
</code></pre>
<p>이 두 가지 손실은 추천의 개인화 순위에 대해 상호 교환 가능합니다.</p>
<h2 id="요약-summary-111"><a class="header" href="#요약-summary-111">요약 (Summary)</a></h2>
<ul>
<li>추천 시스템의 개인화 순위 작업에는 포인트와이즈, 페어와이즈 및 리스트와이즈 방법이라는 세 가지 유형의 순위 손실을 사용할 수 있습니다.</li>
<li>두 가지 페어와이즈 손실인 베이지안 개인화 순위 손실과 힌지 손실은 상호 교환적으로 사용할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-126"><a class="header" href="#연습-문제-exercises-126">연습 문제 (Exercises)</a></h2>
<ul>
<li>BPR 및 힌지 손실의 변형이 있습니까?</li>
<li>BPR 또는 힌지 손실을 사용하는 추천 모델을 찾을 수 있습니까?</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/402">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="개인화-순위를-위한-신경망-협업-필터링-neural-collaborative-filtering-for-personalized-ranking"><a class="header" href="#개인화-순위를-위한-신경망-협업-필터링-neural-collaborative-filtering-for-personalized-ranking">개인화 순위를 위한 신경망 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></h1>
<p>이 섹션에서는 명시적 피드백을 넘어 암시적 피드백을 사용한 추천을 위한 신경망 협업 필터링(NCF) 프레임워크를 소개합니다. 암시적 피드백은 추천 시스템에 널리 퍼져 있습니다. 클릭, 구매, 시청과 같은 행동은 수집하기 쉽고 사용자의 선호도를 나타내는 일반적인 암시적 피드백입니다. 우리가 소개할 모델인 NeuMF :cite:<code>He.Liao.Zhang.ea.2017</code>는 신경망 행렬 분해(neural matrix factorization)의 줄임말로, 암시적 피드백으로 개인화 순위 작업을 해결하는 것을 목표로 합니다. 이 모델은 신경망의 유연성과 비선형성을 활용하여 행렬 분해의 내적을 대체하여 모델 표현력을 향상시키는 것을 목표로 합니다. 구체적으로, 이 모델은 일반화된 행렬 분해(GMF)와 MLP를 포함한 두 개의 서브 네트워크로 구성되며 단순한 내적 대신 두 경로에서 상호 작용을 모델링합니다. 이 두 네트워크의 출력은 최종 예측 점수 계산을 위해 연결됩니다. AutoRec의 평점 예측 작업과 달리 이 모델은 암시적 피드백을 기반으로 각 사용자에게 순위가 매겨진 추천 목록을 생성합니다. 이전 섹션에서 소개한 개인화 순위 손실을 사용하여 이 모델을 훈련할 것입니다.</p>
<h2 id="neumf-모델-the-neumf-model"><a class="header" href="#neumf-모델-the-neumf-model">NeuMF 모델 (The NeuMF model)</a></h2>
<p>앞서 언급했듯이 NeuMF는 두 개의 서브 네트워크를 융합합니다. GMF는 행렬 분해의 일반적인 신경망 버전으로, 입력은 사용자 및 항목 잠재 요인의 요소별 곱입니다. 두 개의 신경망 레이어로 구성됩니다.</p>
<p>$$
\mathbf{x} = \mathbf{p}_u \odot \mathbf{q}<em>i \
\hat{y}</em>{ui} = \alpha(\mathbf{h}^\top \mathbf{x}),
$$</p>
<p>여기서 $\odot$은 벡터의 하다마드 곱을 나타냅니다. $\mathbf{P} \in \mathbb{R}^{m \times k}$ 및 $\mathbf{Q} \in \mathbb{R}^{n \times k}$는 각각 사용자 및 항목 잠재 행렬에 해당합니다. $\mathbf{p}_u \in \mathbb{R}^{ k}$는 $P$의 $u^\textrm{th}$ 행이고 $\mathbf{q}<em>i \in \mathbb{R}^{ k}$는 $Q$의 $i^	extrm{th}$ 행입니다. $\alpha$와 $h$는 출력 레이어의 활성화 함수와 가중치를 나타냅니다. $\hat{y}</em>{ui}$는 사용자 $u$가 항목 $i$에 대해 부여할 수 있는 예측 점수입니다.</p>
<p>이 모델의 또 다른 구성 요소는 MLP입니다. 모델 유연성을 높이기 위해 MLP 서브 네트워크는 GMF와 사용자 및 항목 임베딩을 공유하지 않습니다. 사용자 및 항목 임베딩의 연결을 입력으로 사용합니다. 복잡한 연결과 비선형 변환을 통해 사용자와 항목 간의 복잡한 상호 작용을 추정할 수 있습니다. 더 정확하게는 MLP 서브 네트워크는 다음과 같이 정의됩니다.</p>
<p>$$
\begin{aligned}
z^{(1)} &amp;= \phi_1(\mathbf{U}_u, \mathbf{V}_i) = \left[ \mathbf{U}_u, \mathbf{V}<em>i \right] \
\phi^{(2)}(z^{(1)})  &amp;= \alpha^1(\mathbf{W}^{(2)} z^{(1)} + b^{(2)}) \
&amp;... \
\phi^{(L)}(z^{(L-1)}) &amp;= \alpha^L(\mathbf{W}^{(L)} z^{(L-1)} + b^{(L)})) \
\hat{y}</em>{ui} &amp;= \alpha(\mathbf{h}^\top\phi^L(z^{(L-1)}))
\end{aligned}
$$</p>
<p>여기서 $\mathbf{W}^<em>, \mathbf{b}^</em>$ 및 $\alpha^<em>$는 가중치 행렬, 편향 벡터 및 활성화 함수를 나타냅니다. $\phi^</em>$는 해당 레이어의 함수를 나타냅니다. $\mathbf{z}^*$는 해당 레이어의 출력을 나타냅니다.</p>
<p>GMF와 MLP의 결과를 융합하기 위해 단순한 덧셈 대신 NeuMF는 두 서브 네트워크의 마지막 두 번째 레이어를 연결하여 추가 레이어로 전달할 수 있는 특징 벡터를 만듭니다. 그 후 출력은 행렬 $\mathbf{h}$와 시그모이드 활성화 함수로 투영됩니다. 예측 레이어는 다음과 같이 공식화됩니다.
$$
\hat{y}_{ui} = \sigma(\mathbf{h}^\top[\mathbf{x}, \phi^L(z^{(L-1)})]).
$$</p>
<p>다음 그림은 NeuMF의 모델 아키텍처를 보여줍니다.</p>
<p><img src="chapter_recommender-systems/../img/rec-neumf.svg" alt="NeuMF 모델 그림" /></p>
<pre><code class="language-{.python .input  n=1}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
import mxnet as mx
import random

npx.set_np()
</code></pre>
<h2 id="모델-구현-model-implementation-1"><a class="header" href="#모델-구현-model-implementation-1">모델 구현 (Model Implementation)</a></h2>
<p>다음 코드는 NeuMF 모델을 구현합니다. 일반화된 행렬 분해 모델과 서로 다른 사용자 및 항목 임베딩 벡터가 있는 MLP로 구성됩니다. MLP의 구조는 파라미터 <code>nums_hiddens</code>로 제어됩니다. ReLU가 기본 활성화 함수로 사용됩니다.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
class NeuMF(nn.Block):
    def __init__(self, num_factors, num_users, num_items, nums_hiddens,
                 **kwargs):
        super(NeuMF, self).__init__(**kwargs)
        self.P = nn.Embedding(num_users, num_factors)
        self.Q = nn.Embedding(num_items, num_factors)
        self.U = nn.Embedding(num_users, num_factors)
        self.V = nn.Embedding(num_items, num_factors)
        self.mlp = nn.Sequential()
        for num_hiddens in nums_hiddens:
            self.mlp.add(nn.Dense(num_hiddens, activation='relu',
                                  use_bias=True))
        self.prediction_layer = nn.Dense(1, activation='sigmoid', use_bias=False)

    def forward(self, user_id, item_id):
        p_mf = self.P(user_id)
        q_mf = self.Q(item_id)
        gmf = p_mf * q_mf
        p_mlp = self.U(user_id)
        q_mlp = self.V(item_id)
        mlp = self.mlp(np.concatenate([p_mlp, q_mlp], axis=1))
        con_res = np.concatenate([gmf, mlp], axis=1)
        return self.prediction_layer(con_res)
</code></pre>
<h2 id="네거티브-샘플링을-사용한-사용자-정의-데이터셋-customized-dataset-with-negative-sampling"><a class="header" href="#네거티브-샘플링을-사용한-사용자-정의-데이터셋-customized-dataset-with-negative-sampling">네거티브 샘플링을 사용한 사용자 정의 데이터셋 (Customized Dataset with Negative Sampling)</a></h2>
<p>페어와이즈 순위 손실의 경우 중요한 단계는 네거티브 샘플링입니다. 각 사용자에 대해 사용자가 상호 작용하지 않은 항목은 후보 항목(관찰되지 않은 항목)입니다. 다음 함수는 사용자 ID와 후보 항목을 입력으로 받아 각 사용자에 대해 해당 사용자의 후보 집합에서 무작위로 네거티브 항목을 샘플링합니다. 훈련 단계에서 모델은 사용자가 좋아하는 항목이 싫어하거나 상호 작용하지 않은 항목보다 높은 순위에 오르도록 보장합니다.</p>
<pre><code class="language-{.python .input  n=3}">#@tab mxnet
class PRDataset(gluon.data.Dataset):
    def __init__(self, users, items, candidates, num_items):
        self.users = users
        self.items = items
        self.cand = candidates
        self.all = set([i for i in range(num_items)])

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        neg_items = list(self.all - set(self.cand[int(self.users[idx])]))
        indices = random.randint(0, len(neg_items) - 1)
        return self.users[idx], self.items[idx], neg_items[indices]
</code></pre>
<h2 id="평가기-evaluator"><a class="header" href="#평가기-evaluator">평가기 (Evaluator)</a></h2>
<p>이 섹션에서는 훈련 및 테스트 세트를 구성하기 위해 시간별 분할 전략을 채택합니다. 주어진 컷오프 $\ell$에서의 적중률($\textrm{Hit}@\ell$)과 ROC 곡선 아래 영역(AUC)을 포함한 두 가지 평가 척도를 사용하여 모델 효율성을 평가합니다. 각 사용자에 대한 주어진 위치 $\ell$에서의 적중률은 추천된 항목이 상위 $\ell$ 순위 목록에 포함되는지 여부를 나타냅니다. 공식적인 정의는 다음과 같습니다.</p>
<p>$$
\textrm{Hit}@\ell = \frac{1}{m} \sum_{u \in \mathcal{U}} \textbf{1}(rank_{u, g_u} &lt;= \ell),
$$</p>
<p>여기서 $\textbf{1}$은 정답 항목이 상위 $\ell$ 목록에 순위가 매겨지면 1이고 그렇지 않으면 0인 지시 함수를 나타냅니다. $rank_{u, g_u}$는 추천 목록에서 사용자 $u$의 정답 항목 $g_u$의 순위를 나타냅니다(이상적인 순위는 1입니다). $m$은 사용자 수입니다. $\mathcal{U}$는 사용자 집합입니다.</p>
<p>AUC의 정의는 다음과 같습니다.</p>
<p>$$
\textrm{AUC} = \frac{1}{m} \sum_{u \in \mathcal{U}} \frac{1}{|\mathcal{I} \backslash S_u|} \sum_{j \in I \backslash S_u} \textbf{1}(rank_{u, g_u} &lt; rank_{u, j}),
$$</p>
<p>여기서 $\mathcal{I}$는 항목 집합입니다. $S_u$는 사용자 $u$의 후보 항목입니다. 정밀도, 재현율 및 정규화된 할인 누적 이득(NDCG)과 같은 다른 많은 평가 프로토콜도 사용할 수 있습니다.</p>
<p>다음 함수는 각 사용자의 적중 횟수와 AUC를 계산합니다.</p>
<pre><code class="language-{.python .input  n=4}">#@tab mxnet
#@save
def hit_and_auc(rankedlist, test_matrix, k):
    hits_k = [(idx, val) for idx, val in enumerate(rankedlist[:k])
              if val in set(test_matrix)]
    hits_all = [(idx, val) for idx, val in enumerate(rankedlist)
                if val in set(test_matrix)]
    max = len(rankedlist) - 1
    auc = 1.0 * (max - hits_all[0][0]) / max if len(hits_all) &gt; 0 else 0
    return len(hits_k), auc
</code></pre>
<p>그런 다음 전체 적중률과 AUC는 다음과 같이 계산됩니다.</p>
<pre><code class="language-{.python .input  n=5}">#@tab mxnet
#@save
def evaluate_ranking(net, test_input, seq, candidates, num_users, num_items,
                     devices):
    ranked_list, ranked_items, hit_rate, auc = {}, {}, [], []
    all_items = set([i for i in range(num_users)])
    for u in range(num_users):
        neg_items = list(all_items - set(candidates[int(u)]))
        user_ids, item_ids, x, scores = [], [], [], []
        [item_ids.append(i) for i in neg_items]
        [user_ids.append(u) for _ in neg_items]
        x.extend([np.array(user_ids)])
        if seq is not None:
            x.append(seq[user_ids, :])
        x.extend([np.array(item_ids)])
        test_data_iter = gluon.data.DataLoader(
            gluon.data.ArrayDataset(*x), shuffle=False, last_batch="keep",
            batch_size=1024)
        for index, values in enumerate(test_data_iter):
            x = [gluon.utils.split_and_load(v, devices, even_split=False)
                 for v in values]
            scores.extend([list(net(*t).asnumpy()) for t in zip(*x)])
        scores = [item for sublist in scores for item in sublist]
        item_scores = list(zip(item_ids, scores))
        ranked_list[u] = sorted(item_scores, key=lambda t: t[1], reverse=True)
        ranked_items[u] = [r[0] for r in ranked_list[u]]
        temp = hit_and_auc(ranked_items[u], test_input[u], 50)
        hit_rate.append(temp[0])
        auc.append(temp[1])
    return np.mean(np.array(hit_rate)), np.mean(np.array(auc))
</code></pre>
<h2 id="모델-훈련-및-평가-training-and-evaluating-the-model-6"><a class="header" href="#모델-훈련-및-평가-training-and-evaluating-the-model-6">모델 훈련 및 평가 (Training and Evaluating the Model)</a></h2>
<p>훈련 함수는 아래와 같이 정의됩니다. 우리는 페어와이즈 방식으로 모델을 훈련합니다.</p>
<pre><code class="language-{.python .input  n=6}">#@tab mxnet
#@save
def train_ranking(net, train_iter, test_iter, loss, trainer, test_seq_iter,
                  num_users, num_items, num_epochs, devices, evaluator,
                  candidates, eval_step=1):
    timer, hit_rate, auc = d2l.Timer(), 0, 0
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
                            legend=['test hit rate', 'test AUC'])
    for epoch in range(num_epochs):
        metric, l = d2l.Accumulator(3), 0.
        for i, values in enumerate(train_iter):
            input_data = []
            for v in values:
                input_data.append(gluon.utils.split_and_load(v, devices))
            with autograd.record():
                p_pos = [net(*t) for t in zip(*input_data[:-1])]
                p_neg = [net(*t) for t in zip(*input_data[:-2],
                                              input_data[-1])]
                ls = [loss(p, n) for p, n in zip(p_pos, p_neg)]
            [l.backward(retain_graph=False) for l in ls]
            l += sum([l.asnumpy() for l in ls]).mean()/len(devices)
            trainer.step(values[0].shape[0])
            metric.add(l, values[0].shape[0], values[0].size)
            timer.stop()
        with autograd.predict_mode():
            if (epoch + 1) % eval_step == 0:
                hit_rate, auc = evaluator(net, test_iter, test_seq_iter,
                                          candidates, num_users, num_items,
                                          devices)
                animator.add(epoch + 1, (hit_rate, auc))
    print(f'train loss {metric[0] / metric[1]:.3f}, '
          f'test hit rate {float(hit_rate):.3f}, test AUC {float(auc):.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(devices)}')
</code></pre>
<p>이제 MovieLens 100k 데이터셋을 로드하고 모델을 훈련할 수 있습니다. MovieLens 데이터셋에는 평점만 있으므로, 정확도 손실을 감수하고 이 평점을 0과 1로 이진화합니다. 사용자가 항목을 평가한 경우 암시적 피드백을 1로 간주하고 그렇지 않으면 0으로 간주합니다. 항목을 평가하는 행동은 암시적 피드백을 제공하는 형태로 처리될 수 있습니다. 여기서는 사용자가 가장 최근에 상호 작용한 항목이 테스트를 위해 남겨지는 <code>seq-aware</code> 모드로 데이터셋을 분할합니다.</p>
<pre><code class="language-{.python .input  n=11}">#@tab mxnet
batch_size = 1024
df, num_users, num_items = d2l.read_data_ml100k()
train_data, test_data = d2l.split_data_ml100k(df, num_users, num_items,
                                              'seq-aware')
users_train, items_train, ratings_train, candidates = d2l.load_data_ml100k(
    train_data, num_users, num_items, feedback="implicit")
users_test, items_test, ratings_test, test_iter = d2l.load_data_ml100k(
    test_data, num_users, num_items, feedback="implicit")
train_iter = gluon.data.DataLoader(
    PRDataset(users_train, items_train, candidates, num_items ), batch_size,
    True, last_batch="rollover", num_workers=d2l.get_dataloader_workers())
</code></pre>
<p>그런 다음 모델을 생성하고 초기화합니다. 우리는 은닉 크기가 10인 3레이어 MLP를 사용합니다.</p>
<pre><code class="language-{.python .input  n=8}">#@tab mxnet
devices = d2l.try_all_gpus()
net = NeuMF(10, num_users, num_items, nums_hiddens=[10, 10, 10])
net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))
</code></pre>
<p>다음 코드는 모델을 훈련합니다.</p>
<pre><code class="language-{.python .input  n=12}">#@tab mxnet
lr, num_epochs, wd, optimizer = 0.01, 10, 1e-5, 'adam'
loss = d2l.BPRLoss()
trainer = gluon.Trainer(net.collect_params(), optimizer,
                        {"learning_rate": lr, 'wd': wd})
train_ranking(net, train_iter, test_iter, loss, trainer, None, num_users,
              num_items, num_epochs, devices, evaluate_ranking, candidates)
</code></pre>
<h2 id="요약-summary-112"><a class="header" href="#요약-summary-112">요약 (Summary)</a></h2>
<ul>
<li>행렬 분해 모델에 비선형성을 추가하면 모델 기능과 효율성을 향상시키는 데 도움이 됩니다.</li>
<li>NeuMF는 행렬 분해와 MLP의 조합입니다. MLP는 사용자 및 항목 임베딩의 연결을 입력으로 사용합니다.</li>
</ul>
<h2 id="연습-문제-exercises-127"><a class="header" href="#연습-문제-exercises-127">연습 문제 (Exercises)</a></h2>
<ul>
<li>잠재 요인의 크기를 변경해 보십시오. 잠재 요인의 크기가 모델 성능에 어떤 영향을 줍니까?</li>
<li>MLP의 아키텍처(예: 레이어 수, 각 레이어의 뉴런 수)를 변경하여 성능에 미치는 영향을 확인하십시오.</li>
<li>다른 최적화 도구, 학습률 및 가중치 감소율을 시도해 보십시오.</li>
<li>이 모델을 최적화하기 위해 지난 섹션에서 정의한 힌지 손실을 사용해 보십시오.</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/403">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="시퀀스-인식-추천-시스템-sequence-aware-recommender-systems"><a class="header" href="#시퀀스-인식-추천-시스템-sequence-aware-recommender-systems">시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></h1>
<p>이전 섹션에서는 사용자의 단기 행동을 고려하지 않고 추천 작업을 행렬 완성 문제로 추상화했습니다. 이 섹션에서는 순차적으로 정렬된 사용자 상호 작용 로그를 고려하는 추천 모델을 소개합니다. 이것은 입력이 과거 사용자 행동의 정렬되고 종종 타임스탬프가 있는 목록인 시퀀스 인식 추천기 :cite:<code>Quadrana.Cremonesi.Jannach.2018</code>입니다. 최근의 많은 문헌은 사용자의 시간적 행동 패턴을 모델링하고 관심 변화를 발견하는 데 이러한 정보를 통합하는 것의 유용성을 입증했습니다.</p>
<p>우리가 소개할 모델인 Caser :cite:<code>Tang.Wang.2018</code>는 합성곱 시퀀스 임베딩 추천 모델(convolutional sequence embedding recommendation model)의 약자로, 합성곱 신경망을 채택하여 사용자의 최근 활동의 동적 패턴 영향을 포착합니다. Caser의 주요 구성 요소는 수평 합성곱 네트워크와 수직 합성곱 네트워크로 구성되며, 각각 결합 수준(union-level) 및 포인트 수준(point-level) 시퀀스 패턴을 밝히는 것을 목표로 합니다. 포인트 수준 패턴은 과거 시퀀스의 단일 항목이 대상 항목에 미치는 영향을 나타내는 반면, 결합 수준 패턴은 여러 이전 행동이 후속 대상에 미치는 영향을 나타냅니다. 예를 들어, 우유와 버터를 함께 구매하면 둘 중 하나만 구매하는 것보다 밀가루를 구매할 확률이 더 높습니다. 또한 사용자의 일반적인 관심사 또는 장기적인 선호도도 마지막 완전 연결 레이어에서 모델링되어 사용자 관심사를 보다 포괄적으로 모델링할 수 있습니다. 모델의 세부 사항은 다음과 같습니다.</p>
<h2 id="모델-아키텍처-model-architectures"><a class="header" href="#모델-아키텍처-model-architectures">모델 아키텍처 (Model Architectures)</a></h2>
<p>시퀀스 인식 추천 시스템에서, 각 사용자는 항목 집합의 일부 항목 시퀀스와 연관됩니다. $S^u = (S_1^u, ... S_{|S_u|}^u)$를 정렬된 시퀀스라고 합시다. Caser의 목표는 사용자의 일반적인 취향과 단기 의도를 고려하여 항목을 추천하는 것입니다. 이전 $L$개 항목을 고려한다고 가정하면 타임 스텝 $t$에 대한 이전 상호 작용을 나타내는 임베딩 행렬을 구성할 수 있습니다.</p>
<p>$$
\mathbf{E}^{(u, t)} = [ \mathbf{q}<em>{S</em>{t-L}^u} , ..., \mathbf{q}<em>{S</em>{t-2}^u}, \mathbf{q}<em>{S</em>{t-1}^u} ]^\top,
$$</p>
<p>여기서 $\mathbf{Q} \in \mathbb{R}^{n \times k}$는 항목 임베딩을 나타내고 $\mathbf{q}_i$는 $i^\textrm{th}$ 행을 나타냅니다. $\mathbf{E}^{(u, t)} \in \mathbb{R}^{L \times k}$는 타임 스텝 $t$에서 사용자 $u$의 일시적 관심을 추론하는 데 사용할 수 있습니다. 입력 행렬 $\mathbf{E}^{(u, t)}$를 후속 두 합성곱 구성 요소의 입력인 이미지로 볼 수 있습니다.</p>
<p>수평 합성곱 레이어에는 $d$개의 수평 필터 $\mathbf{F}^j \in \mathbb{R}^{h \times k}, 1 \leq j \leq d, h = {1, ..., L}$가 있고, 수직 합성곱 레이어에는 $d'$개의 수직 필터 $\mathbf{G}^j \in \mathbb{R}^{ L \times 1}, 1 \leq j \leq d'$가 있습니다. 일련의 합성곱 및 풀링 연산 후 두 가지 출력을 얻습니다.</p>
<p>$$
\mathbf{o} = \textrm{HConv}(\mathbf{E}^{(u, t)}, \mathbf{F}) <br />
\mathbf{o}'= \textrm{VConv}(\mathbf{E}^{(u, t)}, \mathbf{G}) ,
$$</p>
<p>여기서 $\mathbf{o} \in \mathbb{R}^d$는 수평 합성곱 네트워크의 출력이고 $\mathbf{o}' \in \mathbb{R}^{kd'}$는 수직 합성곱 네트워크의 출력입니다. 단순화를 위해 합성곱 및 풀링 연산의 세부 사항은 생략합니다. 이들은 연결되어 완전 연결 신경망 레이어에 공급되어 더 높은 수준의 표현을 얻습니다.</p>
<p>$$
\mathbf{z} = \phi(\mathbf{W}[\mathbf{o}, \mathbf{o}']^\top + \mathbf{b}),
$$</p>
<p>여기서 $\mathbf{W} \in \mathbb{R}^{k \times (d + kd')}$는 가중치 행렬이고 $\mathbf{b} \in \mathbb{R}^k$는 편향입니다. 학습된 벡터 $\mathbf{z} \in \mathbb{R}^k$는 사용자의 단기 의도를 나타냅니다.</p>
<p>마지막으로, 예측 함수는 사용자의 단기 및 일반적인 취향을 결합하며 다음과 같이 정의됩니다.</p>
<p>$$
\hat{y}_{uit} = \mathbf{v}_i \cdot [\mathbf{z}, \mathbf{p}_u]^\top + \mathbf{b}'_i,
$$</p>
<p>여기서 $\mathbf{V} \in \mathbb{R}^{n \times 2k}$는 또 다른 항목 임베딩 행렬입니다. $\mathbf{b}' \in \mathbb{R}^n$은 항목별 편향입니다. $\mathbf{P} \in \mathbb{R}^{m \times k}$는 사용자의 일반적인 취향을 위한 사용자 임베딩 행렬입니다. $\mathbf{p}_u \in \mathbb{R}^{ k}$는 $P$의 $u^\textrm{th}$ 행이고 $\mathbf{v}_i \in \mathbb{R}^{2k}$는 $\mathbf{V}$의 $i^\textrm{th}$ 행입니다.</p>
<p>모델은 BPR 또는 힌지 손실로 학습할 수 있습니다. Caser의 아키텍처는 다음과 같습니다.</p>
<p><img src="chapter_recommender-systems/../img/rec-caser.svg" alt="Caser 모델 그림" /></p>
<p>먼저 필요한 라이브러리를 가져옵니다.</p>
<pre><code class="language-{.python .input  n=3}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
from mxnet.gluon import nn
import mxnet as mx
import random

npx.set_np()
</code></pre>
<h2 id="모델-구현-model-implementation-2"><a class="header" href="#모델-구현-model-implementation-2">모델 구현 (Model Implementation)</a></h2>
<p>다음 코드는 Caser 모델을 구현합니다. 수직 합성곱 레이어, 수평 합성곱 레이어 및 완전 연결 레이어로 구성됩니다.</p>
<pre><code class="language-{.python .input  n=4}">#@tab mxnet
class Caser(nn.Block):
    def __init__(self, num_factors, num_users, num_items, L=5, d=16,
                 d_prime=4, drop_ratio=0.05, **kwargs):
        super(Caser, self).__init__(**kwargs)
        self.P = nn.Embedding(num_users, num_factors)
        self.Q = nn.Embedding(num_items, num_factors)
        self.d_prime, self.d = d_prime, d
        # 수직 합성곱 레이어
        self.conv_v = nn.Conv2D(d_prime, (L, 1), in_channels=1)
        # 수평 합성곱 레이어
        h = [i + 1 for i in range(L)]
        self.conv_h, self.max_pool = nn.Sequential(), nn.Sequential()
        for i in h:
            self.conv_h.add(nn.Conv2D(d, (i, num_factors), in_channels=1))
            self.max_pool.add(nn.MaxPool1D(L - i + 1))
        # 완전 연결 레이어
        self.fc1_dim_v, self.fc1_dim_h = d_prime * num_factors, d * len(h)
        self.fc = nn.Dense(in_units=d_prime * num_factors + d * L,
                           activation='relu', units=num_factors)
        self.Q_prime = nn.Embedding(num_items, num_factors * 2)
        self.b = nn.Embedding(num_items, 1)
        self.dropout = nn.Dropout(drop_ratio)

    def forward(self, user_id, seq, item_id):
        item_embs = np.expand_dims(self.Q(seq), 1)
        user_emb = self.P(user_id)
        out, out_h, out_v, out_hs = None, None, None, []
        if self.d_prime:
            out_v = self.conv_v(item_embs)
            out_v = out_v.reshape(out_v.shape[0], self.fc1_dim_v)
        if self.d:
            for conv, maxp in zip(self.conv_h, self.max_pool):
                conv_out = np.squeeze(npx.relu(conv(item_embs)), axis=3)
                t = maxp(conv_out)
                pool_out = np.squeeze(t, axis=2)
                out_hs.append(pool_out)
            out_h = np.concatenate(out_hs, axis=1)
        out = np.concatenate([out_v, out_h], axis=1)
        z = self.fc(self.dropout(out))
        x = np.concatenate([z, user_emb], axis=1)
        q_prime_i = np.squeeze(self.Q_prime(item_id))
        b = np.squeeze(self.b(item_id))
        res = (x * q_prime_i).sum(1) + b
        return res
</code></pre>
<h2 id="네거티브-샘플링을-사용한-순차적-데이터셋-sequential-dataset-with-negative-sampling"><a class="header" href="#네거티브-샘플링을-사용한-순차적-데이터셋-sequential-dataset-with-negative-sampling">네거티브 샘플링을 사용한 순차적 데이터셋 (Sequential Dataset with Negative Sampling)</a></h2>
<p>순차적 상호 작용 데이터를 처리하려면 <code>Dataset</code> 클래스를 재구현해야 합니다. 다음 코드는 <code>SeqDataset</code>이라는 새 데이터셋 클래스를 생성합니다. 각 샘플에서 사용자 ID, 이전 $L$개의 상호 작용 항목을 시퀀스로, 그리고 다음 상호 작용 항목을 대상으로 출력합니다. 다음 그림은 한 사용자의 데이터 로딩 프로세스를 보여줍니다. 이 사용자가 9개의 영화를 좋아했다고 가정하면, 이 9개의 영화를 시간순으로 정리합니다. 최신 영화는 테스트 항목으로 남겨둡니다. 나머지 8개의 영화에 대해 3개의 훈련 샘플을 얻을 수 있으며, 각 샘플에는 5개($L=5$) 영화 시퀀스와 후속 항목이 대상 항목으로 포함됩니다. 네거티브 샘플도 사용자 정의 데이터셋에 포함됩니다.</p>
<p><img src="chapter_recommender-systems/../img/rec-seq-data.svg" alt="데이터 생성 프로세스 그림" /></p>
<pre><code class="language-{.python .input  n=5}">#@tab mxnet
class SeqDataset(gluon.data.Dataset):
    def __init__(self, user_ids, item_ids, L, num_users, num_items,
                 candidates):
        user_ids, item_ids = np.array(user_ids), np.array(item_ids)
        sort_idx = np.array(sorted(range(len(user_ids)),
                                   key=lambda k: user_ids[k]))
        u_ids, i_ids = user_ids[sort_idx], item_ids[sort_idx]
        temp, u_ids, self.cand = {}, u_ids.asnumpy(), candidates
        self.all_items = set([i for i in range(num_items)])
        [temp.setdefault(u_ids[i], []).append(i) for i, _ in enumerate(u_ids)]
        temp = sorted(temp.items(), key=lambda x: x[0])
        u_ids = np.array([i[0] for i in temp])
        idx = np.array([i[1][0] for i in temp])
        self.ns = ns = int(sum([c - L if c &gt;= L + 1 else 1 for c
                                in np.array([len(i[1]) for i in temp])]))
        self.seq_items = np.zeros((ns, L))
        self.seq_users = np.zeros(ns, dtype='int32')
        self.seq_tgt = np.zeros((ns, 1))
        self.test_seq = np.zeros((num_users, L))
        test_users, _uid = np.empty(num_users), None
        for i, (uid, i_seq) in enumerate(self._seq(u_ids, i_ids, idx, L + 1)):
            if uid != _uid:
                self.test_seq[uid][:] = i_seq[-L:]
                test_users[uid], _uid = uid, uid
            self.seq_tgt[i][:] = i_seq[-1:]
            self.seq_items[i][:], self.seq_users[i] = i_seq[:L], uid

    def _win(self, tensor, window_size, step_size=1):
        if len(tensor) - window_size &gt;= 0:
            for i in range(len(tensor), 0, - step_size):
                if i - window_size &gt;= 0:
                    yield tensor[i - window_size:i]
                else:
                    break
        else:
            yield tensor

    def _seq(self, u_ids, i_ids, idx, max_len):
        for i in range(len(idx)):
            stop_idx = None if i &gt;= len(idx) - 1 else int(idx[i + 1])
            for s in self._win(i_ids[int(idx[i]):stop_idx], max_len):
                yield (int(u_ids[i]), s)

    def __len__(self):
        return self.ns

    def __getitem__(self, idx):
        neg = list(self.all_items - set(self.cand[int(self.seq_users[idx])]))
        i = random.randint(0, len(neg) - 1)
        return (self.seq_users[idx], self.seq_items[idx], self.seq_tgt[idx],
                neg[i])
</code></pre>
<h2 id="movielens-100k-데이터셋-로드-load-the-movielens-100k-dataset"><a class="header" href="#movielens-100k-데이터셋-로드-load-the-movielens-100k-dataset">MovieLens 100K 데이터셋 로드 (Load the MovieLens 100K dataset)</a></h2>
<p>그 후, MovieLens 100K 데이터셋을 시퀀스 인식 모드로 읽고 분할하고 위에서 구현된 순차적 데이터 로더를 사용하여 훈련 데이터를 로드합니다.</p>
<pre><code class="language-{.python .input  n=6}">#@tab mxnet
TARGET_NUM, L, batch_size = 1, 5, 4096
df, num_users, num_items = d2l.read_data_ml100k()
train_data, test_data = d2l.split_data_ml100k(df, num_users, num_items,
                                              'seq-aware')
users_train, items_train, ratings_train, candidates = d2l.load_data_ml100k(
    train_data, num_users, num_items, feedback="implicit")
users_test, items_test, ratings_test, test_iter = d2l.load_data_ml100k(
    test_data, num_users, num_items, feedback="implicit")
train_seq_data = SeqDataset(users_train, items_train, L, num_users,
                            num_items, candidates)
train_iter = gluon.data.DataLoader(train_seq_data, batch_size, True,
                                   last_batch="rollover",
                                   num_workers=d2l.get_dataloader_workers())
test_seq_iter = train_seq_data.test_seq
train_seq_data[0]
</code></pre>
<p>훈련 데이터 구조는 위에 나와 있습니다. 첫 번째 요소는 사용자 ID이고, 다음 목록은 이 사용자가 좋아한 지난 5개의 항목을 나타내며, 마지막 요소는 사용자가 5개 항목 이후에 좋아한 항목입니다.</p>
<h2 id="모델-훈련-train-the-model"><a class="header" href="#모델-훈련-train-the-model">모델 훈련 (Train the Model)</a></h2>
<p>이제 모델을 훈련해 봅시다. 결과를 비교할 수 있도록 지난 섹션의 NeuMF와 동일한 설정(학습률, 최적화 도구 및 $k$ 포함)을 사용합니다.</p>
<pre><code class="language-{.python .input  n=7}">#@tab mxnet
devices = d2l.try_all_gpus()
net = Caser(10, num_users, num_items, L)
net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))
lr, num_epochs, wd, optimizer = 0.04, 8, 1e-5, 'adam'
loss = d2l.BPRLoss()
trainer = gluon.Trainer(net.collect_params(), optimizer,
                        {"learning_rate": lr, 'wd': wd})

# 실행에 1시간 이상 소요됨 (MXNet 수정 대기 중)
# d2l.train_ranking(net, train_iter, test_iter, loss, trainer, test_seq_iter, num_users, num_items, num_epochs, devices, d2l.evaluate_ranking, candidates, eval_step=1)
</code></pre>
<h2 id="요약-summary-113"><a class="header" href="#요약-summary-113">요약 (Summary)</a></h2>
<ul>
<li>사용자의 단기 및 장기 관심사를 추론하면 사용자가 선호하는 다음 항목을 더 효과적으로 예측할 수 있습니다.</li>
<li>합성곱 신경망을 활용하여 순차적 상호 작용에서 사용자의 단기 관심사를 포착할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-128"><a class="header" href="#연습-문제-exercises-128">연습 문제 (Exercises)</a></h2>
<ul>
<li>수평 및 수직 합성곱 네트워크 중 하나를 제거하여 절제 연구를 수행하십시오. 어떤 구성 요소가 더 중요합니까?</li>
<li>하이퍼파라미터 $L$을 변경해 보십시오. 과거 상호 작용이 더 길수록 정확도가 높아집니까?</li>
<li>위에서 소개한 시퀀스 인식 추천 작업 외에도 세션 기반 추천 :cite:<code>Hidasi.Karatzoglou.Baltrunas.ea.2015</code>이라는 또 다른 유형의 시퀀스 인식 추천 작업이 있습니다. 이 두 작업의 차이점을 설명할 수 있습니까?</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/404">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="풍부한-기능을-갖춘-추천-시스템-feature-rich-recommender-systems"><a class="header" href="#풍부한-기능을-갖춘-추천-시스템-feature-rich-recommender-systems">풍부한 기능을 갖춘 추천 시스템 (Feature-Rich Recommender Systems)</a></h1>
<p>상호 작용 데이터는 사용자 선호도 및 관심사의 가장 기본적인 표시입니다. 이는 이전에 소개된 모델에서 중요한 역할을 합니다. 그러나 상호 작용 데이터는 일반적으로 매우 희소하며 때때로 노이즈가 있을 수 있습니다. 이 문제를 해결하기 위해 항목의 특성, 사용자 프로필, 심지어 상호 작용이 발생한 상황과 같은 부가 정보를 추천 모델에 통합할 수 있습니다. 이러한 기능을 활용하면 특히 상호 작용 데이터가 부족할 때 사용자 관심사의 효과적인 예측 변수가 될 수 있으므로 추천에 도움이 됩니다. 따라서 추천 모델은 이러한 기능을 처리하고 모델에 콘텐츠/문맥 인식을 제공하는 기능을 갖추는 것이 필수적입니다. 이러한 유형의 추천 모델을 보여주기 위해 온라인 광고 추천을 위한 클릭률(CTR)에 대한 또 다른 작업 :cite:<code>McMahan.Holt.Sculley.ea.2013</code>을 소개하고 익명 광고 데이터셋을 제시합니다. 타겟 광고 서비스는 광범위한 관심을 끌었으며 종종 추천 엔진으로 프레임화됩니다. 사용자의 개인적인 취향과 관심사에 맞는 광고를 추천하는 것은 클릭률 향상에 중요합니다.</p>
<p>디지털 마케팅 담당자는 온라인 광고를 사용하여 고객에게 광고를 표시합니다. 클릭률은 광고주가 노출 수 대비 광고에서 받는 클릭 수를 측정하는 지표이며 다음 공식으로 계산된 백분율로 표시됩니다.</p>
<p>$$ \textrm{CTR} = \frac{#\textrm{Clicks}} {#\textrm{Impressions}} \times 100 % .$$</p>
<p>클릭률은 예측 알고리즘의 효과를 나타내는 중요한 신호입니다. 클릭률 예측은 웹사이트의 무언가가 클릭될 가능성을 예측하는 작업입니다. CTR 예측 모델은 타겟 광고 시스템뿐만 아니라 일반 항목(예: 영화, 뉴스, 제품) 추천 시스템, 이메일 캠페인, 심지어 검색 엔진에서도 사용될 수 있습니다. 또한 사용자 만족도, 전환율과 밀접한 관련이 있으며 광고주가 현실적인 기대치를 설정하는 데 도움이 되므로 캠페인 목표를 설정하는 데 도움이 될 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from collections import defaultdict
from d2l import mxnet as d2l
from mxnet import gluon, np
import os
</code></pre>
<h2 id="온라인-광고-데이터셋-an-online-advertising-dataset"><a class="header" href="#온라인-광고-데이터셋-an-online-advertising-dataset">온라인 광고 데이터셋 (An Online Advertising Dataset)</a></h2>
<p>인터넷과 모바일 기술의 상당한 발전으로 온라인 광고는 중요한 수입원이 되었으며 인터넷 산업에서 대부분의 수익을 창출합니다. 일반 방문자를 유료 고객으로 전환하려면 관련 광고 또는 사용자의 관심을 끄는 광고를 표시하는 것이 중요합니다. 우리가 소개한 데이터셋은 온라인 광고 데이터셋입니다. 34개의 필드로 구성되어 있으며 첫 번째 열은 광고가 클릭되었는지(1) 아닌지(0)를 나타내는 타겟 변수입니다. 다른 모든 열은 범주형 특성입니다. 열은 광고 ID, 사이트 또는 애플리케이션 ID, 장치 ID, 시간, 사용자 프로필 등을 나타낼 수 있습니다. 익명화 및 개인 정보 보호 문제로 인해 기능의 실제 의미는 공개되지 않습니다.</p>
<p>다음 코드는 서버에서 데이터셋을 다운로드하여 로컬 데이터 폴더에 저장합니다.</p>
<pre><code class="language-{.python .input  n=15}">#@tab mxnet
#@save
d2l.DATA_HUB['ctr'] = (d2l.DATA_URL + 'ctr.zip',
                       'e18327c48c8e8e5c23da714dd614e390d369843f')

data_dir = d2l.download_extract('ctr')
</code></pre>
<p>각각 15,000개와 3,000개의 샘플/라인으로 구성된 훈련 세트와 테스트 세트가 있습니다.</p>
<h2 id="데이터셋-래퍼-dataset-wrapper"><a class="header" href="#데이터셋-래퍼-dataset-wrapper">데이터셋 래퍼 (Dataset Wrapper)</a></h2>
<p>데이터 로딩의 편의를 위해 CSV 파일에서 광고 데이터셋을 로드하고 <code>DataLoader</code>에서 사용할 수 있는 <code>CTRDataset</code>을 구현합니다.</p>
<pre><code class="language-{.python .input  n=13}">#@tab mxnet
#@save
class CTRDataset(gluon.data.Dataset):
    def __init__(self, data_path, feat_mapper=None, defaults=None,
                 min_threshold=4, num_feat=34):
        self.NUM_FEATS, self.count, self.data = num_feat, 0, {}
        feat_cnts = defaultdict(lambda: defaultdict(int))
        self.feat_mapper, self.defaults = feat_mapper, defaults
        self.field_dims = np.zeros(self.NUM_FEATS, dtype=np.int64)
        with open(data_path) as f:
            for line in f:
                instance = {}
                values = line.rstrip('\n').split('\t')
                if len(values) != self.NUM_FEATS + 1:
                    continue
                label = np.float32([0, 0])
                label[int(values[0])] = 1
                instance['y'] = [np.float32(values[0])]
                for i in range(1, self.NUM_FEATS + 1):
                    feat_cnts[i][values[i]] += 1
                    instance.setdefault('x', []).append(values[i])
                self.data[self.count] = instance
                self.count = self.count + 1
        if self.feat_mapper is None and self.defaults is None:
            feat_mapper = {i: {feat for feat, c in cnt.items() if c &gt;= 
                               min_threshold} for i, cnt in feat_cnts.items()}
            self.feat_mapper = {i: {feat_v: idx for idx, feat_v in enumerate(feat_values)} 
                                for i, feat_values in feat_mapper.items()}
            self.defaults = {i: len(feat_values) for i, feat_values in feat_mapper.items()}
        for i, fm in self.feat_mapper.items():
            self.field_dims[i - 1] = len(fm) + 1
        self.offsets = np.array((0, *np.cumsum(self.field_dims).asnumpy()
                                 [:-1]))
        
    def __len__(self):
        return self.count
    
    def __getitem__(self, idx):
        feat = np.array([self.feat_mapper[i + 1].get(v, self.defaults[i + 1])
                         for i, v in enumerate(self.data[idx]['x'])])
        return feat + self.offsets, self.data[idx]['y']
</code></pre>
<p>다음 예제는 훈련 데이터를 로드하고 첫 번째 레코드를 출력합니다.</p>
<pre><code class="language-{.python .input  n=16}">#@tab mxnet
train_data = CTRDataset(os.path.join(data_dir, 'train.csv'))
train_data[0]
</code></pre>
<p>보시다시피 34개의 필드 모두 범주형 특성입니다. 각 값은 해당 항목의 원-핫 인덱스를 나타냅니다. 레이블 $0$은 클릭되지 않았음을 의미합니다. 이 <code>CTRDataset</code>은 Criteo 디스플레이 광고 챌린지 <a href="https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/">데이터셋</a> 및 Avazu 클릭률 예측 <a href="https://www.kaggle.com/c/avazu-ctr-prediction">데이터셋</a>과 같은 다른 데이터셋을 로드하는 데에도 사용할 수 있습니다.</p>
<h2 id="요약-summary-114"><a class="header" href="#요약-summary-114">요약 (Summary)</a></h2>
<ul>
<li>클릭률은 광고 시스템 및 추천 시스템의 효과를 측정하는 데 사용되는 중요한 지표입니다.</li>
<li>클릭률 예측은 일반적으로 이진 분류 문제로 변환됩니다. 목표는 주어진 특성을 기반으로 광고/항목이 클릭될지 여부를 예측하는 것입니다.</li>
</ul>
<h2 id="연습-문제-exercises-129"><a class="header" href="#연습-문제-exercises-129">연습 문제 (Exercises)</a></h2>
<ul>
<li>제공된 <code>CTRDataset</code>으로 Criteo 및 Avazu 데이터셋을 로드할 수 있습니까? Criteo 데이터셋은 실수 값 특성으로 구성되어 있으므로 코드를 약간 수정해야 할 수도 있습니다.</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/405">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="인수-분해-머신-factorization-machines"><a class="header" href="#인수-분해-머신-factorization-machines">인수 분해 머신 (Factorization Machines)</a></h1>
<p>:citet:<code>Rendle.2010</code>에 의해 제안된 인수 분해 머신(Factorization Machines, FM)은 분류, 회귀 및 순위 지정 작업에 사용할 수 있는 지도 학습 알고리즘입니다. 이는 빠르게 주목을 받았으며 예측 및 추천을 위한 인기 있고 영향력 있는 방법이 되었습니다. 특히, 이는 선형 회귀 모델과 행렬 분해 모델의 일반화입니다. 더욱이, 이는 다항식 커널을 가진 서포트 벡터 머신을 연상시킵니다. 선형 회귀 및 행렬 분해에 비해 인수 분해 머신의 강점은 다음과 같습니다: (1) $\chi$-way 변수 상호 작용을 모델링할 수 있습니다. 여기서 $\chi$는 다항식 차수이며 일반적으로 2로 설정됩니다. (2) 인수 분해 머신과 관련된 빠른 최적화 알고리즘은 다항식 계산 시간을 선형 복잡도로 줄여줄 수 있어, 특히 고차원 희소 입력에 대해 매우 효율적입니다. 이러한 이유로 인수 분해 머신은 현대적인 광고 및 제품 추천에 널리 사용됩니다. 기술적인 세부 사항과 구현은 아래에 설명되어 있습니다.</p>
<h2 id="2-way-인수-분해-머신"><a class="header" href="#2-way-인수-분해-머신">2-Way 인수 분해 머신</a></h2>
<p>공식적으로, $x \in \mathbb{R}^d$를 한 샘플의 특성 벡터라고 하고, $y$를 그에 대응하는 레이블이라고 합시다. 레이블은 실수 값 레이블이거나 이진 클래스 "클릭/비클릭"과 같은 클래스 레이블일 수 있습니다. 차수가 2인 인수 분해 머신의 모델은 다음과 같이 정의됩니다:</p>
<p>$$
\hat{y}(x) = \mathbf{w}<em>0 + \sum</em>{i=1}^d \mathbf{w}<em>i x_i + \sum</em>{i=1}^d\sum_{j=i+1}^d \langle\mathbf{v}_i, \mathbf{v}_j\rangle x_i x_j
$$</p>
<p>여기서 $\mathbf{w}_0 \in \mathbb{R}$은 전역 편향(global bias)이고; $\mathbf{w} \in \mathbb{R}^d$는 $i$번째 변수의 가중치를 나타내며; $\mathbf{V} \in \mathbb{R}^{d\times k}$는 특성 임베딩을 나타냅니다. $\mathbf{v}_i$는 $\mathbf{V}$의 $i^{\textrm{th}}$번째 행을 나타내고; $k$는 잠재 요인의 차원이며; $\langle\cdot, \cdot \rangle$은 두 벡터의 내적입니다. $\langle \mathbf{v}_i, \mathbf{v}_j \rangle$은 $i^{\textrm{th}}$번째와 $j^{\textrm{th}}$번째 특성 간의 상호 작용을 모델링합니다. 일부 특성 상호 작용은 쉽게 이해될 수 있으므로 전문가에 의해 설계될 수 있습니다. 그러나 대부분의 다른 특성 상호 작용은 데이터 속에 숨겨져 있어 식별하기 어렵습니다. 따라서 특성 상호 작용을 자동으로 모델링하면 특성 엔지니어링의 노력을 크게 줄일 수 있습니다. 처음 두 항이 선형 회귀 모델에 대응하고 마지막 항이 행렬 분해 모델의 확장임은 명백합니다. 특성 $i$가 항목을 나타내고 특성 $j$가 사용자를 나타낸다면, 세 번째 항은 정확히 사용자 임베딩과 항목 임베딩 간의 내적입니다. FM이 더 높은 차수(차수 &gt; 2)로 일반화될 수도 있다는 점은 주목할 가치가 있습니다. 그럼에도 불구하고 수치적 안정성이 일반화 능력을 약화시킬 수 있습니다.</p>
<h2 id="효율적인-최적화-기준-an-efficient-optimization-criterion"><a class="header" href="#효율적인-최적화-기준-an-efficient-optimization-criterion">효율적인 최적화 기준 (An Efficient Optimization Criterion)</a></h2>
<p>인수 분해 머신을 단순한 방식으로 최적화하면 모든 쌍별 상호 작용을 계산해야 하므로 $\mathcal{O}(kd^2)$의 복잡도가 발생합니다. 이 비효율성 문제를 해결하기 위해, 우리는 FM의 세 번째 항을 재구성하여 계산 비용을 크게 줄여 선형 시간 복잡도($\mathcal{O}(kd)$)로 이끌 수 있습니다. 쌍별 상호 작용 항의 재구성은 다음과 같습니다:</p>
<p>$$
\begin{aligned}
&amp;\sum_{i=1}^d \sum_{j=i+1}^d \langle\mathbf{v}<em>i, \mathbf{v}<em>j\rangle x_i x_j \
&amp;= \frac{1}{2} \sum</em>{i=1}^d \sum</em>{j=1}^d\langle\mathbf{v}<em>i, \mathbf{v}<em>j\rangle x_i x_j - \frac{1}{2}\sum</em>{i=1}^d \langle\mathbf{v}<em>i, \mathbf{v}<em>i\rangle x_i x_i \
&amp;= \frac{1}{2} \big (\sum</em>{i=1}^d \sum</em>{j=1}^d \sum</em>{l=1}^k\mathbf{v}<em>{i, l} \mathbf{v}</em>{j, l} x_i x_j - \sum_{i=1}^d \sum_{l=1}^k \mathbf{v}<em>{i, l} \mathbf{v}</em>{i, l} x_i x_i \big)<br />
&amp;= \frac{1}{2} \sum_{l=1}^k \big ((\sum_{i=1}^d \mathbf{v}<em>{i, l} x_i) (\sum</em>{j=1}^d \mathbf{v}<em>{j, l}x_j) - \sum</em>{i=1}^d \mathbf{v}<em>{i, l}^2 x_i^2 \big ) \
&amp;= \frac{1}{2} \sum</em>{l=1}^k \big ((\sum_{i=1}^d \mathbf{v}<em>{i, l} x_i)^2 - \sum</em>{i=1}^d \mathbf{v}_{i, l}^2 x_i^2)
\end{aligned}
$$</p>
<p>이 재구성을 통해 모델 복잡도가 크게 감소합니다. 더욱이, 희소 특성의 경우 0이 아닌 요소만 계산하면 되므로 전체 복잡도는 0이 아닌 특성의 수에 선형적입니다.</p>
<p>FM 모델을 학습하기 위해, 회귀 작업에는 MSE 손실을, 분류 작업에는 크로스 엔트로피 손실을, 순위 지정 작업에는 BPR 손실을 사용할 수 있습니다. 확률적 경사 하강법 및 Adam과 같은 표준 최적화기를 최적화에 사용할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import init, gluon, np, npx
from mxnet.gluon import nn
import os

npx.set_np()
</code></pre>
<h2 id="모델-구현-model-implementation-3"><a class="header" href="#모델-구현-model-implementation-3">모델 구현 (Model Implementation)</a></h2>
<p>다음 코드는 인수 분해 머신을 구현합니다. FM이 선형 회귀 블록과 효율적인 특성 상호 작용 블록으로 구성되어 있음을 명확히 알 수 있습니다. 우리는 CTR 예측을 분류 작업으로 취급하므로 최종 점수에 시그모이드 함수를 적용합니다.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
class FM(nn.Block):
    def __init__(self, field_dims, num_factors):
        super(FM, self).__init__()
        num_inputs = int(sum(field_dims))
        self.embedding = nn.Embedding(num_inputs, num_factors)
        self.fc = nn.Embedding(num_inputs, 1)
        self.linear_layer = nn.Dense(1, use_bias=True)

    def forward(self, x):
        square_of_sum = np.sum(self.embedding(x), axis=1) ** 2
        sum_of_square = np.sum(self.embedding(x) ** 2, axis=1)
        x = self.linear_layer(self.fc(x).sum(1)) \
            + 0.5 * (square_of_sum - sum_of_square).sum(1, keepdims=True)
        x = npx.sigmoid(x)
        return x
</code></pre>
<h2 id="광고-데이터셋-로드-load-the-advertising-dataset"><a class="header" href="#광고-데이터셋-로드-load-the-advertising-dataset">광고 데이터셋 로드 (Load the Advertising Dataset)</a></h2>
<p>우리는 이전 섹션의 CTR 데이터 래퍼를 사용하여 온라인 광고 데이터셋을 로드합니다.</p>
<pre><code class="language-{.python .input  n=3}">#@tab mxnet
batch_size = 2048
data_dir = d2l.download_extract('ctr')
train_data = d2l.CTRDataset(os.path.join(data_dir, 'train.csv'))
test_data = d2l.CTRDataset(os.path.join(data_dir, 'test.csv'),
                           feat_mapper=train_data.feat_mapper,
                           defaults=train_data.defaults)
train_iter = gluon.data.DataLoader(
    train_data, shuffle=True, last_batch='rollover', batch_size=batch_size,
    num_workers=d2l.get_dataloader_workers())
test_iter = gluon.data.DataLoader(
    test_data, shuffle=False, last_batch='rollover', batch_size=batch_size,
    num_workers=d2l.get_dataloader_workers())
</code></pre>
<h2 id="모델-훈련-train-the-model-1"><a class="header" href="#모델-훈련-train-the-model-1">모델 훈련 (Train the Model)</a></h2>
<p>그 후, 모델을 훈련합니다. 학습률은 0.02로 설정하고 임베딩 크기는 기본적으로 20으로 설정합니다. 모델 훈련을 위해 <code>Adam</code> 최적화기와 <code>SigmoidBinaryCrossEntropyLoss</code> 손실을 사용합니다.</p>
<pre><code class="language-{.python .input  n=5}">#@tab mxnet
devices = d2l.try_all_gpus()
net = FM(train_data.field_dims, num_factors=20)
net.initialize(init.Xavier(), ctx=devices)
lr, num_epochs, optimizer = 0.02, 30, 'adam'
trainer = gluon.Trainer(net.collect_params(), optimizer,
                        {'learning_rate': lr})
loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<h2 id="요약-summary-115"><a class="header" href="#요약-summary-115">요약 (Summary)</a></h2>
<ul>
<li>FM은 회귀, 분류 및 순위 지정과 같은 다양한 작업에 적용될 수 있는 일반적인 프레임워크입니다.</li>
<li>특성 상호 작용/교차(crossing)는 예측 작업에 중요하며, 2-way 상호 작용은 FM으로 효율적으로 모델링될 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-130"><a class="header" href="#연습-문제-exercises-130">연습 문제 (Exercises)</a></h2>
<ul>
<li>Avazu, MovieLens, Criteo 데이터셋과 같은 다른 데이터셋에서 FM을 테스트할 수 있습니까?</li>
<li>임베딩 크기를 변경하여 성능에 미치는 영향을 확인해 보십시오. 행렬 분해와 유사한 패턴을 관찰할 수 있습니까?</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/406">토론</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="심층-인수분해-머신-deep-factorization-machines"><a class="header" href="#심층-인수분해-머신-deep-factorization-machines">심층 인수분해 머신 (Deep Factorization Machines)</a></h1>
<p>효과적인 특성 조합을 학습하는 것은 클릭률 예측 작업의 성공에 중요합니다. 인수분해 머신은 선형 패러다임(예: 이중 선형 상호 작용)으로 특성 상호 작용을 모델링합니다. 이는 내재된 특성 교차 구조가 일반적으로 매우 복잡하고 비선형적인 실제 데이터에는 종종 불충분합니다. 설상가상으로 실제로는 인수분해 머신에서 2차 특성 상호 작용이 일반적으로 사용됩니다. 인수분해 머신으로 더 높은 차수의 특성 조합을 모델링하는 것은 이론적으로 가능하지만 수치적 불안정성과 높은 계산 복잡성으로 인해 일반적으로 채택되지 않습니다.</p>
<p>한 가지 효과적인 해결책은 심층 신경망을 사용하는 것입니다. 심층 신경망은 특성 표현 학습에 강력하며 정교한 특성 상호 작용을 학습할 수 있는 잠재력이 있습니다. 따라서 심층 신경망을 인수분해 머신에 통합하는 것은 자연스러운 일입니다. 인수분해 머신에 비선형 변환 레이어를 추가하면 저차 특성 조합과 고차 특성 조합을 모두 모델링할 수 있는 기능이 제공됩니다. 또한 입력의 비선형 고유 구조도 심층 신경망으로 포착할 수 있습니다. 이 섹션에서는 FM과 심층 신경망을 결합한 심층 인수분해 머신(DeepFM) :cite:<code>Guo.Tang.Ye.ea.2017</code>이라는 대표적인 모델을 소개합니다.</p>
<h2 id="모델-아키텍처-model-architectures-1"><a class="header" href="#모델-아키텍처-model-architectures-1">모델 아키텍처 (Model Architectures)</a></h2>
<p>DeepFM은 병렬 구조로 통합된 FM 구성 요소와 딥 구성 요소로 구성됩니다. FM 구성 요소는 저차 특성 상호 작용을 모델링하는 데 사용되는 2-way 인수분해 머신과 동일합니다. 딥 구성 요소는 고차 특성 상호 작용과 비선형성을 포착하는 데 사용되는 MLP입니다. 이 두 구성 요소는 동일한 입력/임베딩을 공유하고 그 출력은 최종 예측으로 합산됩니다. DeepFM의 정신은 암기와 일반화를 모두 포착할 수 있는 Wide &amp; Deep 아키텍처와 유사하다는 점을 지적할 가치가 있습니다. Wide &amp; Deep 모델에 비해 DeepFM의 장점은 특성 조합을 자동으로 식별하여 수작업 특성 엔지니어링 노력을 줄여준다는 것입니다.</p>
<p>간결함을 위해 FM 구성 요소에 대한 설명은 생략하고 출력을 $\hat{y}^{(FM)}$으로 표시합니다. 자세한 내용은 지난 섹션을 참조하십시오. $\mathbf{e}_i \in \mathbb{R}^{k}$를 $i^\textrm{th}$ 필드의 잠재 특성 벡터라고 합시다. 딥 구성 요소의 입력은 희소 범주형 특성 입력으로 조회된 모든 필드의 밀집 임베딩의 연결이며 다음과 같이 표시됩니다.</p>
<p>$$
\mathbf{z}^{(0)}  = [\mathbf{e}_1, \mathbf{e}_2, ..., \mathbf{e}_f],
$$</p>
<p>여기서 $f$는 필드 수입니다. 그런 다음 다음 신경망에 공급됩니다.</p>
<p>$$
\mathbf{z}^{(l)}  = \alpha(\mathbf{W}^{(l)}\mathbf{z}^{(l-1)} + \mathbf{b}^{(l)}),
$$</p>
<p>여기서 $\alpha$는 활성화 함수입니다. $\mathbf{W}<em>{l}$과 $\mathbf{b}</em>{l}$은 $l^\textrm{th}$ 레이어의 가중치와 편향입니다. $y_{DNN}$을 예측의 출력이라고 합시다. DeepFM의 최종 예측은 FM과 DNN의 출력의 합입니다. 따라서 다음을 얻습니다.</p>
<p>$$
\hat{y} = \sigma(\hat{y}^{(FM)} + \hat{y}^{(DNN)}),
$$</p>
<p>여기서 $\sigma$는 시그모이드 함수입니다. DeepFM의 아키텍처는 아래와 같습니다.
<img src="chapter_recommender-systems/../img/rec-deepfm.svg" alt="DeepFM 모델 그림" /></p>
<p>DeepFM만이 심층 신경망과 FM을 결합하는 유일한 방법은 아니라는 점에 유의해야 합니다. 특성 상호 작용에 비선형 레이어를 추가할 수도 있습니다 :cite:<code>He.Chua.2017</code>.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import init, gluon, np, npx
from mxnet.gluon import nn
import os

npx.set_np()
</code></pre>
<h2 id="deepfm-구현-implementation-of-deepfm"><a class="header" href="#deepfm-구현-implementation-of-deepfm">DeepFM 구현 (Implementation of DeepFM)</a></h2>
<p>DeepFM의 구현은 FM의 구현과 유사합니다. FM 부분은 변경하지 않고 활성화 함수로 <code>relu</code>가 있는 MLP 블록을 사용합니다. 드롭아웃도 모델을 정규화하는 데 사용됩니다. MLP의 뉴런 수는 <code>mlp_dims</code> 하이퍼파라미터로 조정할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=2}">#@tab mxnet
class DeepFM(nn.Block):
    def __init__(self, field_dims, num_factors, mlp_dims, drop_rate=0.1):
        super(DeepFM, self).__init__()
        num_inputs = int(sum(field_dims))
        self.embedding = nn.Embedding(num_inputs, num_factors)
        self.fc = nn.Embedding(num_inputs, 1)
        self.linear_layer = nn.Dense(1, use_bias=True)
        input_dim = self.embed_output_dim = len(field_dims) * num_factors
        self.mlp = nn.Sequential()
        for dim in mlp_dims:
            self.mlp.add(nn.Dense(dim, 'relu', True, in_units=input_dim))
            self.mlp.add(nn.Dropout(rate=drop_rate))
            input_dim = dim
        self.mlp.add(nn.Dense(in_units=input_dim, units=1))

    def forward(self, x):
        embed_x = self.embedding(x)
        square_of_sum = np.sum(embed_x, axis=1) ** 2
        sum_of_square = np.sum(embed_x ** 2, axis=1)
        inputs = np.reshape(embed_x, (-1, self.embed_output_dim))
        x = self.linear_layer(self.fc(x).sum(1)) \
            + 0.5 * (square_of_sum - sum_of_square).sum(1, keepdims=True) \
            + self.mlp(inputs)
        x = npx.sigmoid(x)
        return x
</code></pre>
<h2 id="모델-훈련-및-평가-training-and-evaluating-the-model-7"><a class="header" href="#모델-훈련-및-평가-training-and-evaluating-the-model-7">모델 훈련 및 평가 (Training and Evaluating the Model)</a></h2>
<p>데이터 로딩 프로세스는 FM과 동일합니다. DeepFM의 MLP 구성 요소를 피라미드 구조(30-20-10)가 있는 3레이어 밀집 네트워크로 설정합니다. 다른 모든 하이퍼파라미터는 FM과 동일하게 유지됩니다.</p>
<pre><code class="language-{.python .input  n=4}">#@tab mxnet
batch_size = 2048
data_dir = d2l.download_extract('ctr')
train_data = d2l.CTRDataset(os.path.join(data_dir, 'train.csv'))
test_data = d2l.CTRDataset(os.path.join(data_dir, 'test.csv'),
                           feat_mapper=train_data.feat_mapper,
                           defaults=train_data.defaults)
field_dims = train_data.field_dims
train_iter = gluon.data.DataLoader(
    train_data, shuffle=True, last_batch='rollover', batch_size=batch_size,
    num_workers=d2l.get_dataloader_workers())
test_iter = gluon.data.DataLoader(
    test_data, shuffle=False, last_batch='rollover', batch_size=batch_size,
    num_workers=d2l.get_dataloader_workers())
devices = d2l.try_all_gpus()
net = DeepFM(field_dims, num_factors=10, mlp_dims=[30, 20, 10])
net.initialize(init.Xavier(), ctx=devices)
lr, num_epochs, optimizer = 0.01, 30, 'adam'
trainer = gluon.Trainer(net.collect_params(), optimizer,
                        {'learning_rate': lr})
loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
</code></pre>
<p>FM과 비교할 때 DeepFM은 더 빨리 수렴하고 더 나은 성능을 달성합니다.</p>
<h2 id="요약-summary-116"><a class="header" href="#요약-summary-116">요약 (Summary)</a></h2>
<ul>
<li>신경망을 FM에 통합하면 복잡한 고차 상호 작용을 모델링할 수 있습니다.</li>
<li>DeepFM은 광고 데이터셋에서 원래 FM보다 성능이 뛰어납니다.</li>
</ul>
<h2 id="연습-문제-exercises-131"><a class="header" href="#연습-문제-exercises-131">연습 문제 (Exercises)</a></h2>
<ul>
<li>MLP의 구조를 변경하여 모델 성능에 미치는 영향을 확인하십시오.</li>
<li>데이터셋을 Criteo로 변경하고 원래 FM 모델과 비교하십시오.</li>
</ul>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/407">Discussions</a>
:end_tab:</p>
<pre><code></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="부록-딥러닝을-위한-수학-appendix-mathematics-for-deep-learning"><a class="header" href="#부록-딥러닝을-위한-수학-appendix-mathematics-for-deep-learning">부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a></h1>
<p>:label:<code>chap_appendix_math</code></p>
<p><strong>Brent Werness</strong> (<em>Amazon</em>), <strong>Rachel Hu</strong> (<em>Amazon</em>), 및 이 책의 저자들</p>
<p>현대 딥러닝의 멋진 부분 중 하나는 그 아래에 있는 수학에 대한 완전한 이해 없이도 많은 부분을 이해하고 사용할 수 있다는 사실입니다. 이는 분야가 성숙하고 있다는 신호입니다. 대부분의 소프트웨어 개발자가 더 이상 계산 가능한 함수 이론에 대해 걱정할 필요가 없는 것처럼, 딥러닝 실무자도 최대 우도 학습의 이론적 기초에 대해 걱정할 필요가 없어야 합니다.</p>
<p>하지만 우리는 아직 거기에 도달하지 못했습니다.</p>
<p>실제로, 때로는 아키텍처 선택이 기울기 흐름에 어떤 영향을 미치는지, 또는 특정 손실 함수로 훈련함으로써 만드는 암시적 가정을 이해해야 할 수도 있습니다. 엔트로피가 도대체 무엇을 측정하는지, 그리고 모델에서 문자당 비트(bits-per-character)가 정확히 무엇을 의미하는지 이해하는 데 어떻게 도움이 되는지 알아야 할 수도 있습니다. 이 모든 것은 더 깊은 수학적 이해를 필요로 합니다.</p>
<p>이 부록은 현대 딥러닝의 핵심 이론을 이해하는 데 필요한 수학적 배경을 제공하는 것을 목표로 하지만, 철저하지는 않습니다. 우리는 선형 대수를 더 깊이 있게 검토하는 것으로 시작할 것입니다. 우리는 데이터에 대한 다양한 변환의 효과를 시각화할 수 있게 해주는 모든 일반적인 선형 대수 객체와 연산에 대한 기하학적 이해를 개발합니다. 핵심 요소는 고유 분해(eigen-decompositions)의 기초를 개발하는 것입니다.</p>
<p>다음으로 우리는 기울기가 왜 가장 가파른 하강 방향인지, 그리고 왜 역전파가 그러한 형태를 취하는지 완전히 이해할 수 있는 수준까지 미분학 이론을 개발합니다. 그런 다음 다음 주제인 확률론을 지원하는 데 필요한 정도로 적분학을 논의합니다.</p>
<p>실제로 마주치는 문제는 종종 확실하지 않으므로 불확실한 것에 대해 말할 언어가 필요합니다. 우리는 확률 변수 이론과 가장 일반적으로 마주치는 분포를 검토하여 모델을 확률적으로 논의할 수 있도록 합니다. 이는 확률적 분류 기술인 나이브 베이즈 분류기의 기초를 제공합니다.</p>
<p>확률론과 밀접한 관련이 있는 것은 통계학 연구입니다. 통계학은 짧은 섹션에서 다루기에는 너무 방대한 분야이지만, 우리는 모든 머신러닝 실무자가 알아야 할 기본 개념, 특히 추정량 평가 및 비교, 가설 검정 수행, 신뢰 구간 구성을 소개할 것입니다.</p>
<p>마지막으로 정보 이론이라는 주제로 넘어갑니다. 정보 이론은 정보 저장 및 전송에 대한 수학적 연구입니다. 이는 모델이 담론 영역에 대해 얼마나 많은 정보를 보유하고 있는지 정량적으로 논의할 수 있는 핵심 언어를 제공합니다.</p>
<p>이들을 종합하면 딥러닝에 대한 깊은 이해를 향한 길을 시작하는 데 필요한 수학적 개념의 핵심을 형성합니다.</p>
<pre><code class="language-toc">:maxdepth: 2

geometry-linear-algebraic-ops
eigendecomposition
single-variable-calculus
multivariable-calculus
integral-calculus
random-variables
maximum-likelihood
distributions
naive-bayes
statistics
information-theory
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="기하-및-선형-대수-연산-geometry-and-linear-algebraic-operations"><a class="header" href="#기하-및-선형-대수-연산-geometry-and-linear-algebraic-operations">기하 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></h1>
<p>:label:<code>sec_geometry-linear-algebraic-ops</code></p>
<p>:numref:<code>sec_linear-algebra</code>에서 우리는 선형 대수의 기초를 접했고, 데이터를 변환하는 일반적인 연산을 표현하는 데 선형 대수가 어떻게 사용될 수 있는지 보았습니다.
선형 대수는 우리가 딥러닝과 더 넓은 머신러닝에서 수행하는 많은 작업의 핵심적인 수학적 기둥 중 하나입니다.
:numref:<code>sec_linear-algebra</code>는 현대 딥러닝 모델의 메커니즘을 전달하기에 충분한 도구를 포함하고 있었지만, 이 주제에는 훨씬 더 많은 것이 있습니다.
이 섹션에서는 선형 대수 연산의 몇 가지 기하학적 해석을 강조하고 고유값과 고유벡터를 포함한 몇 가지 기본 개념을 소개하면서 더 깊이 파고들 것입니다.</p>
<h2 id="벡터의-기하학-geometry-of-vectors"><a class="header" href="#벡터의-기하학-geometry-of-vectors">벡터의 기하학 (Geometry of Vectors)</a></h2>
<p>먼저 벡터의 두 가지 공통된 기하학적 해석, 즉 공간에서의 점 또는 방향으로의 해석에 대해 논의해야 합니다.
근본적으로 벡터는 아래의 Python 리스트와 같은 숫자들의 목록입니다.</p>
<pre><code class="language-{.python .input}">#@tab all
v = [1, 7, 0, 1]
</code></pre>
<p>수학자들은 이것을 <em>열</em> 벡터 또는 <em>행</em> 벡터로 쓰는 경우가 많습니다. 즉, 다음과 같습니다.</p>
<p>$$
\mathbf{x} = \begin{bmatrix}1\7\0\1\end{bmatrix},
$$</p>
<p>또는</p>
<p>$$
\mathbf{x}^\top = \begin{bmatrix}1 &amp; 7 &amp; 0 &amp; 1\end{bmatrix}.
$$</p>
<p>데이터 예제가 열 벡터이고 가중 합을 형성하는 데 사용되는 가중치가 행 벡터인 경우처럼, 이들은 종종 다른 해석을 갖습니다.
그러나 유연하게 대처하는 것이 유익할 수 있습니다.
:numref:<code>sec_linear-algebra</code>에서 설명했듯이, 단일 벡터의 기본 방향은 열 벡터이지만, 테이블 형식의 데이터셋을 나타내는 행렬의 경우 행렬의 각 데이터 예제를 행 벡터로 취급하는 것이 더 일반적입니다.</p>
<p>벡터가 주어졌을 때 우리가 부여해야 할 첫 번째 해석은 공간에서의 점입니다.
2차원 또는 3차원에서 우리는 벡터의 구성 요소를 사용하여 <em>원점</em>이라고 불리는 고정된 기준점과 비교하여 공간에서 점의 위치를 정의함으로써 이러한 점들을 시각화할 수 있습니다. 이는 :numref:<code>fig_grid</code>에서 볼 수 있습니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/grid-points.svg" alt="벡터를 평면상의 점으로 시각화한 그림. 벡터의 첫 번째 구성 요소는 $\mathit{x}$-좌표를 제공하고, 두 번째 구성 요소는 $\mathit{y}$-좌표를 제공합니다. 고차원도 유사하지만 시각화하기는 훨씬 어렵습니다." />
:label:<code>fig_grid</code></p>
<p>이러한 기하학적 관점은 문제를 더 추상적인 수준에서 고려할 수 있게 해줍니다.
사진을 고양이 또는 개로 분류하는 것과 같은 극복할 수 없어 보이는 문제에 더 이상 직면하지 않고, 작업을 공간에서의 점들의 모음으로 추상적으로 간주하기 시작하고 작업을 두 개의 뚜렷한 점 클러스터를 분리하는 방법을 발견하는 것으로 상상할 수 있습니다.</p>
<p>이와 병행하여 사람들이 종종 벡터에 대해 갖는 두 번째 관점이 있습니다: 공간에서의 방향입니다.
벡터 $\mathbf{v} = [3,2]^\top$를 원점에서 오른쪽으로 3단위, 위로 2단위인 위치로 생각할 수 있을 뿐만 아니라, 오른쪽으로 3걸음, 위로 2걸음을 걷는 방향 자체로 생각할 수도 있습니다.
이런 식으로 우리는 그림 :numref:<code>fig_arrow</code>의 모든 벡터를 동일하게 간주합니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/par-vec.svg" alt="모든 벡터는 평면상의 화살표로 시각화될 수 있습니다. 이 경우 그려진 모든 벡터는 벡터 $(3,2)^\top$의 표현입니다." />
:label:<code>fig_arrow</code></p>
<p>이러한 전환의 이점 중 하나는 벡터 덧셈 행위에 대한 시각적 의미를 부여할 수 있다는 것입니다.
특히, 우리는 한 벡터가 가리키는 방향을 따르고 나서 다른 벡터가 가리키는 방향을 따릅니다. 이는 :numref:<code>fig_add-vec</code>에서 볼 수 있습니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/vec-add.svg" alt="한 벡터를 따르고 나서 다른 벡터를 따름으로써 벡터 덧셈을 시각화할 수 있습니다." />
:label:<code>fig_add-vec</code></p>
<p>벡터 뺄셈도 비슷한 해석을 갖습니다.
$\mathbf{u} = \mathbf{v} + (\mathbf{u}-\mathbf{v})$라는 항등식을 고려함으로써, 우리는 벡터 $\mathbf{u}-\mathbf{v}$가 점 $\mathbf{v}$에서 점 $\mathbf{u}$로 우리를 데려다주는 방향임을 알 수 있습니다.</p>
<h2 id="내적과-각도-dot-products-and-angles"><a class="header" href="#내적과-각도-dot-products-and-angles">내적과 각도 (Dot Products and Angles)</a></h2>
<p>:numref:<code>sec_linear-algebra</code>에서 보았듯이,
두 개의 열 벡터 $\mathbf{u}$와 $\mathbf{v}$를 취하면 다음과 같이 계산하여 내적을 형성할 수 있습니다.</p>
<p>$$\mathbf{u}^\top\mathbf{v} = \sum_i u_i\cdot v_i.$$
:eqlabel:<code>eq_dot_def</code></p>
<p>:eqref:<code>eq_dot_def</code>는 대칭적이므로 고전적인 곱셈의 표기법을 모방하여 다음과 같이 씁니다.</p>
<p>$$
\mathbf{u}\cdot\mathbf{v} = \mathbf{u}^\top\mathbf{v} = \mathbf{v}^\top\mathbf{u},
$$</p>
<p>벡터의 순서를 바꿔도 동일한 답이 나온다는 사실을 강조하기 위함입니다.</p>
<p>내적 :eqref:<code>eq_dot_def</code>는 기하학적 해석도 허용합니다: 이는 두 벡터 사이의 각도와 밀접하게 관련되어 있습니다. :numref:<code>fig_angle</code>에 표시된 각도를 고려하십시오.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/vec-angle.svg" alt="평면상의 임의의 두 벡터 사이에는 잘 정의된 각도 $\theta$가 있습니다. 우리는 이 각도가 내적과 밀접하게 묶여 있음을 알게 될 것입니다." />
:label:<code>fig_angle</code></p>
<p>시작하기 위해 두 개의 특정 벡터를 고려해 봅시다.</p>
<p>$$
\mathbf{v} = (r,0) ; \textrm{및} ; \mathbf{w} = (s\cos(\theta), s \sin(\theta)).
$$</p>
<p>벡터 $\mathbf{v}$는 길이가 $r$이고 $x$축과 평행하며, 벡터 $\mathbf{w}$는 길이가 $s$이고 $x$축과 각도 $\theta$를 이룹니다.
이 두 벡터의 내적을 계산하면 다음과 같음을 알 수 있습니다.</p>
<p>$$
\mathbf{v}\cdot\mathbf{w} = rs\cos(\theta) = |\mathbf{v}||\mathbf{w}|\cos(\theta).
$$</p>
<p>간단한 대수적 조작을 통해 항을 재배열하면 다음을 얻을 수 있습니다.</p>
<p>$$
\theta = \arccos\left(rac{\mathbf{v}\cdot\mathbf{w}}{|\mathbf{v}||\mathbf{w}|}\right).
$$</p>
<p>요컨대, 이 두 특정 벡터에 대해 내적과 노름의 조합은 두 벡터 사이의 각도를 알려줍니다. 이 사실은 일반적으로도 참입니다. 여기서 식을 유도하지는 않겠지만,
$\|\mathbf{v} - \mathbf{w}\|^2$를 두 가지 방식(하나는 내적으로, 다른 하나는 코사인 법칙을 사용한 기하학적 방식)으로 쓰는 것을 고려하면 전체 관계를 얻을 수 있습니다.
실제로 임의의 두 벡터 $\mathbf{v}$와 $\mathbf{w}$에 대해 두 벡터 사이의 각도는 다음과 같습니다.</p>
<p>$$\theta = \arccos\left(rac{\mathbf{v}\cdot\mathbf{w}}{|\mathbf{v}||\mathbf{w}|}\right).$$
:eqlabel:<code>eq_angle_forumla</code></p>
<p>이것은 계산의 어떤 것도 2차원을 참조하지 않기 때문에 멋진 결과입니다.
실제로 우리는 이를 3차원 또는 300만 차원에서 문제없이 사용할 수 있습니다.</p>
<p>간단한 예로 두 벡터 사이의 각도를 계산하는 방법을 살펴봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mxnet import gluon, np, npx
npx.set_np()

def angle(v, w):
    return np.arccos(v.dot(w) / (np.linalg.norm(v) * np.linalg.norm(w)))

angle(np.array([0, 1, 2]), np.array([2, 3, 4]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
import torch
from torchvision import transforms
import torchvision

def angle(v, w):
    return torch.acos(v.dot(w) / (torch.norm(v) * torch.norm(w)))

angle(torch.tensor([0, 1, 2], dtype=torch.float32), torch.tensor([2.0, 3, 4]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
import tensorflow as tf

def angle(v, w):
    return tf.acos(tf.tensordot(v, w, axes=1) / (tf.norm(v) * tf.norm(w)))

angle(tf.constant([0, 1, 2], dtype=tf.float32), tf.constant([2.0, 3, 4]))
</code></pre>
<p>지금은 사용하지 않겠지만, 각도가 $\pi/2$ (또는 동등하게 $90^{\circ}$)인 벡터를 *직교(orthogonal)*한다고 지칭한다는 점을 알아두면 유용합니다.
위의 방정식을 조사하면 $\theta = \pi/2$일 때 이런 일이 발생하며, 이는 $\cos(\theta) = 0$과 같음을 알 수 있습니다.
이것이 발생할 수 있는 유일한 방법은 내적 자체가 0인 경우이며, 두 벡터는 $\mathbf{v}\.\mathbf{w} = 0$인 경우에만 직교합니다.
이는 객체를 기하학적으로 이해할 때 유용한 공식이 될 것입니다.</p>
<p>각도를 계산하는 것이 왜 유용한지 묻는 것은 합리적입니다.
답은 우리가 데이터에 기대하는 일종의 불변성(invariance)에 있습니다.
이미지와 모든 픽셀 값은 같지만 밝기가 $10%$인 복제 이미지를 고려해 보십시오.
개별 픽셀의 값은 일반적으로 원래 값과 거리가 멉니다.
따라서 원본 이미지와 어두운 이미지 사이의 거리를 계산하면 거리가 클 수 있습니다.
그러나 대부분의 머신러닝 응용 프로그램에서 <em>콘텐츠</em>는 동일합니다. 고양이/개 분류기 입장에서는 여전히 고양이 이미지입니다.
그러나 각도를 고려하면 임의의 벡터 $\mathbf{v}$에 대해 $\mathbf{v}$와 $0.1\cdot\mathbf{v}$ 사이의 각도가 0임을 알 수 있습니다.
이는 벡터의 스케일을 조정해도 동일한 방향을 유지하고 길이만 변경된다는 사실에 대응합니다. 각도는 어두운 이미지를 동일한 것으로 간주합니다.</p>
<p>이와 같은 예는 어디에나 있습니다.
텍스트에서 우리는 똑같은 말을 하는 두 배 긴 문서를 작성하더라도 논의되는 주제가 바뀌지 않기를 원할 수 있습니다.
어떤 인코딩(예: 어떤 어휘의 단어 발생 횟수 계산)의 경우 이는 문서를 인코딩하는 벡터를 두 배로 늘리는 것에 대응하므로 다시 각도를 사용할 수 있습니다.</p>
<h3 id="코사인-유사도-cosine-similarity"><a class="header" href="#코사인-유사도-cosine-similarity">코사인 유사도 (Cosine Similarity)</a></h3>
<p>각도를 사용하여 두 벡터의 근접성을 측정하는 머신러닝 맥락에서, 실무자들은 다음과 같은 부분을 지칭하기 위해 *코사인 유사도(cosine similarity)*라는 용어를 채택합니다.
$$
\cos(\theta) = \frac{\mathbf{v}\.\mathbf{w}}{|\mathbf{v}||\mathbf{w}|}.
$$</p>
<p>코사인은 두 벡터가 같은 방향을 가리킬 때 최대값 $1$, 반대 방향을 가리킬 때 최소값 $-1$, 두 벡터가 직교할 때 값 $0$을 갖습니다.
고차원 벡터의 성분이 평균 $0$으로 무작위로 샘플링되면 코사인은 거의 항상 $0$에 가깝습니다.</p>
<h2 id="초평면-hyperplanes"><a class="header" href="#초평면-hyperplanes">초평면 (Hyperplanes)</a></h2>
<p>벡터로 작업하는 것 외에도 선형 대수에서 멀리 나아가기 위해 이해해야 할 또 다른 핵심 객체는 *초평면(hyperplane)*입니다. 이는 직선(2차원) 또는 평면(3차원)을 고차원으로 일반화한 것입니다.
$d$차원 벡터 공간에서 초평면은 $d-1$차원을 가지며 공간을 두 개의 반공간(half-spaces)으로 나눕니다.</p>
<p>예제부터 시작하겠습니다.
열 벡터 $\mathbf{w}=[2,1]^\top$가 있다고 가정합시다. 우리는 "$\mathbf{w}\.\mathbf{v} = 1$인 점 $\mathbf{v}$는 무엇인가?"를 알고 싶습니다.
위의 내적과 각도 사이의 연결 :eqref:<code>eq_angle_forumla</code>을 상기하면, 이것은 다음과 동등함을 알 수 있습니다.
$$
|\mathbf{v}||\mathbf{w}|\cos(\theta) = 1 ; \iff ; |\mathbf{v}|\cos(\theta) = \frac{1}{|\mathbf{w}|} = \frac{1}{\sqrt{5}}.
$$</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/proj-vec.svg" alt="삼각법을 상기하면, 공식 $\|\mathbf{v}|\cos(\theta)$가 $\mathbf{w}$ 방향으로의 벡터 $\mathbf{v}$의 투영(projection) 길이임을 알 수 있습니다." />
:label:<code>fig_vector-project</code></p>
<p>이 식의 기하학적 의미를 고려하면, 이는 $\mathbf{w}$ 방향으로의 $\mathbf{v}$의 투영 길이가 정확히 $1/\|\mathbf{w}\|$\$임을 알 수 있습니다. 이는 :numref:<code>fig_vector-project</code>에 나와 있습니다.
이것이 참인 모든 점의 집합은 벡터 $\mathbf{w}$에 직각인 직선입니다.
원한다면 이 직선의 방정식을 찾아 $2x + y = 1$ 또는 동등하게 $y = 1 - 2x$임을 알 수 있습니다.</p>
<p>이제 $\mathbf{w}\.\mathbf{v} &gt; 1$ 또는 $\mathbf{w}\.\mathbf{v} &lt; 1$인 점들의 집합에 대해 물으면 어떻게 되는지 살펴봅시다.
이들은 각각 투영이 $1/\|\mathbf{w}\|$\$보다 길거나 짧은 경우임을 알 수 있습니다.
따라서 이 두 부등식은 직선의 양쪽을 정의합니다.
이런 식으로 우리는 공간을 두 반쪽으로 나누는 방법을 찾았습니다. 여기서 한쪽의 모든 점은 임계값 미만의 내적을 갖고 다른 쪽은 그 이상을 갖습니다. 이는 :numref:<code>fig_space-division</code>에서 볼 수 있습니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/space-division.svg" alt="이제 식의 부등식 버전을 고려하면, 초평면(이 경우 단순한 직선)이 공간을 두 반쪽으로 분리하는 것을 알 수 있습니다." />
:label:<code>fig_space-division</code></p>
<p>고차원에서의 이야기도 거의 같습니다.
이제 $\mathbf{w} = [1,2,3]^\top$를 취하고 3차원에서 $\mathbf{w}\.\mathbf{v} = 1$인 점들에 대해 물으면, 주어진 벡터 $\mathbf{w}$에 직각인 평면을 얻습니다.
두 부등식은 다시 :numref:<code>fig_higher-division</code>에 표시된 것처럼 평면의 양쪽을 정의합니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/space-division-3d.svg" alt="모든 차원의 초평면은 공간을 두 반쪽으로 분리합니다." />
:label:<code>fig_higher-division</code></p>
<p>이 시점에서 시각화 능력은 다하겠지만, 십 차원, 백 차원 또는 수십억 차원에서 이 작업을 수행하는 것을 막을 수 있는 것은 아무것도 없습니다.
이는 머신러닝 모델을 생각할 때 종종 발생합니다.
예를 들어, 우리는 :numref:<code>sec_softmax</code>의 선형 분류 모델을 서로 다른 타겟 클래스를 분리하는 초평면을 찾는 방법으로 이해할 수 있습니다.
이러한 맥락에서 그러한 초평면은 종종 *결정 평면(decision planes)*이라고 불립니다.
심층 학습 분류 모델의 대부분은 softmax에 공급되는 선형 레이어로 끝나므로, 심층 신경망의 역할을 타겟 클래스가 초평면에 의해 깨끗하게 분리될 수 있도록 하는 비선형 임베딩을 찾는 것으로 해석할 수 있습니다.</p>
<p>수작업으로 만든 예제를 제공하기 위해, Fashion-MNIST 데이터셋(:numref:<code>sec_fashion_mnist</code>에서 본)의 티셔츠와 바지의 작은 이미지를 분류하기 위해 단순히 평균들 사이의 벡터를 취하여 결정 평면을 정의하고 대략적인 임계값을 눈대중으로 잡음으로써 합리적인 모델을 생성할 수 있음에 주목하십시오. 먼저 데이터를 로드하고 평균을 계산하겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 데이터셋 로드
train = gluon.data.vision.FashionMNIST(train=True)
test = gluon.data.vision.FashionMNIST(train=False)

X_train_0 = np.stack([x[0] for x in train if x[1] == 0]).astype(float)
X_train_1 = np.stack([x[0] for x in train if x[1] == 1]).astype(float)
X_test = np.stack(
    [x[0] for x in test if x[1] == 0 or x[1] == 1]).astype(float)
y_test = np.stack(
    [x[1] for x in test if x[1] == 0 or x[1] == 1]).astype(float)

# 평균 계산
ave_0 = np.mean(X_train_0, axis=0)
ave_1 = np.mean(X_train_1, axis=0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 데이터셋 로드
trans = []
trans.append(transforms.ToTensor())
trans = transforms.Compose(trans)
train = torchvision.datasets.FashionMNIST(root="../data", transform=trans,
                                          train=True, download=True)
test = torchvision.datasets.FashionMNIST(root="../data", transform=trans,
                                         train=False, download=True)

X_train_0 = torch.stack(
    [x[0] * 256 for x in train if x[1] == 0]).type(torch.float32)
X_train_1 = torch.stack(
    [x[0] * 256 for x in train if x[1] == 1]).type(torch.float32)
X_test = torch.stack(
    [x[0] * 256 for x in test if x[1] == 0 or x[1] == 1]).type(torch.float32)
y_test = torch.stack([torch.tensor(x[1]) for x in test
                      if x[1] == 0 or x[1] == 1]).type(torch.float32)

# 평균 계산
ave_0 = torch.mean(X_train_0, axis=0)
ave_1 = torch.mean(X_train_1, axis=0)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 데이터셋 로드
((train_images, train_labels), (
    test_images, test_labels)) = tf.keras.datasets.fashion_mnist.load_data()


X_train_0 = tf.cast(tf.stack(train_images[[i for i, label in enumerate(
    train_labels) if label == 0]] * 256), dtype=tf.float32)
X_train_1 = tf.cast(tf.stack(train_images[[i for i, label in enumerate(
    train_labels) if label == 1]] * 256), dtype=tf.float32)
X_test = tf.cast(tf.stack(test_images[[i for i, label in enumerate(
    test_labels) if label == 0]] * 256), dtype=tf.float32)
y_test = tf.cast(tf.stack(test_images[[i for i, label in enumerate(
    test_labels) if label == 1]] * 256), dtype=tf.float32)

# 평균 계산
ave_0 = tf.reduce_mean(X_train_0, axis=0)
ave_1 = tf.reduce_mean(X_train_1, axis=0)
</code></pre>
<p>이러한 평균들을 자세히 살펴보는 것이 유익할 수 있으므로 어떤 모습인지 그려봅시다. 이 경우 평균이 실제로 티셔츠의 흐릿한 이미지와 닮았음을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet, pytorch
# 평균 티셔츠 플롯
d2l.set_figsize()
d2l.plt.imshow(ave_0.reshape(28, 28).tolist(), cmap='Greys')
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 평균 티셔츠 플롯
d2l.set_figsize()
d2l.plt.imshow(tf.reshape(ave_0, (28, 28)), cmap='Greys')
d2l.plt.show()
</code></pre>
<p>두 번째 경우에도 평균이 바지의 흐릿한 이미지와 닮았음을 다시 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet, pytorch
# 평균 바지 플롯
d2l.plt.imshow(ave_1.reshape(28, 28).tolist(), cmap='Greys')
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 평균 바지 플롯
d2l.plt.imshow(tf.reshape(ave_1, (28, 28)), cmap='Greys')
d2l.plt.show()
</code></pre>
<p>완전한 머신러닝 솔루션에서는 데이터셋에서 임계값을 학습할 것입니다. 이 경우 저는 단순히 훈련 데이터에서 수동으로 잘 어울려 보이는 임계값을 눈대중으로 잡았습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 눈대중 임계값을 사용한 테스트 세트 정확도 출력
w = (ave_1 - ave_0).T
predictions = X_test.reshape(2000, -1).dot(w.flatten()) &gt; -1500000

# 정확도
np.mean(predictions.astype(y_test.dtype) == y_test, dtype=np.float64)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 눈대중 임계값을 사용한 테스트 세트 정확도 출력
w = (ave_1 - ave_0).T
# '@'는 pytorch의 행렬 곱셈 연산자입니다.
predictions = X_test.reshape(2000, -1) @ (w.flatten()) &gt; -1500000

# 정확도
torch.mean((predictions.type(y_test.dtype) == y_test).float(), dtype=torch.float64)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 눈대중 임계값을 사용한 테스트 세트 정확도 출력
w = tf.transpose(ave_1 - ave_0)
predictions = tf.reduce_sum(X_test * tf.nest.flatten(w), axis=0) &gt; -1500000

# 정확도
tf.reduce_mean(
    tf.cast(tf.cast(predictions, y_test.dtype) == y_test, tf.float32))
</code></pre>
<h2 id="선형-변환의-기하학-geometry-of-linear-transformations"><a class="header" href="#선형-변환의-기하학-geometry-of-linear-transformations">선형 변환의 기하학 (Geometry of Linear Transformations)</a></h2>
<p>:numref:<code>sec_linear-algebra</code> 및 위의 논의를 통해, 우리는 벡터, 길이 및 각도의 기하학에 대해 확고한 이해를 얻었습니다.
그러나 우리가 논의를 생략한 중요한 객체가 하나 있는데, 그것은 행렬로 표현되는 선형 변환의 기하학적 이해입니다.
잠재적으로 다른 두 고차원 공간 사이에서 데이터를 변환하기 위해 행렬이 무엇을 할 수 있는지 완전히 내면화하려면 상당한 연습이 필요하며, 이는 이 부록의 범위를 벗어납니다.
그러나 우리는 2차원에서 직관을 구축하기 시작할 수 있습니다.</p>
<p>어떤 행렬이 있다고 가정해 봅시다.</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
a &amp; b \ c &amp; d
\end{bmatrix}.
$$</p>
<p>이것을 임의의 벡터 $\mathbf{v} = [x, y]^\top$에 적용하고 싶다면 다음과 같이 곱합니다.</p>
<p>$$
\begin{aligned}
\mathbf{A}\mathbf{v} &amp; = \begin{bmatrix}a &amp; b \ c &amp; d\end{bmatrix}\begin{bmatrix}x \ y\end{bmatrix} \
&amp; = \begin{bmatrix}ax+by\ cx+dy\end{bmatrix} \
&amp; = x\begin{bmatrix}a \ c\end{bmatrix} + y\begin{bmatrix}b \d\end{bmatrix} \
&amp; = x\left{\mathbf{A}\begin{bmatrix}1\0\end{bmatrix}\right} + y\left{\mathbf{A}\begin{bmatrix}0\1\end{bmatrix}\right}.
\end{aligned}
$$</p>
<p>이것은 명확한 것이 다소 난해해진 이상한 계산처럼 보일 수 있습니다.
그러나 이것은 행렬이 <em>임의의</em> 벡터를 변환하는 방식을 <em>두 개의 특정 벡터</em> $[1,0]^\top$ 및 $[0,1]^\top$를 변환하는 방식의 관점에서 쓸 수 있음을 알려줍니다.
이는 잠시 고려해 볼 가치가 있습니다.
우리는 본질적으로 무한한 문제(임의의 실수 쌍에 어떤 일이 일어나는지)를 유한한 문제(이 특정 벡터들에게 어떤 일이 일어나는지)로 축소했습니다. 이러한 벡터들은 *기저(basis)*의 한 예이며, 우리 공간의 모든 벡터를 이러한 <em>기저 벡터</em>들의 가중 합으로 쓸 수 있습니다.</p>
<p>특정 행렬을 사용할 때 어떤 일이 발생하는지 그려봅시다.</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
1 &amp; 2 \ -1 &amp; 3
\end{bmatrix}.
$$</p>
<p>특정 벡터 $\mathbf{v} = [2, -1]^\top$를 보면, 이것은 $2\cdot[1,0]^\top + -1\cdot[0,1]^\top$임을 알 수 있으며, 따라서 행렬 $A$가 이를 $2(\mathbf{A}[1,0]^\top) + -1(\mathbf{A}[0,1])^\top = 2[1, -1]^\top - [2,3]^\top = [0, -5]^\top$로 보낼 것임을 알 수 있습니다.
만약 우리가 이 로직을 신중하게 따라간다면, 가령 모든 정수 쌍 점들의 그리드를 고려함으로써, 행렬 곱셈이 그리드를 비틀고(skew), 회전하고(rotate), 스케일을 조정할 수 있음을 알 수 있습니다. 하지만 그리드 구조는 :numref:<code>fig_grid-transform</code>에서 보듯이 유지되어야 합니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/grid-transform.svg" alt="주어진 기저 벡터에 작용하는 행렬 $\mathbf{A}$. 전체 그리드가 그와 함께 어떻게 이동하는지 주목하십시오." />
:label:<code>fig_grid-transform</code></p>
<p>이것은 행렬로 표현되는 선형 변환에 대해 내면화해야 할 가장 중요한 직관적인 점입니다.
행렬은 공간의 일부 부분을 다른 부분보다 다르게 왜곡할 수 없습니다.
그들이 할 수 있는 전부는 우리 공간의 원래 좌표를 비틀고, 회전하고, 스케일을 조정하는 것뿐입니다.</p>
<p>일부 왜곡은 심할 수 있습니다. 예를 들어 행렬</p>
<p>$$
\mathbf{B} = \begin{bmatrix}
2 &amp; -1 \ 4 &amp; -2
\end{bmatrix},
$$</p>
<p>은 전체 2차원 평면을 하나의 직선으로 압축합니다.
그러한 변환을 식별하고 작업하는 것은 나중 섹션의 주제이지만, 기하학적으로 이것이 우리가 위에서 본 변환 유형과 근본적으로 다르다는 것을 알 수 있습니다.
예를 들어, 행렬 $\mathbf{A}$의 결과는 원래 그리드로 "다시 구부러질" 수 있습니다. 하지만 행렬 $\mathbf{B}$의 결과는 그럴 수 없습니다. 벡터 $[1,2]^\top$가 어디서 왔는지(가령 $[1,1]^\top$에서 왔는지 아니면 $[0, -1]^\top$에서 왔는지) 결코 알 수 없기 때문입니다.</p>
<p>이 그림은 $2\times2$ 행렬에 대한 것이었지만, 학습한 교훈을 고차원으로 가져가는 것을 방해하는 것은 아무것도 없습니다.
$[1,0, \ldots,0]$과 같은 유사한 기저 벡터를 취하고 우리 행렬이 그것들을 어디로 보내는지 본다면, 우리가 다루고 있는 어떤 차원 공간에서든 행렬 곱셈이 전체 공간을 어떻게 왜곡하는지 느끼기 시작할 수 있습니다.</p>
<h2 id="선형-종속-linear-dependence"><a class="header" href="#선형-종속-linear-dependence">선형 종속 (Linear Dependence)</a></h2>
<p>행렬을 다시 고려해 보십시오.</p>
<p>$$
\mathbf{B} = \begin{bmatrix}
2 &amp; -1 \ 4 &amp; -2
\end{bmatrix}.
$$</p>
<p>이것은 전체 평면을 단일 직선 $y = 2x$ 위에 살도록 압축합니다.
이제 질문이 생깁니다: 행렬 자체만 보고 이를 감지할 수 있는 방법이 있을까요?
답은 실제로 가능하다는 것입니다. $\mathbf{b}_1 = [2,4]^\top$ 및 $\mathbf{b}_2 = [-1, -2]^\top$를 $\mathbf{B}$의 두 열이라고 합시다.
행렬 $\mathbf{B}$에 의해 변환된 모든 것은 행렬 열의 가중 합(예: $a_1\mathbf{b}_1 + a_2\mathbf{b}_2$)으로 쓸 수 있음을 기억하십시오.
우리는 이를 *선형 결합(linear combination)*이라고 부릅니다.
$\mathbf{b}_1 = -2\cdot\mathbf{b}_2$라는 사실은 이러한 두 열의 임의의 선형 결합을 다음과 같이 $\mathbf{b}_2$에 대해서만 쓸 수 있음을 의미합니다.</p>
<p>$$
a_1\mathbf{b}_1 + a_2\mathbf{b}_2 = -2a_1\mathbf{b}_2 + a_2\mathbf{b}_2 = (a_2-2a_1)\mathbf{b}_2.
$$</p>
<p>이는 열 중 하나가 공간에서 고유한 방향을 정의하지 않기 때문에 어떤 의미에서 중복된다는 것을 의미합니다.
이 행렬이 전체 평면을 단일 직선으로 무너뜨리는 것을 이미 보았기 때문에 이것은 우리를 너무 놀라게 해서는 안 됩니다.
더욱이, 선형 종속 $\mathbf{b}_1 = -2\cdot\mathbf{b}_2$가 이를 포착함을 알 수 있습니다.
두 벡터 사이에서 이를 더 대칭적으로 만들기 위해 다음과 같이 쓰겠습니다.</p>
<p>$$
\mathbf{b}_1  + 2\cdot\mathbf{b}_2 = 0.
$$</p>
<p>일반적으로 벡터 모음 $\mathbf{v}_1, \ldots, \mathbf{v}_k$는 <em>모두 0은 아닌</em> 계수 $a_1, \ldots, a_k$가 존재하여 다음을 만족할 때 *선형 종속(linearly dependent)*이라고 합니다.</p>
<p>$$
\sum_{i=1}^k a_i\mathbf{v_i} = 0.
$$</p>
<p>이 경우, 우리는 벡터 중 하나를 다른 것들의 어떤 조합의 관점에서 풀 수 있으며, 효과적으로 그것을 중복으로 만들 수 있습니다.
따라서 행렬 열의 선형 종속은 우리 행렬이 공간을 어떤 낮은 차원으로 압축하고 있다는 사실에 대한 증거입니다.
선형 종속이 없으면 벡터들이 *선형 독립(linearly independent)*하다고 합니다.
행렬의 열이 선형 독립이면 압축이 발생하지 않으며 작업을 되돌릴 수 있습니다.</p>
<h2 id="랭크-rank"><a class="header" href="#랭크-rank">랭크 (Rank)</a></h2>
<p>일반적인 $n\times m$ 행렬이 있을 때, 행렬이 어떤 차원 공간으로 매핑되는지 묻는 것은 합리적입니다.
*랭크(rank)*라고 알려진 개념이 우리의 답이 될 것입니다.
이전 섹션에서 우리는 선형 종속이 공간이 더 낮은 차원으로 압축되는 것을 증명한다는 점을 언급했으므로, 이를 사용하여 랭크 개념을 정의할 수 있습니다.
특히, 행렬 $\mathbf{A}$의 랭크는 모든 열 부분 집합 중에서 선형 독립인 열의 최대 개수입니다. 예를 들어 행렬</p>
<p>$$
\mathbf{B} = \begin{bmatrix}
2 &amp; 4 \ -1 &amp; -2
\end{bmatrix},
$$</p>
<p>은 두 열이 선형 종속이지만 어느 한 열 자체는 선형 종속이 아니므로 $\textrm{rank}(B)=1$을 갖습니다.
더 도전적인 예로 다음을 고려할 수 있습니다.</p>
<p>$$
\mathbf{C} = \begin{bmatrix}
1&amp; 3 &amp; 0 &amp; -1 &amp; 0 \ -1 &amp; 0 &amp; 1 &amp; 1 &amp; -1 \ 0 &amp; 3 &amp; 1 &amp; 0 &amp; -1 \ 2 &amp; 3 &amp; -1 &amp; -2 &amp; 1
\end{bmatrix},
$$</p>
<p>그리고 예를 들어 처음 두 열은 선형 독립이지만 세 개의 열로 구성된 네 가지 모음은 모두 종속적임을 보임으로써 $\mathbf{C}$가 랭크 2임을 보여줄 수 있습니다.</p>
<p>설명된 이 절차는 매우 비효율적입니다.
주어진 행렬의 모든 열 부분 집합을 살펴봐야 하므로 열 수에 따라 잠재적으로 기하급수적입니다.
나중에 행렬의 랭크를 계산하는 보다 계산 효율적인 방법을 보겠지만, 지금은 개념이 잘 정의되어 있고 그 의미를 이해하는 것으로 충분합니다.</p>
<h2 id="가역성-invertibility"><a class="header" href="#가역성-invertibility">가역성 (Invertibility)</a></h2>
<p>우리는 위에서 선형 종속 열을 가진 행렬에 의한 곱셈은 되돌릴 수 없음을 보았습니다. 즉, 항상 입력을 복구할 수 있는 역작용이 없습니다.
그러나 풀 랭크 행렬(즉, 랭크가 $n$인 $n \times n$ 행렬 $\mathbf{A}$)에 의한 곱셈은 항상 되돌릴 수 있어야 합니다. 행렬</p>
<p>$$
\mathbf{I} = \begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \ 0 &amp; 1 &amp; \cdots &amp; 0 \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ 0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}.
$$</p>
<p>를 고려해 보십시오. 이는 대각선을 따라 1이 있고 다른 곳은 0인 행렬입니다.
우리는 이를 <em>단위(identity)</em> 행렬이라고 부릅니다.
적용했을 때 우리 데이터를 변경하지 않고 그대로 두는 행렬입니다.
우리 행렬 $\mathbf{A}$가 한 일을 되돌리는 행렬을 찾으려면, 다음을 만족하는 행렬 $\mathbf{A}^{-1}$를 찾고 싶습니다.</p>
<p>$$
\mathbf{A}^{-1}\mathbf{A} = \mathbf{A}\mathbf{A}^{-1} =  \mathbf{I}.
$$</p>
<p>이것을 시스템으로 보면, $n \times n$개의 미지수($\mathbf{A}^{-1}$의 항목들)와 $n \times n$개의 방정식(곱 $\mathbf{A}^{-1}\mathbf{A}$의 모든 항목과 $\mathbf{I}$의 모든 항목 사이에 성립해야 하는 등식)이 있으므로 일반적으로 해가 존재할 것으로 기대해야 합니다.
실제로 다음 섹션에서 *행렬식(determinant)*이라는 수량을 보게 될 텐데, 이는 행렬식이 0이 아닌 한 해를 찾을 수 있다는 속성을 가지고 있습니다. 우리는 그러한 행렬 $\mathbf{A}^{-1}$를 <em>역(inverse)</em> 행렬이라고 부릅니다.
예를 들어 $\mathbf{A}$가 일반적인 $2 \times 2$ 행렬인 경우</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
a &amp; b \ c &amp; d
\end{bmatrix},
$$</p>
<p>역행렬은 다음과 같음을 알 수 있습니다.</p>
<p>$$
\frac{1}{ad-bc}  \begin{bmatrix}
d &amp; -b \ -c &amp; a
\end{bmatrix}.
$$</p>
<p>위의 공식에 의해 주어진 역행렬을 곱하는 것이 실제로 작동하는지 확인함으로써 이를 테스트할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
M = np.array([[1, 2], [1, 4]])
M_inv = np.array([[2, -1], [-0.5, 0.5]])
M_inv.dot(M)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
M = torch.tensor([[1, 2], [1, 4]], dtype=torch.float32)
M_inv = torch.tensor([[2, -1], [-0.5, 0.5]])
M_inv @ M
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
M = tf.constant([[1, 2], [1, 4]], dtype=tf.float32)
M_inv = tf.constant([[2, -1], [-0.5, 0.5]])
tf.matmul(M_inv, M)
</code></pre>
<h3 id="수치적-문제-numerical-issues"><a class="header" href="#수치적-문제-numerical-issues">수치적 문제 (Numerical Issues)</a></h3>
<p>행렬의 역행렬이 이론적으로는 유용하지만, 대부분의 경우 실제 문제 해결을 위해 행렬 역행렬을 <em>사용</em>하고 싶지는 않다는 점을 말해야 합니다.
일반적으로 다음과 같은 선형 방정식을 푸는 데 훨씬 더 수치적으로 안정적인 알고리즘이 있습니다.</p>
<p>$$
\mathbf{A}\mathbf{x} = \mathbf{b},
$$</p>
<p>역행렬을 계산하고 곱하여 다음을 얻는 것보다 말입니다.</p>
<p>$$
\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}.
$$</p>
<p>작은 수로 나누는 것이 수치적 불안정성을 초래할 수 있는 것처럼, 랭크가 낮은 것에 가까운 행렬의 반전도 마찬가지입니다.</p>
<p>게다가 행렬 $\mathbf{A}$가 *희소(sparse)*한 경우가 흔하며, 이는 소량의 0이 아닌 값만 포함하고 있음을 의미합니다.
예제를 탐구해 보면 이것이 역행렬이 희소하다는 것을 의미하지는 않는다는 것을 알게 될 것입니다.
$\mathbf{A}$가 500만 개의 0이 아닌 항목만 있는 100만 x 100만 행렬이라 하더라도(따라서 해당 500만 개만 저장하면 됨), 역행렬은 일반적으로 거의 모든 항목이 0이 아니어서 100만^2개 항목, 즉 1조 개의 항목을 모두 저장해야 합니다!</p>
<p>선형 대수로 작업할 때 자주 마주치는 까다로운 수치적 문제를 끝까지 파고들 시간은 없지만, 언제 주의를 기울여야 하는지에 대한 직관을 제공하고 싶었으며, 일반적으로 실전에서 역행렬 계산을 피하는 것이 좋은 경험 법칙입니다.</p>
<h2 id="행렬식-determinant"><a class="header" href="#행렬식-determinant">행렬식 (Determinant)</a></h2>
<p>선형 대수의 기하학적 관점은 *행렬식(determinant)*으로 알려진 근본적인 수량을 해석하는 직관적인 방법을 제공합니다.
이전의 그리드 이미지를 고려하되 이제 강조된 영역이 있습니다 (:numref:<code>fig_grid-filled</code>).</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/grid-transform-filled.svg" alt="그리드를 다시 왜곡하는 행렬 $\mathbf{A}$. 이번에는 강조된 사각형에 특별히 주의를 기울이고 싶습니다." />
:label:<code>fig_grid-filled</code></p>
<p>강조된 사각형을 보십시오. 이것은 $(0, 1)$ 및 $(1, 0)$에 의해 주어진 모서리를 가진 사각형이므로 넓이가 1입니다.
$\mathbf{A}$가 이 사각형을 변환한 후 평행사변형이 되는 것을 알 수 있습니다.
이 평행사변형이 우리가 시작했을 때와 동일한 넓이를 가져야 할 이유는 없으며, 실제로 여기에 표시된 특정 사례인</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
1 &amp; 2 \ -1 &amp; 3
\end{bmatrix},
$$</p>
<p>에서 이 평행사변형의 넓이를 계산하고 넓이가 5임을 얻는 것은 좌표 기하학의 연습 문제입니다.</p>
<p>일반적으로 행렬이 다음과 같은 경우</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
a &amp; b \ c &amp; d
\end{bmatrix},
$$</p>
<p>약간의 계산을 통해 결과 평행사변형의 넓이가 $ad-bc$임을 알 수 있습니다. 이 넓이를 <em>행렬식</em>이라고 합니다.</p>
<p>예제 코드로 이를 빠르게 확인해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
import numpy as np
np.linalg.det(np.array([[1, -1], [2, 3]]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
torch.det(torch.tensor([[1, -1], [2, 3]], dtype=torch.float32))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
tf.linalg.det(tf.constant([[1, -1], [2, 3]], dtype=tf.float32))
</code></pre>
<p>눈치 빠른 분들은 이 식이 0이거나 심지어 음수일 수 있다는 것을 알아차릴 것입니다. 음수 항의 경우 수학에서 일반적으로 취하는 관례의 문제입니다:
행렬이 도형을 뒤집으면 넓이가 음수가 된다고 말합니다.
이제 행렬식이 0일 때 우리는 더 많은 것을 배우게 됨을 살펴봅시다.</p>
<p>다음을 고려해 보십시오.</p>
<p>$$
\mathbf{B} = \begin{bmatrix}
2 &amp; 4 \ -1 &amp; -2
\end{bmatrix}.
$$</p>
<p>이 행렬의 행렬식을 계산하면 $2\cdot(-2 ) - 4\cdot(-1) = 0$을 얻습니다.
우리의 이해를 고려하면 이것은 이치에 맞습니다.
$\mathbf{B}$는 원래 이미지의 사각형을 넓이가 0인 선분으로 무너뜨립니다.
실제로 변환 후 넓이가 0이 되는 유일한 방법은 더 낮은 차원 공간으로 압축되는 것입니다.
따라서 우리는 다음과 같은 결과가 참임을 알 수 있습니다:
행렬 $A$는 행렬식이 0이 아닐 때만 가역적입니다.</p>
<p>마지막 코멘트로, 평면에 그려진 임의의 도형이 있다고 상상해 보십시오.
컴퓨터 과학자처럼 생각하여 그 도형을 작은 사각형들의 모음으로 분해할 수 있으며, 따라서 도형의 넓이는 본질적으로 분해된 사각형의 수입니다.
이제 그 도형을 행렬로 변환하면 이러한 사각형 각각을 평행사변형으로 보내며, 각 평행사변형은 행렬식에 의해 주어진 넓이를 갖습니다.
우리는 임의의 도형에 대해 행렬식이 행렬이 도형의 넓이를 조정하는 (부호가 있는) 숫자임을 알 수 있습니다.</p>
<p>더 큰 행렬에 대한 행렬식을 계산하는 것은 힘들 수 있지만 직관은 같습니다.
행렬식은 $n\times n$ 행렬이 $n$차원 부피를 조정하는 인수로 남아 있습니다.</p>
<h2 id="텐서-및-일반적인-선형-대수-연산-tensors-and-common-linear-algebra-operations"><a class="header" href="#텐서-및-일반적인-선형-대수-연산-tensors-and-common-linear-algebra-operations">텐서 및 일반적인 선형 대수 연산 (Tensors and Common Linear Algebra Operations)</a></h2>
<p>:numref:<code>sec_linear-algebra</code>에서 텐서의 개념이 소개되었습니다.
이 섹션에서는 텐서 축약(tensor contraction, 행렬 곱셈의 텐서 등가물)에 대해 더 깊이 파고들어, 이것이 수많은 행렬 및 벡터 연산에 대해 어떻게 통합된 관점을 제공할 수 있는지 살펴볼 것입니다.</p>
<p>행렬과 벡터의 경우 데이터를 변환하기 위해 그것들을 곱하는 방법을 알았습니다.
텐서가 우리에게 유용하려면 유사한 정의가 필요합니다. 행렬 곱셈을 생각해 보십시오.</p>
<p>$$
\mathbf{C} = \mathbf{A}\mathbf{B},
$$</p>
<p>또는 동등하게</p>
<p>$$ c_{i, j} = \sum_{k} a_{i, k}b_{k, j}.$$</p>
<p>이 패턴은 텐서에 대해 반복할 수 있는 패턴입니다.
텐서의 경우 보편적으로 선택할 수 있는 합산 대상이 하나만 있는 것은 아니므로 합산하려는 인덱스를 정확하게 지정해야 합니다.
예를 들어 다음을 고려할 수 있습니다.</p>
<p>$$
y_{il} = \sum_{jk} x_{ijkl}a_{jk}.
$$</p>
<p>그러한 변환을 <em>텐서 축약</em>이라고 합니다.
이는 행렬 곱셈만으로 가능한 것보다 훨씬 더 유연한 변환 가족을 나타낼 수 있습니다.</p>
<p>자주 사용되는 표기법상 단순화를 위해, 합계가 식에서 두 번 이상 발생하는 정확히 해당 인덱스들에 대한 것임을 알 수 있으므로 사람들은 종종 *아인슈타인 표기법(Einstein notation)*으로 작업합니다. 여기서 합계는 모든 반복되는 인덱스에 대해 암시적으로 취해집니다.
이는 다음과 같은 간결한 표현을 제공합니다.</p>
<p>$$
y_{il} = x_{ijkl}a_{jk}.
$$</p>
<h3 id="선형-대수의-일반적인-예-common-examples-from-linear-algebra"><a class="header" href="#선형-대수의-일반적인-예-common-examples-from-linear-algebra">선형 대수의 일반적인 예 (Common Examples from Linear Algebra)</a></h3>
<p>이전에 보았던 선형 대수 정의 중 얼마나 많은 것들이 이 압축된 텐서 표기법으로 표현될 수 있는지 살펴봅시다.</p>
<ul>
<li>$\mathbf{v} \cdot \mathbf{w} = \sum_i v_iw_i$</li>
<li>$\|\mathbf{v}\|_2^{2} = \sum_i v_iv_i$</li>
<li>$(\mathbf{A}\mathbf{v})<em>i = \sum_j a</em>{ij}v_j$</li>
<li>$(\mathbf{A}\mathbf{B})<em>{ik} = \sum_j a</em>{ij}b_{jk}$</li>
<li>$\textrm{tr}(\mathbf{A}) = \sum_i a_{ii}$</li>
</ul>
<p>이런 식으로 우리는 무수한 특수 표기법을 짧은 텐서 표현으로 대체할 수 있습니다.</p>
<h3 id="코드로-표현하기-expressing-in-code"><a class="header" href="#코드로-표현하기-expressing-in-code">코드로 표현하기 (Expressing in Code)</a></h3>
<p>텐서는 코드에서도 유연하게 조작될 수 있습니다.
:numref:<code>sec_linear-algebra</code>에서 본 것처럼 다음과 같이 텐서를 생성할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 텐서 정의
B = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
A = np.array([[1, 2], [3, 4]])
v = np.array([1, 2])

# 모양 출력
A.shape, B.shape, v.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 텐서 정의
B = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
A = torch.tensor([[1, 2], [3, 4]])
v = torch.tensor([1, 2])

# 모양 출력
A.shape, B.shape, v.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 텐서 정의
B = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
A = tf.constant([[1, 2], [3, 4]])
v = tf.constant([1, 2])

# 모양 출력
A.shape, B.shape, v.shape
</code></pre>
<p>아인슈타인 합계(Einstein summation)가 직접 구현되었습니다.
아인슈타인 합계에서 발생하는 인덱스는 문자열로 전달될 수 있으며, 그 뒤에 작용할 텐서가 옵니다.
예를 들어 행렬 곱셈을 구현하려면 위에서 본 아인슈타인 합계($\mathbf{A}\mathbf{v} = a_{ij}v_j$)를 고려하고 인덱스 자체를 스트립하여 구현을 얻을 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 행렬 곱셈 재구현
np.einsum("ij, j -&gt; i", A, v), A.dot(v)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 행렬 곱셈 재구현
torch.einsum("ij, j -&gt; i", A, v), A@v
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 행렬 곱셈 재구현
tf.einsum("ij, j -&gt; i", A, v), tf.matmul(A, tf.reshape(v, (2, 1)))
</code></pre>
<p>이것은 매우 유연한 표기법입니다.
예를 들어 전통적으로 다음과 같이 쓰일 계산을 하려면</p>
<p>$$
c_{kl} = \sum_{ij} \mathbf{b}<em>{ijk}\mathbf{a}</em>{il}v_j.
$$</p>
<p>아인슈타인 합계를 통해 다음과 같이 구현할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
np.einsum("ijk, il, j -&gt; kl", B, A, v)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
torch.einsum("ijk, il, j -&gt; kl", B, A, v)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
tf.einsum("ijk, il, j -&gt; kl", B, A, v)
</code></pre>
<p>이 표기법은 인간이 읽기 쉽고 효율적이지만, 어떤 이유로든 프로그래밍 방식으로 텐서 축약을 생성해야 하는 경우에는 번거롭습니다.
이러한 이유로 <code>einsum</code>은 각 텐서에 대해 정수 인덱스를 제공하는 대안 표기법을 제공합니다.
예를 들어 동일한 텐서 축약은 다음과 같이 쓸 수도 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
np.einsum(B, [0, 1, 2], A, [0, 3], v, [1], [2, 3])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# PyTorch는 이 유형의 표기법을 지원하지 않습니다.
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# TensorFlow는 이 유형의 표기법을 지원하지 않습니다.
</code></pre>
<p>두 표기법 중 어느 것이든 코드에서 텐서 축약을 간결하고 효율적으로 표현할 수 있게 해줍니다.</p>
<h2 id="요약-summary-117"><a class="header" href="#요약-summary-117">요약 (Summary)</a></h2>
<ul>
<li>벡터는 기하학적으로 공간에서의 점 또는 방향으로 해석될 수 있습니다.</li>
<li>내적은 임의의 고차원 공간에 대한 각도 개념을 정의합니다.</li>
<li>초평면은 직선과 평면의 고차원 일반화입니다. 분류 작업의 마지막 단계로 자주 사용되는 결정 평면을 정의하는 데 사용될 수 있습니다.</li>
<li>행렬 곱셈은 기본 좌표의 균일한 왜곡으로 기하학적으로 해석될 수 있습니다. 그들은 벡터를 변환하는 매우 제한적이지만 수학적으로 깨끗한 방법을 나타냅니다.</li>
<li>선형 종속은 벡터 모음이 우리가 기대하는 것보다 낮은 차원 공간에 있을 때(가령 2차원 공간에 3개의 벡터가 있을 때)를 알려주는 방법입니다. 행렬의 랭크는 선형 독립인 열의 최대 부분 집합의 크기입니다.</li>
<li>행렬의 역행렬이 정의되면 행렬 반전을 통해 첫 번째 행렬의 작용을 되돌리는 다른 행렬을 찾을 수 있습니다. 행렬 반전은 이론적으로 유용하지만 수치적 불안정성 때문에 실전에서는 주의가 필요합니다.</li>
<li>행렬식은 행렬이 공간을 얼마나 확장하거나 축소하는지 측정할 수 있게 해줍니다. 0이 아닌 행렬식은 가역(비특이) 행렬을 의미하고 0 값 행렬식은 행렬이 비가역(특이)임을 의미합니다.</li>
<li>텐서 축약 및 아인슈타인 합계는 머신러닝에서 볼 수 있는 많은 계산을 표현하기 위한 깔끔하고 명확한 표기법을 제공합니다.</li>
</ul>
<h2 id="연습-문제-exercises-132"><a class="header" href="#연습-문제-exercises-132">연습 문제 (Exercises)</a></h2>
<ol>
<li>다음 사이의 각도는 얼마입니까?
$$
\vec v_1 = \begin{bmatrix}
1 \ 0 \ -1 \ 2
\end{bmatrix}, \qquad \vec v_2 = \begin{bmatrix}
3 \ 1 \ 0 \ 1
\end{bmatrix}?
$$</li>
<li>맞음 또는 틀림: $\begin{bmatrix}1 &amp; 2\0&amp;1\end{bmatrix}$ 및 $\begin{bmatrix}1 &amp; -2\0&amp;1\end{bmatrix}$은 서로의 역행렬입니까?</li>
<li>평면에 넓이가 $100\textrm{m}^2$인 도형을 그린다고 가정해 봅시다. 행렬</li>
</ol>
<p>$$
\begin{bmatrix}
2 &amp; 3\ 1 &amp; 2
\end{bmatrix}.
$$
로 도형을 변환한 후의 넓이는 얼마입니까?
4. 다음 벡터 집합 중 선형 독립인 것은 무엇입니까?</p>
<ul>
<li>$\left{\begin{pmatrix}1\0\-1\end{pmatrix}, \begin{pmatrix}2\1\-1\end{pmatrix}, \begin{pmatrix}3\1\1\end{pmatrix}\right}$</li>
<li>$\left{\begin{pmatrix}3\1\1\end{pmatrix}, \begin{pmatrix}1\1\1\end{pmatrix}, \begin{pmatrix}0\0\0\end{pmatrix}\right}$</li>
<li>$\left{\begin{pmatrix}1\1\0\end{pmatrix}, \begin{pmatrix}0\1\-1\end{pmatrix}, \begin{pmatrix}1\0\1\end{pmatrix}\right}$</li>
</ul>
<ol start="5">
<li>어떤 값 $a, b, c, d$의 선택에 대해 행렬이 $A = \begin{bmatrix}c\d\end{bmatrix}\.\begin{bmatrix}a &amp; b\end{bmatrix}$로 쓰여졌다고 가정합니다. 맞음 또는 틀림: 그러한 행렬의 행렬식은 항상 0입니까?</li>
<li>벡터 $e_1 = \begin{bmatrix}1\0\end{bmatrix}$ 및 $e_2 = \begin{bmatrix}0\1\end{bmatrix}$은 직교합니다. $Ae_1$과 $Ae_2$가 직교하도록 하는 행렬 $A$에 대한 조건은 무엇입니까?</li>
<li>임의의 행렬 $A$에 대해 $\textrm{tr}(\mathbf{A}^4)$를 아인슈타인 표기법으로 어떻게 쓸 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/410">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1084">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1085">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="고유-분해-eigendecompositions"><a class="header" href="#고유-분해-eigendecompositions">고유 분해 (Eigendecompositions)</a></h1>
<p>:label:<code>sec_eigendecompositions</code></p>
<p>고유값(eigenvalues)은 선형 대수를 공부할 때 접하게 되는 가장 유용한 개념 중 하나인 경우가 많지만, 초보자로서 그 중요성을 간과하기 쉽습니다.
아래에서는 고유 분해를 소개하고 그것이 왜 그렇게 중요한지에 대한 느낌을 전달하려고 노력합니다.</p>
<p>다음과 같은 항목을 가진 행렬 $A$가 있다고 가정해 봅시다.</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 0 \ 0 &amp; -1
\end{bmatrix}.
$$</p>
<p>임의의 벡터 $\mathbf{v} = [x, y]^\top$에 $A$를 적용하면,
벡터 $\mathbf{A}\mathbf{v} = [2x, -y]^\top$를 얻습니다.
이것은 직관적인 해석을 갖습니다:
벡터를 $x$ 방향으로 두 배 넓게 늘리고, $y$ 방향으로 뒤집는 것입니다.</p>
<p>그러나 무언가가 변경되지 않고 유지되는 <em>일부</em> 벡터가 있습니다.
즉, $[1, 0]^\top$은 $[2, 0]^\top$으로 보내지고,
$[0, 1]^\top$은 $[0, -1]^\top$으로 보내집니다.
이러한 벡터는 여전히 동일한 직선상에 있으며, 유일한 수정 사항은 행렬이 각각 $2$와 $-1$의 인수로 늘린다는 것입니다.
우리는 그러한 벡터를 *고유 벡터(eigenvectors)*라고 부르고, 그것들이 늘어나는 인수를 *고유값(eigenvalues)*이라고 부릅니다.</p>
<p>일반적으로 다음과 같은 숫자 $\lambda$와 벡터 $\mathbf{v}$를 찾을 수 있다면</p>
<p>$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}.
$$</p>
<p>우리는 $\mathbf{v}$를 $A$에 대한 고유 벡터라고 하고 $\lambda$를 고유값이라고 합니다.</p>
<h2 id="고유값-찾기-finding-eigenvalues"><a class="header" href="#고유값-찾기-finding-eigenvalues">고유값 찾기 (Finding Eigenvalues)</a></h2>
<p>그것들을 찾는 방법을 알아봅시다. 양변에서 $\lambda \mathbf{v}$를 빼고 벡터를 인수분해하면 위의 식이 다음과 동등함을 알 수 있습니다.</p>
<p>$$(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = 0.$$
:eqlabel:<code>eq_eigvalue_der</code></p>
<p>:eqref:<code>eq_eigvalue_der</code>가 발생하려면 $(\mathbf{A} - \lambda \mathbf{I})$가 어떤 방향을 0으로 압축해야 하므로 가역적이지 않으며, 따라서 행렬식이 0입니다.
따라서 우리는 $\det(\mathbf{A}-\lambda \mathbf{I}) = 0$이 되는 $\lambda$가 무엇인지 찾음으로써 <em>고유값</em>을 찾을 수 있습니다.
고유값을 찾으면 $\mathbf{A}\mathbf{v} = \lambda \mathbf{v}$를 풀어 연관된 <em>고유 벡터</em>를 찾을 수 있습니다.</p>
<h3 id="예제-an-example-2"><a class="header" href="#예제-an-example-2">예제 (An Example)</a></h3>
<p>더 어려운 행렬로 이것을 살펴봅시다.</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 1\ 2 &amp; 3
\end{bmatrix}.
$$</p>
<p>$\det(\mathbf{A}-\lambda \mathbf{I}) = 0$을 고려하면,
이것이 다항식 방정식 $0 = (2-\lambda)(3-\lambda)-2 = (4-\lambda)(1-\lambda)$와 동등함을 알 수 있습니다.
따라서 두 개의 고유값은 $4$와 $1$입니다.
연관된 벡터를 찾으려면 다음을 풀어야 합니다.</p>
<p>$$
\begin{bmatrix}
2 &amp; 1\ 2 &amp; 3
\end{bmatrix}\begin{bmatrix}x \ y\end{bmatrix} = \begin{bmatrix}x \ y\end{bmatrix}  ; \textrm{및} ;
\begin{bmatrix}
2 &amp; 1\ 2 &amp; 3
\end{bmatrix}\begin{bmatrix}x \ y\end{bmatrix}  = \begin{bmatrix}4x \ 4y\end{bmatrix} .
$$</p>
<p>우리는 이를 각각 벡터 $[1, -1]^\top$ 및 $[1, 2]^\top$로 풀 수 있습니다.</p>
<p>내장된 <code>numpy.linalg.eig</code> 루틴을 사용하여 코드에서 이를 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
import numpy as np

np.linalg.eig(np.array([[2, 1], [2, 3]]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
import torch

torch.linalg.eig(torch.tensor([[2, 1], [2, 3]], dtype=torch.float64))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
import tensorflow as tf

tf.linalg.eig(tf.constant([[2, 1], [2, 3]], dtype=tf.float64))
</code></pre>
<p><code>numpy</code>는 고유 벡터의 길이를 1로 정규화하는 반면, 우리는 임의의 길이를 취했다는 점에 유의하십시오.
또한 부호의 선택은 임의적입니다.
그러나 계산된 벡터는 동일한 고유값을 가진 우리가 손으로 찾은 벡터와 평행합니다.</p>
<h2 id="행렬-분해-decomposing-matrices"><a class="header" href="#행렬-분해-decomposing-matrices">행렬 분해 (Decomposing Matrices)</a></h2>
<p>이전 예제를 한 단계 더 진행해 봅시다.</p>
<p>$$
\mathbf{W} = \begin{bmatrix}
1 &amp; 1 \ -1 &amp; 2
\end{bmatrix},
$$</p>
<p>를 행렬 $\mathbf{A}$의 고유 벡터들이 열인 행렬이라고 합시다.</p>
<p>$$
\boldsymbol{\Sigma} = \begin{bmatrix}
1 &amp; 0 \ 0 &amp; 4
\end{bmatrix},
$$</p>
<p>를 대각선에 연관된 고유값이 있는 행렬이라고 합시다.
그러면 고유값과 고유 벡터의 정의는 우리에게 다음을 알려줍니다.</p>
<p>$$
\mathbf{A}\mathbf{W} =\mathbf{W} \boldsymbol{\Sigma} .
$$</p>
<p>행렬 $W$는 가역적이므로 양변의 오른쪽에 $W^{-1}$을 곱하면 다음과 같이 쓸 수 있음을 알 수 있습니다.</p>
<p>$$\mathbf{A} = \mathbf{W} \boldsymbol{\Sigma} \mathbf{W}^{-1}.$$
:eqlabel:<code>eq_eig_decomp</code></p>
<p>다음 섹션에서 이것의 몇 가지 좋은 결과를 보겠지만, 지금은 선형 독립인 고유 벡터의 전체 컬렉션을 찾을 수 있는 한(그래서 $W$가 가역적이 되도록) 그러한 분해가 존재한다는 것만 알면 됩니다.</p>
<h2 id="고유-분해에-대한-연산-operations-on-eigendecompositions"><a class="header" href="#고유-분해에-대한-연산-operations-on-eigendecompositions">고유 분해에 대한 연산 (Operations on Eigendecompositions)</a></h2>
<p>고유 분해 :eqref:<code>eq_eig_decomp</code>의 한 가지 좋은 점은 우리가 보통 접하는 많은 연산을 고유 분해의 관점에서 깔끔하게 쓸 수 있다는 것입니다. 첫 번째 예로 다음을 고려하십시오.</p>
<p>$$
\mathbf{A}^n = \overbrace{\mathbf{A}\cdots \mathbf{A}}^{\textrm{$n$ 번}} = \overbrace{(\mathbf{W}\boldsymbol{\Sigma} \mathbf{W}^{-1})\cdots(\mathbf{W}\boldsymbol{\Sigma} \mathbf{W}^{-1})}^{\textrm{$n$ 번}} =  \mathbf{W}\overbrace{\boldsymbol{\Sigma}\cdots\boldsymbol{\Sigma}}^{\textrm{$n$ 번}}\mathbf{W}^{-1} = \mathbf{W}\boldsymbol{\Sigma}^n \mathbf{W}^{-1}.
$$</p>
<p>이것은 행렬의 임의의 양수 거듭제곱에 대해, 고유값을 동일한 거듭제곱으로 올림으로써 고유 분해를 얻을 수 있음을 알려줍니다.
음수 거듭제곱에 대해서도 동일하게 나타낼 수 있으므로, 행렬을 반전시키고 싶다면 다음만 고려하면 됩니다.</p>
<p>$$
\mathbf{A}^{-1} = \mathbf{W}\boldsymbol{\Sigma}^{-1} \mathbf{W}^{-1},
$$</p>
<p>즉, 각 고유값만 반전시키면 됩니다.
이것은 각 고유값이 0이 아닌 한 작동하므로, 가역적이라는 것이 0인 고유값이 없는 것과 같음을 알 수 있습니다.</p>
<p>실제로 $\lambda_1, \ldots, \lambda_n$이 행렬의 고유값이라면 그 행렬의 행렬식은</p>
<p>$$
\det(\mathbf{A}) = \lambda_1 \cdots \lambda_n,
$$</p>
<p>또는 모든 고유값의 곱임을 추가 작업으로 보일 수 있습니다.
이는 $\mathbf{W}$가 어떤 늘림을 하든 $W^{-1}$이 이를 되돌리므로 결국 발생하는 유일한 늘림은 대각 원소들의 곱으로 부피를 늘리는 대각 행렬 $\boldsymbol{\Sigma}$에 의한 곱셈이기 때문에 직관적으로 말이 됩니다.</p>
<p>마지막으로, 랭크(rank)가 행렬의 선형 독립인 열의 최대 개수였음을 상기하십시오.
고유 분해를 자세히 검토함으로써, 랭크가 $\mathbf{A}$의 0이 아닌 고유값의 개수와 같음을 알 수 있습니다.</p>
<p>예제는 계속될 수 있지만 요점은 명확합니다:
고유 분해는 많은 선형 대수 계산을 단순화할 수 있으며 많은 수치 알고리즘과 우리가 선형 대수에서 수행하는 많은 분석의 기초가 되는 근본적인 연산입니다.</p>
<h2 id="대칭-행렬의-고유-분해-eigendecompositions-of-symmetric-matrices"><a class="header" href="#대칭-행렬의-고유-분해-eigendecompositions-of-symmetric-matrices">대칭 행렬의 고유 분해 (Eigendecompositions of Symmetric Matrices)</a></h2>
<p>위의 과정이 작동할 만큼 충분한 선형 독립 고유 벡터를 항상 찾을 수 있는 것은 아닙니다. 예를 들어 행렬</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
1 &amp; 1 \ 0 &amp; 1
\end{bmatrix},
$$</p>
<p>은 단 하나의 고유 벡터, 즉 $(1, 0)^\top$만 갖습니다.
그러한 행렬을 다루기 위해서는 우리가 다룰 수 있는 것보다 더 발전된 기술(예: 조르단 표준형 또는 특이값 분해)이 필요합니다.
우리는 종종 고유 벡터의 전체 집합의 존재를 보장할 수 있는 행렬들에 주의를 집중해야 할 것입니다.</p>
<p>가장 흔히 마주치는 가족은 <em>대칭 행렬</em>입니다. 이는 $\mathbf{A} = \mathbf{A}^\top$인 행렬들입니다.
이 경우 우리는 $W$를 <em>직교 행렬(orthogonal matrix)</em>—모든 열이 서로 직각인 길이 1의 벡터인 행렬, 즉 $\mathbf{W}^\top = \mathbf{W}^{-1}$—로 취할 수 있으며 모든 고유값은 실수가 됩니다.
따라서 이 특별한 경우에 우리는 :eqref:<code>eq_eig_decomp</code>를 다음과 같이 쓸 수 있습니다.</p>
<p>$$
\mathbf{A} = \mathbf{W}\boldsymbol{\Sigma}\mathbf{W}^\top .
$$</p>
<h2 id="거시고린-원판-정리-gershgorin-circle-theorem"><a class="header" href="#거시고린-원판-정리-gershgorin-circle-theorem">거시고린 원판 정리 (Gershgorin Circle Theorem)</a></h2>
<p>고유값은 종종 직관적으로 추론하기 어렵습니다.
임의의 행렬이 제시되면 그것을 계산하지 않고는 고유값이 무엇인지에 대해 말할 수 있는 것이 거의 없습니다.
하지만 가장 큰 값들이 대각선에 있다면 잘 근사하기 쉽게 만들 수 있는 정리가 하나 있습니다.</p>
<p>$\mathbf{A} = (a_{ij})$를 임의의 정사각 행렬($n\times n$)이라고 합시다.
우리는 $r_i = \sum_{j \neq i} |a_{ij}|$로 정의할 것입니다.
$\mathcal{D}<em>i$가 복소평면에서 중심이 $a</em>{ii}$이고 반지름이 $r_i$인 원판을 나타낸다고 합시다.
그러면 $\mathbf{A}$의 모든 고유값은 $\mathcal{D}_i$ 중 하나에 포함됩니다.</p>
<p>이것은 이해하기에 조금 많을 수 있으므로 예제를 살펴봅시다.
행렬을 고려해 보십시오:</p>
<p>$$
\mathbf{A} = \begin{bmatrix}
1.0 &amp; 0.1 &amp; 0.1 &amp; 0.1 \ 0.1 &amp; 3.0 &amp; 0.2 &amp; 0.3 \ 0.1 &amp; 0.2 &amp; 5.0 &amp; 0.5 \ 0.1 &amp; 0.3 &amp; 0.5 &amp; 9.0
\end{bmatrix}.
$$</p>
<p>우리는 $r_1 = 0.3, r_2 = 0.6, r_3 = 0.8, r_4 = 0.9$를 갖습니다.
행렬은 대칭이므로 모든 고유값은 실수입니다.
이는 우리의 모든 고유값이 다음 범위 중 하나에 있게 됨을 의미합니다.</p>
<p>$$[a_{11}-r_1, a_{11}+r_1] = [0.7, 1.3], $$</p>
<p>$$[a_{22}-r_2, a_{22}+r_2] = [2.4, 3.6], $$</p>
<p>$$[a_{33}-r_3, a_{33}+r_3] = [4.2, 5.8], $$</p>
<p>$$[a_{44}-r_4, a_{44}+r_4] = [8.1, 9.9]. $$</p>
<p>수치 계산을 수행하면 고유값이 대략 $0.99, 2.97, 4.95, 9.08$임을 알 수 있으며,
모두 제공된 범위 내에 편안하게 들어옵니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
A = np.array([[1.0, 0.1, 0.1, 0.1],
              [0.1, 3.0, 0.2, 0.3],
              [0.1, 0.2, 5.0, 0.5],
              [0.1, 0.3, 0.5, 9.0]])

v, _ = np.linalg.eig(A)
v
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
A = torch.tensor([[1.0, 0.1, 0.1, 0.1],
              [0.1, 3.0, 0.2, 0.3],
              [0.1, 0.2, 5.0, 0.5],
              [0.1, 0.3, 0.5, 9.0]])

v, _ = torch.linalg.eig(A)
v
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
A = tf.constant([[1.0, 0.1, 0.1, 0.1],
                [0.1, 3.0, 0.2, 0.3],
                [0.1, 0.2, 5.0, 0.5],
                [0.1, 0.3, 0.5, 9.0]])

v, _ = tf.linalg.eigh(A)
v
</code></pre>
<p>이런 식으로 고유값을 근사할 수 있으며,
대각선이 다른 모든 원소보다 상당히 큰 경우 근사는 상당히 정확할 것입니다.</p>
<p>작은 일이지만, 고유 분해와 같이 복잡하고 미묘한 주제에 대해 우리가 할 수 있는 어떤 직관적인 파악이라도 얻는 것은 좋습니다.</p>
<h2 id="유용한-응용-반복-맵의-성장-a-useful-application-the-growth-of-iterated-maps"><a class="header" href="#유용한-응용-반복-맵의-성장-a-useful-application-the-growth-of-iterated-maps">유용한 응용: 반복 맵의 성장 (A Useful Application: The Growth of Iterated Maps)</a></h2>
<p>이제 고유 벡터가 원칙적으로 무엇인지 이해했으므로,
신경망 동작의 중심 문제인 적절한 가중치 초기화에 대한 깊은 이해를 제공하는 데 그것들이 어떻게 사용될 수 있는지 살펴봅시다.</p>
<h3 id="장기-행동으로서의-고유-벡터-eigenvectors-as-long-term-behavior"><a class="header" href="#장기-행동으로서의-고유-벡터-eigenvectors-as-long-term-behavior">장기 행동으로서의 고유 벡터 (Eigenvectors as Long Term Behavior)</a></h3>
<p>심층 신경망 초기화에 대한 완전한 수학적 조사는 텍스트의 범위를 벗어나지만,
고유값이 이러한 모델이 어떻게 작동하는지 이해하는 데 어떻게 도움이 되는지 여기에서 장난감 버전을 볼 수 있습니다.
우리가 알고 있듯이, 신경망은 선형 변환 레이어와 비선형 연산을 산재시켜 작동합니다.
여기서 단순함을 위해 비선형성이 없다고 가정하고 변환이 단일 반복 행렬 연산 $A$라고 가정합시다. 그러면 우리 모델의 출력은 다음과 같습니다.</p>
<p>$$
\mathbf{v}<em>{out} = \mathbf{A}\cdot \mathbf{A}\cdots \mathbf{A} \mathbf{v}</em>{in} = \mathbf{A}^N \mathbf{v}_{in}.
$$</p>
<p>이러한 모델이 초기화될 때 $A$는 가우스(Gaussian) 항목을 가진 무작위 행렬로 취해지므로, 그중 하나를 만들어 봅시다.
구체적으로 평균 0, 분산 1인 가우스 분포 $5 \times 5$ 행렬로 시작합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
np.random.seed(8675309)

k = 5
A = np.random.randn(k, k)
A
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
torch.manual_seed(42)

k = 5
A = torch.randn(k, k, dtype=torch.float64)
A
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
k = 5
A = tf.random.normal((k, k), dtype=tf.float64)
A
</code></pre>
<h3 id="무작위-데이터에서의-행동-behavior-on-random-data"><a class="header" href="#무작위-데이터에서의-행동-behavior-on-random-data">무작위 데이터에서의 행동 (Behavior on Random Data)</a></h3>
<p>우리의 장난감 모델에서 단순함을 위해,
우리가 공급하는 데이터 벡터 $\mathbf{v}_{in}$이 무작위 5차원 가우스 벡터라고 가정합시다.
어떤 일이 일어나기를 원하는지 생각해 봅시다.
문맥을 위해 일반적인 머신러닝 문제를 생각해 봅시다.
우리는 이미지와 같은 입력 데이터를 이미지가 고양이 사진일 확률과 같은 예측으로 바꾸려고 노력하고 있습니다.
만약 $\mathbf{A}$를 반복적으로 적용하여 무작위 벡터를 매우 길게 늘린다면,
입력의 작은 변화가 출력의 큰 변화로 증폭될 것입니다 - 입력 이미지의 아주 작은 수정이 매우 다른 예측으로 이어질 것입니다.
이것은 옳지 않은 것 같습니다!</p>
<p>반대로 $\mathbf{A}$가 무작위 벡터를 더 짧게 수축시킨다면,
많은 레이어를 거친 후 벡터는 본질적으로 아무것도 아닌 것으로 수축할 것이고,
출력은 입력에 의존하지 않을 것입니다. 이것 또한 분명히 옳지 않습니다!</p>
<p>우리는 출력이 입력에 따라 변하되 너무 많이 변하지 않도록 하기 위해
성장과 쇠퇴 사이의 좁은 길을 걸어야 합니다!</p>
<p>무작위 입력 벡터에 대해 행렬 $\mathbf{A}$를 반복적으로 곱할 때 어떤 일이 일어나는지 살펴보고 노름(norm)을 추적해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# `A`를 반복적으로 적용한 후 노름의 시퀀스 계산
v_in = np.random.randn(k, 1)

norm_list = [np.linalg.norm(v_in)]
for i in range(1, 100):
    v_in = A.dot(v_in)
    norm_list.append(np.linalg.norm(v_in))

d2l.plot(np.arange(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# `A`를 반복적으로 적용한 후 노름의 시퀀스 계산
v_in = torch.randn(k, 1, dtype=torch.float64)

norm_list = [torch.norm(v_in).item()]
for i in range(1, 100):
    v_in = A @ v_in
    norm_list.append(torch.norm(v_in).item())

d2l.plot(torch.arange(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# `A`를 반복적으로 적용한 후 노름의 시퀀스 계산
v_in = tf.random.normal((k, 1), dtype=tf.float64)

norm_list = [tf.norm(v_in).numpy()]
for i in range(1, 100):
    v_in = tf.matmul(A, v_in)
    norm_list.append(tf.norm(v_in).numpy())

d2l.plot(tf.range(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<p>노름이 통제할 수 없이 커지고 있습니다!
실제로 몫의 리스트를 취하면 패턴을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 노름의 스케일링 인자 계산
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])

d2l.plot(np.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 노름의 스케일링 인자 계산
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])

d2l.plot(torch.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 노름의 스케일링 인자 계산
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])

d2l.plot(tf.range(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<p>위의 계산 마지막 부분을 보면, 무작위 벡터가 <code>1.974459321485[...]</code>라는 인수로 늘어나는 것을 볼 수 있습니다.
끝부분이 약간씩 바뀌기는 하지만 늘어나는 인수는 안정적입니다.</p>
<h3 id="고유-벡터와-다시-연결하기-relating-back-to-eigenvectors"><a class="header" href="#고유-벡터와-다시-연결하기-relating-back-to-eigenvectors">고유 벡터와 다시 연결하기 (Relating Back to Eigenvectors)</a></h3>
<p>우리는 고유 벡터와 고유값이 무언가가 늘어나는 양에 대응한다는 것을 보았지만, 그것은 특정 벡터와 특정 늘림에 대한 것이었습니다.
$\mathbf{A}$에 대해 그것들이 무엇인지 살펴봅시다.
여기서 약간의 주의사항이 있습니다: 그것들을 모두 보려면 복소수로 가야 한다는 것이 밝혀졌습니다.
이것들을 늘림과 회전으로 생각할 수 있습니다.
복소수의 노름(실수부와 허수부의 제곱 합의 제곱근)을 취함으로써 그 늘림 인수를 측정할 수 있습니다. 그것들을 정렬해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 고유값 계산
eigs = np.linalg.eigvals(A).tolist()
norm_eigs = [np.absolute(x) for x in eigs]
norm_eigs.sort()
print(f'norms of eigenvalues: {norm_eigs}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 고유값 계산
eigs = torch.linalg.eig(A).eigenvalues.tolist()
norm_eigs = [torch.abs(torch.tensor(x)) for x in eigs]
norm_eigs.sort()
print(f'norms of eigenvalues: {norm_eigs}')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 고유값 계산
eigs = tf.linalg.eigh(A)[0].numpy().tolist()
norm_eigs = [tf.abs(tf.constant(x, dtype=tf.float64)) for x in eigs]
norm_eigs.sort()
print(f'norms of eigenvalues: {norm_eigs}')
</code></pre>
<h3 id="관찰-an-observation"><a class="header" href="#관찰-an-observation">관찰 (An Observation)</a></h3>
<p>우리는 여기서 약간 예상치 못한 일이 일어나는 것을 봅니다:
무작위 벡터에 적용된 우리 행렬 $\mathbf{A}$의 장기적인 늘림에 대해 우리가 이전에 식별한 그 숫자가 <em>정확히</em>
(소수점 아래 13자리까지 정확하게!)
$\mathbf{A}$의 가장 큰 고유값입니다.
이것은 분명히 우연이 아닙니다!</p>
<p>하지만 이제 기하학적으로 무슨 일이 일어나고 있는지 생각해보면 이해가 되기 시작합니다.
무작위 벡터를 고려해 보십시오. 이 무작위 벡터는 모든 방향을 조금씩 가리키고 있으므로,
특히 $\mathbf{A}$의 가장 큰 고유값과 연관된 고유 벡터와 동일한 방향을 적어도 조금은 가리킵니다.
이것은 너무 중요해서 <em>주 고유값(principal eigenvalue)</em> 및 *주 고유 벡터(principal eigenvector)*라고 불립니다.
$\mathbf{A}$를 적용한 후, 우리의 무작위 벡터는 가능한 모든 고유 벡터와 연관되어 모든 가능한 방향으로 늘어나지만,
무엇보다도 이 주 고유 벡터와 연관된 방향으로 가장 많이 늘어납니다.
이것이 의미하는 바는 $A$를 적용한 후 우리의 무작위 벡터가 더 길어지고 주 고유 벡터와 더 일치하는 방향을 가리킨다는 것입니다.
행렬을 여러 번 적용한 후 주 고유 벡터와의 일치는 점점 더 가까워져서,
모든 실질적인 목적을 위해 우리의 무작위 벡터는 주 고유 벡터로 변환됩니다!
실제로 이 알고리즘은 행렬의 가장 큰 고유값과 고유 벡터를 찾기 위한 *거듭제곱 반복(power iteration)*으로 알려진 것의 기초입니다. 자세한 내용은 예를 들어 :cite:<code>Golub.Van-Loan.1996</code>을 참조하십시오.</p>
<h3 id="정규화-수정-fixing-the-normalization"><a class="header" href="#정규화-수정-fixing-the-normalization">정규화 수정 (Fixing the Normalization)</a></h3>
<p>이제 위의 논의로부터 우리는 무작위 벡터가 전혀 늘어나거나 줄어들지 않기를 원한다는 결론을 내렸습니다.
우리는 전체 과정 동안 무작위 벡터가 거의 동일한 크기를 유지하기를 바랍니다.
그렇게 하기 위해 이제 우리는 가장 큰 고유값이 대신 1이 되도록 주 고유값으로 우리 행렬의 스케일을 조정합니다.
이 경우 어떤 일이 일어나는지 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 행렬 `A`의 스케일 조정
A /= norm_eigs[-1]

# 동일한 실험 다시 수행
v_in = np.random.randn(k, 1)

norm_list = [np.linalg.norm(v_in)]
for i in range(1, 100):
    v_in = A.dot(v_in)
    norm_list.append(np.linalg.norm(v_in))

d2l.plot(np.arange(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 행렬 `A`의 스케일 조정
A /= norm_eigs[-1]

# 동일한 실험 다시 수행
v_in = torch.randn(k, 1, dtype=torch.float64)

norm_list = [torch.norm(v_in).item()]
for i in range(1, 100):
    v_in = A @ v_in
    norm_list.append(torch.norm(v_in).item())

d2l.plot(torch.arange(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 행렬 `A`의 스케일 조정
A /= norm_eigs[-1]

# 동일한 실험 다시 수행
v_in = tf.random.normal((k, 1), dtype=tf.float64)

norm_list = [tf.norm(v_in).numpy()]
for i in range(1, 100):
    v_in = tf.matmul(A, v_in)
    norm_list.append(tf.norm(v_in).numpy())

d2l.plot(tf.range(0, 100), norm_list, 'Iteration', 'Value')
</code></pre>
<p>이전처럼 연속된 노름 사이의 비율을 플롯할 수도 있으며 실제로 안정화되는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 비율도 플롯
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i-1])

d2l.plot(np.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 비율도 플롯
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i-1])

d2l.plot(torch.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 비율도 플롯
norm_ratio_list = []
for i in range(1, 100):
    norm_ratio_list.append(norm_list[i]/norm_list[i-1])

d2l.plot(tf.range(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
</code></pre>
<h2 id="토론-discussion-5"><a class="header" href="#토론-discussion-5">토론 (Discussion)</a></h2>
<p>우리는 이제 우리가 원했던 것을 정확히 봅니다!
행렬을 주 고유값으로 정규화한 후,
무작위 데이터가 이전처럼 폭발하지 않고 오히려 결국 특정 값으로 평형을 이룹니다.
이런 일들을 제1원리부터 할 수 있으면 좋겠는데,
수학을 깊이 파고들면 독립적인 평균 0, 분산 1인 가우스 항목을 가진 큰 무작위 행렬의 가장 큰 고유값은
*원형 법칙(circular law)*이라는 매혹적인 사실 덕분에 평균적으로 대략 $\sqrt{n}$,
또는 우리의 경우 $\sqrt{5} \approx 2.2$가 된다는 것을 알 수 있습니다 :cite:<code>Ginibre.1965</code>.
무작위 행렬의 고유값(및 특이값이라고 불리는 관련 객체) 사이의 관계는 :citet:<code>Pennington.Schoenholz.Ganguli.2017</code> 및 후속 작업에서 논의된 것처럼 신경망의 적절한 초기화와 깊은 관계가 있음이 밝혀졌습니다.</p>
<h2 id="요약-summary-118"><a class="header" href="#요약-summary-118">요약 (Summary)</a></h2>
<ul>
<li>고유 벡터는 방향을 바꾸지 않고 행렬에 의해 늘어나는 벡터입니다.</li>
<li>고유값은 행렬의 적용에 의해 고유 벡터가 늘어나는 양입니다.</li>
<li>행렬의 고유 분해는 많은 연산을 고유값에 대한 연산으로 축소할 수 있게 해줍니다.</li>
<li>거시고린 원판 정리는 행렬의 고유값에 대한 근사치를 제공할 수 있습니다.</li>
<li>반복되는 행렬 거듭제곱의 행동은 주로 가장 큰 고유값의 크기에 달려 있습니다. 이 이해는 신경망 초기화 이론에서 많은 응용 분야를 갖습니다.</li>
</ul>
<h2 id="연습-문제-exercises-133"><a class="header" href="#연습-문제-exercises-133">연습 문제 (Exercises)</a></h2>
<ol>
<li>다음 행렬의 고유값과 고유 벡터는 무엇입니까?
$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 1 \ 1 &amp; 2
\end{bmatrix}?
$$</li>
<li>다음 행렬의 고유값과 고유 벡터는 무엇이며, 이 예제는 이전 예제와 비교하여 무엇이 이상합니까?
$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 1 \ 0 &amp; 2
\end{bmatrix}.
$$</li>
<li>고유값을 계산하지 않고, 다음 행렬의 가장 작은 고유값이 $0.5$보다 작을 수 있습니까? <em>참고</em>: 이 문제는 머릿속으로 풀 수 있습니다.
$$
\mathbf{A} = \begin{bmatrix}
3.0 &amp; 0.1 &amp; 0.3 &amp; 1.0 \ 0.1 &amp; 1.0 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.1 &amp; 5.0 &amp; 0.0 \ 1.0 &amp; 0.2 &amp; 0.0 &amp; 1.8
\end{bmatrix}.
$$</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/411">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1086">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1087">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="일변수-미적분학-single-variable-calculus"><a class="header" href="#일변수-미적분학-single-variable-calculus">일변수 미적분학 (Single Variable Calculus)</a></h1>
<p>:label:<code>sec_single_variable_calculus</code></p>
<p>:numref:<code>sec_calculus</code>에서 우리는 미분학의 기본 요소들을 보았습니다. 이 섹션에서는 미적분학의 기초를 더 깊이 파고들어 머신러닝의 맥락에서 이를 어떻게 이해하고 적용할 수 있는지 살펴봅니다.</p>
<h2 id="미분학-differential-calculus"><a class="header" href="#미분학-differential-calculus">미분학 (Differential Calculus)</a></h2>
<p>미분학은 근본적으로 함수가 작은 변화 아래에서 어떻게 행동하는지에 대한 연구입니다. 이것이 왜 딥러닝의 핵심인지 알아보기 위해 예제를 하나 고려해 봅시다.</p>
<p>편의를 위해 가중치들이 단일 벡터 $\mathbf{w} = (w_1, \ldots, w_n)$로 연결된 심층 신경망이 있다고 가정합시다. 훈련 데이터셋이 주어졌을 때, 이 데이터셋에 대한 우리 신경망의 손실을 고려하며 이를 $\mathcal{L}(\mathbf{w})$라고 쓰겠습니다.</p>
<p>이 함수는 주어진 아키텍처의 가능한 모든 모델의 이 데이터셋에서의 성능을 인코딩하므로 매우 복잡하며, 어떤 가중치 세트 $\mathbf{w}$가 손실을 최소화할지 알기는 거의 불가능합니다. 따라서 실전에서는 종종 가중치를 <em>무작위로</em> 초기화하는 것부터 시작하여, 손실을 가능한 한 빨리 줄이는 방향으로 작은 단계들을 반복적으로 밟아 나갑니다.</p>
<p>그러면 질문은 표면상으로는 전혀 쉬워지지 않은 어떤 것이 됩니다: 가중치를 가능한 한 빨리 줄이는 방향을 어떻게 찾을까요? 이를 파헤치기 위해, 먼저 가중치가 단 하나뿐인 경우인 단일 실수 값 $x$에 대한 $L(\mathbf{w}) = L(x)$ 사례를 살펴봅시다.</p>
<p>$x$를 취하여 이를 $x + \epsilon$으로 아주 조금 바꾸었을 때 어떤 일이 일어나는지 이해해 보려고 노력합시다. 구체적으로 생각하고 싶다면 $\epsilon = 0.0000001$과 같은 숫자를 떠올려 보십시오. 무슨 일이 일어나는지 시각화하는 데 도움을 주기 위해, $[0, 3]$ 구간에서 예제 함수 $f(x) = \sin(x^x)$를 그래프로 그려봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mxnet import np, npx
npx.set_np()

# 일반적인 범위에서 함수 플롯
x_big = np.arange(0.01, 3.01, 0.01)
ys = np.sin(x_big**x_big)
d2l.plot(x_big, ys, 'x', 'f(x)')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
import torch
torch.pi = torch.acos(torch.zeros(1)).item() * 2  # torch에서 pi 정의

# 일반적인 범위에서 함수 플롯
x_big = torch.arange(0.01, 3.01, 0.01)
ys = torch.sin(x_big**x_big)
d2l.plot(x_big, ys, 'x', 'f(x)')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
import tensorflow as tf
tf.pi = tf.acos(tf.zeros(1)).numpy() * 2  # TensorFlow에서 pi 정의

# 일반적인 범위에서 함수 플롯
x_big = tf.range(0.01, 3.01, 0.01)
ys = tf.sin(x_big**x_big)
d2l.plot(x_big, ys, 'x', 'f(x)')
</code></pre>
<p>이 큰 스케일에서 함수의 동작은 단순하지 않습니다. 그러나 범위를 $[1.75, 2.25]$와 같이 더 작게 줄여보면 그래프가 훨씬 더 단순해지는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 아주 작은 범위에서 동일한 함수 플롯
x_med = np.arange(1.75, 2.25, 0.001)
ys = np.sin(x_med**x_med)
d2l.plot(x_med, ys, 'x', 'f(x)')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 아주 작은 범위에서 동일한 함수 플롯
x_med = torch.arange(1.75, 2.25, 0.001)
ys = torch.sin(x_med**x_med)
d2l.plot(x_med, ys, 'x', 'f(x)')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 아주 작은 범위에서 동일한 함수 플롯
x_med = tf.range(1.75, 2.25, 0.001)
ys = tf.sin(x_med**x_med)
d2l.plot(x_med, ys, 'x', 'f(x)')
</code></pre>
<p>이를 극단으로 가져가서 아주 작은 세그먼트로 확대해 보면, 동작은 훨씬 더 단순해집니다: 그것은 그냥 직선입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 아주 작은 범위에서 동일한 함수 플롯
x_small = np.arange(2.0, 2.01, 0.0001)
ys = np.sin(x_small**x_small)
d2l.plot(x_small, ys, 'x', 'f(x)')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 아주 작은 범위에서 동일한 함수 플롯
x_small = torch.arange(2.0, 2.01, 0.0001)
ys = torch.sin(x_small**x_small)
d2l.plot(x_small, ys, 'x', 'f(x)')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 아주 작은 범위에서 동일한 함수 플롯
x_small = tf.range(2.0, 2.01, 0.0001)
ys = tf.sin(x_small**x_small)
d2l.plot(x_small, ys, 'x', 'f(x)')
</code></pre>
<p>이것이 일변수 미적분학의 핵심 관찰입니다: 친숙한 함수들의 동작은 충분히 작은 범위에서 직선으로 모델링될 수 있습니다. 이는 대부분의 함수에 대해, 함수의 $x$ 값을 아주 조금 옮길 때 출력 $f(x)$ 또한 아주 조금 옮겨질 것이라고 기대하는 것이 합리적임을 의미합니다. 우리가 답해야 할 유일한 질문은 "입력의 변화에 비해 출력의 변화가 얼마나 큰가? 절반만큼 큰가? 두 배만큼 큰가?" 하는 것입니다.</p>
<p>따라서 우리는 함수의 입력에서의 작은 변화에 대한 함수의 출력 변화의 비율을 고려할 수 있습니다. 이를 다음과 같이 공식적으로 쓸 수 있습니다.</p>
<p>$$
\frac{L(x+\epsilon) - L(x)}{(x+\epsilon) - x} = \frac{L(x+\epsilon) - L(x)}{\epsilon}.
$$</p>
<p>이것은 이미 코드에서 가지고 놀기 시작하기에 충분합니다. 예를 들어 $L(x) = x^{2} + 1701(x-4)^3$임을 안다고 가정합시다. 그러면 우리는 $x = 4$ 지점에서 이 값이 얼마나 큰지 다음과 같이 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
# 함수 정의
def L(x):
    return x**2 + 1701*(x-4)**3

# 여러 에필론(epsilon)에 대해 차이를 에필론으로 나눈 값 인쇄
for epsilon in [0.1, 0.001, 0.0001, 0.00001]:
    print(f'epsilon = {epsilon:.5f} -&gt; {(L(4+epsilon) - L(4)) / epsilon:.5f}')
</code></pre>
<p>이제 우리가 주의 깊게 본다면, 이 숫자의 출력이 의심스러울 정도로 $8$에 가깝다는 것을 알아차릴 것입니다. 실제로 $\epsilon$을 줄이면 값이 점진적으로 $8$에 더 가까워지는 것을 볼 수 있습니다. 따라서 우리는 우리가 구하는 값(입력의 변화가 출력을 변화시키는 정도)이 $x=4$ 지점에서 $8$이어야 한다고 올바르게 결론지을 수 있습니다. 수학자가 이 사실을 인코딩하는 방식은 다음과 같습니다.</p>
<p>$$
\lim_{\epsilon \rightarrow 0}\frac{L(4+\epsilon) - L(4)}{\epsilon} = 8.
$$</p>
<p>역사적인 여담으로서: 신경망 연구의 처음 몇십 년 동안, 과학자들은 작은 섭동 하에서 손실 함수가 어떻게 변하는지 평가하기 위해 이 알고리즘(<em>유한 차분법</em>)을 사용했습니다: 그냥 가중치를 바꾸고 손실이 어떻게 변하는지 보는 것이었습니다. 이는 계산적으로 비효율적이며, 하나의 변수의 단일 변화가 손실에 어떤 영향을 미치는지 보기 위해 손실 함수를 두 번 평가해야 합니다. 만약 우리가 단 몇 천 개의 파라미터로라도 이를 시도했다면, 전체 데이터셋에 대해 네트워크를 수천 번 평가해야 했을 것입니다! :citet:<code>Rumelhart.Hinton.Williams.ea.1988</code>에서 도입된 <em>역전파 알고리즘</em>이 데이터셋에 대한 네트워크의 단일 예측과 동일한 계산 시간 내에 가중치의 <em>임의의</em> 변화가 손실을 어떻게 변화시킬지 계산하는 방법을 제공한 1986년에 이르러서야 이 문제가 해결되었습니다.</p>
<p>우리 예제로 돌아가서, 이 값 $8$은 $x$의 값마다 다르므로 이를 $x$의 함수로 정의하는 것이 타당합니다. 더 공식적으로, 이 값 의존적인 변화율을 *도함수(derivative)*라고 부르며 다음과 같이 씁니다.</p>
<p>$$\frac{df}{dx}(x) = \lim_{\epsilon \rightarrow 0}\frac{f(x+\epsilon) - f(x)}{\epsilon}.$$
:eqlabel:<code>eq_der_def</code></p>
<p>교재마다 도함수에 대해 다른 표기법을 사용할 것입니다. 예를 들어 아래의 표기법들은 모두 동일한 것을 나타냅니다.</p>
<p>$$
\frac{df}{dx} = \frac{d}{dx}f = f' = \nabla_xf = D_xf = f_x.
$$</p>
<p>대부분의 저자들은 하나의 표기법을 골라 그것을 고수하겠지만, 그것조차 보장되지는 않습니다. 이러한 모든 것들에 익숙해지는 것이 가장 좋습니다. 우리는 이 텍스트 전체에서 복잡한 식의 도함수를 취하고 싶을 때를 제외하고는 $\frac{df}{dx}$ 표기법을 사용할 것이며, 그 경우에는 다음과 같은 식을 쓰기 위해 $\frac{d}{dx}f$를 사용할 것입니다.
$$
\frac{d}{dx}\left[x^4+\cos\left(\frac{x^2+1}{2x-1}\right)\right].
$$</p>
<p>종종 우리가 $x$를 아주 조금 바꿀 때 함수가 어떻게 변하는지 보기 위해 도함수의 정의 :eqref:<code>eq_der_def</code>를 다시 풀어보는 것이 직관적으로 유용합니다.</p>
<p>$$\begin{aligned} \frac{df}{dx}(x) = \lim_{\epsilon \rightarrow 0}\frac{f(x+\epsilon) - f(x)}{\epsilon} &amp; \implies \frac{df}{dx}(x) \approx \frac{f(x+\epsilon) - f(x)}{\epsilon} \ &amp; \implies \epsilon \frac{df}{dx}(x) \approx f(x+\epsilon) - f(x) \ &amp; \implies f(x+\epsilon) \approx f(x) + \epsilon \frac{df}{dx}(x).
\end{aligned}$$
:eqlabel:<code>eq_small_change</code></p>
<p>마지막 방정식은 명시적으로 불러낼 가치가 있습니다. 이는 당신이 임의의 함수를 취해 입력을 아주 작은 양만큼 바꾼다면, 출력은 도함수에 의해 스케일링된 그 작은 양만큼 바뀔 것임을 알려줍니다.</p>
<p>이런 식으로 우리는 도함수를 입력의 변화로부터 우리가 얻는 출력의 변화가 얼마나 큰지 알려주는 스케일링 인자로 이해할 수 있습니다.</p>
<h2 id="미적분-규칙-rules-of-calculus"><a class="header" href="#미적분-규칙-rules-of-calculus">미적분 규칙 (Rules of Calculus)</a></h2>
<p>:label:<code>sec_derivative_table</code></p>
<p>이제 우리는 명시적인 함수의 도함수를 계산하는 방법을 이해하는 과제로 넘어갑니다. 미적분학의 완전한 정식 처리는 제1원리로부터 모든 것을 유도할 것입니다. 우리는 여기서 이러한 유혹에 빠지지 않고, 흔히 마주치는 일반적인 규칙들에 대한 이해를 제공할 것입니다.</p>
<h3 id="일반적인-도함수-common-derivatives"><a class="header" href="#일반적인-도함수-common-derivatives">일반적인 도함수 (Common Derivatives)</a></h3>
<p>:numref:<code>sec_calculus</code>에서 보았듯이, 도함수를 계산할 때 종종 일련의 규칙을 사용하여 계산을 몇 가지 핵심 함수로 축소할 수 있습니다. 참조의 편의를 위해 여기에서 반복합니다.</p>
<ul>
<li><strong>상수 함수의 미분.</strong> $\frac{d}{dx}c = 0$.</li>
<li><strong>선형 함수의 미분.</strong> $\frac{d}{dx}(ax) = a$.</li>
<li><strong>거듭제곱 법칙.</strong> $\frac{d}{dx}x^n = nx^{n-1}$.</li>
<li><strong>지수 함수의 미분.</strong> $\frac{d}{dx}e^x = e^x$.</li>
<li><strong>로그 함수의 미분.</strong> $\frac{d}{dx}\log(x) = \frac{1}{x}$.</li>
</ul>
<h3 id="미분-규칙-derivative-rules"><a class="header" href="#미분-규칙-derivative-rules">미분 규칙 (Derivative Rules)</a></h3>
<p>모든 도함수를 별도로 계산하여 표에 저장해야 한다면 미분학은 거의 불가능할 것입니다. 위의 도함수들을 일반화하고 $f(x) = \log\left(1+(x-1)^{10}\right)$의 도함수를 찾는 것과 같은 더 복잡한 도함수를 계산할 수 있다는 것은 수학의 선물입니다. :numref:<code>sec_calculus</code>에서 언급했듯이, 그렇게 하기 위한 핵심은 우리가 함수들을 가져와 다양한 방식으로 결합할 때 어떤 일이 일어나는지를 성문화하는 것이며, 가장 중요한 것은 합, 곱, 그리고 합성입니다.</p>
<ul>
<li><strong>합의 법칙.</strong> $\frac{d}{dx}\left(g(x) + h(x)\right) = \frac{dg}{dx}(x) + \frac{dh}{dx}(x)$.</li>
<li><strong>곱의 미분법.</strong> $\frac{d}{dx}\left(g(x)\cdot h(x)\right) = g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)$.</li>
<li><strong>연쇄 법칙.</strong> $\frac{d}{dx}g(h(x)) = \frac{dg}{dh}(h(x))\cdot \frac{dh}{dx}(x)$.</li>
</ul>
<p>이러한 규칙들을 이해하기 위해 :eqref:<code>eq_small_change</code>를 어떻게 사용할 수 있는지 살펴봅시다. 합의 법칙의 경우 다음과 같은 추론 체인을 고려해 보십시오.</p>
<p>$$
\begin{aligned}
f(x+\epsilon) &amp; = g(x+\epsilon) + h(x+\epsilon) \ &amp; \approx g(x) + \epsilon \frac{dg}{dx}(x) + h(x) + \epsilon \frac{dh}{dx}(x) \ &amp; = g(x) + h(x) + \epsilon\left(\frac{dg}{dx}(x) + \frac{dh}{dx}(x)\right) \ &amp; = f(x) + \epsilon\left(\frac{dg}{dx}(x) + \frac{dh}{dx}(x)\right).
\end{aligned}
$$</p>
<p>이 결과를 $f(x+\epsilon) \approx f(x) + \epsilon \frac{df}{dx}(x)$라는 사실과 비교함으로써, 우리가 원하던 대로 $\frac{df}{dx}(x) = \frac{dg}{dx}(x) + \frac{dh}{dx}(x)$임을 알 수 있습니다. 여기서의 직관은 다음과 같습니다: 우리가 입력 $x$를 바꿀 때, $g$와 $h$는 출력의 변화에 $\frac{dg}{dx}(x)$와 $\frac{dh}{dx}(x)$만큼 공동으로 기여합니다.</p>
<p>곱의 미분법은 더 미묘하며, 이러한 식들로 작업하는 방법에 대한 새로운 관찰을 필요로 할 것입니다. 우리는 이전과 같이 :eqref:<code>eq_small_change</code>를 사용하여 시작하겠습니다.</p>
<p>$$
\begin{aligned}
f(x+\epsilon) &amp; = g(x+\epsilon)\cdot h(x+\epsilon) \ &amp; \approx \left(g(x) + \epsilon \frac{dg}{dx}(x)\right)\cdot\left(h(x) + \epsilon \frac{dh}{dx}(x)\right) \ &amp; = g(x)\cdot h(x) + \epsilon\left(g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)\right) + \epsilon^2\frac{dg}{dx}(x)\frac{dh}{dx}(x) \ &amp; = f(x) + \epsilon\left(g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)\right) + \epsilon^2\frac{dg}{dx}(x)\frac{dh}{dx}(x). \
\end{aligned}
$$</p>
<p>이것은 위에서 수행된 계산과 닮았으며, 실제로 우리는 $\epsilon$ 옆에 앉아 있는 우리의 답($\frac{df}{dx}(x) = g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x)$)을 봅니다. 하지만 크기 $\epsilon^{2}$인 그 항의 문제가 있습니다. 우리는 $\epsilon^2$의 차수가 $\epsilon^1$의 차수보다 높기 때문에 이를 *고차 항(higher-order term)*이라고 부를 것입니다. 우리는 나중 섹션에서 가끔 이것들을 추적하고 싶어 할 것임을 보게 되겠지만, 지금은 $\epsilon = 0.0000001$이라면 $\epsilon^{2}= 0.0000000000001$로 훨씬 더 작다는 점에 주목하십시오. 우리가 $\epsilon \rightarrow 0$으로 보냄에 따라 우리는 안전하게 고차 항들을 무시할 수 있습니다. 이 부록에서의 일반적인 관례로서, 우리는 "$\%$"를 두 항이 고차 항들까지 같음을 나타내는 데 사용할 것입니다. 하지만 좀 더 정식으로 하고 싶다면 차분 몫(difference quotient)을 조사하여</p>
<p>$$
\frac{f(x+\epsilon) - f(x)}{\epsilon} = g(x)\frac{dh}{dx}(x) + \frac{dg}{dx}(x)h(x) + \epsilon \frac{dg}{dx}(x)\frac{dh}{dx}(x),
$$</p>
<p>$\epsilon \rightarrow 0$으로 보냄에 따라 우변의 항도 0으로 가는 것을 볼 수 있습니다.</p>
<p>마지막으로 연쇄 법칙의 경우, 우리는 다시 이전과 같이 :eqref:<code>eq_small_change</code>를 사용하여 진행하여 다음을 볼 수 있습니다.</p>
<p>$$
\begin{aligned}
f(x+\epsilon) &amp; = g(h(x+\epsilon)) \ &amp; \approx g\left(h(x) + \epsilon \frac{dh}{dx}(x)\right) \ &amp; \approx g(h(x)) + \epsilon \frac{dh}{dx}(x) \frac{dg}{dh}(h(x))\ &amp; = f(x) + \epsilon \frac{dg}{dh}(h(x))\frac{dh}{dx}(x),
\end{aligned}
$$</p>
<p>여기서 두 번째 줄에서 우리는 함수 $g$가 아주 작은 양 $\epsilon \frac{dh}{dx}(x)$만큼 시프트된 입력($h(x)$)을 갖는 것으로 간주합니다.</p>
<p>이러한 규칙들은 본질적으로 원하는 어떤 식이라도 계산할 수 있는 유연한 도구 세트를 제공합니다. 예를 들어 다음과 같습니다.</p>
<p>$$
\begin{aligned}
\frac{d}{dx}\left[\log\left(1+(x-1)^{10}\right)\right] &amp; = \left(1+(x-1)^{10}\right)^{-1}\frac{d}{dx}\left[1+(x-1)^{10}\right]\ &amp; = \left(1+(x-1)^{10}\right)^{-1}\left(\frac{d}{dx}[1] + \frac{d}{dx}[(x-1)^{10}]\right) \ &amp; = \left(1+(x-1)^{10}\right)^{-1}\left(0 + 10(x-1)^9\frac{d}{dx}[x-1]\right) \ &amp; = 10\left(1+(x-1)^{10}\right)^{-1}(x-1)^9 \ &amp; = \frac{10(x-1)^9}{1+(x-1)^{10}}.
\end{aligned}
$$</p>
<p>여기서 각 줄은 다음 규칙들을 사용했습니다.</p>
<ol>
<li>연쇄 법칙과 로그의 도함수.</li>
<li>합의 법칙.</li>
<li>상수의 도함수, 연쇄 법칙, 그리고 거듭제곱 법칙.</li>
<li>합의 법칙, 선형 함수의 도함수, 상수의 도함수.</li>
</ol>
<p>이 예제를 하고 나서 두 가지가 분명해져야 합니다.</p>
<ol>
<li>합, 곱, 상수, 거듭제곱, 지수, 로그를 사용하여 우리가 쓸 수 있는 임의의 함수는 이러한 규칙들을 따름으로써 기계적으로 그 도함수가 계산될 수 있습니다.</li>
<li>인간이 이러한 규칙들을 따르는 것은 지루하고 오류가 발생하기 쉬울 수 있습니다!</li>
</ol>
<p>다행히도 이러한 두 가지 사실은 함께 앞으로 나아갈 길을 암시합니다: 이것은 기계화하기에 완벽한 후보입니다! 실제로 우리가 나중에 다시 살펴볼 역전파(backpropagation)가 바로 그것입니다.</p>
<h3 id="선형-근사-linear-approximation"><a class="header" href="#선형-근사-linear-approximation">선형 근사 (Linear Approximation)</a></h3>
<p>도함수로 작업할 때, 위에서 사용된 근사를 기하학적으로 해석하는 것이 종종 유용합니다. 특히 다음 방정식은</p>
<p>$$
f(x+\epsilon) \approx f(x) + \epsilon \frac{df}{dx}(x),
$$</p>
<p>$(x, f(x))$ 점을 지나고 기울기가 $\frac{df}{dx}(x)$인 직선으로 $f$의 값을 근사합니다. 이런 식으로 우리는 도함수가 아래 그림과 같이 함수 $f$에 대한 선형 근사를 제공한다고 말합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# sin 계산
xs = np.arange(-np.pi, np.pi, 0.01)
plots = [np.sin(xs)]

# 몇 가지 선형 근사 계산. d(sin(x)) / dx = cos(x) 사용
for x0 in [-1.5, 0, 2]:
    plots.append(np.sin(x0) + (xs - x0) * np.cos(x0))

d2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# sin 계산
xs = torch.arange(-torch.pi, torch.pi, 0.01)
plots = [torch.sin(xs)]

# 몇 가지 선형 근사 계산. d(sin(x))/dx = cos(x) 사용
for x0 in [-1.5, 0.0, 2.0]:
    plots.append(torch.sin(torch.tensor(x0)) + (xs - x0) *
                 torch.cos(torch.tensor(x0)))

d2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# sin 계산
xs = tf.range(-tf.pi, tf.pi, 0.01)
plots = [tf.sin(xs)]

# 몇 가지 선형 근사 계산. d(sin(x))/dx = cos(x) 사용
for x0 in [-1.5, 0.0, 2.0]:
    plots.append(tf.sin(tf.constant(x0)) + (xs - x0) *
                 tf.cos(tf.constant(x0)))

d2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])
</code></pre>
<h3 id="고계-도함수-higher-order-derivatives"><a class="header" href="#고계-도함수-higher-order-derivatives">고계 도함수 (Higher Order Derivatives)</a></h3>
<p>이제 겉보기에는 이상해 보일 수 있는 일을 해봅시다. 함수 $f$를 취해 도함수 $\frac{df}{dx}$를 계산합니다. 이는 우리에게 임의의 점에서의 $f$의 변화율을 제공합니다.</p>
<p>그러나 도함수 $\frac{df}{dx}$ 자체도 하나의 함수로 볼 수 있으므로, $\frac{df}{dx}$의 도함수를 다시 구하여 $\frac{d^2f}{dx^2} = \frac{d}{dx}\left(\frac{df}{dx}\right)$를 얻는 것을 막는 것은 아무것도 없습니다. 우리는 이것을 $f$의 2계 도함수(second derivative)라고 부를 것입니다. 이 함수는 $f$의 변화율의 변화율, 즉 변화율이 어떻게 변하고 있는지를 나타냅니다. 우리는 도함수를 몇 번이라도 적용하여 이른바 $n$계 도함수를 얻을 수 있습니다. 표기법을 깔끔하게 유지하기 위해 $n$계 도함수를 다음과 같이 나타낼 것입니다.</p>
<p>$$
f^{(n)}(x) = \frac{d^{n}f}{dx^{n}} = \left(\frac{d}{dx}\right)^{n} f.
$$</p>
<p>이것이 <em>왜</em> 유용한 개념인지 이해해 봅시다. 아래에서 $f^{(2)}(x)$, $f^{(1)}(x)$, 그리고 $f(x)$를 시각화합니다.</p>
<p>먼저, 2계 도함수 $f^{(2)}(x)$가 양의 상수인 경우를 고려해 보십시오. 이는 1계 도함수의 기울기가 양수임을 의미합니다. 결과적으로 1계 도함수 $f^{(1)}(x)$는 음수에서 시작하여 한 지점에서 0이 되고 마지막에 양수가 될 수 있습니다. 이는 우리 원래 함수 $f$의 기울기를 알려주며, 따라서 함수 $f$ 자체는 감소하다가 평평해졌다가 다시 증가합니다. 즉, 그림 :numref:<code>fig_positive-second</code>에 표시된 것처럼 함수 $f$는 위로 휘어져 있으며 단일 최소값을 갖습니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/posSecDer.svg" alt="우리가 2계 도함수가 양의 상수라고 가정한다면, 1계 도함수는 증가하고 있으며, 이는 함수 자체가 최소값을 가짐을 의미합니다." />
:label:<code>fig_positive-second</code></p>
<p>둘째, 만약 2계 도함수가 음의 상수라면, 이는 1계 도함수가 감소하고 있음을 의미합니다. 이는 1계 도함수가 양수에서 시작하여 한 지점에서 0이 되고 음수가 될 수 있음을 시사합니다. 따라서 함수 $f$ 자체는 증가하다가 평평해졌다가 감소합니다. 즉, 그림 :numref:<code>fig_negative-second</code>에 표시된 것처럼 함수 $f$는 아래로 휘어져 있으며 단일 최대값을 갖습니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/negSecDer.svg" alt="우리가 2계 도함수가 음의 상수라고 가정한다면, 1계 도함수는 감소하고 있으며, 이는 함수 자체가 최대값을 가짐을 의미합니다." />
:label:<code>fig_negative-second</code></p>
<p>셋째, 2계 도함수가 항상 0이라면, 1계 도함수는 결코 변하지 않을 것입니다 - 즉 상수입니다! 이는 $f$가 고정된 비율로 증가(또는 감소)함을 의미하며, $f$는 그림 :numref:<code>fig_zero-second</code>에 표시된 것처럼 그 자체가 직선입니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/zeroSecDer.svg" alt="우리가 2계 도함수가 0이라고 가정한다면, 1계 도함수는 상수이며, 이는 함수 자체가 직선임을 의미합니다." />
:label:<code>fig_zero-second</code></p>
<p>요약하자면, 2계 도함수는 함수 $f$가 휘어지는 방식을 설명하는 것으로 해석될 수 있습니다. 양의 2계 도함수는 위쪽 곡선으로 이어지는 반면, 음의 2계 도함수는 함수가 아래쪽으로 휘어짐을 의미하고, 0인 2계 도함수는 함수가 전혀 휘어지지 않음을 의미합니다.</p>
<p>이를 한 단계 더 진행해 봅시다. 함수 $g(x) = ax^{2}+ bx + c$를 고려해 보십시오. 그러면 다음과 같이 계산할 수 있습니다.</p>
<p>$$
\begin{aligned}
\frac{dg}{dx}(x) &amp; = 2ax + b \
\frac{d^2g}{dx^2}(x) &amp; = 2a.
\end{aligned}
$$</p>
<p>만약 우리가 어떤 원래 함수 $f(x)$를 염두에 두고 있다면, 우리는 처음 두 개의 도함수를 계산하고 이 계산과 일치하도록 $a, b, c$ 값을 찾을 수 있습니다. 1계 도함수가 직선으로 최상의 근사를 제공했던 이전 섹션과 유사하게, 이 구조는 이차식에 의한 최상의 근사를 제공합니다. $f(x) = \sin(x)$에 대해 이를 시각화해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# sin 계산
xs = np.arange(-np.pi, np.pi, 0.01)
plots = [np.sin(xs)]

# 몇 가지 이차 근사 계산. d(sin(x)) / dx = cos(x) 사용
for x0 in [-1.5, 0, 2]:
    plots.append(np.sin(x0) + (xs - x0) * np.cos(x0) -
                              (xs - x0)**2 * np.sin(x0) / 2)

d2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# sin 계산
xs = torch.arange(-torch.pi, torch.pi, 0.01)
plots = [torch.sin(xs)]

# 몇 가지 이차 근사 계산. d(sin(x)) / dx = cos(x) 사용
for x0 in [-1.5, 0.0, 2.0]:
    plots.append(torch.sin(torch.tensor(x0)) + (xs - x0) *
                 torch.cos(torch.tensor(x0)) - (xs - x0)**2 *
                 torch.sin(torch.tensor(x0)) / 2)

d2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# sin 계산
xs = tf.range(-tf.pi, tf.pi, 0.01)
plots = [tf.sin(xs)]

# 몇 가지 이차 근사 계산. d(sin(x)) / dx = cos(x) 사용
for x0 in [-1.5, 0.0, 2.0]:
    plots.append(tf.sin(tf.constant(x0)) + (xs - x0) *
                 tf.cos(tf.constant(x0)) - (xs - x0)**2 *
                 tf.sin(tf.constant(x0)) / 2)

d2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])
</code></pre>
<p>우리는 다음 섹션에서 이 아이디어를 <em>테일러 급수</em>의 개념으로 확장할 것입니다.</p>
<h3 id="테일러-급수-taylor-series"><a class="header" href="#테일러-급수-taylor-series">테일러 급수 (Taylor Series)</a></h3>
<p>*테일러 급수(Taylor series)*는 한 점 $x_0$에서의 처음 $n$개 도함수 값, 즉 $\left{ f(x_0), f^{(1)}(x_0), f^{(2)}(x_0), \ldots, f^{(n)}(x_0) \right}$이 주어졌을 때 함수 $f(x)$를 근사하는 방법을 제공합니다. 아이디어는 $x_0$에서 주어진 모든 도함수와 일치하는 $n$차 다항식을 찾는 것입니다.</p>
<p>우리는 이전 섹션에서 $n=2$인 경우를 보았고 약간의 대수는 이것이 다음과 같음을 보여줍니다.</p>
<p>$$
f(x) \approx \frac{1}{2}\frac{d^2f}{dx^2}(x_0)(x-x_0)^{2}+ \frac{df}{dx}(x_0)(x-x_0) + f(x_0).
$$</p>
<p>위에서 볼 수 있듯이, 분모의 $2$는 우리가 $x^2$의 두 도함수를 취할 때 얻는 $2$를 상쇄하기 위해 있는 반면, 다른 항들은 모두 0입니다. 동일한 로직이 1계 도함수와 값 자체에도 적용됩니다.</p>
<p>로직을 $n=3$까지 밀어붙인다면 다음과 같이 결론지을 것입니다.</p>
<p>$$
f(x) \approx \frac{\frac{d^3f}{dx^3}(x_0)}{6}(x-x_0)^3 + \frac{\frac{d^2f}{dx^2}(x_0)}{2}(x-x_0)^{2}+ \frac{df}{dx}(x_0)(x-x_0) + f(x_0).
$$</p>
<p>여기서 $6 = 3 \times 2 = 3!$은 $x^3$의 세 도함수를 취할 때 앞에 붙는 상수로부터 옵니다.</p>
<p>더 나아가, 우리는 다음과 같이 $n$차 다항식을 얻을 수 있습니다.</p>
<p>$$
P_n(x) = \sum_{i = 0}^{n} \frac{f^{(i)}(x_0)}{i!}(x-x_0)^{i}.
$$</p>
<p>여기서 표기법은 다음과 같습니다.</p>
<p>$$
f^{(n)}(x) = \frac{d^{n}f}{dx^{n}} = \left(\frac{d}{dx}\right)^{n} f.
$$</p>
<p>실제로 $P_n(x)$는 우리 함수 $f(x)$에 대한 최상의 $n$차 다항식 근사로 간주될 수 있습니다.</p>
<p>위 근사의 오차를 끝까지 파고들지는 않겠지만, 무한 극한에 대해 언급할 가치가 있습니다. 이 경우 $\cos(x)$나 $e^{x}$와 같이 잘 행동하는 함수(실해석적 함수로 알려짐)에 대해, 우리는 무한한 수의 항을 써서 정확히 동일한 함수를 근사할 수 있습니다.</p>
<p>$$
f(x) = \sum_{n = 0}^\infty \frac{f^{(n)}(x_0)}{n!}(x-x_0)^{n}.
$$</p>
<p>$f(x) = e^{x}$를 예로 들어봅시다. $e^{x}$는 자기 자신이 도함수이므로 $f^{(n)}(x) = e^{x}$임을 압니다. 따라서 $e^{x}$는 $x_0 = 0$에서 테일러 급수를 취함으로써 재구성될 수 있습니다. 즉 다음과 같습니다.</p>
<p>$$
e^{x} = \sum_{n = 0}^\infty \frac{x^{n}}{n!} = 1 + x + \frac{x^2}{2} + \frac{x^3}{6} + \cdots.
$$</p>
<p>이것이 코드에서 어떻게 작동하는지 살펴보고, 테일러 근사의 차수를 높이는 것이 어떻게 우리가 원하는 함수 $e^x$에 더 가까워지게 하는지 관찰해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 지수 함수 계산
xs = np.arange(0, 3, 0.01)
ys = np.exp(xs)

# 몇 가지 테일러 급수 근사 계산
P1 = 1 + xs
P2 = 1 + xs + xs**2 / 2
P5 = 1 + xs + xs**2 / 2 + xs**3 / 6 + xs**4 / 24 + xs**5 / 120

d2l.plot(xs, [ys, P1, P2, P5], 'x', 'f(x)', legend=[
    "Exponential", "Degree 1 Taylor Series", "Degree 2 Taylor Series",
    "Degree 5 Taylor Series"])
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 지수 함수 계산
xs = torch.arange(0, 3, 0.01)
ys = torch.exp(xs)

# 몇 가지 테일러 급수 근사 계산
P1 = 1 + xs
P2 = 1 + xs + xs**2 / 2
P5 = 1 + xs + xs**2 / 2 + xs**3 / 6 + xs**4 / 24 + xs**5 / 120

d2l.plot(xs, [ys, P1, P2, P5], 'x', 'f(x)', legend=[
    "Exponential", "Degree 1 Taylor Series", "Degree 2 Taylor Series",
    "Degree 5 Taylor Series"])
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 지수 함수 계산
xs = tf.range(0, 3, 0.01)
ys = tf.exp(xs)

# 몇 가지 테일러 급수 근사 계산
P1 = 1 + xs
P2 = 1 + xs + xs**2 / 2
P5 = 1 + xs + xs**2 / 2 + xs**3 / 6 + xs**4 / 24 + xs**5 / 120

d2l.plot(xs, [ys, P1, P2, P5], 'x', 'f(x)', legend=[
    "Exponential", "Degree 1 Taylor Series", "Degree 2 Taylor Series",
    "Degree 5 Taylor Series"])
</code></pre>
<p>테일러 급수에는 두 가지 주요 응용 분야가 있습니다.</p>
<ol>
<li>
<p><em>이론적 응용</em>: 우리가 너무 복잡한 함수를 이해하려고 할 때, 테일러 급수를 사용하면 우리가 직접 다룰 수 있는 다항식으로 변환할 수 있는 경우가 많습니다.</p>
</li>
<li>
<p><em>수치적 응용</em>: $e^{x}$나 $\cos(x)$와 같은 일부 함수는 기계가 계산하기 어렵습니다. 기계는 고정된 정밀도로 값의 테이블을 저장할 수 있고(종종 그렇게 합니다), 하지만 "$\cos(1)$의 1000번째 자리는 무엇인가?"와 같은 미결 질문이 여전히 남습니다. 테일러 급수는 종종 그러한 질문에 답하는 데 도움이 됩니다.</p>
</li>
</ol>
<h2 id="요약-summary-119"><a class="header" href="#요약-summary-119">요약 (Summary)</a></h2>
<ul>
<li>도함수는 우리가 입력을 아주 작은 양만큼 바꿀 때 함수가 어떻게 변하는지 표현하는 데 사용될 수 있습니다.</li>
<li>기초적인 도함수들은 미분 규칙을 사용하여 결합되어 임의로 복잡한 도함수를 생성할 수 있습니다.</li>
<li>도함수를 반복하여 2계 이상의 고계 도함수를 얻을 수 있습니다. 차수가 증가할 때마다 함수의 행동에 대해 더 세밀한 정보를 제공합니다.</li>
<li>단일 데이터 예제의 도함수에 있는 정보를 사용하여, 우리는 테일러 급수로부터 얻은 다항식으로 잘 행동하는 함수를 근사할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-134"><a class="header" href="#연습-문제-exercises-134">연습 문제 (Exercises)</a></h2>
<ol>
<li>$x^3-4x+1$의 도함수는 무엇입니까?</li>
<li>$\log(\frac{1}{x})$의 도함수는 무엇입니까?</li>
<li>맞음 또는 틀림: 만약 $f'(x) = 0$이라면 $f$는 $x$에서 최대값이나 최소값을 갖습니까?</li>
<li>$x\ge0$에 대해 $f(x) = x\log(x)$의 최소값은 어디입니까 (여기서 $f(0)$에서 $f$는 극한값 0을 취한다고 가정합시다)?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/412">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1088">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1089">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="다변수-미적분학-multivariable-calculus"><a class="header" href="#다변수-미적분학-multivariable-calculus">다변수 미적분학 (Multivariable Calculus)</a></h1>
<p>:label:<code>sec_multivariable_calculus</code></p>
<p>이제 단일 변수 함수의 도함수에 대해 꽤 강력한 이해를 얻었으므로, 잠재적으로 수십억 개의 가중치를 가진 손실 함수를 고려하고 있었던 원래 질문으로 돌아가 봅시다.</p>
<h2 id="고차원-미분-higher-dimensional-differentiation"><a class="header" href="#고차원-미분-higher-dimensional-differentiation">고차원 미분 (Higher-Dimensional Differentiation)</a></h2>
<p>:numref:<code>sec_single_variable_calculus</code>가 우리에게 알려주는 것은, 이 수십억 개의 가중치 중 단 하나를 바꾸고 다른 모든 것들은 고정해 둔다면 어떤 일이 일어날지 안다는 것입니다! 이것은 단일 변수 함수에 지나지 않으므로 다음과 같이 쓸 수 있습니다.</p>
<p>$$L(w_1+\epsilon_1, w_2, \ldots, w_N) \approx L(w_1, w_2, \ldots, w_N) + \epsilon_1 \frac{d}{dw_1} L(w_1, w_2, \ldots, w_N).$$
:eqlabel:<code>eq_part_der</code></p>
<p>다른 변수들을 고정하면서 한 변수에 대해 미분하는 것을 *편미분(partial derivative)*이라고 부르며, :eqref:<code>eq_part_der</code>에서의 도함수에 대해 $\frac{\partial}{\partial w_1}$ 표기법을 사용할 것입니다.</p>
<p>이제 이것을 가져와 $w_2$를 $w_2 + \epsilon_2$로 조금 바꿔봅시다.</p>
<p>$$
\begin{aligned}
L(w_1+\epsilon_1, w_2+\epsilon_2, \ldots, w_N) &amp; \approx L(w_1, w_2+\epsilon_2, \ldots, w_N) + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2+\epsilon_2, \ldots, w_N+\epsilon_N) \
&amp; \approx L(w_1, w_2, \ldots, w_N) \
&amp; \quad + \epsilon_2\frac{\partial}{\partial w_2} L(w_1, w_2, \ldots, w_N) \
&amp; \quad + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N) \
&amp; \quad + \epsilon_1\epsilon_2\frac{\partial}{\partial w_2}\frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N) \
&amp; \approx L(w_1, w_2, \ldots, w_N) \
&amp; \quad + \epsilon_2\frac{\partial}{\partial w_2} L(w_1, w_2, \ldots, w_N) \
&amp; \quad + \epsilon_1 \frac{\partial}{\partial w_1} L(w_1, w_2, \ldots, w_N).
\end{aligned}
$$</p>
<p>우리는 이전 섹션에서 $\epsilon^{2}$을 버릴 수 있었던 것과 같은 방식으로 $\epsilon_1\epsilon_2$가 버릴 수 있는 고차 항이라는 아이디어를 :eqref:<code>eq_part_der</code>에서 본 것과 함께 다시 사용했습니다. 이런 방식으로 계속하면 다음과 같이 쓸 수 있습니다.</p>
<p>$$
L(w_1+\epsilon_1, w_2+\epsilon_2, \ldots, w_N+\epsilon_N) \approx L(w_1, w_2, \ldots, w_N) + \sum_i \epsilon_i \frac{\partial}{\partial w_i} L(w_1, w_2, \ldots, w_N).
$$</p>
<p>이것이 엉망으로 보일 수 있지만, 우변의 합계가 정확히 내적처럼 보인다는 점에 주목하여 이를 더 친숙하게 만들 수 있습니다. 따라서 다음과 같이 둔다면</p>
<p>$$
\boldsymbol{\epsilon} = [\epsilon_1, \ldots, \epsilon_N]^\top ; \textrm{및} ;
\nabla_{\mathbf{x}} L = \left[\frac{\partial L}{\partial x_1}, \ldots, \frac{\partial L}{\partial x_N}\right]^\top,
$$</p>
<p>다음과 같습니다.</p>
<p>$$L(\mathbf{w} + \boldsymbol{\epsilon}) \approx L(\mathbf{w}) + \boldsymbol{\epsilon}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}).$$
:eqlabel:<code>eq_nabla_use</code></p>
<p>우리는 벡터 $\nabla_{\mathbf{w}} L$을 $L$의 *기울기(gradient)*라고 부를 것입니다.</p>
<p>방정식 :eqref:<code>eq_nabla_use</code>는 잠시 감상할 가치가 있습니다. 이것은 우리가 1차원에서 마주쳤던 것과 정확히 동일한 형식을 가지고 있으며, 단지 모든 것을 벡터와 내적으로 변환했을 뿐입니다. 이는 입력에 대한 임의의 섭동이 주어졌을 때 함수 $L$이 대략 어떻게 변할지 알려줍니다. 다음 섹션에서 보겠지만, 이는 우리가 기울기에 포함된 정보를 사용하여 어떻게 학습할 수 있는지를 기하학적으로 이해하는 데 중요한 도구를 제공할 것입니다.</p>
<p>하지만 먼저, 이 근사가 작동하는 것을 예제로 살펴봅시다. 다음과 같은 함수로 작업한다고 가정합시다.</p>
<p>$$
f(x, y) = \log(e^x + e^y) ; \textrm{이며 기울기는} ; \nabla f (x, y) = \left[\frac{e^x}{e^x+e^y}, \frac{e^y}{e^x+e^y}\right] ; \textrm{입니다.} $$</p>
<p>$(0, \log(2))$와 같은 점을 보면 다음을 알 수 있습니다.</p>
<p>$$
f(x, y) = \log(3) ; \textrm{이며 기울기는} ; \nabla f (x, y) = \left[\frac{1}{3}, \frac{2}{3}\right] ; \textrm{입니다.} $$</p>
<p>따라서 $(\epsilon_1, \log(2) + \epsilon_2)$에서 $f$를 근사하고 싶다면, :eqref:<code>eq_nabla_use</code>의 구체적인 사례를 가져야 함을 알 수 있습니다.</p>
<p>$$
f(\epsilon_1, \log(2) + \epsilon_2) \approx \log(3) + \frac{1}{3}\epsilon_1 + \frac{2}{3}\epsilon_2. $$</p>
<p>우리는 근사가 얼마나 좋은지 확인하기 위해 코드에서 이를 테스트할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mpl_toolkits import mplot3d
from mxnet import autograd, np, npx
npx.set_np()

def f(x, y):
    return np.log(np.exp(x) + np.exp(y))
def grad_f(x, y):
    return np.array([np.exp(x) / (np.exp(x) + np.exp(y)),
                     np.exp(y) / (np.exp(x) + np.exp(y))])

epsilon = np.array([0.01, -0.03])
grad_approx = f(0, np.log(2)) + epsilon.dot(grad_f(0, np.log(2)))
true_value = f(0 + epsilon[0], np.log(2) + epsilon[1])
f'근사값: {grad_approx}, 참값: {true_value}'
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
from mpl_toolkits import mplot3d
import torch
import numpy as np

def f(x, y):
    return torch.log(torch.exp(x) + torch.exp(y))
def grad_f(x, y):
    return torch.tensor([torch.exp(x) / (torch.exp(x) + torch.exp(y)),
                     torch.exp(y) / (torch.exp(x) + torch.exp(y))])

epsilon = torch.tensor([0.01, -0.03])
grad_approx = f(torch.tensor([0.]), torch.log(
    torch.tensor([2.]))) + epsilon.dot(
    grad_f(torch.tensor([0.]), torch.log(torch.tensor(2.))))
true_value = f(torch.tensor([0.]) + epsilon[0], torch.log(
    torch.tensor([2.])) + epsilon[1])
f'근사값: {grad_approx}, 참값: {true_value}'
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
from mpl_toolkits import mplot3d
import tensorflow as tf
import numpy as np

def f(x, y):
    return tf.math.log(tf.exp(x) + tf.exp(y))
def grad_f(x, y):
    return tf.constant([(tf.exp(x) / (tf.exp(x) + tf.exp(y))).numpy(),
                        (tf.exp(y) / (tf.exp(x) + tf.exp(y))).numpy()])

epsilon = tf.constant([0.01, -0.03])
grad_approx = f(tf.constant([0.]), tf.math.log(
    tf.constant([2.]))) + tf.tensordot(
    epsilon, grad_f(tf.constant([0.]), tf.math.log(tf.constant(2.))), axes=1)
true_value = f(tf.constant([0.]) + epsilon[0], tf.math.log(
    tf.constant([2.])) + epsilon[1])
f'근사값: {grad_approx}, 참값: {true_value}'
</code></pre>
<h2 id="기울기와-경사-하강법의-기하학-geometry-of-gradients-and-gradient-descent"><a class="header" href="#기울기와-경사-하강법의-기하학-geometry-of-gradients-and-gradient-descent">기울기와 경사 하강법의 기하학 (Geometry of Gradients and Gradient Descent)</a></h2>
<p>:eqref:<code>eq_nabla_use</code> 식을 다시 고려해 보십시오.</p>
<p>$$
L(\mathbf{w} + \boldsymbol{\epsilon}) \approx L(\mathbf{w}) + \boldsymbol{\epsilon}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}).
$$</p>
<p>내가 이것을 사용하여 우리의 손실 $L$을 최소화하는 데 도움을 주고 싶다고 가정합시다. :numref:<code>sec_autograd</code>에서 처음 설명한 경사 하강법 알고리즘을 기하학적으로 이해해 봅시다. 우리가 할 일은 다음과 같습니다.</p>
<ol>
<li>초기 파라미터 $\mathbf{w}$에 대해 무작위 선택으로 시작합니다.</li>
<li>$\mathbf{w}$에서 $L$을 가장 빠르게 감소시키는 방향 $\mathbf{v}$를 찾습니다.</li>
<li>해당 방향으로 작은 단계를 밟습니다: $\mathbf{w} \rightarrow \mathbf{w} + \epsilon\mathbf{v}$.</li>
<li>반복합니다.</li>
</ol>
<p>우리가 정확히 어떻게 해야 할지 모르는 유일한 일은 두 번째 단계에서 벡터 $\mathbf{v}$를 계산하는 것입니다. 우리는 그러한 방향을 *가장 가파른 하강 방향(direction of steepest descent)*이라고 부를 것입니다. :numref:<code>sec_geometry-linear-algebraic-ops</code>의 내적에 대한 기하학적 이해를 사용하여, 우리는 :eqref:<code>eq_nabla_use</code>를 다음과 같이 다시 쓸 수 있음을 알 수 있습니다.</p>
<p>$$
L(\mathbf{w} + \mathbf{v}) \approx L(\mathbf{w}) + \mathbf{v}\cdot \nabla_{\mathbf{w}} L(\mathbf{w}) = L(\mathbf{w}) + |\nabla_{\mathbf{w}} L(\mathbf{w})|\cos(\theta).
$$</p>
<p>편의상 우리의 방향이 길이 1을 갖도록 취했고, $\mathbf{v}$와 $\nabla_{\mathbf{w}} L(\mathbf{w})$ 사이의 각도에 대해 $\theta$를 사용했습니다. 만약 $L$을 가능한 한 빨리 감소시키는 방향을 찾고 싶다면, 우리는 이 식을 가능한 한 음수가 되게 만들고 싶습니다. 우리가 선택한 방향이 이 방정식에 들어가는 유일한 방법은 $\cos(\theta)$를 통해서이므로, 우리는 이 코사인을 가능한 한 음수가 되게 만들고 싶습니다. 이제 코사인의 모양을 상기하면, $\cos(\theta) = -1$이 되게 하거나 동등하게 기울기와 우리가 선택한 방향 사이의 각도를 $\pi$ 라디안, 즉 $180$도가 되게 함으로써 이를 가능한 한 음수가 되게 만들 수 있습니다. 이를 달성하는 유일한 방법은 정확히 반대 방향으로 향하는 것입니다: $\nabla_{\mathbf{w}} L(\mathbf{w})$와 정확히 반대 방향을 가리키도록 $\mathbf{v}$를 선택하십시오!</p>
<p>이것은 우리를 머신러닝에서 가장 중요한 수학적 개념 중 하나로 이끕니다: 가장 가파른 하강 방향은 $-\nabla_{\mathbf{w}}L(\mathbf{w})$ 방향을 가리킵니다. 따라서 우리의 비공식적인 알고리즘은 다음과 같이 다시 쓰일 수 있습니다.</p>
<ol>
<li>초기 파라미터 $\mathbf{w}$에 대해 무작위 선택으로 시작합니다.</li>
<li>$\nabla_{\mathbf{w}} L(\mathbf{w})$를 계산합니다.</li>
<li>해당 방향의 반대 방향으로 작은 단계를 밟습니다: $\mathbf{w} \leftarrow \mathbf{w} - \epsilon\nabla_{\mathbf{w}} L(\mathbf{w})$.</li>
<li>반복합니다.</li>
</ol>
<p>이 기본 알고리즘은 많은 연구자에 의해 여러 방식으로 수정되고 조정되었지만 핵심 개념은 그들 모두에서 동일하게 유지됩니다. 기울기를 사용하여 손실을 가능한 한 빨리 줄이는 방향을 찾고, 해당 방향으로 단계를 밟도록 파라미터를 업데이트하십시오.</p>
<h2 id="수학적-최적화에-관한-노트-a-note-on-mathematical-optimization"><a class="header" href="#수학적-최적화에-관한-노트-a-note-on-mathematical-optimization">수학적 최적화에 관한 노트 (A Note on Mathematical Optimization)</a></h2>
<p>이 책 전체에서 우리는 딥러닝 설정에서 마주치는 모든 함수가 명시적으로 최소화하기에 너무 복잡하다는 실질적인 이유 때문에 수치적 최적화 기술에 정면으로 집중합니다.</p>
<p>하지만 우리가 위에서 얻은 기하학적 이해가 함수를 직접 최적화하는 것에 대해 무엇을 말해주는지 고려하는 것은 유용한 연습입니다.</p>
<p>어떤 함수 $L(\mathbf{x})$를 최소화하는 $\mathbf{x}_0$ 값을 찾고 싶다고 가정합시다. 더욱이 누군가가 우리에게 값을 주면서 그것이 $L$을 최소화하는 값이라고 말한다고 가정합시다. 그들의 답이 그럴듯한지 확인하기 위해 우리가 확인할 수 있는 것이 있을까요?</p>
<p>다시 :eqref:<code>eq_nabla_use</code>를 고려하십시오.
$$
L(\mathbf{x}_0 + \boldsymbol{\epsilon}) \approx L(\mathbf{x}<em>0) + \boldsymbol{\epsilon}\cdot \nabla</em>{\mathbf{x}} L(\mathbf{x}_0).
$$</p>
<p>기울기가 0이 아니라면, 우리는 더 작은 $L$ 값을 찾기 위해 $-\epsilon \nabla_{\mathbf{x}} L(\mathbf{x}_0)$ 방향으로 단계를 밟을 수 있음을 압니다. 따라서 우리가 진정으로 최소값에 있다면, 이것은 불가능해야 합니다! 우리는 $\mathbf{x}<em>0$가 최소값이라면 $\nabla</em>{\mathbf{x}} L(\mathbf{x}<em>0) = 0$이라고 결론지을 수 있습니다. 우리는 $\nabla</em>{\mathbf{x}} L(\mathbf{x}_0) = 0$인 점들을 *임계점(critical points)*이라고 부릅니다.</p>
<p>이것은 좋은데, 왜냐하면 드문 설정에서 우리는 기울기가 0인 모든 점을 명시적으로 찾고 가장 작은 값을 가진 점을 찾을 수 있기 때문입니다.</p>
<p>구체적인 예로 다음 함수를 고려해 보십시오.
$$
f(x) = 3x^4 - 4x^3 -12x^2.
$$</p>
<p>이 함수는 다음과 같은 도함수를 갖습니다.
$$
\frac{df}{dx} = 12x^3 - 12x^2 -24x = 12x(x-2)(x+1).
$$</p>
<p>최소값의 가능한 위치는 $x = -1, 0, 2$뿐이며, 여기서 함수는 각각 $-5, 0, -32$ 값을 취하므로, 우리는 $x = 2$일 때 함수를 최소화한다고 결론지을 수 있습니다. 빠른 플롯이 이를 확인해 줍니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.arange(-2, 3, 0.01)
f = (3 * x**4) - (4 * x**3) - (12 * x**2)

d2l.plot(x, f, 'x', 'f(x)')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.arange(-2, 3, 0.01)
f = (3 * x**4) - (4 * x**3) - (12 * x**2)

d2l.plot(x, f, 'x', 'f(x)')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.range(-2, 3, 0.01)
f = (3 * x**4) - (4 * x**3) - (12 * x**2)

d2l.plot(x, f, 'x', 'f(x)')
</code></pre>
<p>이는 이론적으로나 수치적으로 작업할 때 알아야 할 중요한 사실을 강조합니다: 함수를 최소화(또는 최대화)할 수 있는 유일하게 가능한 점들은 기울기가 0인 점들이지만, 기울기가 0인 모든 점이 진정한 <em>전역</em> 최소값(또는 최대값)은 아닙니다.</p>
<h2 id="다변수-연쇄-법칙-multivariate-chain-rule"><a class="header" href="#다변수-연쇄-법칙-multivariate-chain-rule">다변수 연쇄 법칙 (Multivariate Chain Rule)</a></h2>
<p>여러 항을 합성하여 만들 수 있는 네 가지 변수($w, x, y, z$)의 함수가 있다고 가정해 봅시다.</p>
<p>$$\begin{aligned}f(u, v) &amp; = (u+v)^{2} \u(a, b) &amp; = (a+b)^{2}, \qquad v(a, b) = (a-b)^{2}, \a(w, x, y, z) &amp; = (w+x+y+z)^{2}, \qquad b(w, x, y, z) = (w+x-y-z)^2.\end{aligned}$$
:eqlabel:<code>eq_multi_func_def</code></p>
<p>그러한 방정식 체인은 신경망으로 작업할 때 흔하므로, 그러한 함수의 기울기를 계산하는 방법을 이해하는 것이 핵심입니다. 어떤 변수들이 서로 직접적으로 관련되어 있는지 살펴본다면 그림 :numref:<code>fig_chain-1</code>에서 이러한 연결의 시각적 힌트를 보기 시작할 수 있습니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/chain-net1.svg" alt="위의 함수 관계. 노드는 값을 나타내고 엣지는 함수적 의존성을 보여줍니다." />
:label:<code>fig_chain-1</code></p>
<p>:eqref:<code>eq_multi_func_def</code>의 모든 것을 합성하여 다음과 같이 쓰는 것을 막는 것은 아무것도 없습니다.</p>
<p>$$
f(w, x, y, z) = \left(\left((w+x+y+z)^2+(w+x-y-z)^2\right)^2+\left((w+x+y+z)^2-(w+x-y-z)^2\right)^2\right)^2. $$</p>
<p>그런 다음 단일 변수 도함수만 사용하여 도함수를 취할 수도 있겠지만, 그렇게 한다면 금방 수많은 항에 휩싸이게 될 것이고, 그중 다수는 반복될 것입니다! 실제로 예를 들어 다음을 알 수 있습니다.</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial w} &amp; = 2 \left(2 \left(2 (w + x + y + z) - 2 (w + x - y - z)\right) \left((w + x + y + z)^{2}- (w + x - y - z)^{2}\right) + \
&amp; \left. \quad 2 \left(2 (w + x - y - z) + 2 (w + x + y + z)\right) \left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\right)\right) \times \
&amp; \quad \left(\left((w + x + y + z)^{2}- (w + x - y - z)^2\right)^{2}+ \left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\right)^{2}\right).
\end{aligned}
$$</p>
<p>우리가 $\frac{\partial f}{\partial x}$도 계산하고 싶다면, 다시 많은 반복되는 항과 두 도함수 사이의 많은 <em>공유되는</em> 반복 항이 있는 유사한 방정식을 얻게 될 것입니다. 이는 엄청난 양의 낭비되는 작업을 나타내며, 만약 우리가 이런 방식으로 도함수를 계산해야 했다면 전체 딥러닝 혁명은 시작되기도 전에 멈췄을 것입니다!</p>
<p>문제를 쪼개 봅시다. 우리는 본질적으로 $w, x, y, z$가 모두 존재하지 않는다고 가정하고 $a$를 바꿀 때 $f$가 어떻게 변하는지 이해하는 것부터 시작할 것입니다. 우리는 처음으로 기울기로 작업했을 때와 같이 추론할 것입니다. $a$를 취하고 여기에 작은 양 $\epsilon$을 더해 봅시다.</p>
<p>$$
\begin{aligned}
&amp; f(u(a+\epsilon, b), v(a+\epsilon, b)) \
\approx &amp; f\left(u(a, b) + \epsilon\frac{\partial u}{\partial a}(a, b), v(a, b) + \epsilon\frac{\partial v}{\partial a}(a, b)\right) \
\approx &amp; f(u(a, b), v(a, b)) + \epsilon\left[\frac{\partial f}{\partial u}(u(a, b), v(a, b))\frac{\partial u}{\partial a}(a, b) + \frac{\partial f}{\partial v}(u(a, b), v(a, b))\frac{\partial v}{\partial a}(a, b)\right].
\end{aligned}
$$</p>
<p>첫 번째 줄은 편미분의 정의로부터 따르고, 두 번째 줄은 기울기의 정의로부터 따릅니다. 식 $\frac{\partial f}{\partial u}(u(a, b), v(a, b))$에서처럼 우리가 모든 도함수를 어디서 평가하는지 정확하게 추적하는 것은 표기법상 부담스러우므로, 우리는 종종 이를 훨씬 더 기억하기 쉬운 다음과 같은 형태로 줄여 씁니다.</p>
<p>$$
\frac{\partial f}{\partial a} = \frac{\partial f}{\partial u}\frac{\partial u}{\partial a}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial a}. $$</p>
<p>과정의 의미에 대해 생각해보는 것이 유용합니다. 우리는 $a$의 변화에 따라 $f(u(a, b), v(a, b))$ 형태의 함수가 그 값을 어떻게 바꾸는지 이해하려고 노력하고 있습니다. 이것이 일어날 수 있는 두 가지 경로가 있습니다: $a \rightarrow u \rightarrow f$인 경로와 $a \rightarrow v \rightarrow f$인 경로입니다. 우리는 연쇄 법칙을 통해 이러한 두 기여를 각각 $\frac{\partial w}{\partial u} \cdot \frac{\partial u}{\partial x}$와 $\frac{\partial w}{\partial v} \cdot \frac{\partial v}{\partial x}$로 계산하고 합산할 수 있습니다.</p>
<p>그림 :numref:<code>fig_chain-2</code>에 표시된 것처럼 오른쪽의 함수가 왼쪽에서 연결된 함수들에 의존하는 다른 함수 네트워크가 있다고 상상해 보십시오.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/chain-net2.svg" alt="연쇄 법칙의 또 다른 더 미묘한 예." />
:label:<code>fig_chain-2</code></p>
<p>$rac{\partial f}{\partial y}$와 같은 것을 계산하려면, $y$에서 $f$로 가는 모든(이 경우 3개) 경로에 대해 합산해야 하며 다음을 얻습니다.</p>
<p>$$
\frac{\partial f}{\partial y} = \frac{\partial f}{\partial a} \frac{\partial a}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial b} \frac{\partial b}{\partial v} \frac{\partial v}{\partial y}. $$</p>
<p>이런 방식으로 연쇄 법칙을 이해하면 기울기가 네트워크를 통해 어떻게 흐르는지, 그리고 LSTM(:numref:<code>sec_lstm</code>)이나 잔차 레이어(:numref:<code>sec_resnet</code>)의 여러 아키텍처 선택이 기울기 흐름을 제어함으로써 학습 과정을 형성하는 데 어떻게 도움이 되는지 이해할 때 큰 보상을 얻을 수 있을 것입니다.</p>
<h2 id="역전파-알고리즘-the-backpropagation-algorithm"><a class="header" href="#역전파-알고리즘-the-backpropagation-algorithm">역전파 알고리즘 (The Backpropagation Algorithm)</a></h2>
<p>이전 섹션의 :eqref:<code>eq_multi_func_def</code> 예제로 돌아가 봅시다.</p>
<p>$$
\begin{aligned}
f(u, v) &amp; = (u+v)^{2} \u(a, b) &amp; = (a+b)^{2}, \qquad v(a, b) = (a-b)^{2}, \a(w, x, y, z) &amp; = (w+x+y+z)^{2}, \qquad b(w, x, y, z) = (w+x-y-z)^2.
\end{aligned}
$$</p>
<p>가령 $\frac{\partial f}{\partial w}$를 계산하고 싶다면 다변수 연쇄 법칙을 적용하여 다음을 볼 수 있습니다.</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial w} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial w} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial w}, \
\frac{\partial u}{\partial w} &amp; = \frac{\partial u}{\partial a}\frac{\partial a}{\partial w}+\frac{\partial u}{\partial b}\frac{\partial b}{\partial w}, \
\frac{\partial v}{\partial w} &amp; = \frac{\partial v}{\partial a}\frac{\partial a}{\partial w}+\frac{\partial v}{\partial b}\frac{\partial b}{\partial w}.
\end{aligned}
$$</p>
<p>이 분해를 사용하여 $\frac{\partial f}{\partial w}$를 계산해 봅시다. 여기서 우리에게 필요한 것은 다양한 단일 단계 부분들뿐입니다.</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial u} = 2(u+v), &amp; \quad\frac{\partial f}{\partial v} = 2(u+v), \
\frac{\partial u}{\partial a} = 2(a+b), &amp; \quad\frac{\partial u}{\partial b} = 2(a+b), \
\frac{\partial v}{\partial a} = 2(a-b), &amp; \quad\frac{\partial v}{\partial b} = -2(a-b), \
\frac{\partial a}{\partial w} = 2(w+x+y+z), &amp; \quad\frac{\partial b}{\partial w} = 2(w+x-y-z).
\end{aligned}
$$</p>
<p>이것을 코드로 작성하면 상당히 관리하기 쉬운 식이 됩니다.</p>
<pre><code class="language-{.python .input}">#@tab all
# 입력에서 출력으로 함수 값 계산
w, x, y, z = -1, 0, -2, 1
a, b = (w + x + y + z)**2, (w + x - y - z)**2
u, v = (a + b)**2, (a - b)**2
f = (u + v)**2
print(f'    {w}, {x}, {y}, {z} 에서 f는 {f} 입니다')

# 단일 단계 부분 계산
df_du, df_dv = 2*(u + v), 2*(u + v)
du_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)
da_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)

# 입력에서 출력으로 최종 결과 계산
du_dw, dv_dw = du_da*da_dw + du_db*db_dw, dv_da*da_dw + dv_db*db_dw
df_dw = df_du*du_dw + df_dv*dv_dw
print(f'    {w}, {x}, {y}, {z} 에서 df/dw는 {df_dw} 입니다')
</code></pre>
<p>하지만 이 방식이 여전히 $\frac{\partial f}{\partial x}$와 같은 것을 계산하기 쉽게 만들지는 않는다는 점에 유의하십시오. 그 이유는 우리가 연쇄 법칙을 적용하기로 선택한 <em>방식</em> 때문입니다. 위에서 우리가 한 일을 보면, 우리는 가능한 한 분모에 $\partial w$를 유지했습니다. 이런 식으로 우리는 $w$가 다른 모든 변수를 어떻게 바꾸는지 보면서 연쇄 법칙을 적용하기로 선택했습니다. 만약 그것이 우리가 원한 것이라면 이것은 좋은 아이디어였을 것입니다. 하지만 딥러닝에서의 동기를 다시 생각해 보십시오: 우리는 모든 파라미터가 <em>손실</em>을 어떻게 바꾸는지 보고 싶어 합니다. 본질적으로 우리는 가능할 때마다 분자에 $\partial f$를 유지하면서 연쇄 법칙을 적용하고 싶습니다!</p>
<p>더 명시적으로 말하자면, 우리는 다음과 같이 쓸 수 있음에 유의하십시오.</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial w} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial w} + \frac{\partial f}{\partial b}\frac{\partial b}{\partial w}, \
\frac{\partial f}{\partial a} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial a}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial a}, \
\frac{\partial f}{\partial b} &amp; = \frac{\partial f}{\partial u}\frac{\partial u}{\partial b}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial b}.
\end{aligned}
$$</p>
<p>연쇄 법칙의 이 적용은 우리가 $\frac{\partial f}{\partial u}, \frac{\partial f}{\partial v}, \frac{\partial f}{\partial a}, \frac{\partial f}{\partial b}, ; \textrm{및} ; \frac{\partial f}{\partial w}$를 명시적으로 계산하게 합니다. 다음과 같은 방정식을 포함하는 것을 막는 것은 아무것도 없습니다.</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial x} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial x} + \frac{\partial f}{\partial b}\frac{\partial b}{\partial x}, \
\frac{\partial f}{\partial y} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial y}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial y}, \
\frac{\partial f}{\partial z} &amp; = \frac{\partial f}{\partial a}\frac{\partial a}{\partial z}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial z}.
\end{aligned}
$$</p>
<p>그리고 나서 전체 네트워크의 <em>임의의</em> 노드를 바꿀 때 $f$가 어떻게 변하는지 추적합니다. 이를 구현해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
# 입력에서 출력으로 함수 값 계산
w, x, y, z = -1, 0, -2, 1
a, b = (w + x + y + z)**2, (w + x - y - z)**2
u, v = (a + b)**2, (a - b)**2
f = (u + v)**2
print(f'{w}, {x}, {y}, {z} 에서 f는 {f} 입니다')

# 위의 분해를 사용하여 도함수 계산
# 먼저 단일 단계 부분 계산
df_du, df_dv = 2*(u + v), 2*(u + v)
du_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)
da_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)
da_dx, db_dx = 2*(w + x + y + z), 2*(w + x - y - z)
da_dy, db_dy = 2*(w + x + y + z), -2*(w + x - y - z)
da_dz, db_dz = 2*(w + x + y + z), -2*(w + x - y - z)

# 이제 출력에서 입력으로 임의의 값을 바꿀 때 f가 어떻게 변하는지 계산
df_da, df_db = df_du*du_da + df_dv*dv_da, df_du*du_db + df_dv*dv_db
df_dw, df_dx = df_da*da_dw + df_db*db_dw, df_da*da_dx + df_db*db_dx
df_dy, df_dz = df_da*da_dy + df_db*db_dy, df_da*da_dz + df_db*db_dz

print(f'{w}, {x}, {y}, {z} 에서 df/dw는 {df_dw} 입니다')
print(f'{w}, {x}, {y}, {z} 에서 df/dx는 {df_dx} 입니다')
print(f'{w}, {x}, {y}, {z} 에서 df/dy는 {df_dy} 입니다')
print(f'{w}, {x}, {y}, {z} 에서 df/dz는 {df_dz} 입니다')
</code></pre>
<p>우리가 입력에서 출력으로 정방향으로 도함수를 계산하는 대신(위의 첫 번째 코드 스니펫에서 했던 것처럼), $f$에서 입력 방향으로 역방향으로 도함수를 계산한다는 사실이 이 알고리즘에 그 이름을 부여합니다: <em>역전파(backpropagation)</em>. 여기에는 두 단계가 있음에 유의하십시오.</p>
<ol>
<li>함수의 값과 단일 단계 부분들을 앞에서 뒤로 계산합니다. 위에서 하지는 않았지만, 이는 단일 *정방향 패스(forward pass)*로 결합될 수 있습니다.</li>
<li>뒤에서 앞으로 $f$의 기울기를 계산합니다. 우리는 이를 *역방향 패스(backwards pass)*라고 부릅니다.</li>
</ol>
<p>이것이 바로 모든 딥러닝 알고리즘이 한 번의 패스로 네트워크의 모든 가중치에 대한 손실의 기울기 계산을 가능하게 하기 위해 구현하는 것입니다. 우리가 그러한 분해를 가졌다는 것은 놀라운 사실입니다.</p>
<p>이것이 어떻게 캡슐화되는지 보기 위해, 이 예제를 빠르게 살펴봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# ndarray로 초기화한 다음 기울기 첨부
w, x, y, z = np.array(-1), np.array(0), np.array(-2), np.array(1)

w.attach_grad()
x.attach_grad()
y.attach_grad()
z.attach_grad()

# 기울기를 추적하며 평소와 같이 계산 수행
with autograd.record():
    a, b = (w + x + y + z)**2, (w + x - y - z)**2
    u, v = (a + b)**2, (a - b)**2
    f = (u + v)**2

# 역방향 패스 실행
f.backward()

print(f'{w}, {x}, {y}, {z} 에서 df/dw는 {w.grad} 입니다')
print(f'{w}, {x}, {y}, {z} 에서 df/dx는 {x.grad} 입니다')
print(f'{w}, {x}, {y}, {z} 에서 df/dy는 {y.grad} 입니다')
print(f'{w}, {x}, {y}, {z} 에서 df/dz는 {z.grad} 입니다')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# ndarray로 초기화한 다음 기울기 첨부
w = torch.tensor([-1.], requires_grad=True)
x = torch.tensor([0.], requires_grad=True)
y = torch.tensor([-2.], requires_grad=True)
z = torch.tensor([1.], requires_grad=True)
# 기울기를 추적하며 평소와 같이 계산 수행
a, b = (w + x + y + z)**2, (w + x - y - z)**2
u, v = (a + b)**2, (a - b)**2
f = (u + v)**2

# 역방향 패스 실행
f.backward()

print(f'{w.data.item()}, {x.data.item()}, {y.data.item()}, '
      f'{z.data.item()} 에서 df/dw는 {w.grad.data.item()} 입니다')
print(f'{w.data.item()}, {x.data.item()}, {y.data.item()}, '
      f'{z.data.item()} 에서 df/dx는 {x.grad.data.item()} 입니다')
print(f'{w.data.item()}, {x.data.item()}, {y.data.item()}, '
      f'{z.data.item()} 에서 df/dy는 {y.grad.data.item()} 입니다')
print(f'{w.data.item()}, {x.data.item()}, {y.data.item()}, '
      f'{z.data.item()} 에서 df/dz는 {z.grad.data.item()} 입니다')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# ndarray로 초기화한 다음 기울기 첨부
w = tf.Variable(tf.constant([-1.]))
x = tf.Variable(tf.constant([0.]))
y = tf.Variable(tf.constant([-2.]))
z = tf.Variable(tf.constant([1.]))
# 기울기를 추적하며 평소와 같이 계산 수행
with tf.GradientTape(persistent=True) as t:
    a, b = (w + x + y + z)**2, (w + x - y - z)**2
    u, v = (a + b)**2, (a - b)**2
    f = (u + v)**2

# 역방향 패스 실행
w_grad = t.gradient(f, w).numpy()
x_grad = t.gradient(f, x).numpy()
y_grad = t.gradient(f, y).numpy()
z_grad = t.gradient(f, z).numpy()

print(f'{w.numpy()}, {x.numpy()}, {y.numpy()}, '
      f'{z.numpy()} 에서 df/dw는 {w_grad} 입니다')
print(f'{w.numpy()}, {x.numpy()}, {y.numpy()}, '
      f'{z.numpy()} 에서 df/dx는 {x_grad} 입니다')
print(f'{w.numpy()}, {x.numpy()}, {y.numpy()}, '
      f'{z.numpy()} 에서 df/dy는 {y_grad} 입니다')
print(f'{w.numpy()}, {x.numpy()}, {y.numpy()}, '
      f'{z.numpy()} 에서 df/dz는 {z_grad} 입니다')
</code></pre>
<p>우리가 위에서 한 모든 것은 <code>f.backwards()</code>를 호출함으로써 자동으로 수행될 수 있습니다.</p>
<h2 id="헤시안-hessians"><a class="header" href="#헤시안-hessians">헤시안 (Hessians)</a></h2>
<p>단일 변수 미적분학에서와 마찬가지로, 함수에 대한 더 나은 근사를 얻는 방법을 파악하기 위해 고계 도함수를 고려하는 것이 유용합니다. 이는 기울기만 사용하는 것보다 더 나은 근사를 제공합니다.</p>
<p>여러 변수의 함수의 고계 도함수로 작업할 때 즉시 마주치는 한 가지 문제는 그 수가 매우 많다는 것입니다. $n$개 변수의 함수 $f(x_1, \ldots, x_n)$가 있다면, 우리는 $n^{2}$개의 많은 2계 도함수를 취할 수 있습니다. 즉, $i$와 $j$의 임의의 선택에 대해 다음과 같습니다.</p>
<p>$$
\frac{d^2f}{dx_idx_j} = \frac{d}{dx_i}\left(\frac{d}{dx_j}f\right).
$$</p>
<p>이들은 전통적으로 *헤시안(Hessian)*이라고 불리는 행렬로 조립됩니다.</p>
<p>$$\mathbf{H}_f = \begin{bmatrix} \frac{d^2f}{dx_1dx_1} &amp; \cdots &amp; \frac{d^2f}{dx_1dx_n} \ \vdots &amp; \ddots &amp; \vdots \ \frac{d^2f}{dx_ndx_1} &amp; \cdots &amp; \frac{d^2f}{dx_ndx_n} \ \end{bmatrix}.$$
:eqlabel:<code>eq_hess_def</code></p>
<p>이 행렬의 모든 항목이 독립적인 것은 아닙니다. 실제로 양쪽의 *혼합 부분 도함수(mixed partials)*가 존재하고 연속적인 한, 임의의 $i$와 $j$에 대해 다음과 같이 말할 수 있습니다.</p>
<p>$$
\frac{d^2f}{dx_idx_j} = \frac{d^2f}{dx_jdx_i}.
$$</p>
<p>이는 먼저 $x_i$ 방향으로 함수를 섭동시킨 다음 $x_j$로 섭동시키는 것과, 먼저 $x_j$로 섭동시킨 다음 $x_i$로 섭동시키는 것 두 순서 모두 $f$ 출력의 동일한 최종 변화로 이어진다는 지식을 가지고 비교함으로써 따릅니다.</p>
<p>단일 변수에서와 마찬가지로, 우리는 이러한 도함수를 사용하여 함수가 점 근처에서 어떻게 행동하는지에 대해 훨씬 더 나은 아이디어를 얻을 수 있습니다. 특히, 단일 변수에서 보았던 것처럼 점 $\mathbf{x}_0$ 근처에서 가장 잘 맞는 이차식을 찾는 데 이를 사용할 수 있습니다.</p>
<p>예제를 살펴봅시다. $f(x_1, x_2) = a + b_1x_1 + b_2x_2 + c_{11}x_1^{2} + c_{12}x_1x_2 + c_{22}x_2^{2}$라고 가정합시다. 이것은 두 변수의 이차식에 대한 일반적인 형태입니다. 제로(zero) 지점에서의 함수의 값, 기울기, 그리고 헤시안 :eqref:<code>eq_hess_def</code>을 본다면 다음과 같습니다.</p>
<p>$$
\begin{aligned}
f(0,0) &amp; = a, \
\nabla f (0,0) &amp; = \begin{bmatrix}b_1 \ b_2\end{bmatrix}, \
\mathbf{H} f (0,0) &amp; = \begin{bmatrix}2 c_{11} &amp; c_{12} \ c_{12} &amp; 2c_{22}\end{bmatrix},
\end{aligned}
$$</p>
<p>우리는 다음과 같이 말함으로써 원래의 다항식을 다시 얻을 수 있습니다.</p>
<p>$$
f(\mathbf{x}) = f(0) + \nabla f (0) \cdot \mathbf{x} + \frac{1}{2}\mathbf{x}^\top \mathbf{H} f (0) \mathbf{x}. $$</p>
<p>일반적으로 임의의 점 $\mathbf{x}_0$에서 이 전개를 계산한다면 다음을 알 수 있습니다.</p>
<p>$$
f(\mathbf{x}) = f(\mathbf{x}_0) + \nabla f (\mathbf{x}_0) \cdot (\mathbf{x}-\mathbf{x}_0) + \frac{1}{2}(\mathbf{x}-\mathbf{x}_0)^\top \mathbf{H} f (\mathbf{x}_0) (\mathbf{x}-\mathbf{x}_0).
$$</p>
<p>이는 임의의 차원 입력에 대해 작동하며, 임의의 함수에 대해 특정 점에서의 최상의 근사 이차식을 제공합니다. 예를 들어 다음 함수를 플롯해 봅시다.</p>
<p>$$
f(x, y) = xe^{-x^2-y^2}.
$$</p>
<p>기울기와 헤시안이 다음과 같음을 계산할 수 있습니다.
$$
\nabla f(x, y) = e^{-x^2-y^2}\begin{pmatrix}1-2x^2 \ -2xy\end{pmatrix} ; \textrm{및} ; \mathbf{H}f(x, y) = e^{-x^2-y^2}\begin{pmatrix} 4x^3 - 6x &amp; 4x^2y - 2y \ 4x^2y-2y &amp;4xy^2-2x\end{pmatrix}.
$$</p>
<p>따라서 약간의 대수를 통해 $[-1,0]^\top$에서의 근사 이차식이 다음과 같음을 알 수 있습니다.</p>
<p>$$
f(x, y) \approx e^{-1}\left(-1 - (x+1) +(x+1)^2+y^2\right).
$$</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 그리드 구성 및 함수 계산
x, y = np.meshgrid(np.linspace(-2, 2, 101),
                   np.linspace(-2, 2, 101), indexing='ij')
z = x*np.exp(- x**2 - y**2)

# (1, 0)에서 기울기와 헤시안을 사용하여 근사 이차식 계산
w = np.exp(-1)*(-1 - (x + 1) + (x + 1)**2 + y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x.asnumpy(), y.asnumpy(), z.asnumpy(),
                  **{'rstride': 10, 'cstride': 10})
ax.plot_wireframe(x.asnumpy(), y.asnumpy(), w.asnumpy(),
                  **{'rstride': 10, 'cstride': 10}, color='purple')
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(-1, 1)
ax.dist = 12
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 그리드 구성 및 함수 계산
x, y = torch.meshgrid(torch.linspace(-2, 2, 101),
                   torch.linspace(-2, 2, 101))

z = x*torch.exp(- x**2 - y**2)

# (1, 0)에서 기울기와 헤시안을 사용하여 근사 이차식 계산
w = torch.exp(torch.tensor([-1.]))*(-1 - (x + 1) + 2 * (x + 1)**2 + 2 * y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x.numpy(), y.numpy(), z.numpy(),
                  **{'rstride': 10, 'cstride': 10})
ax.plot_wireframe(x.numpy(), y.numpy(), w.numpy(),
                  **{'rstride': 10, 'cstride': 10}, color='purple')
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(-1, 1)
ax.dist = 12
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 그리드 구성 및 함수 계산
x, y = tf.meshgrid(tf.linspace(-2., 2., 101),
                   tf.linspace(-2., 2., 101))

z = x*tf.exp(- x**2 - y**2)

# (1, 0)에서 기울기와 헤시안을 사용하여 근사 이차식 계산
w = tf.exp(tf.constant([-1.]))*(-1 - (x + 1) + 2 * (x + 1)**2 + 2 * y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x.numpy(), y.numpy(), z.numpy(),
                  **{'rstride': 10, 'cstride': 10})
ax.plot_wireframe(x.numpy(), y.numpy(), w.numpy(),
                  **{'rstride': 10, 'cstride': 10}, color='purple')
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(-1, 1)
ax.dist = 12
</code></pre>
<p>이것이 :numref:<code>sec_gd</code>에서 논의된 뉴턴 알고리즘(Newton's Algorithm)의 기초를 형성합니다. 여기서는 최상의 근사 이차식을 반복적으로 찾고 그 이차식을 정확하게 최소화함으로써 수치적 최적화를 수행합니다.</p>
<h2 id="행렬-미적분학-맛보기-a-little-matrix-calculus"><a class="header" href="#행렬-미적분학-맛보기-a-little-matrix-calculus">행렬 미적분학 맛보기 (A Little Matrix Calculus)</a></h2>
<p>행렬을 포함하는 함수의 도함수는 특히 깔끔하게 나옵니다. 이 섹션은 표기법상 무거울 수 있으므로 처음 읽을 때는 건너뛸 수 있지만, 특히 딥러닝 응용 분야에서 행렬 연산이 얼마나 중심적인지를 고려할 때 일반적인 행렬 연산을 포함하는 함수의 도함수가 예상보다 훨씬 깔끔한 경우가 많다는 것을 아는 것은 유용합니다.</p>
<p>예제로 시작해 봅시다. 어떤 고정된 열 벡터 $\boldsymbol{\beta}$가 있다고 가정하고, 곱 함수 $f(\mathbf{x}) = \boldsymbol{\beta}^\top\mathbf{x}$를 취하여 $\mathbf{x}$를 바꿀 때 내적이 어떻게 변하는지 이해하고 싶다고 합시다.</p>
<p>머신러닝에서 행렬 도함수를 다룰 때 유용하게 쓰일 표기법 중 하나는 *분모 레이아웃 행렬 도함수(denominator layout matrix derivative)*라고 불리는데, 여기서는 미분 분모에 있는 벡터, 행렬 또는 텐서의 모양으로 부분 도함수를 조립합니다. 이 경우 다음과 같이 쓸 것입니다.</p>
<p>$$
\frac{df}{d\mathbf{x}} = \begin{bmatrix}
\frac{df}{dx_1} \
\vdots \
\frac{df}{dx_n}
\end{bmatrix},
$$</p>
<p>열 벡터 $\mathbf{x}$의 모양과 일치시켰습니다.</p>
<p>함수를 구성 요소로 써보면 다음과 같습니다.</p>
<p>$$
f(\mathbf{x}) = \sum_{i = 1}^{n} \beta_ix_i = \beta_1x_1 + \cdots + \beta_nx_n. $$</p>
<p>이제 가령 $\beta_1$에 대해 부분 도함수를 취하면, 첫 번째 항을 제외하고는 모든 것이 0이며 이는 단지 $\beta_1$에 $x_1$을 곱한 것이므로 다음을 얻습니다.</p>
<p>$$
\frac{df}{dx_1} = \beta_1, $$</p>
<p>또는 더 일반적으로 다음과 같습니다.</p>
<p>$$
\frac{df}{dx_i} = \beta_i.
$$</p>
<p>이제 이를 행렬로 다시 조립하여 다음을 볼 수 있습니다.</p>
<p>$$
\frac{df}{d\mathbf{x}} = \begin{bmatrix}
\frac{df}{dx_1} \
\vdots \
\frac{df}{dx_n}
\end{bmatrix} = \begin{bmatrix}
\beta_1 \
\vdots \
\beta_n
\end{bmatrix} = \boldsymbol{\beta}.
$$</p>
<p>이는 이 섹션 전체에서 종종 마주하게 될 행렬 미적분학에 대한 몇 가지 요소를 설명합니다.</p>
<ul>
<li>첫째, 계산은 상당히 복잡해질 것입니다.</li>
<li>둘째, 최종 결과는 중간 과정보다 훨씬 깔끔하며 항상 단일 변수의 경우와 유사하게 보일 것입니다. 이 경우 $\frac{d}{dx}(bx) = b$와 $\frac{d}{d\mathbf{x}} (\boldsymbol{\beta}^\top\mathbf{x}) = \boldsymbol{\beta}$가 모두 유사함에 유의하십시오.</li>
<li>셋째, 전치(transposes)가 뜬금없이 나타날 수 있습니다. 이에 대한 핵심 이유는 우리가 분모의 모양과 일치시킨다는 관례 때문이며, 따라서 행렬을 곱할 때 원래 항의 모양과 다시 일치시키기 위해 전치를 취해야 할 것입니다.</li>
</ul>
<p>직관을 계속 구축하기 위해 조금 더 어려운 계산을 시도해 봅시다. 열 벡터 $\mathbf{x}$와 정사각 행렬 $A$가 있다고 가정하고 다음을 계산하고 싶다고 합시다.</p>
<p>$$\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x}).$$
:eqlabel:<code>eq_mat_goal_1</code></p>
<p>조작하기 더 쉬운 표기법을 위해 아인슈타인 표기법을 사용하여 이 문제를 고려해 봅시다. 이 경우 함수를 다음과 같이 쓸 수 있습니다.</p>
<p>$$
\mathbf{x}^\top A \mathbf{x} = x_ia_{ij}x_j. $$</p>
<p>도함수를 계산하려면 모든 $k$에 대해 다음의 값이 무엇인지 이해해야 합니다.</p>
<p>$$
\frac{d}{dx_k}(\mathbf{x}^\top A \mathbf{x}) = \frac{d}{dx_k}x_ia_{ij}x_j.
$$</p>
<p>곱의 미분법에 의해 다음을 얻습니다.</p>
<p>$$
\frac{d}{dx_k}x_ia_{ij}x_j = \frac{dx_i}{dx_k}a_{ij}x_j + x_ia_{ij}\frac{dx_j}{dx_k}.
$$</p>
<p>$rac{dx_i}{dx_k}$와 같은 항의 경우, $i=k$일 때 1이고 그렇지 않으면 0임을 알 수 있습니다. 이는 $i$와 $k$가 다른 모든 항이 이 합계에서 사라짐을 의미하므로, 첫 번째 합계에서 남는 유일한 항은 $i=k$인 항들뿐입니다. $j=k$가 필요한 두 번째 항에 대해서도 동일한 추론이 성립합니다. 이는 다음을 제공합니다.</p>
<p>$$
\frac{d}{dx_k}x_ia_{ij}x_j = a_{kj}x_j + x_ia_{ik}.
$$</p>
<p>이제 아인슈타인 표기법에서 인덱스의 이름은 임의적입니다 - 이 시점에서 이 계산에 있어 $i$와 $j$가 다르다는 사실은 중요하지 않으므로, 둘 다 $i$를 사용하도록 인덱스를 다시 지정하여 다음을 볼 수 있습니다.</p>
<p>$$
\frac{d}{dx_k}x_ia_{ij}x_j = a_{ki}x_i + x_ia_{ik} = (a_{ki} + a_{ik})x_i.
$$</p>
<p>이제 여기서부터 더 나아가기 위해 약간의 연습이 필요합니다. 행렬 연산의 관점에서 이 결과를 식별해 봅시다. $a_{ki} + a_{ik}$는 $\mathbf{A} + \mathbf{A}^\top$의 $k, i$번째 성분입니다. 이는 다음을 제공합니다.</p>
<p>$$
\frac{d}{dx_k}x_ia_{ij}x_j = [\mathbf{A} + \mathbf{A}^\top]_{ki}x_i.
$$</p>
<p>마찬가지로, 이 항은 이제 행렬 $\mathbf{A} + \mathbf{A}^\top$와 벡터 $\mathbf{x}$의 곱이므로 다음과 같음을 알 수 있습니다.</p>
<p>$$
\left[\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x})\right]<em>k = \frac{d}{dx_k}x_ia</em>{ij}x_j = [(\mathbf{A} + \mathbf{A}^\top)\mathbf{x}]_k.
$$</p>
<p>따라서 우리는 :eqref:<code>eq_mat_goal_1</code>로부터 원하는 도함수의 $k$번째 항목이 우변에 있는 벡터의 $k$번째 항목임을 알 수 있으며, 따라서 두 가지는 동일합니다. 따라서 다음을 산출합니다.</p>
<p>$$
\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x}) = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}.
$$</p>
<p>이것은 지난번보다 훨씬 더 많은 작업을 필요로 했지만 최종 결과는 작습니다. 그뿐만 아니라 전통적인 단일 변수 도함수에 대한 다음 계산을 고려해 보십시오.</p>
<p>$$
\frac{d}{dx}(xax) = \frac{dx}{dx}ax + xa\frac{dx}{dx} = (a+a)x.
$$</p>
<p>동등하게 $\frac{d}{dx}(ax^2) = 2ax = (a+a)x$입니다. 다시 말하지만, 우리는 단일 변수 결과와 다소 비슷해 보이지만 전치가 섞여 들어간 결과를 얻습니다.</p>
<p>이 시점에서 패턴이 다소 의심스러워 보일 것이므로 이유를 알아내려고 노력해 봅시다. 이와 같이 행렬 도함수를 취할 때, 우리가 얻는 식이 행렬과 그들의 전치의 곱과 합의 관점에서 쓸 수 있는 또 다른 행렬 식이라고 먼저 가정해 봅시다. 그러한 식이 존재한다면 모든 행렬에 대해 참이어야 할 것입니다. 특히, 행렬 곱이 단지 숫자의 곱이고, 행렬 합이 단지 합이며, 전치가 아무것도 하지 않는 $1 \times 1$ 행렬에 대해 참이어야 할 것입니다! 즉, 우리가 얻는 식은 무엇이든 단일 변수 식과 <em>일치해야</em> 합니다. 이는 약간의 연습을 통해 연관된 단일 변수 식이 어떻게 생겼어야 하는지 아는 것만으로 행렬 도함수를 종종 추측할 수 있음을 의미합니다!</p>
<p>이를 시도해 봅시다. $\mathbf{X}$가 $n \times m$ 행렬이고, $\mathbf{U}$가 $n \times r$이며 $\mathbf{V}$가 $r \times m$이라고 가정합시다. 다음을 계산해 봅시다.</p>
<p>$$\frac{d}{d\mathbf{V}} |\mathbf{X} - \mathbf{U}\mathbf{V}|_2^{2} = ;?$$
:eqlabel:<code>eq_mat_goal_2</code></p>
<p>이 계산은 행렬 분해(matrix factorization)라는 분야에서 중요합니다. 하지만 우리에게는 그저 계산해야 할 도함수일 뿐입니다. 이것이 $1\times1$ 행렬에 대해 어떨지 상상해 봅시다. 그 경우 다음과 같은 식을 얻습니다.</p>
<p>$$
\frac{d}{dv} (x-uv)^{2}= -2(x-uv)u, $$</p>
<p>여기서 도함수는 꽤 표준적입니다. 이를 다시 행렬 식으로 변환하려고 시도하면 다음을 얻습니다.</p>
<p>$$
\frac{d}{d\mathbf{V}} |\mathbf{X} - \mathbf{U}\mathbf{V}|_2^{2}= -2(\mathbf{X} - \mathbf{U}\mathbf{V})\mathbf{U}. $$</p>
<p>하지만 이를 보면 제대로 작동하지 않습니다. $\mathbf{X}$는 $n \times m$이고 $\mathbf{U}\mathbf{V}$도 마찬가지이므로, 행렬 $2(\mathbf{X} - \mathbf{U}\mathbf{V})$는 $n \times m$입니다. 반면 $\mathbf{U}$는 $n \times r$이며, 차원이 일치하지 않으므로 $n \times m$과 $n \times r$ 행렬을 곱할 수 없습니다!</p>
<p>우리는 $\mathbf{V}$와 모양이 같은 $r \times m$인 $\frac{d}{d\mathbf{V}}$를 얻고 싶습니다. 그래서 어떻게든 우리는 $n \times m$ 행렬과 $n \times r$ 행렬을 가져와서(아마도 전치를 사용하여) 곱해서 $r \times m$을 얻어야 합니다. 우리는 $U^\top$에 $(\mathbf{X} - \mathbf{U}\mathbf{V})$를 곱함으로써 이를 할 수 있습니다. 따라서 :eqref:<code>eq_mat_goal_2</code>에 대한 해가 다음과 같을 것이라고 추측할 수 있습니다.</p>
<p>$$
\frac{d}{d\mathbf{V}} |\mathbf{X} - \mathbf{U}\mathbf{V}|_2^{2}= -2\mathbf{U}^\top(\mathbf{X} - \mathbf{U}\mathbf{V}). $$</p>
<p>이것이 작동함을 보여주기 위해 상세한 계산을 제공하지 않는다면 태만한 일일 것입니다. 이 경험 법칙이 작동한다고 이미 믿으신다면 이 유도를 건너뛰셔도 좋습니다. 다음을 계산하려면</p>
<p>$$
\frac{d}{d\mathbf{V}} |\mathbf{X} - \mathbf{U}\mathbf{V}|_2^2,
$$</p>
<p>모든 $a$와 $b$에 대해 다음을 찾아야 합니다.</p>
<p>$$
\frac{d}{dv_{ab}} |\mathbf{X} - \mathbf{U}\mathbf{V}|<em>2^{2}= \frac{d}{dv</em>{ab}} \sum_{i, j}\left(x_{ij} - \sum_k u_{ik}v_{kj}\right)^2.
$$</p>
<p>$rac{d}{dv_{ab}}$에 관한 한 $\mathbf{X}$와 $\mathbf{U}$의 모든 항목이 상수임을 상기하면, 도함수를 합계 안으로 밀어 넣고 제곱에 연쇄 법칙을 적용하여 다음을 얻을 수 있습니다.</p>
<p>$$
\frac{d}{dv_{ab}} |\mathbf{X} - \mathbf{U}\mathbf{V}|<em>2^{2}= \sum</em>{i, j}2\left(x_{ij} - \sum_k u_{ik}v_{kj}\right)\left(-\sum_k u_{ik}\frac{dv_{kj}}{dv_{ab}} \right).
$$</p>
<p>이전 유도에서와 같이, $\frac{dv_{kj}}{dv_{ab}}$는 $k=a$ 및 $j=b$일 때만 0이 아님에 주목할 수 있습니다. 이 조건 중 어느 하나라도 충족되지 않으면 합계의 항은 0이 되며 우리는 이를 자유롭게 버릴 수 있습니다. 다음을 알 수 있습니다.</p>
<p>$$
\frac{d}{dv_{ab}} |\mathbf{X} - \mathbf{U}\mathbf{V}|<em>2^{2}= -2\sum</em>{i}\left(x_{ib} - \sum_k u_{ik}v_{kb}\right)u_{ia}.
$$</p>
<p>여기서 중요한 미묘함은 $k=a$라는 요구 사항이 안쪽 합계 내에서 발생하지 않는다는 것인데, 그 $k$는 안쪽 항 내에서 우리가 합산하고 있는 더미 변수이기 때문입니다. 표기법상 더 깔끔한 예로 다음 이유를 고려해 보십시오.</p>
<p>$$
\frac{d}{dx_1} \left(\sum_i x_i \right)^{2}= 2\left(\sum_i x_i \right).
$$</p>
<p>이 지점으로부터 우리는 합계의 구성 요소들을 식별하기 시작할 수 있습니다. 먼저 다음입니다.</p>
<p>$$
\sum_k u_{ik}v_{kb} = [\mathbf{U}\mathbf{V}]_{ib}.
$$</p>
<p>따라서 합계 내부의 전체 식은 다음과 같습니다.</p>
<p>$$
x_{ib} - \sum_k u_{ik}v_{kb} = [\mathbf{X}-\mathbf{U}\mathbf{V}]_{ib}.
$$</p>
<p>이는 우리가 이제 우리의 도함수를 다음과 같이 쓸 수 있음을 의미합니다.</p>
<p>$$
\frac{d}{dv_{ab}} |\mathbf{X} - \mathbf{U}\mathbf{V}|<em>2^{2}= -2\sum</em>{i}[\mathbf{X}-\mathbf{U}\mathbf{V}]<em>{ib}u</em>{ia}.
$$</p>
<p>우리는 이것이 행렬의 $a, b$ 원소처럼 보이기를 원하므로 이전 예제와 같은 기술을 사용하여 행렬 식에 도달할 수 있으며, 이는 $u_{ia}$의 인덱스 순서를 바꿔야 함을 의미합니다. $u_{ia} = [\mathbf{U}^\top]_{ai}$임을 주목한다면 다음과 같이 쓸 수 있습니다.</p>
<p>$$
\frac{d}{dv_{ab}} |\mathbf{X} - \mathbf{U}\mathbf{V}|<em>2^{2}= -2\sum</em>{i} [\mathbf{U}^\top]<em>{ai}[\mathbf{X}-\mathbf{U}\mathbf{V}]</em>{ib}.
$$</p>
<p>이것은 행렬 곱이며, 따라서 다음과 같이 결론지을 수 있습니다.</p>
<p>$$
\frac{d}{dv_{ab}} |\mathbf{X} - \mathbf{U}\mathbf{V}|<em>2^{2}= -2[\mathbf{U}^\top(\mathbf{X}-\mathbf{U}\mathbf{V})]</em>{ab}.
$$</p>
<p>따라서 우리는 :eqref:<code>eq_mat_goal_2</code>에 대한 해를 다음과 같이 쓸 수 있습니다.</p>
<p>$$
\frac{d}{d\mathbf{V}} |\mathbf{X} - \mathbf{U}\mathbf{V}|_2^{2}= -2\mathbf{U}^\top(\mathbf{X} - \mathbf{U}\mathbf{V}).
$$</p>
<p>이것은 우리가 위에서 추측한 해와 일치합니다!</p>
<p>이 시점에서 "왜 내가 배운 모든 미적분학 규칙의 행렬 버전을 그냥 쓸 수 없나요? 이것이 여전히 기계적이라는 것은 분명합니다. 그냥 끝내 버립시다!"라고 묻는 것이 합리적입니다. 실제로 그러한 규칙들이 있으며 :cite:<code>Petersen.Pedersen.ea.2008</code>은 훌륭한 요약을 제공합니다. 그러나 단일 값에 비해 행렬 연산이 결합될 수 있는 수많은 방식 때문에, 단일 변수보다 훨씬 더 많은 행렬 도함수 규칙이 있습니다. 인덱스로 작업하거나 적절한 경우 자동 미분에 맡기는 것이 최선인 경우가 많습니다.</p>
<h2 id="요약-summary-120"><a class="header" href="#요약-summary-120">요약 (Summary)</a></h2>
<ul>
<li>고차원에서 우리는 1차원에서의 도함수와 동일한 목적을 수행하는 기울기를 정의할 수 있습니다. 이를 통해 입력에 임의의 작은 변화를 주었을 때 다변수 함수가 어떻게 변하는지 알 수 있습니다.</li>
<li>역전파 알고리즘은 많은 부분 도함수의 효율적인 계산을 가능하게 하기 위해 다변수 연쇄 법칙을 조직하는 방법으로 볼 수 있습니다.</li>
<li>행렬 미적분학은 행렬 식의 도함수를 간결한 방식으로 쓸 수 있게 해 줍니다.</li>
</ul>
<h2 id="연습-문제-exercises-135"><a class="header" href="#연습-문제-exercises-135">연습 문제 (Exercises)</a></h2>
<ol>
<li>열 벡터 $\boldsymbol{\beta}$가 주어졌을 때, $f(\mathbf{x}) = \boldsymbol{\beta}^\top\mathbf{x}$와 $g(\mathbf{x}) = \mathbf{x}^\top\boldsymbol{\beta}$의 도함수를 모두 계산하십시오. 왜 같은 답을 얻습니까?</li>
<li>$\mathbf{v}$를 $n$차원 벡터라고 합시다. $\frac{\partial}{\partial\mathbf{v}}|\mathbf{v}|_2$는 무엇입니까?</li>
<li>$L(x, y) = \log(e^x + e^y)$라고 합시다. 기울기를 계산하십시오. 기울기 구성 요소들의 합은 얼마입니까?</li>
<li>$f(x, y) = x^2y + xy^2$라고 합시다. 유일한 임계점이 $(0,0)$임을 보이십시오. $f(x, x)$를 고려하여 $(0,0)$이 최대값, 최소값, 아니면 둘 다 아닌지 결정하십시오.</li>
<li>함수 $f(\mathbf{x}) = g(\mathbf{x}) + h(\mathbf{x})$를 최소화한다고 가정합시다. $\nabla f = 0$ 조건을 $g$와 $h$의 관점에서 어떻게 기하학적으로 해석할 수 있습니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/413">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1090">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1091">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="적분학-integral-calculus"><a class="header" href="#적분학-integral-calculus">적분학 (Integral Calculus)</a></h1>
<p>:label:<code>sec_integral_calculus</code></p>
<p>미분은 전통적인 미적분 교육 내용의 절반만을 구성합니다. 다른 기둥인 적분은 "이 곡선 아래의 넓이는 얼마인가?"라는 다소 별개의 질문으로 시작합니다. 겉보기에는 관련이 없어 보이지만, 적분은 <em>미적분학의 기본 정리</em>라고 알려진 것을 통해 미분과 긴밀하게 얽혀 있습니다.</p>
<p>이 책에서 논의하는 머신러닝 수준에서 적분에 대한 깊은 이해가 필요하지는 않을 것입니다. 하지만 나중에 마주칠 추가 응용 분야를 위한 토대를 마련하기 위해 짧은 소개를 제공할 것입니다.</p>
<h2 id="기하학적-해석-geometric-interpretation"><a class="header" href="#기하학적-해석-geometric-interpretation">기하학적 해석 (Geometric Interpretation)</a></h2>
<p>함수 $f(x)$가 있다고 가정해 봅시다. 단순함을 위해 $f(x)$가 비음수(결코 0보다 작은 값을 취하지 않음)라고 가정합시다. 우리가 이해하고자 하는 것은: $f(x)$와 $x$축 사이에 포함된 넓이가 얼마인가 하는 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mpl_toolkits import mplot3d
from mxnet import np, npx
npx.set_np()

x = np.arange(-2, 2, 0.01)
f = np.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist(), f.tolist())
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
from mpl_toolkits import mplot3d
import torch

x = torch.arange(-2, 2, 0.01)
f = torch.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist(), f.tolist())
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
from mpl_toolkits import mplot3d
import tensorflow as tf

x = tf.range(-2, 2, 0.01)
f = tf.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.numpy(), f.numpy())
d2l.plt.show()
</code></pre>
<p>대부분의 경우 이 넓이는 무한하거나 정의되지 않을 것이므로($f(x) = x^{2}$ 아래의 넓이를 고려해 보십시오), 사람들은 종종 한 쌍의 끝점, 가령 $a$와 $b$ 사이의 넓이에 대해 이야기할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.arange(-2, 2, 0.01)
f = np.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250])
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.arange(-2, 2, 0.01)
f = torch.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250])
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.range(-2, 2, 0.01)
f = tf.exp(-x**2)

d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.numpy()[50:250], f.numpy()[50:250])
d2l.plt.show()
</code></pre>
<p>우리는 이 넓이를 아래의 적분 기호로 나타낼 것입니다.</p>
<p>$$ 	extrm{Area}(\mathcal{A}) = \int_a^b f(x) ;dx. $$</p>
<p>내부 변수는 $\sum$에서의 합의 인덱스와 매우 흡사한 더미 변수(dummy variable)이므로, 우리가 원하는 어떤 내부 값으로도 동등하게 쓰일 수 있습니다.</p>
<p>$$ \int_a^b f(x) ;dx = \int_a^b f(z) ;dz. $$</p>
<p>우리가 그러한 적분을 어떻게 근사할 수 있는지 이해하려는 전통적인 방법이 있습니다: $a$와 $b$ 사이의 영역을 취해 $N$개의 수직 슬라이스로 자르는 것을 상상할 수 있습니다. $N$이 크면 각 슬라이스의 넓이를 직사각형으로 근사할 수 있으며, 그런 다음 넓이들을 더해 곡선 아래의 총 넓이를 얻을 수 있습니다. 코드에서 이를 수행하는 예제를 살펴봅시다. 나중에 실제 값을 얻는 방법을 볼 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
epsilon = 0.05
a = 0
b = 2

x = np.arange(a, b, epsilon)
f = x / (1 + x**2)

approx = np.sum(epsilon*f)
true = np.log(2) / 2

d2l.set_figsize()
d2l.plt.bar(x.asnumpy(), f.asnumpy(), width=epsilon, align='edge')
d2l.plt.plot(x, f, color='black')
d2l.plt.ylim([0, 1])
d2l.plt.show()

f'근사값: {approx}, 참값: {true}'
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
epsilon = 0.05
a = 0
b = 2

x = torch.arange(a, b, epsilon)
f = x / (1 + x**2)

approx = torch.sum(epsilon*f)
true = torch.log(torch.tensor([5.])) / 2

d2l.set_figsize()
d2l.plt.bar(x, f, width=epsilon, align='edge')
d2l.plt.plot(x, f, color='black')
d2l.plt.ylim([0, 1])
d2l.plt.show()

f'근사값: {approx}, 참값: {true}'
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
epsilon = 0.05
a = 0
b = 2

x = tf.range(a, b, epsilon)
f = x / (1 + x**2)

approx = tf.reduce_sum(epsilon*f)
true = tf.math.log(tf.constant([5.])) / 2

d2l.set_figsize()
d2l.plt.bar(x, f, width=epsilon, align='edge')
d2l.plt.plot(x, f, color='black')
d2l.plt.ylim([0, 1])
d2l.plt.show()

f'근사값: {approx}, 참값: {true}'
</code></pre>
<p>문제는 이것이 수치적으로는 수행될 수 있지만, 분석적으로는 다음과 같은 아주 간단한 함수에 대해서만 이 접근 방식을 사용할 수 있다는 점입니다.</p>
<p>$$ \int_a^b x ;dx. $$</p>
<p>위 코드 예제와 같이 다소 더 복잡한 것들은</p>
<p>$$ \int_a^b \frac{x}{1+x^{2}} ;dx. $$</p>
<p>그러한 직접적인 방법으로 해결할 수 있는 범위를 넘어섭니다.</p>
<p>우리는 대신 다른 접근 방식을 취할 것입니다. 넓이의 개념과 함께 직관적으로 작업하고, 적분을 찾는 데 사용되는 주요 계산 도구인 <em>미적분학의 기본 정리</em>를 배울 것입니다. 이것이 우리 적분 연구의 기초가 될 것입니다.</p>
<h2 id="미적분학의-기본-정리-the-fundamental-theorem-of-calculus"><a class="header" href="#미적분학의-기본-정리-the-fundamental-theorem-of-calculus">미적분학의 기본 정리 (The Fundamental Theorem of Calculus)</a></h2>
<p>적분 이론을 깊이 파고들기 위해 함수를 하나 도입합시다.</p>
<p>$$ F(x) = \int_0^x f(y) dy. $$</p>
<p>이 함수는 $x$를 어떻게 바꾸느냐에 따라 $0$과 $x$ 사이의 넓이를 측정합니다. 다음이 성립하므로 이것이 우리가 필요한 전부임에 유의하십시오.</p>
<p>$$ \int_a^b f(x) ;dx = F(b) - F(a). $$</p>
<p>이는 그림 :numref:<code>fig_area-subtract</code>에 표시된 것처럼 우리가 먼 끝점까지의 넓이를 측정하고 가까운 끝점까지의 넓이를 뺄 수 있다는 사실을 수학적으로 인코딩한 것입니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/sub-area.svg" alt="곡선 아래 두 점 사이의 넓이를 계산하는 문제를 특정 점의 왼쪽 넓이를 계산하는 것으로 왜 축소할 수 있는지 시각화한 그림." />
:label:<code>fig_area-subtract</code></p>
<p>따라서 우리는 $F(x)$가 무엇인지 알아냄으로써 임의의 구간에 대한 적분이 무엇인지 알아낼 수 있습니다.</p>
<p>그렇게 하기 위해 실험을 하나 고려해 봅시다. 미적분에서 자주 하듯이, 값을 아주 조금 옮겼을 때 어떤 일이 일어나는지 상상해 봅시다. 위의 언급으로부터 다음을 압니다.</p>
<p>$$ F(x+\epsilon) - F(x) = \int_x^{x+\epsilon} f(y) ; dy. $$</p>
<p>이는 함수가 함수의 아주 작은 조각 아래의 넓이만큼 변한다는 것을 알려줍니다.</p>
<p>이것이 우리가 근사를 하는 지점입니다. 만약 우리가 이와 같이 아주 작은 넓이의 조각을 본다면, 이 넓이는 높이가 $f(x)$이고 밑변의 너비가 $\epsilon$인 직사각형의 넓이에 가깝게 보입니다. 실제로 $\epsilon \rightarrow 0$에 따라 이 근사가 점점 더 좋아진다는 것을 보일 수 있습니다. 따라서 다음과 같이 결론지을 수 있습니다.</p>
<p>$$ F(x+\epsilon) - F(x) \approx \epsilon f(x). $$</p>
<p>하지만 이제 주목할 수 있습니다: 이것은 우리가 $F$의 도함수를 계산하고 있을 때 기대하는 바로 그 패턴입니다! 따라서 우리는 다음과 같은 다소 놀라운 사실을 봅니다.</p>
<p>$$ \frac{dF}{dx}(x) = f(x). $$</p>
<p>이것이 <em>미적분학의 기본 정리</em>입니다. 우리는 이를 확장된 형태로 다음과 같이 쓸 수 있습니다.
$$\frac{d}{dx}\int_0^x  f(y) ; dy = f(x).$$
:eqlabel:<code>eq_ftc</code></p>
<p>이것은 넓이를 찾는 개념(선험적으로 다소 어려움)을 취하여 도함수(훨씬 더 완전히 이해된 것)에 대한 진술로 축소합니다. 우리가 반드시 해야 할 마지막 한 가지 코멘트는 이것이 $F(x)$가 정확히 무엇인지 알려주지는 않는다는 것입니다. 실제로 임의의 $C$에 대해 $F(x) + C$는 동일한 도함수를 갖습니다. 이는 적분 이론에서의 삶의 단면(fact-of-life)입니다. 고맙게도 정적분으로 작업할 때 상수들은 상쇄되어 결과와 무관하게 됨에 유의하십시오.</p>
<p>$$ \int_a^b f(x) ; dx = (F(b) + C) - (F(a) + C) = F(b) - F(a). $$</p>
<p>이것이 추상적인 헛소리처럼 보일 수 있지만, 적분 계산에 대한 완전히 새로운 관점을 우리에게 제공했다는 것을 잠시 감상해 봅시다. 우리의 목표는 더 이상 넓이를 복구하기 위해 어떤 종류의 자르기 및 합산 과정을 수행하는 것이 아니라, 단순히 도함수가 우리가 가진 함수인 함수를 찾는 것입니다! 이는 이제 :numref:<code>sec_derivative_table</code>의 표를 반대로 함으로써 많은 다소 어려운 적분들을 나열할 수 있기 때문에 놀라운 일입니다. 예를 들어, 우리는 $x^{n}$의 도함수가 $nx^{n-1}$임을 압니다. 따라서 기본 정리 :eqref:<code>eq_ftc</code>를 사용하여 다음과 같이 말할 수 있습니다.</p>
<p>$$ \int_0^{x} ny^{n-1} ; dy = x^n - 0^n = x^n. $$</p>
<p>마찬가지로, 우리는 $e^{x}$의 도함수가 자기 자신임을 압니다. 이는 다음을 의미합니다.</p>
<p>$$ \int_0^{x} e^x ; dx = e^x - e^0 = e^x - 1. $$</p>
<p>이런 식으로 우리는 미분학의 아이디어들을 자유롭게 활용하여 적분학 전체 이론을 개발할 수 있습니다. 모든 적분 규칙은 이 한 가지 사실에서 파생됩니다.</p>
<h2 id="변수-변환-change-of-variables"><a class="header" href="#변수-변환-change-of-variables">변수 변환 (Change of Variables)</a></h2>
<p>:label:<code>subsec_integral_example</code></p>
<p>미분과 마찬가지로 적분 계산을 더 다루기 쉽게 만드는 여러 규칙이 있습니다. 실제로 미분학의 모든 규칙(곱의 미분법, 합의 법칙, 연쇄 법칙과 같은)은 각각 적분학의 대응하는 규칙(부분 적분법, 적분의 선형성, 변수 변환 공식)을 갖습니다. 이 섹션에서는 목록 중에서 가장 중요하다고 할 수 있는 변수 변환 공식을 깊이 파고들 것입니다.</p>
<p>먼저, 함수 자체가 적분인 함수가 있다고 가정합시다.</p>
<p>$$ F(x) = \int_0^x f(y) ; dy. $$</p>
<p>이 함수를 다른 함수와 합성하여 $F(u(x))$를 얻었을 때 어떻게 보이는지 알고 싶다고 가정해 봅시다. 연쇄 법칙에 의해 다음을 압니다.</p>
<p>$$ \frac{d}{dx}F(u(x)) = \frac{dF}{du}(u(x))\cdot \frac{du}{dx}. $$</p>
<p>우리는 위에서와 같이 기본 정리 :eqref:<code>eq_ftc</code>를 사용하여 이를 적분에 대한 진술로 바꿀 수 있습니다. 이는 다음을 제공합니다.</p>
<p>$$ F(u(x)) - F(u(0)) = \int_0^x \frac{dF}{du}(u(y))\cdot \frac{du}{dy} ;dy. $$</p>
<p>$F$ 자체가 적분임을 상기하면 좌변은 다음과 같이 다시 쓰일 수 있습니다.</p>
<p>$$ \int_{u(0)}^{u(x)} f(y) ; dy = \int_0^x \frac{dF}{du}(u(y))\cdot \frac{du}{dy} ;dy. $$</p>
<p>마찬가지로 $F$가 적분임을 상기하면 기본 정리 :eqref:<code>eq_ftc</code>를 사용하여 $\frac{dF}{dx} = f$임을 인식할 수 있으며, 따라서 다음과 같이 결론지을 수 있습니다.</p>
<p>$$\int_{u(0)}^{u(x)} f(y) ; dy = \int_0^x f(u(y))\cdot \frac{du}{dy} ;dy.$$
:eqlabel:<code>eq_change_var</code></p>
<p>이것이 <em>변수 변환(change of variables)</em> 공식입니다.</p>
<p>더 직관적인 유도를 위해, $x$와 $x+\epsilon$ 사이의 $f(u(x))$ 적분을 취할 때 어떤 일이 일어나는지 고려해 보십시오. 작은 $\epsilon$에 대해 이 적분은 대략 연관된 직사각형의 넓이인 $\epsilon f(u(x))$입니다. 이제 이것을 $u(x)$에서 $u(x+\epsilon)$까지의 $f(y)$ 적분과 비교해 봅시다. 우리는 $u(x+\epsilon) \approx u(x) + \epsilon \frac{du}{dx}(x)$임을 알고 있으므로, 이 직사각형의 넓이는 대략 $\epsilon \frac{du}{dx}(x)f(u(x))$입니다. 따라서 이러한 두 직사각형의 넓이가 일치하게 하려면, 그림 :numref:<code>fig_rect-transform</code>에 설명된 것처럼 첫 번째 것에 $\frac{du}{dx}(x)$를 곱해야 합니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/rect-trans.svg" alt="변수 변환 하에서 단일 얇은 직사각형의 변환을 시각화한 그림." />
:label:<code>fig_rect-transform</code></p>
<p>이는 다음을 알려줍니다.</p>
<p>$$ \int_x^{x+\epsilon} f(u(y))\frac{du}{dy}(y);dy = \int_{u(x)}^{u(x+\epsilon)} f(y) ; dy. $$</p>
<p>이것이 단일 작은 직사각형에 대해 표현된 변수 변환 공식입니다.</p>
<p>만약 $u(x)$와 $f(x)$가 적절하게 선택된다면, 이는 믿을 수 없을 정도로 복잡한 적분의 계산을 가능하게 할 수 있습니다. 예를 들어, 우리가 $f(y) = 1$ 및 $u(x) = e^{-x^{2}}$($\frac{du}{dx}(x) = -2xe^{-x^{2}}$를 의미함)를 선택하더라도, 이는 예를 들어 다음을 보여줄 수 있습니다.</p>
<p>$$ e^{-1} - 1 = \int_{e^{-0}}^{e^{-1}} 1 ; dy = -2\int_0^{1} ye^{-y^2};dy, $$</p>
<p>따라서 재배열하면 다음과 같습니다.</p>
<p>$$ \int_0^{1} ye^{-y^2}; dy = \frac{1-e^{-1}}{2}. $$</p>
<h2 id="부호-관례에-대한-코멘트-a-comment-on-sign-conventions"><a class="header" href="#부호-관례에-대한-코멘트-a-comment-on-sign-conventions">부호 관례에 대한 코멘트 (A Comment on Sign Conventions)</a></h2>
<p>눈치 빠른 독자들은 위의 계산에서 이상한 점을 발견할 것입니다. 즉, 다음과 같은 계산입니다.</p>
<p>$$ \int_{e^{-0}}^{e^{-1}} 1 ; dy = e^{-1} -1 &lt; 0, $$</p>
<p>음수를 생성할 수 있습니다. 넓이에 대해 생각할 때 음수 값을 보는 것이 이상할 수 있으므로 관례가 무엇인지 파헤쳐 볼 가치가 있습니다.</p>
<p>수학자들은 부호가 있는 넓이(signed areas)의 개념을 취합니다. 이는 두 가지 방식으로 나타납니다. 첫째, 때때로 0보다 작은 함수 $f(x)$를 고려하면 넓이 또한 음수가 될 것입니다. 예를 들어 다음과 같습니다.</p>
<p>$$ \int_0^{1} (-1);dx = -1. $$</p>
<p>마찬가지로, 왼쪽에서 오른쪽이 아니라 오른쪽에서 왼쪽으로 진행되는 적분 또한 음수 넓이로 취해집니다.</p>
<p>$$ \int_0^{-1} 1; dx = -1. $$</p>
<p>표준 넓이(양수 함수의 왼쪽에서 오른쪽까지)는 항상 양수입니다. 그것을 뒤집어서 얻은 것(가령 $x$축을 뒤집어 음수 함수의 적분을 얻거나, $y$축을 뒤집어 잘못된 순서의 적분을 얻는 경우)은 음수 넓이를 생성할 것입니다. 실제로 두 번 뒤집으면 상쇄되는 한 쌍의 음수 부호를 주어 양수 넓이를 갖게 될 것입니다.</p>
<p>$$ \int_0^{-1} (-1);dx =  1. $$</p>
<p>이 논의가 익숙하게 들린다면 그렇습니다! :numref:<code>sec_geometry-linear-algebraic-ops</code>에서 우리는 행렬식이 거의 동일한 방식으로 부호가 있는 넓이를 어떻게 나타내는지 논의했습니다.</p>
<h2 id="다중-적분-multiple-integrals"><a class="header" href="#다중-적분-multiple-integrals">다중 적분 (Multiple Integrals)</a></h2>
<p>어떤 경우에는 고차원에서 작업해야 할 것입니다. 예를 들어 $f(x, y)$와 같은 두 변수 함수가 있고 $x$가 $[a, b]$를 범위로 하고 $y$가 $[c, d]$를 범위로 할 때 $f$ 아래의 부피를 알고 싶다고 가정해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 그리드 구성 및 함수 계산
x, y = np.meshgrid(np.linspace(-2, 2, 101), np.linspace(-2, 2, 101),
                   indexing='ij')
z = np.exp(- x**2 - y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x.asnumpy(), y.asnumpy(), z.asnumpy())
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.plt.xticks([-2, -1, 0, 1, 2])
d2l.plt.yticks([-2, -1, 0, 1, 2])
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(0, 1)
ax.dist = 12
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 그리드 구성 및 함수 계산
x, y = torch.meshgrid(torch.linspace(-2, 2, 101), torch.linspace(-2, 2, 101))
z = torch.exp(- x**2 - y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.plt.xticks([-2, -1, 0, 1, 2])
d2l.plt.yticks([-2, -1, 0, 1, 2])
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(0, 1)
ax.dist = 12
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 그리드 구성 및 함수 계산
x, y = tf.meshgrid(tf.linspace(-2., 2., 101), tf.linspace(-2., 2., 101))
z = tf.exp(- x**2 - y**2)

# 함수 플롯
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.plt.xticks([-2, -1, 0, 1, 2])
d2l.plt.yticks([-2, -1, 0, 1, 2])
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(0, 1)
ax.dist = 12
</code></pre>
<p>우리는 이를 다음과 같이 씁니다.</p>
<p>$$ \int_{[a, b]\times[c, d]} f(x, y);dx;dy. $$</p>
<p>이 적분을 계산하고 싶다고 가정합시다. 저의 주장은 우리가 먼저 $x$에 대해 반복적으로 적분한 다음 $y$에 대한 적분으로 전환함으로써 이를 수행할 수 있다는 것입니다. 즉, 다음과 같습니다.</p>
<p>$$ \int_{[a, b]\times[c, d]} f(x, y);dx;dy = \int_c^{d} \left(\int_a^{b} f(x, y) ;dx\right) ; dy. $$</p>
<p>왜 그런지 봅시다.</p>
<p>우리가 함수를 정수 좌표 $i, j$로 인덱싱할 $\epsilon \times \epsilon$ 사각형들로 나눈 위 그림을 고려하십시오. 이 경우 우리의 적분은 대략 다음과 같습니다.</p>
<p>$$ \sum_{i, j} \epsilon^{2} f(\epsilon i, \epsilon j). $$</p>
<p>문제를 이산화하고 나면, 우리는 이러한 사각형의 값들을 원하는 순서대로 더할 수 있으며 값을 바꾸는 것에 대해 걱정하지 않아도 됩니다. 이는 그림 :numref:<code>fig_sum-order</code>에 설명되어 있습니다. 특히 다음과 같이 말할 수 있습니다.</p>
<p>$$  \sum _ {j} \epsilon \left(\sum_{i} \epsilon f(\epsilon i, \epsilon j)\right). $$</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/sum-order.svg" alt="많은 사각형에 대한 합을 먼저 열에 대한 합(1)으로 분해한 다음 열 합계들을 함께 더하는(2) 방법의 그림." />
:label:<code>fig_sum-order</code></p>
<p>안쪽의 합은 정확히 다음 적분의 이산화입니다.</p>
<p>$$ G(\epsilon j) = \int _a^{b} f(x, \epsilon j) ; dx. $$</p>
<p>마지막으로 이러한 두 식을 결합하면 다음을 얻는다는 점에 유의하십시오.</p>
<p>$$ \sum _ {j} \epsilon G(\epsilon j) \approx \int _ {c}^{d} G(y) ; dy = \int _ {[a, b]\times[c, d]} f(x, y);dx;dy. $$</p>
<p>따라서 이 모든 것을 종합하면 다음을 갖게 됩니다.</p>
<p>$$ \int _ {[a, b]\times[c, d]} f(x, y);dx;dy = \int _ c^{d} \left(\int _ a^{b} f(x, y) ;dx\right) ; dy. $$</p>
<p>이산화하고 나면 우리가 한 모든 것은 숫자 목록을 더하는 순서를 재배열한 것뿐임에 유의하십시오. 이것이 아무것도 아닌 것처럼 보일 수 있지만, 이 결과(*푸비니의 정리(Fubini's Theorem)*라고 불림)가 항상 참인 것은 아닙니다! 머신러닝을 할 때 접하는 수학 유형(연속 함수)의 경우 걱정할 필요가 없지만, 그것이 실패하는 예제를 만드는 것은 가능합니다(예를 들어 직사각형 $[0,2]\times[0,1]$에서 함수 $f(x, y) = xy(x^2-y^2)/(x^2+y^2)^3$).</p>
<p>먼저 $x$에 대해 적분한 다음 $y$에 대해 적분하기로 한 선택은 임의적이었음에 유의하십시오. 우리는 똑같이 $y$를 먼저 하고 그다음 $x$를 하도록 선택하여 다음을 볼 수도 있었습니다.</p>
<p>$$ \int _ {[a, b]\times[c, d]} f(x, y);dx;dy = \int _ a^{b} \left(\int _ c^{d} f(x, y) ;dy\right) ; dx. $$</p>
<p>종종 우리는 벡터 표기법으로 응축하여, $U = [a, b]\times [c, d]$에 대해 다음과 같이 말할 것입니다.</p>
<p>$$ \int _ U f(\mathbf{x});d\mathbf{x}. $$</p>
<h2 id="다중-적분에서의-변수-변환-change-of-variables-in-multiple-integrals"><a class="header" href="#다중-적분에서의-변수-변환-change-of-variables-in-multiple-integrals">다중 적분에서의 변수 변환 (Change of Variables in Multiple Integrals)</a></h2>
<p>:eqref:<code>eq_change_var</code>의 단일 변수와 마찬가지로, 고차원 적분 내에서 변수를 변환하는 능력은 핵심 도구입니다. 유도 없이 결과를 요약해 봅시다.</p>
<p>우리는 적분 도메인을 재파라미터화하는 함수가 필요합니다. 우리는 이것을 $\phi : \mathbb{R}^n \rightarrow \mathbb{R}^n$로 취할 수 있는데, 이는 $n$개의 실수 변수를 받아 다른 $n$개를 반환하는 임의의 함수입니다. 식을 깔끔하게 유지하기 위해 $\phi$가 *단사(injective)*라고 가정하겠습니다. 즉, 결코 자기 자신 위로 접히지 않습니다($\phi(\mathbf{x}) = \phi(\mathbf{y}) \implies \mathbf{x} = \mathbf{y}$).</p>
<p>이 경우 다음과 같이 말할 수 있습니다.</p>
<p>$$ \int _ {\phi(U)} f(\mathbf{x});d\mathbf{x} = \int _ {U} f(\phi(\mathbf{x})) \left|\det(D\phi(\mathbf{x}))\right|;d\mathbf{x}. $$</p>
<p>여기서 $D\phi$는 $\phi$의 *야코비 행렬(Jacobian)*로, $\boldsymbol{\phi} = (\phi_1(x_1, \ldots, x_n), \ldots, \phi_n(x_1, \ldots, x_n))$의 편미분 행렬입니다.</p>
<p>$$ D\boldsymbol{\phi} = \begin{bmatrix}
\frac{\partial \phi _ 1}{\partial x _ 1} &amp; \cdots &amp; \frac{\partial \phi _ 1}{\partial x _ n} \
\vdots &amp; \ddots &amp; \vdots \
\frac{\partial \phi _ n}{\partial x _ 1} &amp; \cdots &amp; \frac{\partial \phi _ n}{\partial x _ n}
\end{bmatrix}. $$</p>
<p>자세히 살펴보면, 이것이 $\frac{du}{dx}(x)$ 항을 $\left|\det(D\phi(\mathbf{x}))\right|$로 대체한 것을 제외하고는 단일 변수 연쇄 법칙 :eqref:<code>eq_change_var</code>과 유사함을 알 수 있습니다. 이 항을 어떻게 해석할 수 있는지 살펴봅시다. $\frac{du}{dx}(x)$ 항이 $u$를 적용하여 $x$축을 얼마나 늘렸는지를 말하기 위해 존재했음을 상기하십시오. 고차원에서의 동일한 과정은 $\boldsymbol{\phi}$를 적용하여 작은 사각형(또는 작은 <em>하이퍼큐브</em>)의 넓이(또는 부피, 또는 하이퍼볼륨)를 얼마나 늘리는지 결정하는 것입니다. 만약 $\boldsymbol{\phi}$가 행렬에 의한 곱셈이었다면, 우리는 행렬식이 이미 답을 준다는 것을 압니다.</p>
<p>약간의 작업을 통해, 도함수와 기울기로 직선이나 평면으로 근사할 수 있었던 것과 동일한 방식으로 <em>야코비 행렬</em>이 한 점에서의 다변수 함수 $\boldsymbol{\phi}$에 대한 최선의 근사를 제공함을 보일 수 있습니다. 따라서 야코비 행렬의 행렬식은 우리가 1차원에서 식별한 스케일링 인자를 정확히 반영합니다.</p>
<p>이에 대한 세부 사항을 채우는 데는 약간의 작업이 필요하므로, 지금 당장 명확하지 않더라도 걱정하지 마십시오. 나중에 활용할 예제를 하나라도 살펴봅시다. 적분을 고려해 보십시오.</p>
<p>$$ \int _ {-\infty}^{\infty} \int _ {-\infty}^{\infty} e^{-x^{2}-y^{2}} ;dx;dy. $$</p>
<p>이 적분을 직접 다루는 것은 아무데도 도달하지 못할 것이지만, 변수를 변환하면 상당한 진전을 이룰 수 있습니다. 만약 $\boldsymbol{\phi}(r, \theta) = (r \cos(\theta),  r\sin(\theta))$라고 하면($x = r \cos(\theta), y = r \sin(\theta)$임을 의미함), 변수 변환 공식을 적용하여 이것이 다음과 같음을 볼 수 있습니다.</p>
<p>$$ \int _ 0^\infty \int_0 ^ {2\pi} e^{-r^{2}} \left|\det(D\mathbf{\phi}(\mathbf{x}))\right|;d\theta;dr, $$</p>
<p>여기서</p>
<p>$$ \left|\det(D\mathbf{\phi}(\mathbf{x}))\right| = \left|\det\begin{bmatrix}
\cos(\theta) &amp; -r\sin(\theta) \
\sin(\theta) &amp; r\cos(\theta)
\end{bmatrix}\right| = r(\cos^{2}(\theta) + \sin^{2}(\theta)) = r. $$</p>
<p>따라서 적분은 다음과 같습니다.</p>
<p>$$ \int _ 0^\infty \int _ 0 ^ {2\pi} re^{-r^{2}} ;d\theta;dr = 2\pi\int _ 0^\infty re^{-r^{2}} ;dr = \pi, $$</p>
<p>여기서 마지막 등식은 섹션 :numref:<code>subsec_integral_example</code>에서 사용한 것과 동일한 계산에 의해 따릅니다.</p>
<p>우리는 :numref:<code>sec_random_variables</code>에서 연속 확률 변수를 공부할 때 이 적분을 다시 만날 것입니다.</p>
<h2 id="요약-summary-121"><a class="header" href="#요약-summary-121">요약 (Summary)</a></h2>
<ul>
<li>적분 이론은 넓이나 부피에 대한 질문에 답할 수 있게 해 줍니다.</li>
<li>미적분학의 기본 정리는 어떤 점까지의 넓이의 도함수가 적분되는 함수의 값에 의해 주어진다는 관찰을 통해, 넓이를 계산하는 데 미분에 대한 지식을 활용할 수 있게 해 줍니다.</li>
<li>고차원 적분은 단일 변수 적분을 반복하여 계산할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-136"><a class="header" href="#연습-문제-exercises-136">연습 문제 (Exercises)</a></h2>
<ol>
<li>$\int_1^2 \frac{1}{x} ;dx$는 얼마입니까?</li>
<li>변수 변환 공식을 사용하여 $\int_0^{\sqrt{\pi}}x\sin(x^2);dx$를 적분하십시오.</li>
<li>$\int_{[0,1]^2} xy ;dx;dy$는 얼마입니까?</li>
<li>변수 변환 공식을 사용하여 $\int_0^2\int_0^1xy(x^2-y^2)/(x^2+y^2)^3;dy;dx$와 $\int_0^1\int_0^2f(x, y) = xy(x^2-y^2)/(x^2+y^2)^3;dx;dy$를 계산하여 그들이 다름을 확인하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/414">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1092">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1093">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="확률-변수-random-variables-1"><a class="header" href="#확률-변수-random-variables-1">확률 변수 (Random Variables)</a></h1>
<p>:label:<code>sec_random_variables</code></p>
<p>:numref:<code>sec_prob</code>에서 우리는 이산 확률 변수를 다루는 방법에 대한 기초를 보았습니다. 우리의 경우 이산 확률 변수는 유한한 가능한 값의 집합 또는 정수를 취하는 확률 변수를 지칭합니다. 이 섹션에서는 임의의 실수 값을 취할 수 있는 확률 변수인 <em>연속 확률 변수(continuous random variables)</em> 이론을 전개합니다.</p>
<h2 id="연속-확률-변수-continuous-random-variables"><a class="header" href="#연속-확률-변수-continuous-random-variables">연속 확률 변수 (Continuous Random Variables)</a></h2>
<p>연속 확률 변수는 이산 확률 변수보다 현저히 더 미묘한 주제입니다. 적절한 비유를 들자면, 기술적인 도약은 숫자 목록을 더하는 것과 함수를 적분하는 것 사이의 도약에 필적합니다. 따라서 이론을 전개하는 데 시간을 좀 들여야 합니다.</p>
<h3 id="이산에서-연속으로-from-discrete-to-continuous"><a class="header" href="#이산에서-연속으로-from-discrete-to-continuous">이산에서 연속으로 (From Discrete to Continuous)</a></h3>
<p>연속 확률 변수로 작업할 때 마주치는 추가적인 기술적 과제를 이해하기 위해 사고 실험을 하나 해봅시다. 우리가 다트판에 다트를 던지고 있고, 판의 중심으로부터 정확히 $2 \textrm{cm}$ 떨어진 곳에 맞을 확률을 알고 싶다고 가정해 봅시다. 우선, 한 자리 정도의 정확도로 측정하는 것을 상상해 봅니다. 즉, $0 \textrm{cm}$, $1 \textrm{cm}$, $2 \textrm{cm}$ 등의 칸(bins)이 있는 것입니다. 우리는 다트판에 가령 100개의 다트를 던지고, 그중 20개가 $2\textrm{cm}$ 칸에 떨어진다면 던진 다트의 $20%$가 중심에서 $2 \textrm{cm}$ 떨어진 판에 맞았다고 결론을 내립니다.</p>
<p>하지만 자세히 살펴보면, 이것은 우리의 질문과 맞지 않습니다! 우리는 정확한 일치를 원했지만, 이 칸들은 가령 $1.5\textrm{cm}$와 $2.5\textrm{cm}$ 사이에 떨어진 모든 것을 담고 있습니다.</p>
<p>굴하지 않고 더 진행해 봅니다. 더 정밀하게 측정합니다. 가령 $1.9\textrm{cm}$, $2.0\textrm{cm}$, $2.1\textrm{cm}$로 측정하고, 이제 아마도 100개의 다트 중 3개가 $2.0\textrm{cm}$ 양동이에 맞았음을 봅니다. 따라서 우리는 확률이 $3%$라고 결론짓습니다.</p>
<p>하지만 이것은 아무것도 해결하지 못합니다! 단지 문제를 한 자리 더 아래로 밀어냈을 뿐입니다. 조금 추상화해 봅시다. 처음 $k$자리가 $2.00000\ldots$와 일치할 확률을 알고 있고, 처음 $k+1$자리가 일치할 확률을 알고 싶다고 상상해 봅시다. $k+1$번째 자리가 본질적으로 {0, 1, 2, \ldots, 9} 집합에서의 무작위 선택이라고 가정하는 것은 꽤 합리적입니다. 적어도, 중심으로부터 떨어진 마이크로미터 수가 $3$보다 $7$로 끝나는 것을 선호하게 만드는 물리적으로 의미 있는 과정을 상상할 수 없습니다.</p>
<p>이것이 의미하는 바는 본질적으로 우리가 요구하는 정확도가 한 자리 늘어날 때마다 일치할 확률이 10분의 1로 줄어들어야 한다는 것입니다. 또는 다른 방식으로 표현하면 다음과 같이 기대할 것입니다.</p>
<p>$$ P(\textrm{거리가}; 2.00\ldots \textrm{이며,}; k ;\textrm{자리까지 일치함} ) \approx p\cdot10^{-k}. $$</p>
<p>값 $p$는 본질적으로 처음 몇 자리에서 일어나는 일을 인코딩하고, $10^{-k}$가 나머지를 처리합니다.</p>
<p>소수점 이하 $k=4$자리까지 정확한 위치를 안다면, 이는 값이 가령 $[1.99995, 2.00005]$ 구간 내에 떨어진다는 것을 의미하며, 이 구간의 길이는 $2.00005-1.99995 = 10^{-4}$입니다. 따라서 이 구간의 길이를 $\epsilon$이라고 부르면 다음과 같이 말할 수 있습니다.</p>
<p>$$ P(\textrm{거리가}; 2 ;\textrm{근처의}; \epsilon ;\textrm{크기 구간 내에 있음} ) \approx \epsilon \cdot p. $$</p>
<p>이것을 마지막으로 한 단계 더 진행해 봅시다. 우리는 줄곧 점 $2$에 대해서만 생각해 왔지만, 다른 점들에 대해서는 생각하지 않았습니다. 근본적으로 다른 점은 없지만, 값 $p$는 아마도 다를 것입니다. 우리는 적어도 다트 던지는 사람이 $20\textrm{cm}$보다는 $2\textrm{cm}$와 같은 중심 근처의 점에 맞힐 가능성이 더 높기를 바랄 것입니다. 따라서 값 $p$는 고정된 것이 아니라 점 $x$에 의존해야 합니다. 이는 우리가 다음을 기대해야 함을 알려줍니다.</p>
<p>$$P(\textrm{거리가}; x ;\textrm{근처의}; \epsilon ;\textrm{크기 구간 내에 있음} ) \approx \epsilon \cdot p(x).$$ :eqlabel:<code>eq_pdf_deriv</code></p>
<p>실제로 :eqref:<code>eq_pdf_deriv</code>는 *확률 밀도 함수(probability density function)*를 정확하게 정의합니다. 이는 한 점 근처에 맞을 상대적 확률을 다른 점과 비교하여 인코딩하는 함수 $p(x)$입니다. 그러한 함수가 어떻게 생겼을지 시각화해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mxnet import np, npx
npx.set_np()

# 어떤 확률 변수에 대한 확률 밀도 함수 플롯
x = np.arange(-5, 5, 0.01)
p = 0.2*np.exp(-(x - 3)**2 / 2)/np.sqrt(2 * np.pi) + \
    0.8*np.exp(-(x + 1)**2 / 2)/np.sqrt(2 * np.pi)

d2l.plot(x, p, 'x', 'Density')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
import torch
torch.pi = torch.acos(torch.zeros(1)).item() * 2  # torch에서 pi 정의

# 어떤 확률 변수에 대한 확률 밀도 함수 플롯
x = torch.arange(-5, 5, 0.01)
p = 0.2*torch.exp(-(x - 3)**2 / 2)/torch.sqrt(2 * torch.tensor(torch.pi)) + \
    0.8*torch.exp(-(x + 1)**2 / 2)/torch.sqrt(2 * torch.tensor(torch.pi))

d2l.plot(x, p, 'x', 'Density')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
import tensorflow as tf
tf.pi = tf.acos(tf.zeros(1)).numpy() * 2  # TensorFlow에서 pi 정의

# 어떤 확률 변수에 대한 확률 밀도 함수 플롯
x = tf.range(-5, 5, 0.01)
p = 0.2*tf.exp(-(x - 3)**2 / 2)/tf.sqrt(2 * tf.constant(tf.pi)) + \
    0.8*tf.exp(-(x + 1)**2 / 2)/tf.sqrt(2 * tf.constant(tf.pi))

d2l.plot(x, p, 'x', 'Density')
</code></pre>
<p>함수 값이 큰 위치는 우리가 무작위 값을 찾을 가능성이 더 높은 영역을 나타냅니다. 낮은 부분은 무작위 값을 찾을 가능성이 낮은 영역입니다.</p>
<h3 id="확률-밀도-함수-probability-density-functions"><a class="header" href="#확률-밀도-함수-probability-density-functions">확률 밀도 함수 (Probability Density Functions)</a></h3>
<p>이제 이를 더 조사해 봅시다. 우리는 확률 변수 $X$에 대해 확률 밀도 함수가 직관적으로 무엇인지 이미 보았습니다. 즉, 밀도 함수는 다음을 만족하는 함수 $p(x)$입니다.</p>
<p>$$P(X ;\textrm{가}; x ;\textrm{근처의}; \epsilon ;\textrm{크기 구간 내에 있음} ) \approx \epsilon \cdot p(x).$$ :eqlabel:<code>eq_pdf_def</code></p>
<p>그렇다면 이것이 $p(x)$의 속성에 대해 무엇을 의미할까요?</p>
<p>먼저, 확률은 결코 음수가 아니므로 $p(x) \ge 0$일 것으로 기대해야 합니다.</p>
<p>둘째, $\mathbb{R}$을 $(\epsilon\cdot i, \epsilon \cdot (i+1)]$로 주어지는 $\epsilon$ 너비의 무한한 수의 슬라이스로 쪼갠다고 상상해 봅시다. 이들 각각에 대해, 우리는 :eqref:<code>eq_pdf_def</code>로부터 확률이 대략 다음과 같음을 압니다.</p>
<p>$$ P(X ;\textrm{가}; x ;\textrm{근처의}; \epsilon ;\textrm{크기 구간 내에 있음} ) \approx \epsilon \cdot p(\epsilon \cdot i), $$</p>
<p>따라서 이들 모두에 대해 합산하면 다음과 같아야 합니다.</p>
<p>$$ P(X\in\mathbb{R}) \approx \sum_i \epsilon \cdot p(\epsilon\cdot i). $$</p>
<p>이것은 :numref:<code>sec_integral_calculus</code>에서 논의된 적분의 근사에 지나지 않으므로 다음과 같이 말할 수 있습니다.</p>
<p>$$ P(X\in\mathbb{R}) = \int_{-\infty}^{\infty} p(x) ; dx. $$</p>
<p>확률 변수는 <em>어떤</em> 숫자를 반드시 가져야 하므로 $P(X\in\mathbb{R}) = 1$임을 압니다. 따라서 임의의 밀도에 대해 다음과 같이 결론지을 수 있습니다.</p>
<p>$$ \int_{-\infty}^{\infty} p(x) ; dx = 1. $$</p>
<p>실제로 이를 더 파고들면 임의의 $a$와 $b$에 대해 다음을 보게 됩니다.</p>
<p>$$ P(X\in(a, b]) = \int _ {a}^{b} p(x) ; dx. $$</p>
<p>우리는 이전과 동일한 이산 근사 방법을 사용하여 코드에서 이를 근사할 수 있습니다. 이 경우 파란색 영역에 떨어질 확률을 근사할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 수치 적분을 사용하여 확률 근사
epsilon = 0.01
x = np.arange(-5, 5, 0.01)
p = 0.2*np.exp(-(x - 3)**2 / 2) / np.sqrt(2 * np.pi) + \
    0.8*np.exp(-(x + 1)**2 / 2) / np.sqrt(2 * np.pi)

d2l.set_figsize()
d2l.plt.plot(x, p, color='black')
d2l.plt.fill_between(x.tolist()[300:800], p.tolist()[300:800])
d2l.plt.show()

f'근사 확률: {np.sum(epsilon*p[300:800])}'
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 수치 적분을 사용하여 확률 근사
epsilon = 0.01
x = torch.arange(-5, 5, 0.01)
p = 0.2*torch.exp(-(x - 3)**2 / 2) / torch.sqrt(2 * torch.tensor(torch.pi)) + \
    0.8*torch.exp(-(x + 1)**2 / 2) / torch.sqrt(2 * torch.tensor(torch.pi))

d2l.set_figsize()
d2l.plt.plot(x, p, color='black')
d2l.plt.fill_between(x.tolist()[300:800], p.tolist()[300:800])
d2l.plt.show()

f'근사 확률: {torch.sum(epsilon*p[300:800])}'
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 수치 적분을 사용하여 확률 근사
epsilon = 0.01
x = tf.range(-5, 5, 0.01)
p = 0.2*tf.exp(-(x - 3)**2 / 2) / tf.sqrt(2 * tf.constant(tf.pi)) + \
    0.8*tf.exp(-(x + 1)**2 / 2) / tf.sqrt(2 * tf.constant(tf.pi))

d2l.set_figsize()
d2l.plt.plot(x, p, color='black')
d2l.plt.fill_between(x.numpy().tolist()[300:800], p.numpy().tolist()[300:800])
d2l.plt.show()

f'근사 확률: {tf.reduce_sum(epsilon*p[300:800])}'
</code></pre>
<p>이러한 두 가지 속성이 가능한 확률 밀도 함수(줄여서 <em>p.d.f.</em>)의 공간을 정확하게 설명한다는 것이 밝혀졌습니다. 이들은 다음을 만족하는 비음수 함수 $p(x) \ge 0$입니다.</p>
<p>$$\int_{-\infty}^{\infty} p(x) ; dx = 1.$$ :eqlabel:<code>eq_pdf_int_one</code></p>
<p>우리는 적분을 사용하여 확률 변수가 특정 구간에 있을 확률을 얻음으로써 이 함수를 해석합니다.</p>
<p>$$P(X\in(a, b]) = \int _ {a}^{b} p(x) ; dx.$$ :eqlabel:<code>eq_pdf_int_int</code></p>
<p>:numref:<code>sec_distributions</code>에서 여러 일반적인 분포를 보겠지만, 계속해서 추상적으로 작업해 봅시다.</p>
<h3 id="누적-분포-함수-cumulative-distribution-functions"><a class="header" href="#누적-분포-함수-cumulative-distribution-functions">누적 분포 함수 (Cumulative Distribution Functions)</a></h3>
<p>이전 섹션에서 우리는 p.d.f.의 개념을 보았습니다. 실전에서 이는 연속 확률 변수를 논의하기 위해 흔히 마주치는 방법이지만, 한 가지 중요한 함정이 있습니다: p.d.f.의 값 자체가 확률이 아니라, 확률을 산출하기 위해 우리가 적분해야 하는 함수라는 점입니다. 밀도가 10보다 크더라도, 그것이 $1/10$보다 긴 구간에 대해 10보다 크지 않은 한 아무런 문제가 없습니다. 이는 직관에 반할 수 있으므로, 사람들은 종종 그 자체로 <em>확률</em>인 <em>누적 분포 함수(cumulative distribution function)</em> 또는 c.d.f.의 관점에서도 생각합니다.</p>
<p>특히 :eqref:<code>eq_pdf_int_int</code>를 사용하여, 밀도 $p(x)$를 갖는 확률 변수 $X$에 대한 c.d.f.를 다음과 같이 정의합니다.</p>
<p>$$ F(x) = \int _ {-\infty}^{x} p(x) ; dx = P(X \le x). $$</p>
<p>몇 가지 속성을 관찰해 봅시다.</p>
<ul>
<li>$x\rightarrow -\infty$에 따라 $F(x) \rightarrow 0$.</li>
<li>$x\rightarrow \infty$에 따라 $F(x) \rightarrow 1$.</li>
<li>$F(x)$는 비감소 함수입니다 ($y &gt; x \implies F(y) \ge F(x)$).</li>
<li>$X$가 연속 확률 변수라면 $F(x)$는 연속적입니다 (도약이 없음).</li>
</ul>
<p>네 번째 불렛 포인트와 관련하여, 가령 확률 $1/2$로 $0$과 $1$ 값을 취하는 이산 확률 변수 $X$라면 이것이 참이 아님에 유의하십시오. 그 경우 다음과 같습니다.</p>
<p>$$ F(x) = \begin{cases} 0 &amp; x &lt; 0, \ \frac{1}{2} &amp; x &lt; 1, \ 1 &amp; x \ge 1. \end{cases} $$</p>
<p>이 예제에서 우리는 c.d.f.로 작업할 때의 이점 중 하나를 봅니다. 즉, 동일한 프레임워크 내에서 연속 또는 이산 확률 변수, 또는 실제로 두 가지의 혼합(동전을 던져 앞면이면 주사위 굴리기 결과를 반환하고, 뒷면이면 다트 던지기 거리를 반환함)을 다룰 수 있다는 능력입니다.</p>
<h3 id="평균-means"><a class="header" href="#평균-means">평균 (Means)</a></h3>
<p>확률 변수 $X$를 다루고 있다고 가정합시다. 분포 자체는 해석하기 어려울 수 있습니다. 확률 변수의 행동을 간결하게 요약할 수 있는 것이 종종 유용합니다. 확률 변수의 행동을 포착하는 데 도움이 되는 숫자를 *요약 통계량(summary statistics)*이라고 합니다. 가장 흔히 마주치는 것들은 <em>평균(mean)</em>, <em>분산(variance)</em>, 그리고 *표준 편차(standard deviation)*입니다.</p>
<p><em>평균</em>은 확률 변수의 평균적인 값을 인코딩합니다. 확률 $p_i$로 값 $x_i$를 취하는 이산 확률 변수 $X$가 있다면, 평균은 가중 평균에 의해 주어집니다: 값들에 확률 변수가 그 값을 취할 확률을 곱한 합계입니다.</p>
<p>$$\mu_X = E[X] = \sum_i x_i p_i.$$ :eqlabel:<code>eq_exp_def</code></p>
<p>우리가 평균을 해석해야 할 방식은(비록 주의가 필요하지만) 본질적으로 확률 변수가 위치하려는 경향이 있는 곳을 알려준다는 것입니다.</p>
<p>이 섹션 전체에서 살펴볼 미니멀한 예제로서, 확률 $p$로 $a-2$ 값을, 확률 $p$로 $a+2$ 값을, 그리고 확률 $1-2p$로 $a$ 값을 취하는 확률 변수 $X$를 들어봅시다. 우리는 임의의 가능한 $a$와 $p$ 선택에 대해 :eqref:<code>eq_exp_def</code>를 사용하여 평균이 다음과 같음을 계산할 수 있습니다.</p>
<p>$$ \mu_X = E[X] = \sum_i x_i p_i = (a-2)p + a(1-2p) + (a+2)p = a. $$</p>
<p>따라서 평균이 $a$임을 알 수 있습니다. 이는 $a$가 우리가 확률 변수를 중심으로 한 위치이므로 직관과 일치합니다.</p>
<p>도움이 되므로 몇 가지 속성을 요약해 봅시다.</p>
<ul>
<li>임의의 확률 변수 $X$와 숫자 $a, b$에 대해, $\mu_{aX+b} = a\mu_X + b$가 성립합니다.</li>
<li>두 확률 변수 $X$와 $Y$가 있다면, $\mu_{X+Y} = \mu_X+\mu_Y$입니다.</li>
</ul>
<p>평균은 확률 변수의 평균적인 행동을 이해하는 데 유용하지만, 충분하고 완전한 직관적 이해를 갖기에는 평균만으로는 부족합니다. 판매당 $10 \pm $1의 이익을 내는 것과 판매당 $10 \pm $15의 이익을 내는 것은 동일한 평균 값을 가짐에도 불구하고 매우 다릅니다. 두 번째 것은 훨씬 더 큰 변동 정도를 가지며, 따라서 훨씬 더 큰 위험을 나타냅니다. 따라서 확률 변수의 행동을 이해하려면 최소한 하나의 척도가 더 필요할 것입니다: 확률 변수가 얼마나 넓게 요동치는지를 측정하는 척도입니다.</p>
<h3 id="분산-variances"><a class="header" href="#분산-variances">분산 (Variances)</a></h3>
<p>이는 우리로 하여금 확률 변수의 *분산(variance)*을 고려하게 합니다. 이는 확률 변수가 평균으로부터 얼마나 멀리 벗어나는지에 대한 정량적 측정입니다. 식 $X - \mu_X$를 고려해 보십시오. 이것은 확률 변수의 평균으로부터의 편차입니다. 이 값은 양수일 수도 음수일 수도 있으므로, 우리가 편차의 크기를 측정하도록 양수로 만들기 위해 무언가를 해야 합니다.</p>
<p>시도해 볼 수 있는 합리적인 방법은 $|X-\mu_X|$를 살펴보는 것이며, 실제로 이는 *평균 절대 편차(mean absolute deviation)*라고 불리는 유용한 양으로 이어지지만, 수학 및 통계의 다른 분야와의 연결 때문에 사람들은 종종 다른 해결책을 사용합니다.</p>
<p>특히, 그들은 $(X-\mu_X)^2$을 살펴봅니다. 평균을 취하여 이 양의 전형적인 크기를 본다면 분산에 도달하게 됩니다.</p>
<p>$$\sigma_X^2 = \textrm{Var}(X) = E\left[(X-\mu_X)^2\right] = E[X^2] - \mu_X^2.$$ :eqlabel:<code>eq_var_def</code></p>
<p>:eqref:<code>eq_var_def</code>의 마지막 등식은 중간의 정의를 확장하고 기댓값의 속성을 적용함으로써 성립합니다.</p>
<p>확률 $p$로 $a-2$ 값을, 확률 $p$로 $a+2$ 값을, 그리고 확률 $1-2p$로 $a$ 값을 취하는 우리 예제를 살펴봅시다. 이 경우 $\mu_X = a$이므로, $E[X^2]$만 계산하면 됩니다. 이는 쉽게 수행될 수 있습니다.</p>
<p>$$ E\left[X^2\right] = (a-2)^2p + a^2(1-2p) + (a+2)^2p = a^2 + 8p. $$</p>
<p>따라서 :eqref:<code>eq_var_def</code>에 의해 우리의 분산은 다음과 같음을 알 수 있습니다.</p>
<p>$$ \sigma_X^2 = \textrm{Var}(X) = E[X^2] - \mu_X^2 = a^2 + 8p - a^2 = 8p. $$</p>
<p>이 결과 또한 말이 됩니다. $p$가 가질 수 있는 최대값은 $1/2$이며, 이는 동전 던지기로 $a-2$ 또는 $a+2$를 선택하는 것에 해당합니다. 이것의 분산이 $4$인 것은 $a-2$와 $a+2$가 모두 평균에서 $2$ 단위 떨어져 있고 $2^2 = 4$라는 사실에 대응합니다. 스펙트럼의 다른 쪽 끝에서 $p=0$이면, 이 확률 변수는 항상 $a$ 값을 취하므로 분산이 전혀 없습니다.</p>
<p>아래에 분산의 몇 가지 속성을 나열합니다.</p>
<ul>
<li>임의의 확률 변수 $X$에 대해 $\textrm{Var}(X) \ge 0$이며, $\textrm{Var}(X) = 0$인 것은 $X$가 상수인 것과 동등합니다.</li>
<li>임의의 확률 변수 $X$와 숫자 $a, b$에 대해, $\textrm{Var}(aX+b) = a^2\textrm{Var}(X)$가 성립합니다.</li>
<li>두 개의 <em>독립</em> 확률 변수 $X$와 $Y$가 있다면, $\textrm{Var}(X+Y) = \textrm{Var}(X) + \textrm{Var}(Y)$입니다.</li>
</ul>
<p>이러한 값들을 해석할 때 약간의 걸림돌이 있을 수 있습니다. 특히, 이 계산 과정에서 단위를 추적한다고 상상해 봅시다. 웹페이지의 제품에 할당된 별점 평점을 다루고 있다고 가정해 봅시다. 그러면 $a, a-2, a+2$는 모두 별 단위로 측정됩니다. 마찬가지로 평균 $\mu_X$도 별 단위로 측정됩니다(가중 평균이므로). 그러나 분산에 이르면 즉시 문제에 직면하는데, 우리가 $(X-\mu_X)^2$을 보고 싶어 하며 이는 <em>제곱된 별</em> 단위이기 때문입니다. 이는 분산 자체가 원래의 측정값과 비교 가능하지 않음을 의미합니다. 이를 해석 가능하게 만들려면 원래의 단위로 돌아가야 할 것입니다.</p>
<h3 id="표준-편차-standard-deviations"><a class="header" href="#표준-편차-standard-deviations">표준 편차 (Standard Deviations)</a></h3>
<p>이 요약 통계량은 항상 분산의 제곱근을 취함으로써 유도될 수 있습니다! 따라서 우리는 *표준 편차(standard deviation)*를 다음과 같이 정의합니다.</p>
<p>$$ \sigma_X = \sqrt{\textrm{Var}(X)}. $$</p>
<p>우리 예제에서 이는 이제 표준 편차가 $\sigma_X = 2\sqrt{2p}$임을 의미합니다. 별점 리뷰 예제에 대해 단위를 다루고 있다면, $\sigma_X$는 다시 별 단위입니다.</p>
<p>분산에 대해 가졌던 속성들은 표준 편차에 대해서도 다시 기술될 수 있습니다.</p>
<ul>
<li>임의의 확률 변수 $X$에 대해 $\sigma_{X} \ge 0$입니다.</li>
<li>임의의 확률 변수 $X$와 숫자 $a, b$에 대해, $\sigma_{aX+b} = |a|\sigma_{X}$입니다.</li>
<li>두 개의 <em>독립</em> 확률 변수 $X$와 $Y$가 있다면, $\sigma_{X+Y} = \sqrt{\sigma_{X}^2 + \sigma_{Y}^2}$입니다.</li>
</ul>
<p>이 시점에서</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="최대-우도-maximum-likelihood"><a class="header" href="#최대-우도-maximum-likelihood">최대 우도 (Maximum Likelihood)</a></h1>
<p>:label:<code>sec_maximum_likelihood</code></p>
<p>머신러닝에서 가장 흔히 마주치는 사고방식 중 하나는 최대 우도 관점입니다. 이것은 알려지지 않은 파라미터가 있는 확률 모델로 작업할 때, 데이터를 가장 높은 확률로 만드는 파라미터가 가장 가능성 있는 것이라는 개념입니다.</p>
<h2 id="최대-우도-원칙-the-maximum-likelihood-principle"><a class="header" href="#최대-우도-원칙-the-maximum-likelihood-principle">최대 우도 원칙 (The Maximum Likelihood Principle)</a></h2>
<p>생각해보면 도움이 될 수 있는 베이지안 해석이 있습니다. 파라미터 $\boldsymbol{\theta}$를 가진 모델과 데이터 예제 모음 $X$가 있다고 가정합시다. 구체적으로, $\boldsymbol{\theta}$가 동전을 던졌을 때 앞면이 나올 확률을 나타내는 단일 값이고, $X$가 독립적인 동전 던지기 시퀀스라고 상상할 수 있습니다. 이 예제는 나중에 자세히 살펴볼 것입니다.</p>
<p>우리 모델의 파라미터에 대해 가장 가능성 있는 값을 찾고 싶다면, 이는 다음을 찾고 싶다는 의미입니다.</p>
<p>$$\mathop{\mathrm{argmax}} P(\boldsymbol{\theta}\mid X).$$ :eqlabel:<code>eq_max_like</code></p>
<p>베이즈 규칙에 의해 이는 다음과 같습니다.</p>
<p>$$ \mathop{\mathrm{argmax}} \frac{P(X \mid \boldsymbol{\theta})P(\boldsymbol{\theta})}{P(X)}. $$</p>
<p>데이터를 생성하는 파라미터에 구애받지 않는 확률인 $P(X)$ 식은 $\boldsymbol{\theta}$에 전혀 의존하지 않으므로, $\boldsymbol{\theta}$의 최선의 선택을 바꾸지 않고도 삭제할 수 있습니다. 마찬가지로, 우리는 이제 어떤 파라미터 세트가 다른 것보다 더 낫다는 사전 가정이 없다고 상정할 수 있으므로, $P(\boldsymbol{\theta})$도 세타에 의존하지 않는다고 선언할 수 있습니다! 예를 들어 동전 던지기 예제에서 앞면이 나올 확률은 공정한지 여부에 대한 사전 믿음 없이 $[0,1]$의 어떤 값이라도 될 수 있습니다(종종 *무정보 사전 분포(uninformative prior)*라고 함). 따라서 베이즈 규칙의 적용은 $\boldsymbol{\theta}$의 최선의 선택이 $\boldsymbol{\theta}$에 대한 최대 우도 추정치임을 보여줍니다.</p>
<p>$$ \hat{\boldsymbol{\theta}} = \mathop{\mathrm{argmax}} _ {\boldsymbol{\theta}} P(X \mid \boldsymbol{\theta}). $$</p>
<p>일반적인 용어로서, 파라미터가 주어졌을 때 데이터의 확률($P(X \mid \boldsymbol{\theta})$)을 *우도(likelihood)*라고 합니다.</p>
<h3 id="구체적인-예-a-concrete-example"><a class="header" href="#구체적인-예-a-concrete-example">구체적인 예 (A Concrete Example)</a></h3>
<p>구체적인 예에서 이것이 어떻게 작동하는지 살펴봅시다. 동전 던지기가 앞면일 확률을 나타내는 단일 파라미터 $\theta$가 있다고 가정합시다. 그러면 뒷면이 나올 확률은 $1-\theta$이므로, 관찰된 데이터 $X$가 앞면 $n_H$번과 뒷면 $n_T$번인 시퀀스라면, 독립 확률은 곱해진다는 사실을 사용하여 다음을 알 수 있습니다.</p>
<p>$$ P(X \mid \theta) = \theta^{n_H}(1-\theta)^{n_T}. $$</p>
<p>동전을 $13$번 던져서 "HHHTHTTHHHHHT" 시퀀스를 얻었다면($n_H = 9, n_T = 4$), 다음과 같음을 알 수 있습니다.</p>
<p>$$ P(X \mid \theta) = \theta^9(1-\theta)^4. $$</p>
<p>이 예제의 한 가지 좋은 점은 답을 미리 알고 있다는 것입니다. 실제로 말로 "동전을 13번 던져서 9번이 앞면이 나왔다면, 동전이 앞면으로 나올 확률에 대한 우리의 최선의 추측은 무엇일까요?"라고 묻는다면 누구나 정확하게 $9/13$라고 추측할 것입니다. 이 최대 우도 방법이 우리에게 줄 것은 훨씬 더 복잡한 상황으로 일반화될 수 있는 방식으로 제1원리로부터 그 숫자를 얻는 방법입니다.</p>
<p>우리 예제의 경우, $P(X \mid \theta)$의 플롯은 다음과 같습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()

theta = np.arange(0, 1, 0.001)
p = theta**9 * (1 - theta)**4.

d2l.plot(theta, p, 'theta', 'likelihood')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch

theta = torch.arange(0, 1, 0.001)
p = theta**9 * (1 - theta)**4.

d2l.plot(theta, p, 'theta', 'likelihood')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf

theta = tf.range(0, 1, 0.001)
p = theta**9 * (1 - theta)**4.

d2l.plot(theta, p, 'theta', 'likelihood')
</code></pre>
<p>이것은 우리가 예상한 $9/13 \approx 0.7\ldots$ 근처 어딘가에서 최대값을 갖습니다. 정확히 그곳에 있는지 확인하기 위해 미적분학으로 눈을 돌릴 수 있습니다. 최대값에서 함수의 기울기는 평평하다는 점에 유의하십시오. 따라서 우리는 도함수가 0인 $\theta$ 값을 찾고 가장 높은 확률을 주는 값을 찾음으로써 최대 우도 추정치 :eqref:<code>eq_max_like</code>를 찾을 수 있습니다. 다음과 같이 계산합니다.</p>
<p>$$ \begin{aligned}
0 &amp; = \frac{d}{d\theta} P(X \mid \theta) \ &amp; = \frac{d}{d\theta} \theta^9(1-\theta)^4 \ &amp; = 9\theta^8(1-\theta)^4 - 4\theta^9(1-\theta)^3 \ &amp; = \theta^8(1-\theta)^3(9-13\theta).
\end{aligned} $$</p>
<p>이는 세 가지 해를 갖습니다: $0, 1, 9/13$. 처음 두 개는 우리 시퀀스에 확률 0을 할당하므로 최대값이 아니라 최소값임이 분명합니다. 마지막 값은 우리 시퀀스에 0이 아닌 확률을 할당하므로 최대 우도 추정치 $\hat \theta = 9/13$여야 합니다.</p>
<h2 id="수치-최적화와-음의-로그-우도-numerical-optimization-and-the-negative-log-likelihood"><a class="header" href="#수치-최적화와-음의-로그-우도-numerical-optimization-and-the-negative-log-likelihood">수치 최적화와 음의 로그 우도 (Numerical Optimization and the Negative Log-Likelihood)</a></h2>
<p>이전 예제는 좋지만, 수십억 개의 파라미터와 데이터 예제가 있다면 어떨까요?</p>
<p>먼저, 모든 데이터 예제가 독립적이라는 가정을 한다면, 많은 확률의 곱이 되기 때문에 우도 자체를 실질적으로 고려할 수 없다는 점에 유의하십시오. 실제로 각 확률은 $[0,1]$에 있고, 가령 전형적인 값이 약 $1/2$라면, $(1/2)^{1000000000}$의 곱은 기계 정밀도보다 훨씬 낮습니다. 우리는 그것을 직접 다룰 수 없습니다.</p>
<p>하지만 로그가 곱을 합으로 바꾼다는 사실을 상기하면 다음과 같습니다.</p>
<p>$$ \log((1/2)^{1000000000}) = 1000000000\cdot\log(1/2) \approx -301029995.6\ldots $$</p>
<p>이 숫자는 단정밀도 32비트 부동 소수점 내에도 완벽하게 들어맞습니다. 따라서 우리는 다음과 같은 *로그 우도(log-likelihood)*를 고려해야 합니다.</p>
<p>$$ \log(P(X \mid \boldsymbol{\theta})). $$</p>
<p>함수 $x \mapsto \log(x)$가 증가 함수이므로, 우도를 최대화하는 것은 로그 우도를 최대화하는 것과 같습니다. 실제로 :numref:<code>sec_naive_bayes</code>에서 우리는 나이브 베이즈 분류기의 특정 예제를 다룰 때 이 추론이 적용되는 것을 보게 될 것입니다.</p>
<p>우리는 종종 손실을 최소화하고 싶은 손실 함수로 작업합니다. 우리는 최대 우도를 $-\log(P(X \mid \boldsymbol{\theta}))$를 취함으로써 손실 최소화로 바꿀 수 있으며, 이것이 *음의 로그 우도(negative log-likelihood)*입니다.</p>
<p>이를 설명하기 위해 이전의 동전 던지기 문제를 고려하고 폐쇄형 해를 모른다고 가정해 봅시다. 우리는 다음을 계산할 수 있습니다.</p>
<p>$$ -\log(P(X \mid \boldsymbol{\theta})) = -\log(\theta^{n_H}(1-\theta)^{n_T}) = -(n_H\log(\theta) + n_T\log(1-\theta)). $$</p>
<p>이것은 코드로 작성될 수 있으며 수십억 번의 동전 던지기에 대해서도 자유롭게 최적화될 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 데이터 설정
n_H = 8675309
n_T = 256245

# 파라미터 초기화
theta = np.array(0.5)
theta.attach_grad()

# 경사 하강법 수행
lr = 1e-9
for iter in range(100):
    with autograd.record():
        loss = -(n_H * np.log(theta) + n_T * np.log(1 - theta))
    loss.backward()
    theta -= lr * theta.grad

# 출력 확인
theta, n_H / (n_H + n_T)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 데이터 설정
n_H = 8675309
n_T = 256245

# 파라미터 초기화
theta = torch.tensor(0.5, requires_grad=True)

# 경사 하강법 수행
lr = 1e-9
for iter in range(100):
    loss = -(n_H * torch.log(theta) + n_T * torch.log(1 - theta))
    loss.backward()
    with torch.no_grad():
        theta -= lr * theta.grad
    theta.grad.zero_()

# 출력 확인
theta, n_H / (n_H + n_T)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 데이터 설정
n_H = 8675309
n_T = 256245

# 파라미터 초기화
theta = tf.Variable(tf.constant(0.5))

# 경사 하강법 수행
lr = 1e-9
for iter in range(100):
    with tf.GradientTape() as t:
        loss = -(n_H * tf.math.log(theta) + n_T * tf.math.log(1 - theta))
    theta.assign_sub(lr * t.gradient(loss, theta))

# 출력 확인
theta, n_H / (n_H + n_T)
</code></pre>
<p>수치적 편의성만이 사람들이 음의 로그 우도를 사용하기 좋아하는 유일한 이유는 아닙니다. 그것이 선호되는 다른 여러 이유가 있습니다.</p>
<p>로그 우도를 고려하는 두 번째 이유는 미적분 규칙의 단순화된 적용입니다. 위에서 논의한 대로 독립성 가정 때문에 머신러닝에서 마주치는 대부분의 확률은 개별 확률의 곱입니다.</p>
<p>$$ P(X\mid\boldsymbol{\theta}) = p(x_1\mid\boldsymbol{\theta})\cdot p(x_2\mid\boldsymbol{\theta})\cdots p(x_n\mid\boldsymbol{\theta}). $$</p>
<p>이는 우리가 도함수를 계산하기 위해 곱의 미분법을 직접 적용한다면 다음을 얻음을 의미합니다.</p>
<p>$$ \begin{aligned}
\frac{\partial}{\partial \boldsymbol{\theta}} P(X\mid\boldsymbol{\theta}) &amp; = \left(\frac{\partial}{\partial \boldsymbol{\theta}}P(x_1\mid\boldsymbol{\theta})\right)\cdot P(x_2\mid\boldsymbol{\theta})\cdots P(x_n\mid\boldsymbol{\theta}) \ &amp; \quad + P(x_1\mid\boldsymbol{\theta})\cdot \left(\frac{\partial}{\partial \boldsymbol{\theta}}P(x_2\mid\boldsymbol{\theta})\right)\cdots P(x_n\mid\boldsymbol{\theta}) \ &amp; \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \vdots \ &amp; \quad + P(x_1\mid\boldsymbol{\theta})\cdot P(x_2\mid\boldsymbol{\theta}) \cdots \left(\frac{\partial}{\partial \boldsymbol{\theta}}P(x_n\mid\boldsymbol{\theta})\right).
\end{aligned} $$</p>
<p>이것은 $(n-1)$번의 덧셈과 함께 $n(n-1)$번의 곱셈을 필요로 하므로 입력에 대해 시간적으로 이차식에 비례합니다! 항들을 그룹화하는 충분한 기발함은 이를 선형 시간으로 줄여주겠지만 생각이 필요합니다. 음의 로그 우도의 경우 대신 다음을 갖습니다.</p>
<p>$$ -\log\left(P(X\mid\boldsymbol{\theta})\right) = -\log(P(x_1\mid\boldsymbol{\theta})) - \log(P(x_2\mid\boldsymbol{\theta})) \cdots - \log(P(x_n\mid\boldsymbol{\theta})), $$</p>
<p>그러면 다음을 제공합니다.</p>
<p>$$ - \frac{\partial}{\partial \boldsymbol{\theta}} \log\left(P(X\mid\boldsymbol{\theta})\right) = \frac{1}{P(x_1\mid\boldsymbol{\theta})}\left(\frac{\partial}{\partial \boldsymbol{\theta}}P(x_1\mid\boldsymbol{\theta})\right) + \cdots + \frac{1}{P(x_n\mid\boldsymbol{\theta})}\left(\frac{\partial}{\partial \boldsymbol{\theta}}P(x_n\mid\boldsymbol{\theta})\right). $$</p>
<p>이것은 단지 $n$번의 나눗셈과 $n-1$번의 합계만 필요로 하므로 입력에 대해 선형 시간입니다.</p>
<p>음의 로그 우도를 고려하는 세 번째이자 마지막 이유는 :numref:<code>sec_information_theory</code>에서 자세히 논의할 정보 이론과의 관계입니다. 이것은 확률 변수에서의 정보 또는 무작위성의 정도를 측정하는 방법을 제공하는 엄격한 수학적 이론입니다. 해당 분야의 주요 연구 대상은 다음과 같은 엔트로피입니다.</p>
<p>$$ H(p) = -\sum_{i} p_i \log_2(p_i), $$</p>
<p>소스의 무작위성을 측정합니다. 이것이 평균 $-\log$ 확률에 지나지 않는다는 점에 주목하십시오. 따라서 우리의 음의 로그 우도를 데이터 예제의 수로 나누면 크로스 엔트로피(cross-entropy)라고 알려진 엔트로피의 친척을 얻게 됩니다. 이 이론적 해석만으로도 모델 성능을 측정하는 방법으로 데이터셋 전체에 대한 평균 음의 로그 우도를 보고하도록 동기를 부여하기에 충분히 설득력이 있습니다.</p>
<h2 id="연속-변수에-대한-최대-우도-maximum-likelihood-for-continuous-variables"><a class="header" href="#연속-변수에-대한-최대-우도-maximum-likelihood-for-continuous-variables">연속 변수에 대한 최대 우도 (Maximum Likelihood for Continuous Variables)</a></h2>
<p>지금까지 우리가 한 모든 것은 이산 확률 변수로 작업한다고 가정했지만, 연속 확률 변수로 작업하고 싶다면 어떨까요?</p>
<p>짧은 요약은 확률의 모든 인스턴스를 확률 밀도로 대체하는 것 외에는 전혀 변하는 것이 없다는 것입니다. 밀도를 소문자 $p$로 쓴다는 점을 상기하면, 이는 예를 들어 이제 다음과 같이 말함을 의미합니다.</p>
<p>$$ -\log\left(p(X\mid\boldsymbol{\theta})\right) = -\log(p(x_1\mid\boldsymbol{\theta})) - \log(p(x_2\mid\boldsymbol{\theta})) \cdots - \log(p(x_n\mid\boldsymbol{\theta})) = -\sum_i \log(p(x_i \mid \theta)). $$</p>
<p>질문은 "왜 이것이 괜찮은가?"가 됩니다. 결국, 우리가 밀도를 도입한 이유는 특정 결과를 얻을 확률 자체가 0이었기 때문인데, 그렇다면 임의의 파라미터 세트에 대해 우리 데이터를 생성할 확률은 0이 아닐까요?</p>
<p>실제로 그렇습니다. 그리고 왜 우리가 밀도로 전환할 수 있는지 이해하는 것은 입실론(epsilons)에 어떤 일이 일어나는지 추적하는 연습입니다.</p>
<p>먼저 우리의 목표를 재정의해 봅시다. 연속 확률 변수에 대해 더 이상 정확히 맞는 값을 얻을 확률이 아니라 대신 어떤 범위 $\epsilon$ 이내에 맞추는 것을 계산하고 싶다고 가정합시다. 단순함을 위해 우리 데이터가 독립 동일 분포(i.i.d.) 확률 변수 $X_1, \ldots, X_N$의 반복된 관찰 $x_1, \ldots, x_N$이라고 가정합니다. 이전에 보았듯이 이는 다음과 같이 쓰일 수 있습니다.</p>
<p>$$ \begin{aligned}
&amp;P(X_1 \in [x_1, x_1+\epsilon], X_2 \in [x_2, x_2+\epsilon], \ldots, X_N \in [x_N, x_N+\epsilon]\mid\boldsymbol{\theta}) \ &amp;\approx \epsilon^Np(x_1\mid\boldsymbol{\theta})\cdot p(x_2\mid\boldsymbol{\theta}) \cdots p(x_n\mid\boldsymbol{\theta}).
\end{aligned} $$</p>
<p>따라서 여기에 음의 로그를 취하면 다음을 얻습니다.</p>
<p>$$ \begin{aligned}
&amp;-\log(P(X_1 \in [x_1, x_1+\epsilon], X_2 \in [x_2, x_2+\epsilon], \ldots, X_N \in [x_N, x_N+\epsilon]\mid\boldsymbol{\theta})) \ &amp;\approx -N\log(\epsilon) - \sum_{i} \log(p(x_i\mid\boldsymbol{\theta})).
\end{aligned} $$</p>
<p>이 식을 살펴보면, $\epsilon$이 발생하는 유일한 곳은 가산 상수 $-N\log(\epsilon)$입니다. 이것은 파라미터 $\boldsymbol{\theta}$에 전혀 의존하지 않으므로, $\boldsymbol{\theta}$의 최적 선택은 우리의 $\epsilon$ 선택에 의존하지 않습니다! 우리가 네 자리 숫자를 요구하든 사백 자리 숫자를 요구하든, $\boldsymbol{\theta}$의 최적 선택은 동일하게 유지되므로 우리는 입실론을 자유롭게 삭제하여 우리가 최적화하고 싶은 것이 다음과 같음을 볼 수 있습니다.</p>
<p>$$ - \sum_{i} \log(p(x_i\mid\boldsymbol{\theta})). $$</p>
<p>따라서 우리는 확률을 확률 밀도로 대체함으로써 연속 확률 변수에서도 이산 확률 변수에서와 같이 쉽게 최대 우도 관점을 운용할 수 있음을 알 수 있습니다.</p>
<h2 id="요약-summary-122"><a class="header" href="#요약-summary-122">요약 (Summary)</a></h2>
<ul>
<li>최대 우도 원칙은 주어진 데이터셋에 대해 가장 적합한 모델이 데이터를 가장 높은 확률로 생성하는 모델이라는 것을 알려줍니다.</li>
<li>사람들은 종종 수치적 안정성, 곱을 합으로 변환(및 그에 따른 기울기 계산의 단순화), 정보 이론과의 이론적 연계 등 다양한 이유로 음의 로그 우도를 대신 사용합니다.</li>
<li>이산 설정에서 동기를 부여하는 것이 가장 간단하지만, 데이터 포인트에 할당된 확률 밀도를 최대화함으로써 연속 설정으로도 자유롭게 일반화될 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-137"><a class="header" href="#연습-문제-exercises-137">연습 문제 (Exercises)</a></h2>
<ol>
<li>비음수 확률 변수가 어떤 값 $\alpha&gt;0$에 대해 $\alpha e^{-\alpha x}$ 밀도를 갖는다는 것을 안다고 가정합시다. 확률 변수로부터 숫자 $3$인 단일 관찰을 얻었습니다. $\alpha$에 대한 최대 우도 추정치는 무엇입니까?</li>
<li>알려지지 않은 평균을 갖지만 분산이 $1$인 가우스 분포에서 추출된 샘플 {x_i}_{i=1}^N 데이터셋이 있다고 가정합시다. 평균에 대한 최대 우도 추정치는 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/416">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1096">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1097">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="분포-distributions"><a class="header" href="#분포-distributions">분포 (Distributions)</a></h1>
<p>:label:<code>sec_distributions</code></p>
<p>이산형 및 연속형 설정 모두에서 확률을 다루는 방법을 배웠으므로, 이제 흔히 마주치는 몇 가지 일반적인 분포를 알아봅시다. 머신러닝 분야에 따라 이보다 훨씬 더 많은 분포에 익숙해져야 할 수도 있고, 딥러닝의 일부 분야에서는 아예 필요하지 않을 수도 있습니다. 하지만 이것은 익숙해지기에 좋은 기본 목록입니다. 먼저 몇 가지 일반적인 라이브러리를 가져옵시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from math import erf, factorial
import numpy as np
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
from IPython import display
from math import erf, factorial
import torch

torch.pi = torch.acos(torch.zeros(1)) * 2  # torch에서 pi 정의
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
from IPython import display
from math import erf, factorial
import tensorflow as tf
import tensorflow_probability as tfp

tf.pi = tf.acos(tf.zeros(1)) * 2  # TensorFlow에서 pi 정의
</code></pre>
<h2 id="베르누이-분포-bernoulli"><a class="header" href="#베르누이-분포-bernoulli">베르누이 분포 (Bernoulli)</a></h2>
<p>이것은 일반적으로 마주치는 가장 단순한 확률 변수입니다. 이 확률 변수는 $p$의 확률로 $1$이 나오고 $1-p$의 확률로 $0$이 나오는 동전 던지기를 인코딩합니다. 이 분포를 가진 확률 변수 $X$가 있다면 다음과 같이 씁니다.</p>
<p>$$
X sim 	extrm{Bernoulli}(p).
$$</p>
<p>누적 분포 함수는 다음과 같습니다.</p>
<p>$$F(x) = \begin{cases}
0 &amp; x &lt; 0, \
1-p &amp; 0 \le x &lt; 1, \
1 &amp; x &gt;= 1 .
\end{cases}$$
:eqlabel:<code>eq_bernoulli_cdf</code></p>
<p>확률 질량 함수(pmf)는 아래에 플롯되어 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
p = 0.3

d2l.set_figsize()
d2l.plt.stem([0, 1], [1 - p, p], use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
</code></pre>
<p>이제 누적 분포 함수 :eqref:<code>eq_bernoulli_cdf</code>를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.arange(-1, 2, 0.01)

def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; 1 else 1 - p

d2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.arange(-1, 2, 0.01)

def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; 1 else 1 - p

d2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.range(-1, 2, 0.01)

def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; 1 else 1 - p

d2l.plot(x, tf.constant([F(y) for y in x]), 'x', 'c.d.f.')
</code></pre>
<p>$X sim 	extrm{Bernoulli}(p)$이면 다음이 성립합니다.</p>
<ul>
<li>$\mu_X = p$,</li>
<li>$\sigma_X^2 = p(1-p)$.</li>
</ul>
<p>다음과 같이 베르누이 확률 변수로부터 임의의 모양의 배열을 샘플링할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
1*(np.random.rand(10, 10) &lt; p)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
1*(torch.rand(10, 10) &lt; p)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
tf.cast(tf.random.uniform((10, 10)) &lt; p, dtype=tf.float32)
</code></pre>
<h2 id="이산-균등-분포-discrete-uniform"><a class="header" href="#이산-균등-분포-discrete-uniform">이산 균등 분포 (Discrete Uniform)</a></h2>
<p>다음으로 흔히 마주치는 확률 변수는 이산 균등 분포입니다. 여기서 논의를 위해 정수 ${1, 2, \ldots, n}$에서 지원된다고 가정하겠지만, 다른 어떤 값의 집합도 자유롭게 선택할 수 있습니다. 이 문맥에서 *균등(uniform)*이라는 단어의 의미는 모든 가능한 값이 동등하게 가능성이 높다는 것입니다. 각 값 $i sin {1, 2, 3, \ldots, n}$에 대한 확률은 $p_i = \frac{1}{n}$입니다. 이 분포를 가진 확률 변수 $X$를 다음과 같이 나타낼 것입니다.</p>
<p>$$
X sim U(n).
$$</p>
<p>누적 분포 함수는 다음과 같습니다.</p>
<p>$$F(x) = \begin{cases}
0 &amp; x &lt; 1, \
\frac{k}{n} &amp; k \le x &lt; k+1 \textrm{ 이며 } 1 \le k &lt; n, \
1 &amp; x &gt;= n .
\end{cases}$$
:eqlabel:<code>eq_discrete_uniform_cdf</code></p>
<p>먼저 확률 질량 함수를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
n = 5

d2l.plt.stem([i+1 for i in range(n)], n*[1 / n], use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
</code></pre>
<p>이제 누적 분포 함수 :eqref:<code>eq_discrete_uniform_cdf</code>를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.arange(-1, 6, 0.01)

def F(x):
    return 0 if x &lt; 1 else 1 if x &gt; n else np.floor(x) / n

d2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.arange(-1, 6, 0.01)

def F(x):
    return 0 if x &lt; 1 else 1 if x &gt; n else torch.floor(x) / n

d2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.range(-1, 6, 0.01)

def F(x):
    return 0 if x &lt; 1 else 1 if x &gt; n else tf.floor(x) / n

d2l.plot(x, [F(y) for y in x], 'x', 'c.d.f.')
</code></pre>
<p>$X sim U(n)$이면 다음이 성립합니다.</p>
<ul>
<li>$\mu_X = \frac{1+n}{2}$,</li>
<li>$\sigma_X^2 = \frac{n^2-1}{12}$.</li>
</ul>
<p>다음과 같이 이산 균등 확률 변수로부터 임의의 모양의 배열을 샘플링할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
np.random.randint(1, n, size=(10, 10))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
torch.randint(1, n, size=(10, 10))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
tf.random.uniform((10, 10), 1, n, dtype=tf.int32)
</code></pre>
<h2 id="연속-균등-분포-continuous-uniform"><a class="header" href="#연속-균등-분포-continuous-uniform">연속 균등 분포 (Continuous Uniform)</a></h2>
<p>다음으로 연속 균등 분포에 대해 논의해 봅시다. 이 확률 변수 뒤에 숨겨진 아이디어는 이산 균등 분포에서 $n$을 늘리고 구간 $[a, b]$ 내에 맞도록 스케일을 조정하면, $[a, b]$ 내의 임의의 값을 모두 동일한 확률로 선택하는 연속 확률 변수에 접근하게 된다는 것입니다. 이 분포를 다음과 같이 나타낼 것입니다.</p>
<p>$$
X sim U(a, b).
$$</p>
<p>확률 밀도 함수(pdf)는 다음과 같습니다.</p>
<p>$$p(x) = \begin{cases}
\frac{1}{b-a} &amp; x sin [a, b], \
0 &amp; x \notsin [a, b].
\end{cases}$$
:eqlabel:<code>eq_cont_uniform_pdf</code></p>
<p>누적 분포 함수는 다음과 같습니다.</p>
<p>$$F(x) = \begin{cases}
0 &amp; x &lt; a, \
\frac{x-a}{b-a} &amp; x sin [a, b], \
1 &amp; x &gt;= b .
\end{cases}$$
:eqlabel:<code>eq_cont_uniform_cdf</code></p>
<p>먼저 확률 밀도 함수 :eqref:<code>eq_cont_uniform_pdf</code>를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
a, b = 1, 3

x = np.arange(0, 4, 0.01)
p = (x &gt; a)*(x &lt; b)/(b - a)

d2l.plot(x, p, 'x', 'p.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
a, b = 1, 3

x = torch.arange(0, 4, 0.01)
p = (x &gt; a).type(torch.float32)*(x &lt; b).type(torch.float32)/(b-a)
d2l.plot(x, p, 'x', 'p.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
a, b = 1, 3

x = tf.range(0, 4, 0.01)
p = tf.cast(x &gt; a, tf.float32) * tf.cast(x &lt; b, tf.float32) / (b - a)
d2l.plot(x, p, 'x', 'p.d.f.')
</code></pre>
<p>이제 누적 분포 함수 :eqref:<code>eq_cont_uniform_cdf</code>를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def F(x):
    return 0 if x &lt; a else 1 if x &gt; b else (x - a) / (b - a)

d2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def F(x):
    return 0 if x &lt; a else 1 if x &gt; b else (x - a) / (b - a)

d2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def F(x):
    return 0 if x &lt; a else 1 if x &gt; b else (x - a) / (b - a)

d2l.plot(x, [F(y) for y in x], 'x', 'c.d.f.')
</code></pre>
<p>$X sim U(a, b)$이면 다음이 성립합니다.</p>
<ul>
<li>$\mu_X = \frac{a+b}{2}$,</li>
<li>$\sigma_X^2 = \frac{(b-a)^2}{12}$.</li>
</ul>
<p>다음과 같이 균등 확률 변수로부터 임의의 모양의 배열을 샘플링할 수 있습니다. 기본적으로 $U(0,1)$에서 샘플링하므로 다른 범위를 원하면 스케일을 조정해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
(b - a) * np.random.rand(10, 10) + a
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
(b - a) * torch.rand(10, 10) + a
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
(b - a) * tf.random.uniform((10, 10)) + a
</code></pre>
<h2 id="이항-분포-binomial"><a class="header" href="#이항-분포-binomial">이항 분포 (Binomial)</a></h2>
<p>상황을 좀 더 복잡하게 만들어 <em>이항(binomial)</em> 확률 변수를 살펴봅시다. 이 확률 변수는 성공 확률이 $p$인 $n$개의 독립적인 실험 시퀀스를 수행하고, 우리가 얼마나 많은 성공을 볼 것으로 기대하는지 묻는 것에서 비롯됩니다.</p>
<p>이를 수학적으로 표현해 봅시다. 각 실험은 독립 확률 변수 $X_i$이며, 여기서 성공을 인코딩하기 위해 $1$을 사용하고 실패를 인코딩하기 위해 $0$을 사용합니다. 각각이 확률 $p$로 성공하는 독립적인 동전 던지기이므로, $X_i sim 	extrm{Bernoulli}(p)$라고 말할 수 있습니다. 그러면 이항 확률 변수는 다음과 같습니다.</p>
<p>$$
X = \sum_{i=1}^n X_i.
$$</p>
<p>이 경우 다음과 같이 씁니다.</p>
<p>$$
X sim 	extrm{Binomial}(n, p).
$$</p>
<p>누적 분포 함수를 얻으려면, 정확히 $k$번 성공하는 것이 $inom{n}{k} = \frac{n!}{k!(n-k)!}$가지 방식으로 발생할 수 있고 각 방식은 발생 확률 $p^k(1-p)^{n-k}$를 갖는다는 점에 유의해야 합니다. 따라서 누적 분포 함수는 다음과 같습니다.</p>
<p>$$F(x) = \begin{cases}
0 &amp; x &lt; 0, \
\sum_{m \le k} inom{n}{m} p^m(1-p)^{n-m}  &amp; k \le x &lt; k+1 \textrm{ 이며 } 0 \le k &lt; n, \
1 &amp; x &gt;= n .
\end{cases}$$
:eqlabel:<code>eq_binomial_cdf</code></p>
<p>먼저 확률 질량 함수를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
n, p = 10, 0.2

# 이항 계수 계산
def binom(n, k):
    comb = 1
    for i in range(min(k, n - k)):
        comb = comb * (n - i) // (i + 1)
    return comb

pmf = np.array([p**i * (1-p)**(n - i) * binom(n, i) for i in range(n + 1)])

d2l.plt.stem([i for i in range(n + 1)], pmf, use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
n, p = 10, 0.2

# 이항 계수 계산
def binom(n, k):
    comb = 1
    for i in range(min(k, n - k)):
        comb = comb * (n - i) // (i + 1)
    return comb

pmf = d2l.tensor([p**i * (1-p)**(n - i) * binom(n, i) for i in range(n + 1)])

d2l.plt.stem([i for i in range(n + 1)], pmf, use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
n, p = 10, 0.2

# 이항 계수 계산
def binom(n, k):
    comb = 1
    for i in range(min(k, n - k)):
        comb = comb * (n - i) // (i + 1)
    return comb

pmf = tf.constant([p**i * (1-p)**(n - i) * binom(n, i) for i in range(n + 1)])

d2l.plt.stem([i for i in range(n + 1)], pmf, use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
</code></pre>
<p>이제 누적 분포 함수 :eqref:<code>eq_binomial_cdf</code>를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.arange(-1, 11, 0.01)
cmf = np.cumsum(pmf)

def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; n else cmf[int(x)]

d2l.plot(x, np.array([F(y) for y in x.tolist()]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.arange(-1, 11, 0.01)
cmf = torch.cumsum(pmf, dim=0)

def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; n else cmf[int(x)]

d2l.plot(x, torch.tensor([F(y) for y in x.tolist()]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.range(-1, 11, 0.01)
cmf = tf.cumsum(pmf)

def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; n else cmf[int(x)]

d2l.plot(x, [F(y) for y in x.numpy().tolist()], 'x', 'c.d.f.')
</code></pre>
<p>$X sim 	extrm{Binomial}(n, p)$이면 다음이 성립합니다.</p>
<ul>
<li>$\mu_X = np$,</li>
<li>$\sigma_X^2 = np(1-p)$.</li>
</ul>
<p>이는 $n$개의 베르누이 확률 변수의 합에 대한 기댓값의 선형성과, 독립 확률 변수 합의 분산은 분산의 합이라는 사실로부터 따릅니다. 이는 다음과 같이 샘플링될 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
np.random.binomial(n, p, size=(10, 10))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
m = torch.distributions.binomial.Binomial(n, p)
m.sample(sample_shape=(10, 10))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
m = tfp.distributions.Binomial(n, p)
m.sample(sample_shape=(10, 10))
</code></pre>
<h2 id="포아송-분포-poisson"><a class="header" href="#포아송-분포-poisson">포아송 분포 (Poisson)</a></h2>
<p>이제 사고 실험을 해봅시다. 우리는 버스 정류장에 서 있고 다음 1분 동안 몇 대의 버스가 도착할지 알고 싶습니다. 먼저 1분 윈도우 내에 버스가 도착할 확률인 $X^{(1)} sim 	extrm{Bernoulli}(p)$를 고려하는 것으로 시작하겠습니다. 도심에서 멀리 떨어진 버스 정류장의 경우, 이것은 꽤 좋은 근사일 수 있습니다. 우리는 1분 내에 한 대 이상의 버스를 결코 보지 못할 수도 있습니다.</p>
<p>그러나 우리가 붐비는 지역에 있다면 두 대의 버스가 도착할 가능성이 있거나 심지어 높을 수 있습니다. 우리는 우리의 확률 변수를 처음 30초 또는 뒤의 30초에 대한 두 부분으로 나누어 이를 모델링할 수 있습니다. 이 경우 다음과 같이 쓸 수 있습니다.</p>
<p>$$
X^{(2)} sim X^{(2)}_1 + X^{(2)}_2,
$$</p>
<p>여기서 $X^{(2)}$는 총 합이고 $X^{(2)}_i sim 	extrm{Bernoulli}(p/2)$입니다. 그러면 총 분포는 $X^{(2)} sim 	extrm{Binomial}(2, p/2)$가 됩니다.</p>
<p>여기서 멈출 이유가 있을까요? 그 1분을 $n$개 부분으로 계속 나누어 봅시다. 위와 동일한 추론에 의해 다음을 알 수 있습니다.</p>
<p>$$X^{(n)} sim 	extrm{Binomial}(n, p/n).$$
:eqlabel:<code>eq_eq_poisson_approx</code></p>
<p>이러한 확률 변수들을 고려해 보십시오. 이전 섹션에 의해 :eqref:<code>eq_eq_poisson_approx</code>은 평균 $\mu_{X^{(n)}} = n(p/n) = p$와 분산 $\sigma_{X^{(n)}}^2 = n(p/n)(1-(p/n)) = p(1-p/n)$를 가짐을 압니다. 만약 $n
ightarrow \infty$이면, 이러한 수치들이 평균 $\mu_{X^{(\infty)}} = p$와 분산 $\sigma_{X^{(\infty)}}^2 = p$로 안정화되는 것을 볼 수 있습니다. 이는 이 무한 분할 극한에서 우리가 정의할 수 있는 어떤 확률 변수가 <em>있을 수 있음</em>을 나타냅니다.</p>
<p>실제 세계에서 우리는 단순히 버스 도착 횟수를 셀 수 있기 때문에 이것은 그리 놀라운 일이 아니어야 하지만, 우리의 수학적 모델이 잘 정의되어 있다는 것을 보는 것은 좋습니다. 이 논의는 *희귀 사건의 법칙(law of rare events)*으로 공식화될 수 있습니다.</p>
<p>이 추론을 신중하게 따라가면 다음과 같은 모델에 도달할 수 있습니다. 확률 변수 $X$가 ${0,1,2, \ldots}$ 값을 다음 확률로 가질 때 $X sim 	extrm{Poisson}(\lambda)$라고 말할 것입니다.</p>
<p>$$p_k = \frac{\lambda^ke^{-\lambda}}{k!}.$$
:eqlabel:<code>eq_poisson_mass</code></p>
<p>값 $\lambda &gt; 0$은 <em>강도(rate)</em> (또는 <em>형태(shape)</em> 파라미터)라고 알려져 있으며, 단위 시간당 우리가 기대하는 평균 도착 횟수를 나타냅니다.</p>
<p>이 확률 질량 함수를 합산하여 누적 분포 함수를 얻을 수 있습니다.</p>
<p>$$F(x) = \begin{cases}
0 &amp; x &lt; 0, \
e^{-\lambda}\sum_{m = 0}^k rac{\lambda^m}{m!} &amp; k \le x &lt; k+1 \textrm{ 이며 } 0 \le k.
\end{cases}$$
:eqlabel:<code>eq_poisson_cdf</code></p>
<p>먼저 확률 질량 함수 :eqref:<code>eq_poisson_mass</code>를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
lam = 5.0

xs = [i for i in range(20)]
pmf = np.array([np.exp(-lam) * lam**k / factorial(k) for k in xs])

d2l.plt.stem(xs, pmf, use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
lam = 5.0

xs = [i for i in range(20)]
pmf = torch.tensor([torch.exp(torch.tensor(-lam)) * lam**k
                    / factorial(k) for k in xs])

d2l.plt.stem(xs, pmf, use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
lam = 5.0

xs = [i for i in range(20)]
pmf = tf.constant([tf.exp(tf.constant(-lam)).numpy() * lam**k
                    / factorial(k) for k in xs])

d2l.plt.stem(xs, pmf, use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
</code></pre>
<p>이제 누적 분포 함수 :eqref:<code>eq_poisson_cdf</code>를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
x = np.arange(-1, 21, 0.01)
cmf = np.cumsum(pmf)
def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; n else cmf[int(x)]

d2l.plot(x, np.array([F(y) for y in x.tolist()]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
x = torch.arange(-1, 21, 0.01)
cmf = torch.cumsum(pmf, dim=0)
def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; n else cmf[int(x)]

d2l.plot(x, torch.tensor([F(y) for y in x.tolist()]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
x = tf.range(-1, 21, 0.01)
cmf = tf.cumsum(pmf)
def F(x):
    return 0 if x &lt; 0 else 1 if x &gt; n else cmf[int(x)]

d2l.plot(x, [F(y) for y in x.numpy().tolist()], 'x', 'c.d.f.')
</code></pre>
<p>위에서 보았듯이 평균과 분산은 특히 간결합니다. $X sim 	extrm{Poisson}(\lambda)$이면 다음이 성립합니다.</p>
<ul>
<li>$\mu_X = \lambda$,</li>
<li>$\sigma_X^2 = \lambda$.</li>
</ul>
<p>이는 다음과 같이 샘플링될 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
np.random.poisson(lam, size=(10, 10))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
m = torch.distributions.poisson.Poisson(lam)
m.sample((10, 10))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
m = tfp.distributions.Poisson(lam)
m.sample((10, 10))
</code></pre>
<h2 id="가우스-분포-gaussian"><a class="header" href="#가우스-분포-gaussian">가우스 분포 (Gaussian)</a></h2>
<p>이제 다르지만 관련된 실험을 시도해 봅시다. 우리가 다시 $n$개의 독립적인 $	extrm{Bernoulli}(p)$ 측정 $X_i$를 수행한다고 가정합시다. 이들의 합의 분포는 $X^{(n)} sim 	extrm{Binomial}(n, p)$입니다. $n$이 증가하고 $p$가 감소할 때 극한을 취하는 대신, $p$를 고정하고 $n
ightarrow \infty$로 보냅시다. 이 경우 $\mu_{X^{(n)}} = np
ightarrow \infty$이고 $\sigma_{X^{(n)}}^2 = np(1-p)
ightarrow \infty$이므로, 이 극한이 잘 정의될 것이라고 생각할 이유가 없습니다.</p>
<p>그러나 모든 희망이 사라진 것은 아닙니다! 다음과 같이 정의하여 평균과 분산이 잘 작동하도록 만들어 봅시다.</p>
<p>$$
Y^{(n)} = \frac{X^{(n)} - \mu_{X^{(n)}}}{\sigma_{X^{(n)}}}.
$$</p>
<p>이것은 평균 0과 분산 1을 갖는 것을 알 수 있으며, 따라서 어떤 극한 분포로 수렴할 것이라고 믿는 것이 그럴듯합니다. 이러한 분포들이 어떻게 생겼는지 플롯해 보면, 그것이 작동할 것이라고 더욱 확신하게 될 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
p = 0.2
ns = [1, 10, 100, 1000]
d2l.plt.figure(figsize=(10, 3))
for i in range(4):
    n = ns[i]
    pmf = np.array([p**i * (1-p)**(n-i) * binom(n, i) for i in range(n + 1)])
    d2l.plt.subplot(1, 4, i + 1)
    d2l.plt.stem([(i - n*p)/np.sqrt(n*p*(1 - p)) for i in range(n + 1)], pmf,
                 use_line_collection=True)
    d2l.plt.xlim([-4, 4])
    d2l.plt.xlabel('x')
    d2l.plt.ylabel('p.m.f.')
    d2l.plt.title("n = {}".format(n))
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
p = 0.2
ns = [1, 10, 100, 1000]
d2l.plt.figure(figsize=(10, 3))
for i in range(4):
    n = ns[i]
    pmf = torch.tensor([p**i * (1-p)**(n-i) * binom(n, i)
                        for i in range(n + 1)])
    d2l.plt.subplot(1, 4, i + 1)
    d2l.plt.stem([(i - n*p)/torch.sqrt(torch.tensor(n*p*(1 - p)))
                  for i in range(n + 1)], pmf,
                 use_line_collection=True)
    d2l.plt.xlim([-4, 4])
    d2l.plt.xlabel('x')
    d2l.plt.ylabel('p.m.f.')
    d2l.plt.title("n = {}".format(n))
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
p = 0.2
ns = [1, 10, 100, 1000]
d2l.plt.figure(figsize=(10, 3))
for i in range(4):
    n = ns[i]
    pmf = tf.constant([p**i * (1-p)**(n-i) * binom(n, i)
                        for i in range(n + 1)])
    d2l.plt.subplot(1, 4, i + 1)
    d2l.plt.stem([(i - n*p)/tf.sqrt(tf.constant(n*p*(1 - p)))
                  for i in range(n + 1)], pmf,
                 use_line_collection=True)
    d2l.plt.xlim([-4, 4])
    d2l.plt.xlabel('x')
    d2l.plt.ylabel('p.m.f.')
    d2l.plt.title("n = {}".format(n))
d2l.plt.show()
</code></pre>
<p>한 가지 주목할 점은 포아송 사례와 비교하여 이제 표준 편차로 나누고 있다는 점인데, 이는 가능한 결과들을 점점 더 작은 영역으로 짜내고 있다는 것을 의미합니다. 이는 우리의 극한이 더 이상 이산적이지 않고 오히려 연속적일 것임을 나타냅니다.</p>
<p>발생하는 일에 대한 유도는 이 문서의 범위를 벗어나지만, *중심 극한 정리(central limit theorem)*는 $n
ightarrow \infty$에 따라 이것이 가우스 분포(또는 정규 분포)를 낳을 것이라고 기술합니다. 더 명시적으로, 임의의 $a, b$에 대해 다음과 같습니다.</p>
<p>$$
\lim_{n
ightarrow \infty} P(Y^{(n)} sin [a, b]) = P(r(0,1) sin [a, b]),
$$</p>
<p>여기서 우리는 확률 변수가 주어진 평균 $\mu$와 분산 $\sigma^2$를 가진 정규 분포를 따른다고 말하며, $X sim \mathcal N(\mu, \sigma^2)$라고 씁니다. 만약 $X$가 다음과 같은 밀도를 갖는다면 말입니다.</p>
<p>$$p_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.$$
:eqlabel:<code>eq_gaussian_pdf</code></p>
<p>먼저 확률 밀도 함수 :eqref:<code>eq_gaussian_pdf</code>를 플롯해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
mu, sigma = 0, 1

x = np.arange(-3, 3, 0.01)
p = 1 / np.sqrt(2 * np.pi * sigma**2) * np.exp(-(x - mu)**2 / (2 * sigma**2))

d2l.plot(x, p, 'x', 'p.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
mu, sigma = 0, 1

x = torch.arange(-3, 3, 0.01)
p = 1 / torch.sqrt(2 * torch.pi * sigma**2) * torch.exp(
    -(x - mu)**2 / (2 * sigma**2))

d2l.plot(x, p, 'x', 'p.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
mu, sigma = 0, 1

x = tf.range(-3, 3, 0.01)
p = 1 / tf.sqrt(2 * tf.pi * sigma**2) * tf.exp(
    -(x - mu)**2 / (2 * sigma**2))

d2l.plot(x, p, 'x', 'p.d.f.')
</code></pre>
<p>이제 누적 분포 함수를 플롯해 봅시다. 이 부록의 범위를 벗어나지만, 가우스 c.d.f.는 더 기초적인 함수들로 된 닫힌 형식의 공식이 없습니다. 우리는 이 적분을 수치적으로 계산하는 방법을 제공하는 <code>erf</code>를 사용할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def phi(x):
    return (1.0 + erf((x - mu) / (sigma * np.sqrt(2)))) / 2.0

d2l.plot(x, np.array([phi(y) for y in x.tolist()]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def phi(x):
    return (1.0 + erf((x - mu) / (sigma * torch.sqrt(d2l.tensor(2.))))) / 2.0

d2l.plot(x, torch.tensor([phi(y) for y in x.tolist()]), 'x', 'c.d.f.')
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def phi(x):
    return (1.0 + erf((x - mu) / (sigma * tf.sqrt(tf.constant(2.))))) / 2.0

d2l.plot(x, [phi(y) for y in x.numpy().tolist()], 'x', 'c.d.f.')
</code></pre>
<p>눈치 빠른 독자들은 이러한 용어 중 일부를 알아볼 것입니다. 실제로 우리는 이 적분을 :numref:<code>sec_integral_calculus</code>에서 만났습니다. 실제로 이 $p_X(x)$가 총 면적 1을 가지며 따라서 유효한 밀도임을 확인하려면 정확히 그 계산이 필요합니다.</p>
<p>동전 던지기로 작업하기로 한 우리의 선택은 계산을 짧게 만들었지만, 그 선택에 근본적인 것은 아무것도 없었습니다. 실제로 임의의 독립적인 동일 분포 확률 변수 $X_i$ 모음을 취하고 다음을 형성하면</p>
<p>$$
X^{(N)} = \sum_{i=1}^N X_i.
$$</p>
<p>그러면</p>
<p>$$
\frac{X^{(N)} - \mu_{X^{(N)}}}{\sigma_{X^{(N)}}}
$$</p>
<p>은 대략적으로 가우스 분포를 따를 것입니다. 이를 작동시키기 위해 필요한 추가 요구 사항들이 있으며, 가장 흔한 것은 $E[X^4] &lt; \infty$이지만 철학은 명확합니다.</p>
<p>중심 극한 정리는 왜 가우스 분포가 확률, 통계, 머신러닝의 기초가 되는지 설명해 주는 이유입니다. 우리가 측정한 무언가가 많은 작은 독립적인 기여의 합이라고 말할 수 있을 때마다, 측정되는 대상이 가우스 분포에 가까울 것이라고 가정할 수 있습니다.</p>
<p>가우스 분포에는 훨씬 더 많은 흥미로운 속성들이 있으며, 여기서 하나 더 논의하고 싶습니다. 가우스 분포는 *최대 엔트로피 분포(maximum entropy distribution)*로 알려진 것입니다. 우리는 :numref:<code>sec_information_theory</code>에서 엔트로피에 대해 더 깊이 다루겠지만, 지금 시점에서 알아야 할 모든 것은 그것이 무작위성의 척도라는 것입니다. 엄밀한 수학적 의미에서, 우리는 가우스 분포를 고정된 평균과 분산을 가진 확률 변수의 <em>가장</em> 무작위적인 선택으로 생각할 수 있습니다. 따라서 우리의 확률 변수가 어떤 평균과 분산을 갖는다는 것을 안다면, 가우스 분포는 어떤 의미에서 우리가 할 수 있는 가장 보수적인 분포 선택입니다.</p>
<p>섹션을 마무리하기 위해 $X sim \mathcal N(\mu, \sigma^2)$이면 다음이 성립함을 상기합시다.</p>
<ul>
<li>$\mu_X = \mu$,</li>
<li>$\sigma_X^2 = \sigma^2$.</li>
</ul>
<p>아래와 같이 가우스(또는 표준 정규) 분포로부터 샘플링할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
np.random.normal(mu, sigma, size=(10, 10))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
torch.normal(mu, sigma, size=(10, 10))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
tf.random.normal((10, 10), mu, sigma)
</code></pre>
<h2 id="지수족-exponential-family"><a class="header" href="#지수족-exponential-family">지수족 (Exponential Family)</a></h2>
<p>:label:<code>subsec_exponential_family</code></p>
<p>위에 나열된 모든 분포의 한 가지 공유된 속성은 그것들이 모두 *지수족(exponential family)*이라고 알려진 것에 속한다는 것입니다. 지수족은 밀도가 다음과 같은 형태로 표현될 수 있는 분포들의 집합입니다.</p>
<p>$$p(\mathbf{x} \mid \boldsymbol{\eta}) = h(\mathbf{x}) \cdot \exp
\left( \boldsymbol{\eta}^{\top} \cdot T(\mathbf{x}) - A(\boldsymbol{\eta})
\right)
$$
:eqlabel:<code>eq_exp_pdf</code></p>
<p>이 정의는 약간 미묘할 수 있으므로 자세히 살펴봅시다.</p>
<p>먼저, $h(\mathbf{x})$는 <em>기저 척도(underlying measure)</em> 또는 *베이스 척도(base measure)*로 알려져 있습니다. 이는 우리가 지수 가중치로 수정하고 있는 원래의 척도 선택으로 볼 수 있습니다.</p>
<p>둘째, <em>자연 파라미터(natural parameters)</em> 또는 *표준 파라미터(canonical parameters)*라고 불리는 벡터 $\boldsymbol{\eta} = (\eta_1, \eta_2, ..., \eta_l) sin
\mathbb{R}^l$가 있습니다. 이들은 베이스 척도가 어떻게 수정될지를 정의합니다. 자연 파라미터는 이러한 파라미터와 $\mathbf{x}= (x_1, x_2, ..., x_n) sin
\mathbb{R}^n$의 어떤 함수 $T(\cdot)$ 사이의 내적을 취하고 지수화함으로써 새로운 척도로 들어갑니다. 벡터 $T(\mathbf{x})= (T_1(\mathbf{x}), T_2(\mathbf{x}), ..., T_l(\mathbf{x}))$는 $\boldsymbol{\eta}$에 대한 *충분 통계량(sufficient statistics)*이라고 불립니다. 이 이름은 $T(\mathbf{x})$로 표현된 정보가 확률 밀도를 계산하기에 충분하며 샘플 $\mathbf{x}$로부터의 다른 정보는 필요하지 않기 때문에 사용됩니다.</p>
<p>셋째, *큐뮬런트 함수(cumulant function)*라고 지칭되는 $A(\boldsymbol{\eta})$가 있으며, 이는 위의 분포 :eqref:<code>eq_exp_pdf</code>가 1로 적분되도록 보장합니다. 즉, 다음과 같습니다.</p>
<p>$$A(\boldsymbol{\eta})  = \log
\left[\int h(\mathbf{x}) \cdot \exp
\left(\boldsymbol{\eta}^{\top} \cdot T(\mathbf{x})
\right) d\mathbf{x}
\right].$$</p>
<p>구체적으로 가우스 분포를 고려해 봅시다. $\mathbf{x}$가 일변량 변수라고 가정할 때, 우리는 그것이 다음과 같은 밀도를 가짐을 보았습니다.</p>
<p>$$
\begin{aligned}
p(x \mid \mu, \sigma) &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot
\exp
\left{
\frac{-(x-\mu)^2}{2 \sigma^2}
\right} \
&amp;= \frac{1}{\sqrt{2 \pi}} \cdot
\exp
\left{
\frac{\mu}{\sigma^2}x
-\frac{1}{2 \sigma^2} x^2 -
\left(
\frac{1}{2 \sigma^2} \mu^2
+\log(\sigma)
\right)
\right}.
\end{aligned}
$$</p>
<p>이는 지수족의 정의와 다음과 같이 일치합니다.</p>
<ul>
<li><em>기저 척도</em>: $h(x) = \frac{1}{\sqrt{2 \pi}}$,</li>
<li><em>자연 파라미터</em>: $\boldsymbol{\eta} = egin{bmatrix} ̣́\eta_1 \ \eta_2
\end{bmatrix} = egin{bmatrix} rac{\mu}{\sigma^2} \ rac{1}{2 \sigma^2}
\end{bmatrix}$,</li>
<li><em>충분 통계량</em>: $T(x) = egin{bmatrix}x-x^2
\end{bmatrix}$,</li>
<li><em>큐뮬런트 함수</em>: $A({\boldsymboḷ́̃}) = \frac{1}{2 \sigma^2} \mu^2 + \log(\sigma)
= rac{\eta_1^2}{4 \eta_2} - rac{1}{2}\log(2 \eta_2)$.</li>
</ul>
<p>위의 각 항의 정확한 선택은 다소 임의적이라는 점에 주목할 가치가 있습니다. 실제로 중요한 특징은 분포가 이 형태로 표현될 수 있다는 것이지, 정확한 형태 그 자체가 아닙니다.</p>
<p>:numref:<code>subsec_softmax_and_derivatives</code>에서 암시했듯이, 널리 사용되는 기술은 최종 출력 $\mathbf{y}$가 지수족 분포를 따른다고 가정하는 것입니다. 지수족은 머신러닝에서 빈번하게 마주치는 흔하고 강력한 분포 가족입니다.</p>
<h2 id="요약-summary-123"><a class="header" href="#요약-summary-123">요약 (Summary)</a></h2>
<ul>
<li>베르누이 확률 변수는 예/아니오 결과가 있는 이벤트를 모델링하는 데 사용될 수 있습니다.</li>
<li>이산 균등 분포는 유한한 가능성 세트로부터의 선택을 모델링합니다.</li>
<li>연속 균등 분포는 구간으로부터의 선택을 모델링합니다.</li>
<li>이항 분포는 일련의 베르누이 확률 변수를 모델링하고 성공 횟수를 셉니다.</li>
<li>포아송 확률 변수는 희귀 사건의 도착을 모델링합니다.</li>
<li>가우스 확률 변수는 많은 수의 독립 확률 변수를 함께 더한 결과를 모델링합니다.</li>
<li>위의 모든 분포는 지수족에 속합니다.</li>
</ul>
<h2 id="연습-문제-exercises-138"><a class="header" href="#연습-문제-exercises-138">연습 문제 (Exercises)</a></h2>
<ol>
<li>두 독립적인 이항 확률 변수 $X, Y sim 	extrm{Binomial}(16, 1/2)$의 차이인 $X-Y$ 확률 변수의 표준 편차는 얼마입니까?</li>
<li>포아송 확률 변수 $X sim 	extrm{Poisson}(\lambda)$를 취하고 $\lambda
ightarrow \infty$에 따라 $(X - \lambda)/\sqrt{\lambda}$를 고려하면, 이것이 대략 가우스 분포가 됨을 보일 수 있습니다. 이것이 왜 말이 됩니까?</li>
<li>$n$개 요소에 대한 두 이산 균등 확률 변수의 합에 대한 확률 질량 함수는 무엇입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/417">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1098">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1099">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="나이브-베이즈-naive-bayes"><a class="header" href="#나이브-베이즈-naive-bayes">나이브 베이즈 (Naive Bayes)</a></h1>
<p>:label:<code>sec_naive_bayes</code></p>
<p>이전 섹션들을 통해 우리는 확률 이론과 확률 변수에 대해 배웠습니다. 이 이론을 활용하기 위해 <em>나이브 베이즈(naive Bayes)</em> 분류기를 소개하겠습니다. 이것은 확률론적 기초만을 사용하여 숫자의 분류를 수행할 수 있게 해줍니다.</p>
<p>학습은 가정을 만드는 것에 관한 것입니다. 우리가 이전에 본 적 없는 새로운 데이터 예제를 분류하고 싶다면, 어떤 데이터 예제들이 서로 유사한지에 대해 몇 가지 가정을 해야 합니다. 나이브 베이즈 분류기는 대중적이고 현저히 명확한 알고리즘으로, 계산을 단순화하기 위해 모든 특성이 서로 독립적이라고 가정합니다. 이 섹션에서는 이미지 내의 문자를 인식하기 위해 이 모델을 적용할 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import gluon, np, npx
npx.set_np()
d2l.use_svg_display()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
import torchvision
d2l.use_svg_display()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
d2l.use_svg_display()
</code></pre>
<h2 id="광학-문자-인식-optical-character-recognition"><a class="header" href="#광학-문자-인식-optical-character-recognition">광학 문자 인식 (Optical Character Recognition)</a></h2>
<p>MNSIT :cite:<code>LeCun.Bottou.Bengio.ea.1998</code>는 널리 사용되는 데이터셋 중 하나입니다. 훈련용 이미지 60,000개와 검증용 이미지 10,000개를 포함하고 있습니다. 각 이미지는 0부터 9까지의 손글씨 숫자를 포함합니다. 과제는 각 이미지를 해당 숫자로 분류하는 것입니다.</p>
<p>Gluon은 인터넷에서 자동으로 데이터셋을 검색하기 위해 <code>data.vision</code> 모듈에 <code>MNIST</code> 클래스를 제공합니다. 이후에 Gluon은 이미 다운로드된 로컬 복사본을 사용할 것입니다. 파라미터 <code>train</code>의 값을 각각 <code>True</code> 또는 <code>False</code>로 설정하여 훈련 세트를 요청할지 테스트 세트를 요청할지 지정합니다. 각 이미지는 너비와 높이가 모두 $28$이고 모양이 ($28$, $28$, $1$)인 그레이스케일 이미지입니다. 마지막 채널 차원을 제거하기 위해 맞춤형 변환을 사용합니다. 또한 데이터셋은 각 픽셀을 부호 없는 $8$비트 정수로 나타냅니다. 문제를 단순화하기 위해 우리는 그것들을 이진 특성으로 양자화합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def transform(data, label):
    return np.floor(data.astype('float32') / 128).squeeze(axis=-1), label

mnist_train = gluon.data.vision.MNIST(train=True, transform=transform)
mnist_test = gluon.data.vision.MNIST(train=False, transform=transform)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
data_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    lambda x: torch.floor(x * 255 / 128).squeeze(dim=0)
])

mnist_train = torchvision.datasets.MNIST(
    root='./temp', train=True, transform=data_transform, download=True)
mist_test = torchvision.datasets.MNIST(
    root='./temp', train=False, transform=data_transform, download=True)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
((train_images, train_labels), (
    test_images, test_labels)) = tf.keras.datasets.mnist.load_data()

# MNIST의 원래 픽셀 값 범위는 0-255입니다(숫자가 uint8로 저장되기 때문). 
# 이 섹션에서는 (원래 이미지에서) 128보다 큰 픽셀 값은 1로 변환되고 
# 128보다 작은 값은 0으로 변환됩니다. 이유에 대해서는 섹션 18.9.2 및 18.9.3을 참조하십시오.
train_images = tf.floor(tf.constant(train_images / 128, dtype = tf.float32))
test_images = tf.floor(tf.constant(test_images / 128, dtype = tf.float32))

train_labels = tf.constant(train_labels, dtype = tf.int32)
test_labels = tf.constant(test_labels, dtype = tf.int32)
</code></pre>
<p>이미지와 해당 레이블을 포함하는 특정 예제에 액세스할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
image, label = mnist_train[2]
image.shape, label
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
image, label = mnist_train[2]
image.shape, label
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
image, label = train_images[2], train_labels[2]
image.shape, label.numpy()
</code></pre>
<p>여기 <code>image</code> 변수에 저장된 우리의 예제는 높이와 너비가 $28$픽셀인 이미지에 해당합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
image.shape, image.dtype
</code></pre>
<p>우리의 코드는 각 이미지의 레이블을 스칼라로 저장합니다. 그 타입은 $32$비트 정수입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
label, type(label), label.dtype
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
label, type(label)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
label.numpy(), label.dtype
</code></pre>
<p>동시에 여러 예제에 액세스할 수도 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
images, labels = mnist_train[10:38]
images.shape, labels.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
images = torch.stack([mnist_train[i][0] for i in range(10, 38)], dim=0)
labels = torch.tensor([mnist_train[i][1] for i in range(10, 38)])
images.shape, labels.shape
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
images = tf.stack([train_images[i] for i in range(10, 38)], axis=0)
labels = tf.constant([train_labels[i].numpy() for i in range(10, 38)])
images.shape, labels.shape
</code></pre>
<p>이 예제들을 시각화해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab all
d2l.show_images(images, 2, 9);
</code></pre>
<h2 id="분류를-위한-확률-모델-the-probabilistic-model-for-classification"><a class="header" href="#분류를-위한-확률-모델-the-probabilistic-model-for-classification">분류를 위한 확률 모델 (The Probabilistic Model for Classification)</a></h2>
<p>분류 과제에서 우리는 예제를 범주로 매핑합니다. 여기서 예제는 그레이스케일 $28\times 28$ 이미지이고, 범주는 숫자입니다. (자세한 설명은 :numref:<code>sec_softmax</code>를 참조하십시오.)</p>
<p>분류 과제를 표현하는 자연스러운 방법 중 하나는 확률적 질문을 통하는 것입니다: 특성(즉, 이미지 픽셀)이 주어졌을 때 가장 가능성 있는 레이블은 무엇인가? 예제의 특성을 $\mathbf x\in\mathbb R^d$로, 레이블을 $y\in\mathbb R$로 표시합시다. 여기서 특성은 이미지 픽셀이며, $2$차원 이미지를 벡터로 재구성하여 $d=28^2=784$가 되게 할 수 있고 레이블은 숫자입니다.</p>
<p>특성이 주어졌을 때 레이블의 확률은 $p(y  \mid  \mathbf{x})$입니다. 만약 우리가 이러한 확률들을 계산할 수 있다면(우리 예제에서 $y=0, \ldots,9$에 대한 $p(y  \mid  \mathbf{x})$), 분류기는 다음 식에 의해 주어지는 예측 $\hat{y}$를 출력할 것입니다.</p>
<p>$$\hat{y} = \mathrm{argmax} &gt; p(y  \mid  \mathbf{x}).$$</p>
<p>불행히도 이것은 $\mathbf{x} = x_1, ..., x_d$의 모든 값에 대해 $p(y  \mid  \mathbf{x})$를 추정할 것을 요구합니다. 각 특성이 $2$가지 값 중 하나를 가질 수 있다고 상상해 보십시오. 예를 들어, 특성 $x_1 = 1$은 주어진 문서에 사과라는 단어가 나타남을 의미하고 $x_1 = 0$은 그렇지 않음을 의미할 수 있습니다. 만약 우리가 그러한 30개의 이진 특성을 가지고 있다면, 이는 입력 벡터 $\mathbf{x}$의 $2^{30}$개(10억 개 이상!)의 가능한 값 중 어느 것이든 분류할 준비가 되어 있어야 함을 의미합니다.</p>
<p>더욱이, 학습은 어디에 있습니까? 만약 우리가 해당 레이블을 예측하기 위해 모든 단일 가능한 예제를 봐야 한다면, 우리는 정말로 패턴을 학습하는 것이 아니라 단지 데이터셋을 암기하는 것뿐입니다.</p>
<h2 id="나이브-베이즈-분류기-the-naive-bayes-classifier"><a class="header" href="#나이브-베이즈-분류기-the-naive-bayes-classifier">나이브 베이즈 분류기 (The Naive Bayes Classifier)</a></h2>
<p>다행히도 조건부 독립에 대한 몇 가지 가정을 함으로써, 우리는 약간의 귀납적 편향(inductive bias)을 도입하고 상대적으로 적은 양의 훈련 예제로부터 일반화할 수 있는 모델을 구축할 수 있습니다. 먼저 베이즈 정리를 사용하여 분류기를 다음과 같이 표현해 봅시다.</p>
<p>$$\hat{y} = \mathrm{argmax}_y &gt; p(y  \mid  \mathbf{x}) = \mathrm{argmax}_y &gt; \frac{p( \mathbf{x}  \mid  y) p(y)}{p(\mathbf{x})}.$$</p>
<p>분모는 레이블 $y$의 값에 의존하지 않는 정규화 항 $p(\mathbf{x})$임에 유의하십시오. 결과적으로, 우리는 서로 다른 $y$ 값들에 걸쳐 분자를 비교하는 것만 걱정하면 됩니다. 분모를 계산하는 것이 다루기 힘든 것으로 판명되더라도, 분자를 평가할 수 있는 한 그것을 무시하고 넘어갈 수 있습니다. 다행히도 정규화 상수를 복구하고 싶더라도 가능합니다. $\sum_y p(y  \mid  \mathbf{x}) = 1$이므로 언제든지 정규화 항을 복구할 수 있습니다.</p>
<p>이제 $p( \mathbf{x}  \mid  y)$에 집중해 봅시다. 확률의 연쇄 법칙을 사용하면 $p( \mathbf{x}  \mid  y)$ 항을 다음과 같이 표현할 수 있습니다.</p>
<p>$$p(x_1  \mid y) \cdot p(x_2  \mid  x_1, y) \cdot ... \cdot p( x_d  \mid  x_1, ..., x_{d-1}, y).$$</p>
<p>이 식 자체만으로는 우리를 더 멀리 데려다주지 못합니다. 우리는 여전히 대략 $2^d$개의 파라미터를 추정해야 합니다. 그러나 만약 우리가 <em>레이블이 주어졌을 때 특성들이 서로 조건부 독립</em>이라고 가정한다면, 이 항이 $\prod_i p(x_i  \mid  y)$로 단순화되어 우리에게 다음과 같은 예측기를 제공하므로 갑자기 훨씬 더 나은 상태가 됩니다.</p>
<p>$$\hat{y} = \mathrm{argmax}<em>y &gt; \prod</em>{i=1}^d p(x_i  \mid  y) p(y).$$</p>
<p>모든 $i$와 $y$에 대해 $p(x_i=1  \mid  y)$를 추정하고 그 값을 $P_{xy}[i, y]$에 저장할 수 있다면(여기서 $P_{xy}$는 $d\times n$ 행렬이고 $n$은 클래스 수이며 $y\in{1, \ldots, n}$), 우리는 이를 사용하여 $p(x_i = 0 \mid y)$도 추정할 수 있습니다. 즉, 다음과 같습니다.</p>
<p>$$
p(x_i = t_i \mid y) =
\begin{cases}
P_{xy}[i, y] &amp; \textrm{ for } t_i=1 ;\
1 - P_{xy}[i, y] &amp; \textrm{ for } t_i = 0 .
\end{cases}
$$</p>
<p>추가적으로, 모든 $y$에 대해 $p(y)$를 추정하고 이를 $n$ 길이의 벡터인 $P_y$에 저장합니다. 그러면 임의의 새로운 예제 $\mathbf t = (t_1, t_2, \ldots, t_d)$에 대해 다음을 계산할 수 있습니다.</p>
<p>$$\begin{aligned} \hat{y} &amp;= \mathrm{argmax}_ y &gt; p(y)\prod_{i=1}^d   p(x_t = t_i \mid y) \ &amp;= \mathrm{argmax}<em>y &gt; P_y[y]\prod</em>{i=1}^d &gt; P_{xy}[i, y]^{t_i}, \left(1 - P_{xy}[i, y]\right)^{1-t_i} <br />
end{aligned}$$
:eqlabel:<code>eq_naive_bayes_estimation</code></p>
<p>임의의 $y$에 대해서 말입니다. 따라서 조건부 독립에 대한 우리의 가정은 우리 모델의 복잡성을 특성 수에 대한 지수적 의존성 $\mathcal{O}(2^dn)$에서 선형 의존성 $\mathcal{O}(dn)$으로 가져왔습니다.</p>
<h2 id="훈련-training-31"><a class="header" href="#훈련-training-31">훈련 (Training)</a></h2>
<p>이제 문제는 우리가 $P_{xy}$와 $P_y$를 모른다는 것입니다. 그래서 먼저 훈련 데이터가 주어졌을 때 그들의 값을 추정해야 합니다. 이것이 모델을 <em>훈련</em>하는 것입니다. $P_y$를 추정하는 것은 그리 어렵지 않습니다. $10$개의 클래스만 다루고 있으므로, 각 숫자의 발생 횟수 $n_y$를 세고 이를 총 데이터 양 $n$으로 나눌 수 있습니다. 예를 들어 숫자 8이 $n_8 = 5,800$번 발생하고 총 $n = 60,000$개의 이미지가 있다면, 확률 추정치는 $p(y=8) = 0.0967$입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
X, Y = mnist_train[:]  # 모든 훈련 예제

n_y = np.zeros((10))
for y in range(10):
    n_y[y] = (Y == y).sum()
P_y = n_y / n_y.sum()
P_y
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
X = torch.stack([mnist_train[i][0] for i in range(len(mnist_train))], dim=0)
Y = torch.tensor([mnist_train[i][1] for i in range(len(mnist_train))])

n_y = torch.zeros(10)
for y in range(10):
    n_y[y] = (Y == y).sum()
P_y = n_y / n_y.sum()
P_y
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
X = train_images
Y = train_labels

n_y = tf.Variable(tf.zeros(10))
for y in range(10):
    n_y[y].assign(tf.reduce_sum(tf.cast(Y == y, tf.float32)))
P_y = n_y / tf.reduce_sum(n_y)
P_y
</code></pre>
<p>이제 약간 더 어려운 것들인 $P_{xy}$로 넘어가 봅시다. 우리가 흑백 이미지를 선택했으므로, $p(x_i  \mid  y)$는 클래스 $y$에 대해 픽셀 $i$가 켜져 있을 확률을 나타냅니다. 이전과 마찬가지로 이벤트가 발생하는 횟수 $n_{iy}$를 세고 이를 $y$의 총 발생 횟수인 $n_y$로 나눌 수 있습니다. 하지만 약간 골치 아픈 점이 있습니다: 특정 픽셀이 결코 검은색이 아닐 수도 있습니다(예를 들어, 잘 잘린 이미지의 경우 모서리 픽셀은 항상 흰색일 수 있습니다). 통계학자들이 이 문제를 다루는 편리한 방법은 모든 발생 횟수에 의사 카운트(pseudo counts)를 추가하는 것입니다. 따라서 $n_{iy}$ 대신 $n_{iy}+1$을 사용하고 $n_y$ 대신 $n_y+2$를 사용합니다(픽셀 $i$가 가질 수 있는 가능한 값이 두 가지 - 검은색 또는 흰색 - 이기 때문입니다). 이것은 *라플라스 평활화(Laplace Smoothing)*라고도 불립니다. 임시방편처럼 보일 수 있지만, 베타-이항 모델에 의해 베이지안 관점에서 동기가 부여될 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
n_x = np.zeros((10, 28, 28))
for y in range(10):
    n_x[y] = np.array(X.asnumpy()[Y.asnumpy() == y].sum(axis=0))
P_xy = (n_x + 1) / (n_y + 2).reshape(10, 1, 1)

d2l.show_images(P_xy, 2, 5);
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
n_x = torch.zeros((10, 28, 28))
for y in range(10):
    n_x[y] = torch.tensor(X.numpy()[Y.numpy() == y].sum(axis=0))
P_xy = (n_x + 1) / (n_y + 2).reshape(10, 1, 1)

d2l.show_images(P_xy, 2, 5);
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
n_x = tf.Variable(tf.zeros((10, 28, 28)))
for y in range(10):
    n_x[y].assign(tf.cast(tf.reduce_sum(
        X.numpy()[Y.numpy() == y], axis=0), tf.float32))
P_xy = (n_x + 1) / tf.reshape((n_y + 2), (10, 1, 1))

d2l.show_images(P_xy, 2, 5);
</code></pre>
<p>이러한 $10\times 28\times 28$ 확률(각 클래스별 각 픽셀에 대해)을 시각화함으로써 우리는 비열해 보이는 숫자들을 얻을 수 있었습니다.</p>
<p>이제 새로운 이미지를 예측하기 위해 :eqref:<code>eq_naive_bayes_estimation</code>을 사용할 수 있습니다. $\mathbf x$가 주어졌을 때, 다음 함수는 모든 $y$에 대해 $p(\mathbf x \mid y)p(y)$를 계산합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def bayes_pred(x):
    x = np.expand_dims(x, axis=0)  # (28, 28) -&gt; (1, 28, 28)
    p_xy = P_xy * x + (1 - P_xy)*(1 - x)
    p_xy = p_xy.reshape(10, -1).prod(axis=1)  # p(x|y)
    return np.array(p_xy) * P_y

image, label = mnist_test[0]
bayes_pred(image)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def bayes_pred(x):
    x = x.unsqueeze(0)  # (28, 28) -&gt; (1, 28, 28)
    p_xy = P_xy * x + (1 - P_xy)*(1 - x)
    p_xy = p_xy.reshape(10, -1).prod(dim=1)  # p(x|y)
    return p_xy * P_y

image, label = mnist_test[0]
bayes_pred(image)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def bayes_pred(x):
    x = tf.expand_dims(x, axis=0)  # (28, 28) -&gt; (1, 28, 28)
    p_xy = P_xy * x + (1 - P_xy)*(1 - x)
    p_xy = tf.math.reduce_prod(tf.reshape(p_xy, (10, -1)), axis=1)  # p(x|y)
    return p_xy * P_y

image, label = train_images[0], train_labels[0]
bayes_pred(image)
</code></pre>
<p>이것은 끔찍하게 잘못되었습니다! 왜 그런지 알아내기 위해 픽셀당 확률을 살펴봅시다. 그것들은 보통 $0.001$과 $1$ 사이의 숫자입니다. 우리는 그것들 중 $784$개를 곱하고 있습니다. 이 시점에서 우리가 이러한 숫자들을 컴퓨터에서 계산하고 있으며, 따라서 지수에 대한 고정된 범위를 가지고 있다는 점을 언급할 가치가 있습니다. 무슨 일이 일어나는가 하면, 우리는 *수치적 언더플로(numerical underflow)*를 경험하게 됩니다. 즉, 모든 작은 숫자들을 곱하면 0으로 반올림될 때까지 훨씬 더 작은 결과로 이어집니다. 우리는 이를 :numref:<code>sec_maximum_likelihood</code>에서 이론적 문제로 논의했지만, 실전에서 그 현상을 명확하게 봅니다.</p>
<p>해당 섹션에서 논의한 대로, 우리는 $\log a b = \log a + \log b$라는 사실을 사용하여 이를 해결합니다. 즉, 로그를 합산하는 것으로 전환합니다. $a$와 $b$가 모두 작은 숫자일지라도 로그 값은 적절한 범위 내에 있어야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
a = 0.1
print('언더플로:', a**784)
print('로그는 정상임:', 784*math.log(a))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
a = 0.1
print('언더플로:', a**784)
print('로그는 정상임:', 784*math.log(a))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
a = 0.1
print('언더플로:', a**784)
print('로그는 정상임:', 784*tf.math.log(a).numpy())
</code></pre>
<p>로그는 증가 함수이므로, 우리는 :eqref:<code>eq_naive_bayes_estimation</code>를 다음과 같이 다시 쓸 수 있습니다.</p>
<p>$$ \hat{y} = \mathrm{argmax}<em>y &gt; \log P_y[y] + \sum</em>{i=1}^d \Big[t_i\log P_{xy}[x_i, y] + (1-t_i) \log (1 - P_{xy}[x_i, y]) \Big].$$</p>
<p>우리는 다음과 같은 안정적인 버전을 구현할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
log_P_xy = np.log(P_xy)
log_P_xy_neg = np.log(1 - P_xy)
log_P_y = np.log(P_y)

def bayes_pred_stable(x):
    x = np.expand_dims(x, axis=0)  # (28, 28) -&gt; (1, 28, 28)
    p_xy = log_P_xy * x + log_P_xy_neg * (1 - x)
    p_xy = p_xy.reshape(10, -1).sum(axis=1)  # p(x|y)
    return p_xy + log_P_y

py = bayes_pred_stable(image)
py
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
log_P_xy = torch.log(P_xy)
log_P_xy_neg = torch.log(1 - P_xy)
log_P_y = torch.log(P_y)

def bayes_pred_stable(x):
    x = x.unsqueeze(0)  # (28, 28) -&gt; (1, 28, 28)
    p_xy = log_P_xy * x + log_P_xy_neg * (1 - x)
    p_xy = p_xy.reshape(10, -1).sum(axis=1)  # p(x|y)
    return p_xy + log_P_y

py = bayes_pred_stable(image)
py
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
log_P_xy = tf.math.log(P_xy)
log_P_xy_neg = tf.math.log(1 - P_xy)
log_P_y = tf.math.log(P_y)

def bayes_pred_stable(x):
    x = tf.expand_dims(x, axis=0)  # (28, 28) -&gt; (1, 28, 28)
    p_xy = log_P_xy * x + log_P_xy_neg * (1 - x)
    p_xy = tf.math.reduce_sum(tf.reshape(p_xy, (10, -1)), axis=1)  # p(x|y)
    return p_xy + log_P_y

py = bayes_pred_stable(image)
py
</code></pre>
<p>이제 예측이 올바른지 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 비교를 위해 int32 dtype의 스칼라 텐서인 레이블을 Python 스칼라 정수로 변환
py.argmax(axis=0) == int(label)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
py.argmax(dim=0) == label
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
tf.argmax(py, axis=0, output_type = tf.int32) == label
</code></pre>
<p>이제 몇 가지 검증 예제를 예측해 보면 베이즈 분류기가 꽤 잘 작동하는 것을 볼 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def predict(X):
    return [bayes_pred_stable(x).argmax(axis=0).astype(np.int32) for x in X]

X, y = mnist_test[:18]
preds = predict(X)
d2l.show_images(X, 2, 9, titles=[str(d) for d in preds]);
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def predict(X):
    return [bayes_pred_stable(x).argmax(dim=0).type(torch.int32).item()
            for x in X]

X = torch.stack([mnist_test[i][0] for i in range(18)], dim=0)
y = torch.tensor([mnist_test[i][1] for i in range(18)])
preds = predict(X)
d2l.show_images(X, 2, 9, titles=[str(d) for d in preds]);
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def predict(X):
    return [tf.argmax(
        bayes_pred_stable(x), axis=0, output_type = tf.int32).numpy()
            for x in X]

X = tf.stack([train_images[i] for i in range(10, 38)], axis=0)
y = tf.constant([train_labels[i].numpy() for i in range(10, 38)])
preds = predict(X)
d2l.show_images(X, 2, 9, titles=[str(d) for d in preds]);
</code></pre>
<p>마지막으로 분류기의 전체 정확도를 계산해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
X, y = mnist_test[:]
preds = np.array(predict(X), dtype=np.int32)
float((preds == y).sum()) / len(y)  # 검증 정확도
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
X = torch.stack([mnist_test[i][0] for i in range(len(mnist_test))], dim=0)
y = torch.tensor([mnist_test[i][1] for i in range(len(mnist_test))])
preds = torch.tensor(predict(X), dtype=torch.int32)
float((preds == y).sum()) / len(y)  # 검증 정확도
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
X = test_images
y = test_labels
preds = tf.constant(predict(X), dtype=tf.int32)
# 검증 정확도
tf.reduce_sum(tf.cast(preds == y, tf.float32)).numpy() / len(y)
</code></pre>
<p>현대 심층 네트워크는 $0.01$ 미만의 오차율을 달성합니다. 상대적으로 떨어지는 성능은 우리 모델에서 만든 잘못된 통계적 가정 때문입니다: 우리는 각 픽셀이 레이블에만 의존하며 <em>독립적으로</em> 생성된다고 가정했습니다. 이것은 분명히 인간이 숫자를 쓰는 방식이 아니며, 이 잘못된 가정은 우리의 지나치게 나이브한 (베이즈) 분류기의 몰락으로 이어졌습니다.</p>
<h2 id="요약-summary-124"><a class="header" href="#요약-summary-124">요약 (Summary)</a></h2>
<ul>
<li>베이즈 규칙을 사용하여 관찰된 모든 특성이 독립적이라고 가정함으로써 분류기를 만들 수 있습니다.</li>
<li>이 분류기는 레이블과 픽셀 값의 조합이 발생하는 횟수를 세어 데이터셋에서 훈련될 수 있습니다.</li>
<li>이 분류기는 스팸 감지와 같은 과제에서 수십 년 동안 골드 표준이었습니다.</li>
</ul>
<h2 id="연습-문제-exercises-139"><a class="header" href="#연습-문제-exercises-139">연습 문제 (Exercises)</a></h2>
<ol>
<li>두 요소의 XOR에 의해 레이블 $[0,1,1,0]$이 주어지는 데이터셋 $[[0,0], [0,1], [1,0], [1,1]]$을 고려해 보십시오. 이 데이터셋에서 구축된 나이브 베이즈 분류기에 대한 확률은 무엇입니까? 우리 점들을 성공적으로 분류하나요? 그렇지 않다면 어떤 가정이 위반되었습니까?</li>
<li>확률을 추정할 때 라플라스 평활화를 사용하지 않았고 훈련에서 관찰되지 않은 값을 포함하는 데이터 예제가 테스트 시에 도착했다고 가정해 보십시오. 모델은 무엇을 출력할까요?</li>
<li>나이브 베이즈 분류기는 확률 변수의 의존성이 그래프 구조로 인코딩되는 베이지안 네트워크의 특정 예입니다. 전체 이론은 이 섹션의 범위를 벗어나지만(:citet:<code>Koller.Friedman.2009</code>에 자세한 내용이 있음), XOR 모델에서 두 입력 변수 사이의 명시적 의존성을 허용하는 것이 왜 성공적인 분류기를 만들 수 있게 하는지 설명해 보십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/418">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1100">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1101">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="통계-statistics"><a class="header" href="#통계-statistics">통계 (Statistics)</a></h1>
<p>:label:<code>sec_statistics</code></p>
<p>의심할 여지 없이, 최고의 딥러닝 전문가가 되기 위해서는 최첨단의 고정밀 모델을 훈련하는 능력이 매우 중요합니다. 그러나 개선 사항이 언제 유의미한지, 아니면 단지 훈련 과정의 무작위 변동의 결과인지가 불분명한 경우가 많습니다. 추정값의 불확실성에 대해 논의하려면 통계를 배워야 합니다.</p>
<p>*통계(statistics)*에 대한 가장 이른 기록은 9세기의 아랍 학자 알킨디(Al-Kindi)로 거슬러 올라갑니다. 그는 암호화된 메시지를 해독하기 위해 통계와 빈도 분석을 사용하는 방법에 대해 자세히 설명했습니다. 800년 후, 현대 통계학은 1700년대 독일에서 연구자들이 인구통계학적 및 경제적 데이터 수집과 분석에 집중하면서 시작되었습니다. 오늘날 통계학은 데이터의 수집, 처리, 분석, 해석 및 시각화를 다루는 과학 과목입니다. 게다가 통계학의 핵심 이론은 학계, 산업계 및 정부 내의 연구에서 널리 사용되어 왔습니다.</p>
<p>더 구체적으로, 통계학은 *기술 통계(descriptive statistics)*와 *통계적 추론(statistical inference)*으로 나뉠 수 있습니다. 전자는 *표본(sample)*이라고 불리는 관찰된 데이터 모음의 특징을 요약하고 설명하는 데 집중합니다. 표본은 *모집단(population)*에서 추출되며, 이는 우리 실험 관심사의 유사한 개인, 항목 또는 사건의 전체 집합을 나타냅니다. 기술 통계와 반대로, <em>통계적 추론</em>은 표본 분포가 어느 정도 모집단 분포를 재현할 수 있다는 가정 하에 주어진 <em>표본</em>으로부터 모집단의 특성을 추론합니다.</p>
<p>여러분은 "머신러닝과 통계의 본질적인 차이점은 무엇인가?"라고 궁금해할 수 있습니다. 근본적으로 말해서, 통계학은 추론 문제에 집중합니다. 이러한 유형의 문제에는 인과 추론과 같은 변수 간의 관계 모델링, A/B 테스팅과 같은 모델 파라미터의 통계적 유의성 테스트가 포함됩니다. 반대로 머신러닝은 각 파라미터의 기능을 명시적으로 프로그래밍하고 이해하지 않고도 정확한 예측을 하는 것을 강조합니다.</p>
<p>이 섹션에서는 추정량 평가 및 비교, 가설 검정 수행, 신뢰 구간 구축의 세 가지 유형의 통계 추론 방법을 소개합니다. 이러한 방법들은 주어진 모집단의 특성, 즉 실제 파라미터 $\theta$를 추론하는 데 도움이 될 수 있습니다. 간결함을 위해 주어진 모집단의 실제 파라미터 $\theta$가 스칼라 값이라고 가정합니다. $\theta$가 벡터나 텐서인 경우로 확장하는 것은 간단하므로 논의에서는 생략합니다.</p>
<h2 id="추정량-평가-및-비교-evaluating-and-comparing-estimators"><a class="header" href="#추정량-평가-및-비교-evaluating-and-comparing-estimators">추정량 평가 및 비교 (Evaluating and Comparing Estimators)</a></h2>
<p>통계학에서 *추정량(estimator)*은 실제 파라미터 $\theta$를 추정하기 위해 사용되는 주어진 표본들의 함수입니다. 표본 {$x_1, x_2, \ldots, x_n$}을 관찰한 후 $\theta$에 대한 추정값을 $\hat{\theta}_n = \hat{f}(x_1, \ldots, x_n)$이라고 씁니다.</p>
<p>우리는 이전에 섹션 :numref:<code>sec_maximum_likelihood</code>에서 추정량의 간단한 예를 보았습니다. 베르누이 확률 변수로부터 여러 표본을 가지고 있다면, 확률 변수가 1일 확률에 대한 최대 우도 추정량은 관찰된 1의 개수를 세고 총 표본 수로 나눔으로써 얻을 수 있습니다. 마찬가지로, 연습 문제에서는 여러 표본이 주어졌을 때 가우시안 평균의 최대 우도 추정량이 모든 표본의 평균값으로 주어진다는 것을 보여달라고 요청했습니다. 이러한 추정량들이 파라미터의 실제 값을 제공하는 경우는 거의 없지만, 이상적으로는 표본 수가 많을 때 추정값이 실제값에 가까울 것입니다.</p>
<p>예를 들어, 아래에 평균이 0이고 분산이 1인 가우시안 확률 변수의 실제 밀도와 그 가우시안에서 추출한 표본 모음을 보여줍니다. 모든 점이 보이고 원래 밀도와의 관계가 더 명확해지도록 $y$ 좌표를 구성했습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
import random
npx.set_np()

# 데이터 포인트 샘플링 및 y 좌표 생성
epsilon = 0.1
random.seed(8675309)
xs = np.random.normal(loc=0, scale=1, size=(300,))

ys = [np.sum(np.exp(-(xs[:i] - xs[i])**2 / (2 * epsilon**2))
             / np.sqrt(2*np.pi*epsilon**2)) / len(xs) for i in range(len(xs))]

# 실제 밀도 계산
xd = np.arange(np.min(xs), np.max(xs), 0.01)

d = np.exp(-xd**2/2) / np.sqrt(2 * np.pi)

# 결과 플롯
d2l.plot(xd, yd, 'x', 'density')
d2l.plt.scatter(xs, ys)
d2l.plt.axvline(x=0)
d2l.plt.axvline(x=np.mean(xs), linestyle='--', color='purple')
d2l.plt.title(f'sample mean: {float(np.mean(xs)):.2f}')
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
from d2l import torch as d2l
import torch

torch.pi = torch.acos(torch.zeros(1)) * 2  # torch에서 pi 정의

# 데이터 포인트 샘플링 및 y 좌표 생성
epsilon = 0.1
torch.manual_seed(8675309)
xs = torch.randn(size=(300,))

ys = torch.tensor(
    [torch.sum(torch.exp(-(xs[:i] - xs[i])**2 / (2 * epsilon**2))
               / torch.sqrt(2*torch.pi*epsilon**2)) / len(xs)
     for i in range(len(xs))])

# 실제 밀도 계산
xd = torch.arange(torch.min(xs), torch.max(xs), 0.01)

d = torch.exp(-xd**2/2) / torch.sqrt(2 * torch.pi)

# 결과 플롯
d2l.plot(xd, yd, 'x', 'density')
d2l.plt.scatter(xs, ys)
d2l.plt.axvline(x=0)
d2l.plt.axvline(x=torch.mean(xs), linestyle='--', color='purple')
d2l.plt.title(f'sample mean: {float(torch.mean(xs).item()):.2f}')
d2l.plt.show()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf

tf.pi = tf.acos(tf.zeros(1)) * 2  # TensorFlow에서 pi 정의

# 데이터 포인트 샘플링 및 y 좌표 생성
epsilon = 0.1
xs = tf.random.normal((300,))

ys = tf.constant(
    [(tf.reduce_sum(tf.exp(-(xs[:i] - xs[i])**2 / (2 * epsilon**2)) \
               / tf.sqrt(2*tf.pi*epsilon**2)) / tf.cast(
        tf.size(xs), dtype=tf.float32)).numpy() \
     for i in range(tf.size(xs))])

# 실제 밀도 계산
xd = tf.range(tf.reduce_min(xs), tf.reduce_max(xs), 0.01)

d = tf.exp(-xd**2/2) / tf.sqrt(2 * tf.pi)

# 결과 플롯
d2l.plot(xd, yd, 'x', 'density')
d2l.plt.scatter(xs, ys)
d2l.plt.axvline(x=0)
d2l.plt.axvline(x=tf.reduce_mean(xs), linestyle='--', color='purple')
d2l.plt.title(f'sample mean: {float(tf.reduce_mean(xs).numpy()):.2f}')
d2l.plt.show()
</code></pre>
<p>파라미터의 추정량 $\hat{\theta}_n$을 계산하는 방법은 많을 수 있습니다. 이 섹션에서는 추정량을 평가하고 비교하는 세 가지 일반적인 방법인 평균 제곱 오차, 표준 편차 및 통계적 편향을 소개합니다.</p>
<h3 id="평균-제곱-오차-mean-squared-error"><a class="header" href="#평균-제곱-오차-mean-squared-error">평균 제곱 오차 (Mean Squared Error)</a></h3>
<p>추정량을 평가하는 데 사용되는 아마도 가장 간단한 메트릭은 <em>평균 제곱 오차(mean squared error, MSE)</em> (또는 $l_2$ 손실) 추정량이며 다음과 같이 정의될 수 있습니다.</p>
<p>$$\textrm{MSE} (\hat{\theta}_n, \theta) = E[(\hat{\theta}_n - \theta)^2].$$
:eqlabel:<code>eq_mse_est</code></p>
<p>이를 통해 실제 값으로부터의 평균 제곱 편차를 정량화할 수 있습니다. MSE는 항상 음수가 아닙니다. :numref:<code>sec_linear_regression</code>을 읽었다면 이를 가장 흔히 사용되는 회귀 손실 함수로 인식할 것입니다. 추정량을 평가하는 척도로서, 그 값이 0에 가까울수록 추정량이 실제 파라미터 $\theta$에 더 가깝습니다.</p>
<h3 id="통계적-편향-statistical-bias"><a class="header" href="#통계적-편향-statistical-bias">통계적 편향 (Statistical Bias)</a></h3>
<p>MSE는 자연스러운 메트릭을 제공하지만, 이를 크게 만들 수 있는 여러 다른 현상을 쉽게 상상할 수 있습니다. 근본적으로 중요한 두 가지는 데이터셋의 무작위성으로 인한 추정량의 변동과 추정 절차로 인한 추정량의 계통 오차(systematic error)입니다.</p>
<p>먼저 계통 오차를 측정해 봅시다. 추정량 $\hat{\theta}_n$에 대해, *통계적 편향(statistical bias)*의 수학적 설명은 다음과 같이 정의될 수 있습니다.</p>
<p>$$\textrm{bias}(\hat{\theta}_n) = E(\hat{\theta}_n - \theta) = E(\hat{\theta}_n) - \theta.$$
:eqlabel:<code>eq_bias</code></p>
<p>$\textrm{bias}(\hat{\theta}_n) = 0$일 때 추정량 $\hat{\theta}_n$의 기댓값은 파라미터의 실제 값과 같습니다. 이 경우 $\hat{\theta}_n$을 불편 추정량(unbiased estimator)이라고 합니다. 일반적으로 불편 추정량은 기댓값이 실제 파라미터와 같기 때문에 편향 추정량보다 낫습니다.</p>
<p>그러나 편향 추정량이 실제에서 자주 사용된다는 점을 알아두는 것이 좋습니다. 추가적인 가정 없이는 불편 추정량이 존재하지 않거나 계산하기 어려운 경우가 있습니다. 이는 추정량의 중대한 결함처럼 보일 수 있지만, 실제에서 만나는 대다수의 추정량은 가용한 표본 수가 무한대로 갈 때 편향이 0으로 수렴한다는 의미에서 적어도 점근적 불편 추정량(asymptotically unbiased)입니다: $\lim_{n \rightarrow \infty} \textrm{bias}(\hat{\theta}_n) = 0$.</p>
<h3 id="분산과-표준-편차-variance-and-standard-deviation"><a class="header" href="#분산과-표준-편차-variance-and-standard-deviation">분산과 표준 편차 (Variance and Standard Deviation)</a></h3>
<p>둘째, 추정량의 무작위성을 측정해 봅시다. :numref:<code>sec_random_variables</code>에서 상기했듯이, <em>표준 편차(standard deviation)</em> (또는 <em>표준 오차(standard error)</em>)는 분산의 제곱근으로 정의됩니다. 우리는 해당 추정량의 표준 편차나 분산을 측정함으로써 추정량의 변동 정도를 측정할 수 있습니다.</p>
<p>$$\sigma_{\hat{\theta}_n} = \sqrt{\textrm{Var} (\hat{\theta}_n )} = \sqrt{E[(\hat{\theta}_n - E(\hat{\theta}_n))^2]}.$$
:eqlabel:<code>eq_var_est</code></p>
<p>:eqref:<code>eq_var_est</code>를 :eqref:<code>eq_mse_est</code>와 비교하는 것이 중요합니다. 이 방정식에서는 실제 모집단 값 $\theta$와 비교하는 것이 아니라, 기대 표본 평균인 $E(\hat{\theta}_n)$과 비교합니다. 따라서 추정량이 실제 값에서 얼마나 떨어져 있는지를 측정하는 것이 아니라, 추정량 자체의 변동을 측정하는 것입니다.</p>
<h3 id="편향-분산-트레이드오프-the-bias-variance-trade-off"><a class="header" href="#편향-분산-트레이드오프-the-bias-variance-trade-off">편향-분산 트레이드오프 (The Bias-Variance Trade-off)</a></h3>
<p>이 두 가지 주요 구성 요소가 평균 제곱 오차에 기여한다는 것은 직관적으로 명확합니다. 다소 놀라운 점은 이것이 실제로 평균 제곱 오차를 이 두 기여도와 세 번째 기여도로 <em>분해</em>한 것임을 보여줄 수 있다는 것입니다. 즉, 평균 제곱 오차를 편향의 제곱, 분산, 그리고 줄일 수 없는 오차의 합으로 쓸 수 있습니다.</p>
<p>$$
\begin{aligned}
\textrm{MSE} (\hat{\theta}_n, \theta) &amp;= E[(\hat{\theta}_n - \theta)^2] \
&amp;= E[(\hat{\theta}_n)^2] + E[\theta^2] - 2E[\hat{\theta}_n\theta] \
&amp;= \textrm{Var} [\hat{\theta}_n] + E[\hat{\theta}_n]^2 + \textrm{Var} [\theta] + E[\theta]^2 - 2E[\hat{\theta}_n]E[\theta] \
&amp;= (E[\hat{\theta}_n] - E[\theta])^2 + \textrm{Var} [\hat{\theta}_n] + \textrm{Var} [\theta] \
&amp;= (E[\hat{\theta}_n - \theta])^2 + \textrm{Var} [\hat{\theta}_n] + \textrm{Var} [\theta] \
&amp;= (\textrm{bias} [\hat{\theta}_n])^2 + \textrm{Var} (\hat{\theta}_n) + \textrm{Var} [\theta].\
\end{aligned}
$$</p>
<p>우리는 위의 공식을 *편향-분산 트레이드오프(bias-variance trade-off)*라고 부릅니다. 평균 제곱 오차는 세 가지 오차 원인으로 나뉠 수 있습니다: 높은 편향으로 인한 오차, 높은 분산으로 인한 오차, 그리고 줄일 수 없는 오차입니다. 편향 오차는 특성과 출력 사이의 고차원 관계를 추출할 수 없는 단순한 모델(예: 선형 회귀 모델)에서 흔히 보입니다. 모델이 높은 편향 오차를 겪는다면, 우리는 종종 이를 (:numref:<code>sec_generalization_basics</code>에서 도입된 것처럼) <em>과소적합(underfitting)</em> 또는 <em>유연성(flexibility)</em> 부족이라고 말합니다. 높은 분산은 일반적으로 훈련 데이터에 과대적합되는 너무 복잡한 모델에서 비롯됩니다. 결과적으로 <em>과대적합(overfitting)</em> 모델은 데이터의 작은 변동에 민감합니다. 모델이 높은 분산을 겪는다면, 우리는 종종 이를 (:numref:<code>sec_generalization_basics</code>에서 도입된 것처럼) <em>과대적합</em> 및 <em>일반화(generalization)</em> 부족이라고 말합니다. 줄일 수 없는 오차는 $\theta$ 자체의 노이즈로 인한 결과입니다.</p>
<h3 id="코드로-추정량-평가하기-evaluating-estimators-in-code"><a class="header" href="#코드로-추정량-평가하기-evaluating-estimators-in-code">코드로 추정량 평가하기 (Evaluating Estimators in Code)</a></h3>
<p>추정량의 표준 편차는 텐서 <code>a</code>에 대해 단순히 <code>a.std()</code>를 호출함으로써 구현되어 왔으므로, 여기서는 생략하고 통계적 편향과 평균 제곱 오차를 구현해 보겠습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 통계적 편향
def stat_bias(true_theta, est_theta):
    return(np.mean(est_theta) - true_theta)

# 평균 제곱 오차
def mse(data, true_theta):
    return(np.mean(np.square(data - true_theta)))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# 통계적 편향
def stat_bias(true_theta, est_theta):
    return(torch.mean(est_theta) - true_theta)

# 평균 제곱 오차
def mse(data, true_theta):
    return(torch.mean(torch.square(data - true_theta)))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 통계적 편향
def stat_bias(true_theta, est_theta):
    return(tf.reduce_mean(est_theta) - true_theta)

# 평균 제곱 오차
def mse(data, true_theta):
    return(tf.reduce_mean(tf.square(data - true_theta)))
</code></pre>
<p>편향-분산 트레이드오프 방정식을 설명하기 위해, 10,000개의 표본으로 정규 분포 $\mathcal{N}(\theta, \sigma^2)$를 시뮬레이션해 봅시다. 여기서는 $\theta = 1$ 및 $\sigma = 4$를 사용합니다. 추정량은 주어진 표본들의 함수이므로, 여기서는 이 정규 분포 $\mathcal{N}(\theta, \sigma^2)$에서 실제 $\theta$에 대한 추정량으로 표본의 평균을 사용합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
theta_true = 1
sigma = 4
sample_len = 10000
samples = np.random.normal(theta_true, sigma, sample_len)
theta_est = np.mean(samples)
theta_est
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
theta_true = 1
sigma = 4
sample_len = 10000
samples = torch.normal(theta_true, sigma, size=(sample_len, 1))
theta_est = torch.mean(samples)
theta_est
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
theta_true = 1
sigma = 4
sample_len = 10000
samples = tf.random.normal((sample_len, 1), theta_true, sigma)
theta_est = tf.reduce_mean(samples)
theta_est
</code></pre>
<p>우리 추정량의 편향 제곱과 분산의 합을 계산하여 트레이드오프 방정식을 검증해 봅시다. 먼저 우리 추정량의 MSE를 계산합니다.</p>
<pre><code class="language-{.python .input}">#@tab all
mse(samples, theta_true)
</code></pre>
<p>다음으로, 아래와 같이 $\textrm{Var} (\hat{\theta}_n) + [\textrm{bias} (\hat{\theta}_n)]^2$를 계산합니다. 보시다시피, 두 값은 수치적 정밀도 내에서 일치합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
bias = stat_bias(theta_true, theta_est)
np.square(samples.std()) + np.square(bias)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
bias = stat_bias(theta_true, theta_est)
torch.square(samples.std(unbiased=False)) + torch.square(bias)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
bias = stat_bias(theta_true, theta_est)
tf.square(tf.math.reduce_std(samples)) + tf.square(bias)
</code></pre>
<h2 id="가설-검정-수행하기-conducting-hypothesis-tests"><a class="header" href="#가설-검정-수행하기-conducting-hypothesis-tests">가설 검정 수행하기 (Conducting Hypothesis Tests)</a></h2>
<p>통계적 추론에서 가장 흔히 접하는 주제는 가설 검정입니다. 가설 검정은 20세기 초에 대중화되었지만, 첫 번째 사용은 1700년대의 존 아버트넛(John Arbuthnot)으로 거슬러 올라갑니다. 존은 런던의 80년 치 출생 기록을 추적하여 매년 여성보다 남성이 더 많이 태어났다는 결론을 내렸습니다. 그 후, 현대적인 유의성 검정은 $p$-값과 피어슨의 카이제곱 검정을 발명한 칼 피어슨(Karl Pearson), 스튜던트 t-분포의 아버지인 윌리엄 고셋(William Gosset), 그리고 귀무 가설과 유의성 검정을 처음 시작한 로널드 피셔(Ronald Fisher)에 의한 지적 유산입니다.</p>
<p>*가설 검정(hypothesis test)*은 모집단에 대한 기본 진술에 반하는 어떤 증거를 평가하는 방법입니다. 우리는 기본 진술을 <em>귀무 가설(null hypothesis)</em> $H_0$라고 부르며, 관찰된 데이터를 사용하여 이를 기각하려고 시도합니다. 여기서 우리는 $H_0$를 통계적 유의성 검정의 출발점으로 사용합니다. <em>대립 가설(alternative hypothesis)</em> $H_A$ (또는 $H_1$)는 귀무 가설과 상반되는 진술입니다. 귀무 가설은 종종 변수 간의 관계를 가정하는 서술형 형식으로 명시됩니다. 그것은 가능한 한 명확하게 신념을 반영해야 하며, 통계 이론에 의해 테스트 가능해야 합니다.</p>
<p>당신이 화학자라고 상상해 보십시오. 실험실에서 수천 시간을 보낸 후, 당신은 수학을 이해하는 능력을 비약적으로 향상시킬 수 있는 새로운 약을 개발합니다. 그 마법 같은 힘을 보여주기 위해, 당신은 그것을 테스트해야 합니다. 당연히, 약을 복용하고 수학을 더 잘 배우는 데 도움이 되는지 확인해 줄 자원봉사자들이 필요할 것입니다. 어떻게 시작하시겠습니까?</p>
<p>첫째, 어떤 메트릭으로 측정했을 때 수학적 이해 능력에 차이가 없도록 신중하게 무작위로 선택된 두 그룹의 자원봉사자가 필요할 것입니다. 두 그룹은 흔히 실험군(test group)과 대조군(control group)으로 불립니다. <em>실험군</em> (또는 <em>처치군(treatment group)</em>)은 약을 경험하게 될 개인 그룹인 반면, <em>대조군</em>은 벤치마크로 설정된 사용자 그룹을 나타냅니다. 즉, 이 약을 복용하는 것을 제외하고는 동일한 환경 설정입니다. 이런 식으로, 처치에서의 독립 변수의 영향을 제외하고 모든 변수의 영향이 최소화됩니다.</p>
<p>둘째, 일정 기간 약을 복용한 후, 새로운 수학 공식을 배운 후 자원봉사자들에게 동일한 테스트를 보게 하는 것과 같이 동일한 메트릭으로 두 그룹의 수학적 이해도를 측정해야 할 것입니다. 그런 다음 그들의 성적을 수집하고 결과를 비교할 수 있습니다. 이 경우, 우리의 귀무 가설은 두 그룹 사이에 차이가 없다는 것이고, 대립 가설은 차이가 있다는 것입니다.</p>
<p>이것은 여전히 완전히 형식적이지는 않습니다. 당신이 신중하게 생각해야 할 많은 세부 사항들이 있습니다. 예를 들어, 그들의 수학적 이해 능력을 테스트하기에 적합한 메트릭은 무엇입니까? 약의 효과를 주장하는 데 확신을 가질 수 있도록 얼마나 많은 자원봉사자가 필요합니까? 테스트를 얼마나 오래 실행해야 합니까? 두 그룹 사이에 차이가 있는지 어떻게 결정합니까? 평균 성적에만 관심이 있습니까, 아니면 점수의 변동 범위에도 관심이 있습니까? 등등.</p>
<p>이런 방식으로, 가설 검정은 실험 설계와 관찰된 결과의 확실성에 대한 추론을 위한 프레임워크를 제공합니다. 이제 귀무 가설이 참일 가능성이 매우 낮다는 것을 보여줄 수 있다면, 우리는 확신을 가지고 이를 기각할 수 있습니다.</p>
<p>가설 검정을 수행하는 방법에 대한 이야기를 완성하기 위해, 이제 몇 가지 추가 용어를 도입하고 위의 개념들을 형식화해야 합니다.</p>
<h3 id="통계적-유의성-statistical-significance"><a class="header" href="#통계적-유의성-statistical-significance">통계적 유의성 (Statistical Significance)</a></h3>
<p>*통계적 유의성(statistical significance)*은 귀무 가설 $H_0$를 기각해서는 안 될 때 잘못 기각할 확률을 측정합니다. 즉,</p>
<p>$$\textrm{통계적 유의성 }= 1 - \alpha = 1 - P(\textrm{기각 } H_0 \mid H_0 \textrm{ 가 참} ).$$</p>
<p>이를 <em>제1종 오류(type I error)</em> 또는 *위양성(false positive)*이라고도 합니다. $\alpha$는 *유의 수준(significance level)*이라고 불리며 그 흔히 사용되는 값은 $5%$입니다. 즉, $1-\alpha = 95%$입니다. 유의 수준은 참인 귀무 가설을 기각할 때 우리가 감수할 용의가 있는 위험 수준으로 설명될 수 있습니다.</p>
<p>:numref:<code>fig_statistical_significance</code>는 두 표본 가설 검정에서 주어진 정규 분포의 관찰값과 확률을 보여줍니다. 관찰 데이터 예제가 $95%$ 임계값 밖에 위치한다면, 이는 귀무 가설 가정 하에서 매우 일어날 법하지 않은 관찰이 될 것입니다. 따라서 귀무 가설에 무언가 잘못되었을 수 있으며 우리는 이를 기각할 것입니다.</p>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/statistical-significance.svg" alt="통계적 유의성." />
:label:<code>fig_statistical_significance</code></p>
<h3 id="통계적-검정력-statistical-power"><a class="header" href="#통계적-검정력-statistical-power">통계적 검정력 (Statistical Power)</a></h3>
<p><em>통계적 검정력(statistical power)</em> (또는 <em>민감도(sensitivity)</em>)은 귀무 가설 $H_0$를 기각해야 할 때 기각할 확률을 측정합니다. 즉,</p>
<p>$$\textrm{통계적 검정력 }= 1 - \beta = 1 - P(\textrm{ 기각 실패 } H_0  \mid H_0 \textrm{ 가 거짓} ).$$</p>
<p><em>제1종 오류</em>는 귀무 가설이 참일 때 기각함으로써 발생하는 오류인 반면, *제2종 오류(type II error)*는 귀무 가설이 거짓일 때 기각하지 못함으로써 발생하는 오류임을 상기하십시오. 제2종 오류는 보통 $\beta$로 표시되며, 따라서 해당 통계적 검정력은 $1-\beta$입니다.</p>
<p>직관적으로, 통계적 검정력은 우리의 검정이 원하는 통계적 유의 수준에서 어떤 최소 규모의 실제 불일치를 얼마나 잘 감지할 것인지를 나타내는 것으로 해석될 수 있습니다. $80%$는 흔히 사용되는 통계적 검정력 임계값입니다. 통계적 검정력이 높을수록 실제 차이를 감지할 가능성이 높아집니다.</p>
<p>통계적 검정력의 가장 일반적인 용도 중 하나는 필요한 표본 수를 결정하는 것입니다. 귀무 가설이 거짓일 때 이를 기각할 확률은 그것이 얼마나 거짓인지( *효과 크기(effect size)*라고 함)와 당신이 가진 표본 수에 달려 있습니다. 예상할 수 있듯이, 작은 효과 크기는 높은 확률로 감지하기 위해 매우 많은 수의 표본을 필요로 할 것입니다. 이 짧은 부록의 범위를 벗어나 자세히 유도하지는 않겠지만, 예를 들어 우리 표본이 평균 0, 분산 1인 가우시안에서 왔다는 귀무 가설을 기각하고 싶고, 우리 표본의 평균이 실제로 1에 가깝다고 믿는다면, 단 $8$개의 표본 크기만으로도 허용 가능한 오차율로 그렇게 할 수 있습니다. 그러나 우리 표본 모집단의 실제 평균이 $0.01$에 가깝다고 생각한다면, 그 차이를 감지하기 위해 거의 $80,000$개의 표본 크기가 필요할 것입니다.</p>
<p>검정력을 정수기 필터로 상상해 볼 수 있습니다. 이 비유에서, 고검정력 가설 검정은 물속의 유해 물질을 가능한 많이 줄여주는 고품질 정수 시스템과 같습니다. 반면에, 더 작은 불일치는 저품질 정수 필터와 같아서 일부 상대적으로 작은 물질들이 틈새로 쉽게 빠져나갈 수 있습니다. 마찬가지로, 통계적 검정력이 충분히 높지 않으면 검정에서 더 작은 불일치를 잡아내지 못할 수 있습니다.</p>
<h3 id="검정-통계량-test-statistic"><a class="header" href="#검정-통계량-test-statistic">검정 통계량 (Test Statistic)</a></h3>
<p><em>검정 통계량(test statistic)</em> $T(x)$는 표본 데이터의 어떤 특성을 요약하는 스칼라입니다. 이러한 통계량을 정의하는 목표는 우리가 서로 다른 분포를 구별하고 가설 검정을 수행할 수 있도록 하는 것입니다. 우리의 화학자 예제를 다시 생각해 보면, 한 집단이 다른 집단보다 더 잘 수행된다는 것을 보여주고 싶다면 평균을 검정 통계량으로 취하는 것이 합리적일 수 있습니다. 검정 통계량의 다른 선택은 현저히 다른 통계적 검정력을 가진 통계 검정으로 이어질 수 있습니다.</p>
<p>종종 $T(X)$ (귀무 가설 하에서의 검정 통계량 분포)는 귀무 가설 하에서 고려될 때 정규 분포와 같은 일반적인 확률 분포를 적어도 근사적으로 따를 것입니다. 우리가 그러한 분포를 명시적으로 유도할 수 있고 우리 데이터셋에서 검정 통계량을 측정할 수 있다면, 우리 통계량이 우리가 예상하는 범위를 크게 벗어날 때 안전하게 귀무 가설을 기각할 수 있습니다. 이를 정량화하는 것은 $p$-값의 개념으로 이어집니다.</p>
<h3 id="p-값-p-value"><a class="header" href="#p-값-p-value">$p$-값 ($p$-value)</a></h3>
<p>$p$-값 (또는 <em>확률 값</em>)은 귀무 가설이 <em>참</em>이라고 가정할 때, $T(X)$가 관찰된 검정 통계량 $T(x)$만큼 극단적일 확률입니다. 즉,</p>
<p>$$ p\textrm{-값} = P_{H_0}(T(X) \geq T(x)).$$</p>
<p>$p$-값이 미리 정의되고 고정된 통계적 유의 수준 $\alpha$보다 작거나 같으면, 우리는 귀무 가설을 기각할 수 있습니다. 그렇지 않으면, 우리는 귀무 가설을 기각할 증거가 부족하다고 결론 내릴 것입니다. 주어진 모집단 분포에 대해, *기각역(region of rejection)*은 통계적 유의 수준 $\alpha$보다 작은 $p$-값을 가진 모든 점들을 포함하는 구간이 될 것입니다.</p>
<h3 id="단측-검정-및-양측-검정-one-side-test-and-two-sided-test"><a class="header" href="#단측-검정-및-양측-검정-one-side-test-and-two-sided-test">단측 검정 및 양측 검정 (One-side Test and Two-sided Test)</a></h3>
<p>보통 유의성 검정에는 두 가지 종류가 있습니다: 단측 검정과 양측 검정입니다. <em>단측 검정(one-sided test)</em> (또는 <em>한쪽 꼬리 검정</em>)은 귀무 가설과 대립 가설이 한 방향만 가질 때 적용 가능합니다. 예를 들어, 귀무 가설은 실제 파라미터 $\theta$가 어떤 값 $c$보다 작거나 같다고 명시할 수 있습니다. 대립 가설은 $\theta$가 $c$보다 크다는 것이 될 것입니다. 즉, 기각역은 표본 분포의 한쪽에만 있습니다. 단측 검정과 반대로, <em>양측 검정(two-sided test)</em> (또는 <em>양쪽 꼬리 검정</em>)은 기각역이 표본 분포의 양쪽에 있을 때 적용 가능합니다. 이 경우의 예로는 귀무 가설이 실제 파라미터 $\theta$가 어떤 값 $c$와 같다고 명시하는 경우가 있습니다. 대립 가설은 $\theta$가 $c$와 같지 않다는 것이 될 것입니다.</p>
<h3 id="가설-검정의-일반적인-단계-general-steps-of-hypothesis-testing"><a class="header" href="#가설-검정의-일반적인-단계-general-steps-of-hypothesis-testing">가설 검정의 일반적인 단계 (General Steps of Hypothesis Testing)</a></h3>
<p>위의 개념들에 익숙해진 후, 가설 검정의 일반적인 단계를 살펴봅시다.</p>
<ol>
<li>질문을 명시하고 귀무 가설 $H_0$를 설정합니다.</li>
<li>통계적 유의 수준 $\alpha$와 통계적 검정력 ($1 - \beta$)을 설정합니다.</li>
<li>실험을 통해 표본을 얻습니다. 필요한 표본 수는 통계적 검정력과 예상 효과 크기에 달려 있습니다.</li>
<li>검정 통계량과 $p$-값을 계산합니다.</li>
<li>$p$-값과 통계적 유의 수준 $\alpha$를 바탕으로 귀무 가설을 유지할지 기각할지 결정합니다.</li>
</ol>
<p>가설 검정을 수행하기 위해, 우리는 귀무 가설과 우리가 감수할 용의가 있는 위험 수준을 정의하는 것으로 시작합니다. 그런 다음 표본의 검정 통계량을 계산하고, 검정 통계량의 극단적인 값을 귀무 가설에 반하는 증거로 삼습니다. 검정 통계량이 기각역에 떨어지면, 우리는 대립 가설을 지지하며 귀무 가설을 기각할 수 있습니다.</p>
<p>가설 검정은 임상 시험 및 A/B 테스팅과 같은 다양한 시나리오에서 적용 가능합니다.</p>
<h2 id="신뢰-구간-구축하기-constructing-confidence-intervals"><a class="header" href="#신뢰-구간-구축하기-constructing-confidence-intervals">신뢰 구간 구축하기 (Constructing Confidence Intervals)</a></h2>
<p>파라미터 $\theta$의 값을 추정할 때, $\hat \theta$와 같은 점 추정량은 불확실성의 개념을 포함하지 않기 때문에 유용성이 제한적입니다. 오히려, 높은 확률로 실제 파라미터 $\theta$를 포함할 구간을 생성할 수 있다면 훨씬 더 좋을 것입니다. 만약 당신이 한 세기 전에 그러한 아이디어에 관심이 있었다면, 1937년에 신뢰 구간의 개념을 처음 도입한 예르지 네이만(Jerzy Neyman)의 "Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability" :cite:<code>Neyman.1937</code>를 읽고 흥분했을 것입니다.</p>
<p>유용하려면, 신뢰 구간은 주어진 확실성 정도에 대해 가능한 한 작아야 합니다. 이를 유도하는 방법을 알아봅시다.</p>
<h3 id="정의-definition-1"><a class="header" href="#정의-definition-1">정의 (Definition)</a></h3>
<p>수학적으로, 실제 파라미터 $\theta$에 대한 *신뢰 구간(confidence interval)*은 표본 데이터로부터 계산된 구간 $C_n$으로, 다음을 만족합니다.</p>
<p>$$P_{\theta} (C_n \ni \theta) \geq 1 - \alpha, \forall \theta.$$
:eqlabel:<code>eq_confidence</code></p>
<p>여기서 $\alpha \in (0, 1)$이고, $1 - \alpha$는 구간의 <em>신뢰 수준(confidence level)</em> 또는 *커버리지(coverage)*라고 불립니다. 이것은 위에서 논의한 유의 수준과 동일한 $\alpha$입니다.</p>
<p>:eqref:<code>eq_confidence</code>는 고정된 $\theta$가 아니라 변수 $C_n$에 관한 것임에 유의하십시오. 이를 강조하기 위해, $P_{\theta} (\theta \in C_n)$ 대신 $P_{\theta} (C_n \ni \theta)$라고 씁니다.</p>
<h3 id="해석-interpretation"><a class="header" href="#해석-interpretation">해석 (Interpretation)</a></h3>
<p>$95%$ 신뢰 구간을 실제 파라미터가 존재할 확률이 $95%$인 구간으로 해석하고 싶은 유혹이 매우 강하지만, 아쉽게도 이는 사실이 아닙니다. 실제 파라미터는 고정되어 있고, 무작위인 것은 바로 구간입니다. 따라서 더 나은 해석은 이 절차에 의해 많은 수의 신뢰 구간을 생성했다면, 생성된 구간의 $95%$가 실제 파라미터를 포함할 것이라고 말하는 것입니다.</p>
<p>이는 매우 까다롭게 보일 수 있지만, 결과의 해석에 실제적인 영향을 미칠 수 있습니다. 특히, 우리는 거의 드물게만 그렇게 한다면, 실제 값을 포함하지 않는다는 것이 <em>거의 확실한</em> 구간을 구축함으로써 :eqref:<code>eq_confidence</code>를 충족할 수 있습니다. 이 섹션을 마치며 유혹적이지만 틀린 세 가지 진술을 제공합니다. 이러한 점들에 대한 심층적인 논의는 :citet:<code>Morey.Hoekstra.Rouder.ea.2016</code>에서 찾을 수 있습니다.</p>
<ul>
<li><strong>오류 1</strong>. 좁은 신뢰 구간은 파라미터를 정밀하게 추정할 수 있음을 의미한다.</li>
<li><strong>오류 2</strong>. 신뢰 구간 내부의 값이 구간 외부의 값보다 실제 값일 가능성이 더 높다.</li>
<li><strong>오류 3</strong>. 특정하게 관찰된 $95%$ 신뢰 구간이 실제 값을 포함할 확률은 $95%$이다.</li>
</ul>
<p>말하자면, 신뢰 구간은 미묘한 대상입니다. 그러나 해석을 명확히 유지한다면 강력한 도구가 될 수 있습니다.</p>
<h3 id="가우시안-예제-a-gaussian-example"><a class="header" href="#가우시안-예제-a-gaussian-example">가우시안 예제 (A Gaussian Example)</a></h3>
<p>가장 고전적인 예제인, 평균과 분산을 모르는 가우시안 평균에 대한 신뢰 구간을 논의해 봅시다. 가우시안 $\mathcal{N}(\mu, \sigma^2)$에서 $n$개의 표본 {$x_i$}$_{i=1}^n$을 수집한다고 가정합시다. 우리는 다음과 같이 평균과 분산에 대한 추정량을 계산할 수 있습니다.</p>
<p>$$\hat\mu_n = \frac{1}{n}\sum_{i=1}^n x_i ;\textrm{및}; \hat{\sigma}^2_n = \frac{1}{n-1}\sum_{i=1}^n (x_i - \hat{\mu})^2.$$</p>
<p>이제 확률 변수</p>
<p>$$
T = \frac{\hat{\mu}_n - \mu}{\hat{\sigma}_n/\sqrt{n}},
$$</p>
<p>를 고려하면, 우리는 <em>자유도가</em> $n-1$ <em>인 스튜던트 t-분포</em>라고 불리는 잘 알려진 분포를 따르는 확률 변수를 얻습니다.</p>
<p>이 분포는 매우 잘 연구되어 있으며, 예를 들어 $n\rightarrow \infty$임에 따라 대략 표준 가우시안이 된다는 것이 알려져 있습니다. 따라서 표에서 가우시안 c.d.f. 값을 찾아봄으로써, $T$의 값이 적어도 $95%$의 시간에 구간 $[-1.96, 1.96]$에 있다고 결론 내릴 수 있습니다. 유한한 $n$ 값의 경우 구간은 다소 더 커야 하지만, 잘 알려져 있으며 표에 미리 계산되어 있습니다.</p>
<p>따라서 우리는 큰 $n$에 대해 다음과 같이 결론 내릴 수 있습니다.</p>
<p>$$ P\left(\frac{\hat{\mu}_n - \mu}{\hat{\sigma}_n/\sqrt{n}} \in [-1.96, 1.96]\right) \geq 0.95. $$</p>
<p>양변에 $\hat{\sigma}_n/\sqrt{n}$을 곱한 다음 $\hat{\mu}_n$을 더하여 이를 재배열하면 다음을 얻습니다.</p>
<p>$$ P\left(\mu \in \left[\hat{\mu}_n - 1.96\frac{\hat{\sigma}_n}{\sqrt{n}}, \hat{\mu}_n + 1.96\frac{\hat{\sigma}_n}{\sqrt{n}}\right]\right) \geq 0.95. $$</p>
<p>이렇게 해서 우리는 $95%$ 신뢰 구간을 찾았음을 알게 됩니다:
$$\left[\hat{\mu}_n - 1.96\frac{\hat{\sigma}_n}{\sqrt{n}}, \hat{\mu}_n + 1.96\frac{\hat{\sigma}_n}{\sqrt{n}}\right].$$
:eqlabel:<code>eq_gauss_confidence</code></p>
<p>:eqref:<code>eq_gauss_confidence</code>는 통계학에서 가장 많이 사용되는 공식 중 하나라고 해도 과언이 아닙니다. 이 공식을 구현함으로써 통계에 대한 논의를 마칩니다. 단순함을 위해 점근적 영역(asymptotic regime)에 있다고 가정합니다. $N$의 작은 값은 프로그래밍 방식이나 $t$-표로부터 얻은 <code>t_star</code>의 올바른 값을 포함해야 합니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
# 표본 수
N = 1000

# 표본 데이터셋
samples = np.random.normal(loc=0, scale=1, size=(N,))

# 스튜던트 t-분포 c.d.f. 조회
t_star = 1.96

# 구간 구축
mu_hat = np.mean(samples)
sigma_hat = samples.std(ddof=1)
(mu_hat - t_star*sigma_hat/np.sqrt(N), mu_hat + t_star*sigma_hat/np.sqrt(N))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# PyTorch는 기본적으로 베셀 보정(Bessel's correction)을 사용하며, 이는 numpy의 기본 ddof=0 대신 
# ddof=1을 사용함을 의미합니다. ddof=0을 모방하기 위해 unbiased=False를 사용할 수 있습니다.

# 표본 수
N = 1000

# 표본 데이터셋
samples = torch.normal(0, 1, size=(N,))

# 스튜던트 t-분포 c.d.f. 조회
t_star = 1.96

# 구간 구축
mu_hat = torch.mean(samples)
sigma_hat = samples.std(unbiased=True)
(mu_hat - t_star*sigma_hat/torch.sqrt(torch.tensor(N, dtype=torch.float32)),\
 mu_hat + t_star*sigma_hat/torch.sqrt(torch.tensor(N, dtype=torch.float32)))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
# 표본 수
N = 1000

# 표본 데이터셋
samples = tf.random.normal((N,), 0, 1)

# 스튜던트 t-분포 c.d.f. 조회
t_star = 1.96

# 구간 구축
mu_hat = tf.reduce_mean(samples)
sigma_hat = tf.math.reduce_std(samples)
(mu_hat - t_star*sigma_hat/tf.sqrt(tf.constant(N, dtype=tf.float32)), \
 mu_hat + t_star*sigma_hat/tf.sqrt(tf.constant(N, dtype=tf.float32)))
</code></pre>
<h2 id="요약-summary-125"><a class="header" href="#요약-summary-125">요약 (Summary)</a></h2>
<ul>
<li>통계학은 추론 문제에 집중하는 반면, 딥러닝은 명시적인 프로그래밍과 이해 없이 정확한 예측을 하는 것을 강조합니다.</li>
<li>세 가지 일반적인 통계 추론 방법이 있습니다: 추정량 평가 및 비교, 가설 검정 수행, 신뢰 구간 구축입니다.</li>
<li>가장 일반적인 세 가지 추정량이 있습니다: 통계적 편향, 표준 편차, 그리고 평균 제곱 오차입니다.</li>
<li>신뢰 구간은 주어진 표본들로부터 구축할 수 있는 실제 모집단 파라미터의 추정 범위입니다.</li>
<li>가설 검정은 모집단에 대한 기본 진술에 반하는 어떤 증거를 평가하는 방법입니다.</li>
</ul>
<h2 id="연습-문제-exercises-140"><a class="header" href="#연습-문제-exercises-140">연습 문제 (Exercises)</a></h2>
<ol>
<li>$X_1, X_2, \ldots, X_n \overset{\textrm{iid}}{\sim} \textrm{Unif}(0, \theta)$라고 합시다. 여기서 "iid"는 *독립적이고 동일하게 분포된(independent and identically distributed)*을 의미합니다. $\theta$의 다음 추정량들을 고려해 보십시오:
$$\hat{\theta} = \max {X_1, X_2, \ldots, X_n };
$$
$$\tilde{\theta} = 2 \bar{X_n} = \frac{2}{n} \sum_{i=1}^n X_i.$$
<ul>
<li>$\hat{\theta}$의 통계적 편향, 표준 편차, 그리고 평균 제곱 오차를 구하십시오.</li>
<li>$\tilde{\theta}$의 통계적 편향, 표준 편차, 그리고 평균 제곱 오차를 구하십시오.</li>
<li>어떤 추정량이 더 좋습니까?</li>
</ul>
</li>
<li>서론의 화학자 예제에 대해, 양측 가설 검정을 수행하기 위한 5단계를 유도할 수 있습니까? 통계적 유의 수준 $\alpha = 0.05$와 통계적 검정력 $1 - \beta = 0.8$이 주어졌다고 가정합니다.</li>
<li>독립적으로 생성된 100개의 데이터셋에 대해 $N=2$ 및 $\alpha = 0.5$로 신뢰 구간 코드를 실행하고, 결과 구간들을 플롯하십시오 (이 경우 <code>t_star = 1.0</code>). 실제 평균 0을 포함하는 것과는 거리가 먼 매우 짧은 구간들을 몇 개 보게 될 것입니다. 이것이 신뢰 구간의 해석과 모순됩니까? 짧은 구간을 고정밀 추정치를 나타내는 데 사용하는 것이 편안하게 느껴집니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/419">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1102">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1103">토론</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="정보-이론-information-theory"><a class="header" href="#정보-이론-information-theory">정보 이론 (Information Theory)</a></h1>
<p>:label:<code>sec_information_theory</code></p>
<p>우주는 정보로 넘쳐나고 있습니다. 정보는 학문적 차이를 가로지르는 공통 언어를 제공합니다: 셰익스피어의 소네트에서 코넬 ArXiv의 연구자 논문에 이르기까지, 반 고흐의 별이 빛나는 밤에서 베토벤의 교향곡 5번에 이르기까지, 최초의 프로그래밍 언어 Plankalkül에서 최첨단 머신러닝 알고리즘에 이르기까지 말입니다. 모든 것은 형식이 무엇이든 정보 이론의 규칙을 따라야 합니다. 정보 이론을 통해 우리는 서로 다른 신호에 얼마나 많은 정보가 존재하는지 측정하고 비교할 수 있습니다. 이 섹션에서는 정보 이론의 기본 개념과 머신러닝에서의 정보 이론 응용을 조사할 것입니다.</p>
<p>시작하기 전에 머신러닝과 정보 이론 사이의 관계를 개략적으로 설명해 봅시다. 머신러닝은 데이터에서 흥미로운 신호를 추출하고 중요한 예측을 하는 것을 목표로 합니다. 반면에 정보 이론은 정보를 인코딩, 디코딩, 전송 및 조작하는 것을 연구합니다. 결과적으로 정보 이론은 머신러닝 시스템에서의 정보 처리를 논의하기 위한 근본적인 언어를 제공합니다. 예를 들어, 많은 머신러닝 응용 프로그램은 :numref:<code>sec_softmax</code>에서 설명한 대로 크로스 엔트로피 손실을 사용합니다. 이 손실은 정보 이론적 고려 사항에서 직접 도출될 수 있습니다.</p>
<h2 id="정보-information"><a class="header" href="#정보-information">정보 (Information)</a></h2>
<p>정보 이론의 "영혼"인 정보부터 시작하겠습니다. *정보(Information)*는 하나 이상의 인코딩 형식의 특정 시퀀스를 가진 무엇이든 인코딩될 수 있습니다. 우리가 정보라는 개념을 정의하려고 노력하는 과제를 스스로에게 맡겼다고 가정해 봅시다. 우리의 시작점은 무엇이 될 수 있을까요?</p>
<p>다음 사고 실험을 고려해 보십시오. 카드 한 덱을 가진 친구가 있습니다. 그들은 덱을 섞고, 카드 몇 장을 뒤집고, 카드에 대한 진술을 우리에게 말해줄 것입니다. 우리는 각 진술의 정보 내용을 평가하려고 노력할 것입니다.</p>
<p>먼저, 그들은 카드를 한 장 뒤집고 "카드가 보여."라고 말합니다. 이것은 우리에게 아무런 정보도 제공하지 않습니다. 우리는 이미 그것이 사실이라는 것을 확신했으므로 정보가 0이 되기를 바랍니다.</p>
<p>다음으로, 그들은 카드를 한 장 뒤집고 "하트가 보여."라고 말합니다. 이것은 우리에게 약간의 정보를 제공하지만, 실제로는 가능한 4가지 다른 무늬가 있고 각각의 가능성이 동일했으므로 우리는 이 결과에 놀라지 않습니다. 정보의 척도가 무엇이든 이 이벤트는 낮은 정보 내용을 가져야 할 것입니다.</p>
<p>다음으로, 그들은 카드를 한 장 뒤집고 "이것은 스페이드 3이야."라고 말합니다. 이것은 더 많은 정보입니다. 실제로 52가지의 동등하게 가능성 있는 결과가 있었고, 우리 친구는 그중 어느 것인지 알려주었습니다. 이것은 중간 정도의 정보량이어야 합니다.</p>
<p>이것을 논리적 극단으로 가져가 봅시다. 마지막으로 그들이 덱의 모든 카드를 뒤집고 섞인 덱의 전체 시퀀스를 읽어준다고 가정해 봅시다. 덱에는 $52!$가지의 다른 순서가 있고, 다시 모두 동등하게 가능성이 높으므로 그것이 어느 것인지 알기 위해 많은 정보가 필요합니다.</p>
<p>우리가 개발하는 정보의 개념은 이 직관에 부합해야 합니다. 실제로 다음 섹션에서 우리는 이러한 이벤트들이 각각 $0\textrm{ 비트}$, $2\textrm{ 비트}$, $~5.7\textrm{ 비트}$, $~225.6\textrm{ 비트}$의 정보를 가지고 있음을 계산하는 방법을 배울 것입니다.</p>
<p>이러한 사고 실험을 읽어보면 자연스러운 아이디어가 보입니다. 시작점으로서 지식에 신경 쓰기보다는, 정보가 놀라움의 정도나 이벤트의 추상적인 가능성을 나타낸다는 아이디어를 기반으로 구축할 수 있습니다. 예를 들어, 특이한 이벤트를 설명하려면 많은 정보가 필요합니다. 흔한 이벤트의 경우 많은 정보가 필요하지 않을 수 있습니다.</p>
<p>1948년, 클로드 E. 섀넌(Claude E. Shannon)은 정보 이론을 정립한 <em>통신의 수학적 이론(A Mathematical Theory of Communication)</em> :cite:<code>Shannon.1948</code>을 발표했습니다. 그의 기사에서 섀넌은 정보 엔트로피라는 개념을 처음으로 도입했습니다. 여기서 우리의 여정을 시작하겠습니다.</p>
<h3 id="자기-정보-self-information"><a class="header" href="#자기-정보-self-information">자기 정보 (Self-information)</a></h3>
<p>정보가 이벤트의 추상적인 가능성을 구체화한다면, 그 가능성을 비트 수로 어떻게 매핑할까요? 섀넌은 존 투키(John Tukey)가 처음 만든 정보의 단위로 *비트(bit)*라는 용어를 도입했습니다. 그렇다면 "비트"란 무엇이며 왜 정보를 측정하는 데 그것을 사용할까요? 역사적으로 오래된 송신기는 $0$과 $1$이라는 두 가지 유형의 코드만 보내거나 받을 수 있었습니다. 실제로 이진 인코딩은 모든 현대 디지털 컴퓨터에서 여전히 공통적으로 사용됩니다. 이런 식으로 모든 정보는 일련의 $0$과 $1$로 인코딩됩니다. 따라서 길이가 $n$인 일련의 이진 숫자는 $n$비트의 정보를 포함합니다.</p>
<p>이제 임의의 일련의 코드에 대해 각 $0$ 또는 $1$이 $rac{1}{2}$의 확률로 발생한다고 가정합시다. 따라서 길이가 $n$인 일련의 코드를 가진 이벤트 $X$는 $rac{1}{2^n}$의 확률로 발생합니다. 동시에 앞서 언급했듯이 이 시리즈는 $n$비트의 정보를 포함합니다. 그렇다면 확률 $p$를 비트 수로 변환할 수 있는 수학적 함수로 일반화할 수 있을까요? 섀넌은 *자기 정보(self-information)*를 다음과 같이 정의하여 답을 주었습니다.</p>
<p>$$I(X) = - \log_2 (p),$$</p>
<p>이 이벤트 $X$에 대해 우리가 받은 정보의 <em>비트</em>입니다. 이 섹션에서는 항상 밑이 2인 로그를 사용할 것임에 유의하십시오. 단순함을 위해 이 섹션의 나머지 부분에서는 로그 표기법에서 아래 첨자 2를 생략할 것입니다. 즉, $\log(.)$은 항상 $\log_2(.)$을 지칭합니다. 예를 들어 코드 "0010"은 다음과 같은 자기 정보를 갖습니다.</p>
<p>$$I(\textrm{"0010"}) = - \log (p(\textrm{"0010"})) = - \log \left( \frac{1}{2^4} \right) = 4 \textrm{ 비트}.$$</p>
<p>아래와 같이 자기 정보를 계산할 수 있습니다. 그 전에 먼저 이 섹션에 필요한 모든 패키지를 가져옵시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
from mxnet import np
from mxnet.metric import NegativeLogLikelihood
from mxnet.ndarray import nansum
import random

def self_information(p):
    return -np.log2(p)

self_information(1 / 64)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
import torch
from torch.nn import NLLLoss

def nansum(x):
    # pytorch에는 nansum이 내장되어 있지 않으므로 정의합니다.
    return x[~torch.isnan(x)].sum()

def self_information(p):
    return -torch.log2(torch.tensor(p)).item()

self_information(1 / 64)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
import tensorflow as tf

def log2(x):
    return tf.math.log(x) / tf.math.log(2.)

def nansum(x):
    return tf.reduce_sum(tf.where(tf.math.is_nan(
        x), tf.zeros_like(x), x), axis=-1)

def self_information(p):
    return -log2(tf.constant(p)).numpy()

self_information(1 / 64)
</code></pre>
<h2 id="엔트로피-entropy-1"><a class="header" href="#엔트로피-entropy-1">엔트로피 (Entropy)</a></h2>
<p>자기 정보는 단일 이산 이벤트의 정보만 측정하므로, 이산 또는 연속 분포의 임의의 확률 변수에 대해 더 일반화된 척도가 필요합니다.</p>
<h3 id="엔트로피의-동기-motivating-entropy"><a class="header" href="#엔트로피의-동기-motivating-entropy">엔트로피의 동기 (Motivating Entropy)</a></h3>
<p>우리가 원하는 것을 구체화해 봅시다. 이것은 <em>섀넌 엔트로피의 공리</em>라고 알려진 것들에 대한 비공식적인 진술이 될 것입니다. 다음의 상식적인 진술 모음이 우리를 정보의 고유한 정의로 이끌 것이라는 점이 밝혀질 것입니다. 이러한 공리들의 공식적인 버전은 다른 여러 공리들과 함께 :citet:<code>Csiszar.2008</code>에서 찾을 수 있습니다.</p>
<ol>
<li>확률 변수를 관찰함으로써 얻는 정보는 우리가 요소를 무엇이라고 부르는지, 또는 확률이 0인 추가 요소의 존재 여부에 의존하지 않습니다.</li>
<li>두 확률 변수를 관찰함으로써 얻는 정보는 그것들을 따로 관찰함으로써 얻는 정보의 합보다 크지 않습니다. 만약 그것들이 독립적이라면 정확히 그 합과 같습니다.</li>
<li>(거의) 확실한 이벤트를 관찰할 때 얻는 정보는 (거의) 0입니다.</li>
</ol>
<p>이 사실을 증명하는 것은 우리 텍스트의 범위를 벗어나지만, 이것이 엔트로피가 취해야 할 형태를 고유하게 결정한다는 것을 아는 것이 중요합니다. 이것들이 허용하는 유일한 모호함은 기본 단위의 선택에 있으며, 이는 단일 공정한 동전 던지기에 의해 제공되는 정보가 1비트라는 우리가 이전에 보았던 선택을 함으로써 가장 자주 정규화됩니다.</p>
<h3 id="정의-definition-2"><a class="header" href="#정의-definition-2">정의 (Definition)</a></h3>
<p>확률 밀도 함수(p.d.f.) 또는 확률 질량 함수(p.m.f.) $p(x)$를 갖는 확률 분포 $P$를 따르는 임의의 확률 변수 $X$에 대해, 우리는 <em>엔트로피(entropy)</em> (또는 <em>섀넌 엔트로피</em>)를 통해 예상 정보량을 측정합니다.</p>
<p>$$H(X) = - E_{x \sim P} [\log p(x)].$$
:eqlabel:<code>eq_ent_def</code></p>
<p>구체적으로, $X$가 이산형인 경우, $$H(X) = - \sum_i p_i \log p_i \textrm{, 여기서 } p_i = P(X_i).$$</p>
<p>그렇지 않고 $X$가 연속형인 경우 엔트로피를 *미분 엔트로피(differential entropy)*라고도 부릅니다.</p>
<p>$$H(X) = - \int_x p(x) \log p(x) ; dx.$$</p>
<p>아래와 같이 엔트로피를 정의할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def entropy(p):
    entropy = - p * np.log2(p)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(entropy.as_nd_ndarray())
    return out

entropy(np.array([0.1, 0.5, 0.1, 0.3]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def entropy(p):
    entropy = - p * torch.log2(p)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(entropy)
    return out

entropy(torch.tensor([0.1, 0.5, 0.1, 0.3]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def entropy(p):
    return nansum(- p * log2(p))

entropy(tf.constant([0.1, 0.5, 0.1, 0.3]))
</code></pre>
<h3 id="해석-interpretations"><a class="header" href="#해석-interpretations">해석 (Interpretations)</a></h3>
<p>궁금하실 수 있습니다: 엔트로피 정의 :eqref:<code>eq_ent_def</code>에서 왜 음의 로그의 기대값을 사용할까요? 여기 몇 가지 직관이 있습니다.</p>
<p>먼저, 왜 <em>로그</em> 함수 $\log$를 사용할까요? $p(x) = f_1(x) f_2(x) \ldots, f_n(x)$라고 가정해 봅시다. 여기서 각 성분 함수 $f_i(x)$는 서로 독립적입니다. 이는 각 $f_i(x)$가 $p(x)$로부터 얻은 총 정보에 독립적으로 기여함을 의미합니다. 위에서 논의한 대로, 우리는 엔트로피 공식이 독립 확률 변수들에 대해 가산적(additive)이기를 원합니다. 다행히 $\log$는 자연스럽게 확률 분포의 곱을 개별 항들의 합으로 바꿀 수 있습니다.</p>
<p>다음으로, 왜 <em>음의</em> $\log$를 사용할까요? 직관적으로, 더 빈번한 이벤트는 덜 흔한 이벤트보다 적은 정보를 포함해야 합니다. 평범한 경우보다 특이한 경우에서 종종 더 많은 정보를 얻기 때문입니다. 그러나 $\log$는 확률에 따라 단조 증가하며, 실제로 $[0, 1]$의 모든 값에 대해 음수입니다. 우리는 이벤트의 확률과 그들의 엔트로피 사이에 단조 감소 관계를 구축해야 하며, 이는 이상적으로 항상 양수여야 합니다(우리가 관찰하는 어떤 것도 우리가 알고 있는 것을 잊도록 강제해서는 안 되기 때문입니다). 따라서 $\log$ 함수 앞에 음수 부호를 붙입니다.</p>
<p>마지막으로, <em>기대값</em> 함수는 어디에서 왔을까요? 확률 변수 $X$를 고려해 보십시오. 자기 정보($-\log(p)$)를 특정 결과를 볼 때 우리가 느끼는 <em>놀라움</em>의 양으로 해석할 수 있습니다. 실제로 확률이 0에 가까워질수록 놀라움은 무한대가 됩니다. 마찬가지로, 엔트로피를 $X$를 관찰함으로써 얻는 평균 놀라움의 양으로 해석할 수 있습니다. 예를 들어, 슬롯머신 시스템이 각각 확률 ${p_1, \ldots, p_k}$로 기호 ${s_1, \ldots, s_k}$를 통계적으로 독립적으로 내보낸다고 상상해 보십시오. 그러면 이 시스템의 엔트로피는 각 출력을 관찰함으로써 얻는 평균 자기 정보와 같습니다. 즉,</p>
<p>$$H(S) = \sum_i {p_i \cdot I(s_i)} = - \sum_i {p_i \cdot \log p_i}.$$</p>
<h3 id="엔트로피의-속성-properties-of-entropy"><a class="header" href="#엔트로피의-속성-properties-of-entropy">엔트로피의 속성 (Properties of Entropy)</a></h3>
<p>위의 예제와 해석을 통해 엔트로피 :eqref:<code>eq_ent_def</code>의 다음과 같은 속성을 도출할 수 있습니다. 여기서 $X$를 이벤트로, $P$를 $X$의 확률 분포로 지칭합니다.</p>
<ul>
<li>
<p>모든 이산 $X$에 대해 $H(X) \geq 0$입니다(연속 $X$의 경우 엔트로피가 음수일 수 있습니다).</p>
</li>
<li>
<p>$X \sim P$가 p.d.f. 또는 p.m.f. $p(x)$를 갖고, 우리가 $P$를 p.d.f. 또는 p.m.f. $q(x)$를 갖는 새로운 확률 분포 $Q$로 추정하려고 한다면, $$H(X) = - E_{x \sim P} [\log p(x)] \leq  - E_{x \sim P} [\log q(x)] \textrm{이며, 등호는 } P = Q \textrm{일 때만 성립합니다.}}$$ 대안적으로, $H(X)$는 $P$에서 추출된 기호를 인코딩하는 데 필요한 평균 비트 수의 하한을 제공합니다.</p>
</li>
<li>
<p>$X \sim P$인 경우, $x$가 가능한 모든 결과에 고르게 퍼져 있을 때 최대량의 정보를 전달합니다. 구체적으로, 확률 분포 $P$가 $k$-클래스 ${p_1, \ldots, p_k }$를 갖는 이산형인 경우, $$H(X) \leq \log(k) \textrm{이며, 등호는 모든 } i \textrm{에 대해 } p_i = \frac{1}{k} \textrm{일 때만 성립합니다.}}$$ $P$가 연속 확률 변수라면 이야기는 훨씬 더 복잡해집니다. 그러나 추가적으로 $P$가 유한한 구간(0과 1 사이의 모든 값)에서 지원된다고 부과하면, $P$가 해당 구간에서 균등 분포일 때 가장 높은 엔트로피를 갖습니다.</p>
</li>
</ul>
<h2 id="상호-정보량-mutual-information"><a class="header" href="#상호-정보량-mutual-information">상호 정보량 (Mutual Information)</a></h2>
<p>이전에 단일 확률 변수 $X$의 엔트로피를 정의했는데, 한 쌍의 확률 변수 $(X, Y)$의 엔트로피는 어떨까요? 우리는 이러한 기술을 다음과 같은 유형의 질문에 답하려는 시도로 생각할 수 있습니다. "X와 Y가 따로따로 있을 때와 비교하여 함께 있을 때 어떤 정보가 포함되어 있는가? 중복된 정보가 있는가, 아니면 모두 고유한가?"</p>
<p>다음 논의를 위해, 우리는 항상 $(X, Y)$를 결합 확률 분포 $P$(p.d.f. 또는 p.m.f. $p_{X, Y}(x, y)$ 가짐)를 따르는 확률 변수 쌍으로 사용하고, $X$와 $Y$는 각각 확률 분포 $p_X(x)$와 $p_Y(y)$를 따릅니다.</p>
<h3 id="결합-엔트로피-joint-entropy"><a class="header" href="#결합-엔트로피-joint-entropy">결합 엔트로피 (Joint Entropy)</a></h3>
<p>단일 확률 변수의 엔트로피 :eqref:<code>eq_ent_def</code>와 유사하게, 우리는 확률 변수 쌍 $(X, Y)$의 <em>결합 엔트로피(joint entropy)</em> $H(X, Y)$를 다음과 같이 정의합니다.</p>
<p>$$H(X, Y) = -E_{(x, y) \sim P} [\log p_{X, Y}(x, y)]. $$
:eqlabel:<code>eq_joint_ent_def</code></p>
<p>정확하게 말하면, 한편으로 $(X, Y)$가 이산 확률 변수 쌍이라면 다음과 같습니다.</p>
<p>$$H(X, Y) = - \sum_{x} \sum_{y} p_{X, Y}(x, y) \log p_{X, Y}(x, y).$$</p>
<p>다른 한편으로, $(X, Y)$가 연속 확률 변수 쌍이라면 <em>미분 결합 엔트로피</em>를 다음과 같이 정의합니다.</p>
<p>$$H(X, Y) = - \int_{x, y} p_{X, Y}(x, y) \ \log p_{X, Y}(x, y) ;dx ;dy.$$</p>
<p>우리는 :eqref:<code>eq_joint_ent_def</code>를 확률 변수 쌍의 총 무작위성을 알려주는 것으로 생각할 수 있습니다. 극단적인 한 쌍으로서, $X = Y$가 두 개의 동일한 확률 변수라면 쌍의 정보는 정확히 하나의 정보와 같으며 $H(X, Y) = H(X) = H(Y)$를 갖습니다. 다른 극단으로서, $X$와 $Y$가 독립적이라면 $H(X, Y) = H(X) + H(Y)$입니다. 실제로 한 쌍의 확률 변수에 포함된 정보는 어느 한 확률 변수의 엔트로피보다 작지 않고 두 엔트로피의 합보다 크지 않음을 항상 갖게 될 것입니다.</p>
<p>$$
H(X), H(Y) \le H(X, Y) \le H(X) + H(Y).
$$</p>
<p>결합 엔트로피를 밑바닥부터 구현해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def joint_entropy(p_xy):
    joint_ent = -p_xy * np.log2(p_xy)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(joint_ent.as_nd_ndarray())
    return out

joint_entropy(np.array([[0.1, 0.5], [0.1, 0.3]]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def joint_entropy(p_xy):
    joint_ent = -p_xy * torch.log2(p_xy)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(joint_ent)
    return out

joint_entropy(torch.tensor([[0.1, 0.5], [0.1, 0.3]]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def joint_entropy(p_xy):
    joint_ent = -p_xy * log2(p_xy)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(joint_ent)
    return out

joint_entropy(tf.constant([[0.1, 0.5], [0.1, 0.3]]))
</code></pre>
<p>이것은 이전과 동일한 <em>코드</em>이지만, 이제는 두 확률 변수의 결합 분포에 대해 작동하는 것으로 다르게 해석한다는 점에 유의하십시오.</p>
<h3 id="조건부-엔트로피-conditional-entropy"><a class="header" href="#조건부-엔트로피-conditional-entropy">조건부 엔트로피 (Conditional Entropy)</a></h3>
<p>위에서 정의된 결합 엔트로피는 확률 변수 쌍에 포함된 정보량입니다. 이것은 유용하지만, 우리가 관심을 갖는 것이 아닌 경우가 많습니다. 머신러닝의 설정을 고려해 보십시오. $X$를 이미지의 픽셀 값을 설명하는 확률 변수(또는 확률 변수 벡터)로, $Y$를 클래스 레이블인 확률 변수로 취해 봅시다. $X$는 상당한 정보를 포함해야 합니다 - 자연 이미지는 복잡한 것입니다. 그러나 이미지가 보여진 후 $Y$에 포함된 정보는 낮아야 합니다. 실제로 숫자의 이미지는 숫자가 읽을 수 없는 경우가 아니면 그것이 어떤 숫자인지에 대한 정보를 이미 포함하고 있어야 합니다. 따라서 정보 이론의 어휘를 계속 확장하려면 다른 확률 변수에 조건부인 확률 변수의 정보 내용을 추론할 수 있어야 합니다.</p>
<p>확률론에서 우리는 변수 간의 관계를 측정하기 위해 <em>조건부 확률</em>의 정의를 보았습니다. 우리는 이제 유사하게 <em>조건부 엔트로피(conditional entropy)</em> $H(Y \mid X)$를 정의하고자 합니다. 우리는 이를 다음과 같이 쓸 수 있습니다.</p>
<p>$$ H(Y \mid X) = - E_{(x, y) \sim P} [\log p(y \mid x)],$$
:eqlabel:<code>eq_cond_ent_def</code></p>
<p>여기서 $p(y \mid x) = \frac{p_{X, Y}(x, y)}{p_X(x)}$는 조건부 확률입니다. 구체적으로, $(X, Y)$가 이산 확률 변수 쌍이라면 다음과 같습니다.</p>
<p>$$H(Y \mid X) = - \sum_{x} \sum_{y} p(x, y) \log p(y \mid x).$$</p>
<p>$(X, Y)$가 연속 확률 변수 쌍이라면 <em>미분 조건부 엔트로피</em>는 다음과 같이 유사하게 정의됩니다.</p>
<p>$$H(Y \mid X) = - \int_x \int_y p(x, y) \ \log p(y \mid x) ;dx ;dy.$$</p>
<p>이제 <em>조건부 엔트로피</em> $H(Y \mid X)$가 엔트로피 $H(X)$ 및 결합 엔트로피 $H(X, Y)$와 어떤 관련이 있는지 묻는 것이 자연스럽습니다. 위의 정의를 사용하면 이를 다음과 같이 깔끔하게 표현할 수 있습니다.</p>
<p>$$H(Y \mid X) = H(X, Y) - H(X).$$</p>
<p>이것은 직관적인 해석을 갖습니다: $X$가 주어졌을 때 $Y$의 정보($H(Y \mid X)$)는 $X$와 $Y$가 함께 있을 때의 정보($H(X, Y)$)에서 이미 $X$에 포함된 정보($H(X)$)를 뺀 것과 같습니다. 이는 $X$에도 표현되지 않은 $Y$의 정보를 우리에게 제공합니다.</p>
<p>이제 조건부 엔트로피 :eqref:<code>eq_cond_ent_def</code>를 밑바닥부터 구현해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def conditional_entropy(p_xy, p_x):
    p_y_given_x = p_xy/p_x
    cond_ent = -p_xy * np.log2(p_y_given_x)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(cond_ent.as_nd_ndarray())
    return out

conditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def conditional_entropy(p_xy, p_x):
    p_y_given_x = p_xy/p_x
    cond_ent = -p_xy * torch.log2(p_y_given_x)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(cond_ent)
    return out

conditional_entropy(torch.tensor([[0.1, 0.5], [0.2, 0.3]]),
                    torch.tensor([0.2, 0.8]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def conditional_entropy(p_xy, p_x):
    p_y_given_x = p_xy/p_x
    cond_ent = -p_xy * log2(p_y_given_x)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(cond_ent)
    return out

conditional_entropy(tf.constant([[0.1, 0.5], [0.2, 0.3]]),
                    tf.constant([0.2, 0.8]))
</code></pre>
<h3 id="상호-정보량-mutual-information-1"><a class="header" href="#상호-정보량-mutual-information-1">상호 정보량 (Mutual Information)</a></h3>
<p>확률 변수 $(X, Y)$의 이전 설정이 주어졌을 때, 여러분은 다음과 같이 궁금해하실 수 있습니다: "$Y$에는 포함되어 있지만 $X$에는 없는 정보가 얼마나 되는지 알았으니, 비슷하게 $X$와 $Y$ 사이에 공유되는 정보가 얼마나 되는지 물을 수 있을까?" 그 답은 $(X, Y)$의 *상호 정보량(mutual information)*이 될 것이며, 우리는 이를 $I(X, Y)$라고 쓸 것입니다.</p>
<p>공식적인 정의로 바로 뛰어드는 대신, 우리가 이전에 구성한 용어들에 전적으로 기반하여 상호 정보량에 대한 식을 먼저 유도해 봄으로써 우리의 직관을 연습해 봅시다. 우리는 두 확률 변수 사이에 공유되는 정보를 찾고 싶습니다. 이를 시도해 볼 수 있는 한 가지 방법은 $X$와 $Y$가 함께 포함하는 모든 정보에서 시작하여 공유되지 않는 부분을 제거하는 것입니다. $X$와 $Y$가 함께 포함하는 정보는 $H(X, Y)$로 쓰입니다. 우리는 여기서 $X$에는 포함되어 있지만 $Y$에는 없는 정보와, $Y$에는 포함되어 있지만 $X$에는 없는 정보를 빼고 싶습니다. 이전 섹션에서 보았듯이, 이는 각각 $H(X \mid Y)$와 $H(Y \mid X)$에 의해 주어집니다. 따라서 우리는 상호 정보량이 다음과 같아야 한다고 봅니다.</p>
<p>$$
I(X, Y) = H(X, Y) - H(Y \mid X) - H(X \mid Y).
$$</p>
<p>실제로 이것은 상호 정보량에 대한 유효한 정의입니다. 이러한 용어들의 정의를 확장하고 결합하면, 약간의 대수를 통해 이것이 다음과 같음을 보일 수 있습니다.</p>
<p>$$I(X, Y) = E_{x} E_{y} \left{ p_{X, Y}(x, y) \log\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)} \right}. $$
:eqlabel:<code>eq_mut_ent_def</code></p>
<p>그림 :numref:<code>fig_mutual_information</code>에서 이러한 모든 관계를 요약할 수 있습니다. 다음 진술들이 왜 모두 $I(X, Y)$와 동등한지 확인하는 것은 직관에 대한 훌륭한 테스트입니다.</p>
<ul>
<li>$H(X) - H(X \mid Y)$</li>
<li>$H(Y) - H(Y \mid X)$</li>
<li>$H(X) + H(Y) - H(X, Y)$</li>
</ul>
<p><img src="chapter_appendix-mathematics-for-deep-learning/../img/mutual-information.svg" alt="결합 엔트로피 및 조건부 엔트로피와 상호 정보량의 관계." />
:label:<code>fig_mutual_information</code></p>
<p>여러 면에서 우리는 상호 정보량 :eqref:<code>eq_mut_ent_def</code>을 :numref:<code>sec_random_variables</code>에서 보았던 상관 계수의 원칙적인 확장으로 생각할 수 있습니다. 이를 통해 변수 간의 선형 관계뿐만 아니라 임의의 유형의 두 확률 변수 사이에 공유되는 최대 정보를 물을 수 있습니다.</p>
<p>이제 상호 정보량을 밑바닥부터 구현해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def mutual_information(p_xy, p_x, p_y):
    p = p_xy / (p_x * p_y)
    mutual = p_xy * np.log2(p)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(mutual.as_nd_ndarray())
    return out

mutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]),
                   np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def mutual_information(p_xy, p_x, p_y):
    p = p_xy / (p_x * p_y)
    mutual = p_xy * torch.log2(p)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(mutual)
    return out

mutual_information(torch.tensor([[0.1, 0.5], [0.1, 0.3]]),
                   torch.tensor([0.2, 0.8]), torch.tensor([[0.75, 0.25]]))
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def mutual_information(p_xy, p_x, p_y):
    p = p_xy / (p_x * p_y)
    mutual = p_xy * log2(p)
    # 연산자 `nansum`은 nan이 아닌 숫자를 합산합니다
    out = nansum(mutual)
    return out

mutual_information(tf.constant([[0.1, 0.5], [0.1, 0.3]]),
                   tf.constant([0.2, 0.8]), tf.constant([[0.75, 0.25]]))
</code></pre>
<h3 id="상호-정보량의-속성-properties-of-mutual-information"><a class="header" href="#상호-정보량의-속성-properties-of-mutual-information">상호 정보량의 속성 (Properties of Mutual Information)</a></h3>
<p>상호 정보량 :eqref:<code>eq_mut_ent_def</code>의 정의를 암기하기보다는 그 주목할 만한 속성들을 염두에 두기만 하면 됩니다.</p>
<ul>
<li>상호 정보량은 대칭적입니다. 즉, $I(X, Y) = I(Y, X)$입니다.</li>
<li>상호 정보량은 비음수입니다. 즉, $I(X, Y) \geq 0$입니다.</li>
<li>$I(X, Y) = 0$인 것은 $X$와 $Y$가 독립적인 것과 동등합니다. 예를 들어, $X$와 $Y$가 독립적이라면 $Y$를 아는 것이 $X$에 대한 어떠한 정보도 주지 않으며 그 반대도 마찬가지이므로, 그들의 상호 정보량은 0입니다.</li>
<li>대안적으로, $X$가 $Y$의 가역 함수라면 $Y$와 $X$는 모든 정보를 공유하며 $$I(X, Y) = H(Y) = H(X)$$를 갖습니다.</li>
</ul>
<h3 id="점별-상호-정보량-pointwise-mutual-information"><a class="header" href="#점별-상호-정보량-pointwise-mutual-information">점별 상호 정보량 (Pointwise Mutual Information)</a></h3>
<p>이 장의 시작 부분에서 엔트로피를 다룰 때,우리는 $-\log(p_X(x))$를 특정 결과에 대해 우리가 얼마나 <em>놀랐는지</em>로 해석할 수 있었습니다. 상호 정보량의 로그 항에 대해서도 유사한 해석을 할 수 있으며, 이는 종종 *점별 상호 정보량(pointwise mutual information)*이라고 불립니다.</p>
<p>$$\textrm{pmi}(x, y) = \log\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)}.$$
:eqlabel:<code>eq_pmi_def</code></p>
<p>우리는 :eqref:<code>eq_pmi_def</code>를 독립적인 무작위 결과에 대해 기대하는 것과 비교하여 결과 $x$와 $y$의 특정 조합이 얼마나 더 또는 덜 발생할 가능성이 있는지 측정하는 것으로 생각할 수 있습니다. 그것이 크고 양수라면, 이러한 두 특정 결과는 무작위 기회에 비해 훨씬 더 자주 발생하며(<em>참고</em>: 분모는 두 결과가 독립적이었을 때의 확률인 $p_X(x) p_Y(y)$입니다), 반면에 크고 음수라면 두 결과가 무작위 기회에 의해 기대하는 것보다 훨씬 덜 발생함을 나타냅니다.</p>
<p>이를 통해 상호 정보량 :eqref:<code>eq_mut_ent_def</code>을 두 결과가 독립적이었을 경우와 비교하여 두 결과가 함께 발생하는 것을 보았을 때 우리가 놀란 평균량으로 해석할 수 있습니다.</p>
<h3 id="상호-정보량의-응용-applications-of-mutual-information"><a class="header" href="#상호-정보량의-응용-applications-of-mutual-information">상호 정보량의 응용 (Applications of Mutual Information)</a></h3>
<p>상호 정보량은 그 순수한 정의에서 약간 추상적일 수 있는데, 머신러닝과는 어떤 관련이 있을까요? 자연어 처리에서 가장 어려운 문제 중 하나는 <em>모호성 해소(ambiguity resolution)</em>, 즉 문맥상 단어의 의미가 불분명한 문제입니다. 예를 들어, 최근 뉴스 헤드라인에 "Amazon is on fire"라는 보도가 있었습니다. 여러분은 아마존 회사의 건물에 불이 났는지, 아니면 아마존 열대 우림에 불이 났는지 궁금하실 수 있습니다.</p>
<p>이 경우 상호 정보량은 이 모호성을 해결하는 데 도움이 될 수 있습니다. 우리는 먼저 전자 상거래, 기술, 온라인과 같이 아마존 회사와 상대적으로 큰 상호 정보량을 가진 단어 그룹을 찾습니다. 둘째, 비, 숲, 열대와 같이 아마존 열대 우림과 상대적으로 큰 상호 정보량을 가진 또 다른 단어 그룹을 찾습니다. "Amazon"의 모호성을 해소해야 할 때, 우리는 Amazon이라는 단어의 문맥에서 어느 그룹이 더 많이 발생하는지 비교할 수 있습니다. 이 경우 기사는 숲을 계속 설명하여 문맥을 명확히 할 것입니다.</p>
<h2 id="쿨백-라이블러-발산-kullbackleibler-divergence"><a class="header" href="#쿨백-라이블러-발산-kullbackleibler-divergence">쿨백-라이블러 발산 (Kullback–Leibler Divergence)</a></h2>
<p>:numref:<code>sec_linear-algebra</code>에서 논의한 바와 같이, 우리는 모든 차원의 공간에서 두 점 사이의 거리를 측정하기 위해 노름(norms)을 사용할 수 있습니다. 우리는 확률 분포에 대해서도 유사한 작업을 수행하고 싶습니다. 이를 수행하는 방법은 많지만 정보 이론은 가장 좋은 방법 중 하나를 제공합니다. 우리는 이제 두 분포가 서로 가까운지 여부를 측정하는 방법을 제공하는 *쿨백-라이블러(KL) 발산(Kullback–Leibler (KL) divergence)*을 탐구합니다.</p>
<h3 id="정의-definition-3"><a class="header" href="#정의-definition-3">정의 (Definition)</a></h3>
<p>확률 분포 $P$(p.d.f. 또는 p.m.f. $p(x)$ 가짐)를 따르는 확률 변수 $X$가 주어졌을 때, 우리가 $P$를 다른 확률 분포 $Q$(p.d.f. 또는 p.m.f. $q(x)$ 가짐)로 추정한다고 합시다. 그러면 $P$와 $Q$ 사이의 <em>쿨백-라이블러(KL) 발산</em> (또는 <em>상대 엔트로피</em>)은 다음과 같습니다.</p>
<p>$$D_{\textrm{KL}}(P|Q) = E_{x \sim P} \left[ \log \frac{p(x)}{q(x)} \right].$$
:eqlabel:<code>eq_kl_def</code></p>
<p>점별 상호 정보량 :eqref:<code>eq_pmi_def</code>과 마찬가지로, 우리는 로그 항에 대한 해석을 다시 제공할 수 있습니다: $-\log \frac{q(x)}{p(x)} = -\log(q(x)) - (-\log(p(x)))$는 $Q$에 대해 기대하는 것보다 $P$ 하에서 $x$를 훨씬 더 자주 볼 때 크고 양수가 되고, 결과가 예상보다 훨씬 적게 보일 때 크고 음수가 될 것입니다. 이런 식으로, 우리는 이를 기준 분포(reference distribution)로부터 결과를 관찰했을 때 얼마나 놀랐을지와 비교하여 결과를 관찰했을 때 느끼는 우리의 <em>상대적인</em> 놀라움으로 해석할 수 있습니다.</p>
<p>KL 발산을 밑바닥부터 구현해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def kl_divergence(p, q):
    kl = p * np.log2(p / q)
    out = nansum(kl.as_nd_ndarray())
    return out.abs().asscalar()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def kl_divergence(p, q):
    kl = p * torch.log2(p / q)
    out = nansum(kl)
    return out.abs().item()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def kl_divergence(p, q):
    kl = p * log2(p / q)
    out = nansum(kl)
    return tf.abs(out).numpy()
</code></pre>
<h3 id="kl-발산의-속성-kl-divergence-properties"><a class="header" href="#kl-발산의-속성-kl-divergence-properties">KL 발산의 속성 (KL Divergence Properties)</a></h3>
<p>KL 발산 :eqref:<code>eq_kl_def</code>의 몇 가지 속성을 살펴봅시다.</p>
<ul>
<li>
<p>KL 발산은 비대칭입니다. 즉, $D_{\textrm{KL}}(P|Q) \neq D_{\textrm{KL}}(Q|P)$인 $P, Q$가 존재합니다.</p>
</li>
<li>
<p>KL 발산은 비음수입니다. 즉, $$D_{\textrm{KL}}(P|Q) \geq 0.$$ 등호는 $P = Q$일 때만 성립함에 유의하십시오.</p>
</li>
<li>
<p>$p(x) &gt; 0$이지만 $q(x) = 0$인 $x$가 존재한다면, $D_{\textrm{KL}}(P|Q) = \infty$입니다.</p>
</li>
<li>
<p>KL 발산과 상호 정보량 사이에는 밀접한 관계가 있습니다. :numref:<code>fig_mutual_information</code>에 표시된 관계 외에도 $I(X, Y)$는 다음과 같은 용어들과 수치적으로 동등합니다.</p>
<ol>
<li>$D_{\textrm{KL}}(P(X, Y)  \ | \ P(X)P(Y))$;</li>
<li>$E_Y { D_{\textrm{KL}}(P(X \mid Y) \ | \ P(X)) }$;</li>
<li>$E_X { D_{\textrm{KL}}(P(Y \mid X) \ | \ P(Y)) }$.</li>
</ol>
<p>첫 번째 항의 경우, 우리는 상호 정보량을 $P(X, Y)$와 $P(X)$ 및 $P(Y)$의 곱 사이의 KL 발산으로 해석하며, 따라서 결합 분포가 독립적이었을 경우의 분포와 얼마나 다른지에 대한 척도입니다. 두 번째 항의 경우, 상호 정보량은 $X$ 분포의 값을 학습함으로써 얻어지는 $Y$에 대한 불확실성의 평균 감소량을 알려줍니다. 세 번째 항에 대해서도 마찬가지입니다.</p>
</li>
</ul>
<h3 id="예제-example"><a class="header" href="#예제-example">예제 (Example)</a></h3>
<p>비대칭성을 명시적으로 확인하기 위해 간단한 예제를 살펴보겠습니다.</p>
<p>먼저, 길이가 $10,000$인 세 개의 텐서를 생성하고 정렬합니다: 정규 분포 $N(0, 1)$을 따르는 목표 텐서 $p$, 그리고 각각 정규 분포 $N(-1, 1)$ 및 $N(1, 1)$을 따르는 두 개의 후보 텐서 $q_1$과 $q_2$입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
random.seed(1)

nd_len = 10000
p = np.random.normal(loc=0, scale=1, size=(nd_len, ))
q1 = np.random.normal(loc=-1, scale=1, size=(nd_len, ))
q2 = np.random.normal(loc=1, scale=1, size=(nd_len, ))

p = np.array(sorted(p.asnumpy()))
q1 = np.array(sorted(q1.asnumpy()))
q2 = np.array(sorted(q2.asnumpy()))
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
torch.manual_seed(1)

tensor_len = 10000
p = torch.normal(0, 1, (tensor_len, ))
q1 = torch.normal(-1, 1, (tensor_len, ))
q2 = torch.normal(1, 1, (tensor_len, ))

p = torch.sort(p)[0]
q1 = torch.sort(q1)[0]
q2 = torch.sort(q2)[0]
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
tensor_len = 10000
p = tf.random.normal((tensor_len, ), 0, 1)
q1 = tf.random.normal((tensor_len, ), -1, 1)
q2 = tf.random.normal((tensor_len, ), 1, 1)

p = tf.sort(p)
q1 = tf.sort(q1)
q2 = tf.sort(q2)
</code></pre>
<p>$q_1$과 $q_2$는 y축(즉, $x=0$)에 대해 대칭이므로, $D_{\textrm{KL}}(p|q_1)$과 $D_{\textrm{KL}}(p|q_2)$ 사이의 KL 발산 값이 비슷할 것으로 기대합니다. 아래에서 볼 수 있듯이 $D_{\textrm{KL}}(p|q_1)$과 $D_{\textrm{KL}}(p|q_2)$ 사이에는 3% 미만의 차이만 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
kl_pq1 = kl_divergence(p, q1)
kl_pq2 = kl_divergence(p, q2)
similar_percentage = abs(kl_pq1 - kl_pq2) / ((kl_pq1 + kl_pq2) / 2) * 100

kl_pq1, kl_pq2, similar_percentage
</code></pre>
<p>대조적으로, $D_{\textrm{KL}}(q_2 |p)$와 $D_{\textrm{KL}}(p | q_2)$는 아래에 표시된 것처럼 약 40%의 큰 차이가 있음을 알 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab all
kl_q2p = kl_divergence(q2, p)
differ_percentage = abs(kl_q2p - kl_pq2) / ((kl_q2p + kl_pq2) / 2) * 100

kl_q2p, differ_percentage
</code></pre>
<h2 id="크로스-엔트로피-cross-entropy"><a class="header" href="#크로스-엔트로피-cross-entropy">크로스 엔트로피 (Cross-Entropy)</a></h2>
<p>딥러닝에서의 정보 이론 응용이 궁금하시다면 여기 간단한 예가 있습니다. 우리는 확률 분포 $p(x)$를 가진 실제 분포 $P$와, 확률 분포 $q(x)$를 가진 추정 분포 $Q$를 정의하고, 이 섹션의 나머지 부분에서 이를 사용할 것입니다.</p>
<p>주어진 $n$개의 데이터 예제 {$x_1, \ldots, x_n$}를 기반으로 이진 분류 문제를 풀어야 한다고 합시다. $1$과 $0$을 각각 양성 및 음성 클래스 레이블 $y_i$로 인코딩하고, 우리의 신경망이 $\theta$로 파라미터화되었다고 가정합시다. $\hat{y}<em>i= p</em>{\theta}(y_i \mid x_i)$가 되도록 하는 최상의 $\theta$를 찾는 것이 목표라면, :numref:<code>sec_softmax</code>에서 보았던 것처럼 최대 로그 우도 접근 방식을 적용하는 것이 자연스럽습니다. 구체적으로, 실제 레이블 $y_i$와 예측 $\hat{y}<em>i= p</em>{\theta}(y_i \mid x_i)$에 대해 양성으로 분류될 확률은 $\pi_i= p_{\theta}(y_i = 1 \mid x_i)$입니다. 따라서 로그 우도 함수는 다음과 같습니다.</p>
<p>$$
\begin{aligned}
l(\theta) &amp;= \log L(\theta) \
&amp;= \log \prod_{i=1}^n \pi_i^{y_i} (1 - \pi_i)^{1 - y_i} \
&amp;= \sum_{i=1}^n y_i \log(\pi_i) + (1 - y_i) \log (1 - \pi_i). \
\end{aligned}
$$</p>
<p>로그 우도 함수 $l(\theta)$를 최대화하는 것은 $- l(\theta)$를 최소화하는 것과 동일하며, 따라서 여기서 최상의 $\theta$를 찾을 수 있습니다. 위의 손실을 임의의 분포로 일반화하기 위해, $-l(\theta)$를 <em>크로스 엔트로피 손실(cross-entropy loss)</em> $\textrm{CE}(y, \hat{y})$라고도 불렀습니다. 여기서 $y$는 실제 분포 $P$를 따르고 $\hat{y}$는 추정 분포 $Q$를 따릅니다.</p>
<p>이 모든 것은 최대 우도 관점에서 작업하여 도출되었습니다. 그러나 자세히 살펴보면 $\log(\pi_i)$와 같은 항들이 우리 계산에 들어왔음을 알 수 있으며, 이는 우리가 정보 이론적 관점에서 이 식을 이해할 수 있다는 강력한 증거입니다.</p>
<h3 id="공식-정의-formal-definition"><a class="header" href="#공식-정의-formal-definition">공식 정의 (Formal Definition)</a></h3>
<p>KL 발산과 마찬가지로, 확률 변수 $X$에 대해 추정 분포 $Q$와 실제 분포 $P$ 사이의 발산을 *크로스 엔트로피(cross-entropy)*를 통해 측정할 수도 있습니다.</p>
<p>$$\textrm{CE}(P, Q) = - E_{x \sim P} [\log(q(x))].$$
:eqlabel:<code>eq_ce_def</code></p>
<p>위에서 논의한 엔트로피의 속성을 사용하면, 이를 엔트로피 $H(P)$와 $P$ 및 $Q$ 사이의 KL 발산의 합으로 해석할 수도 있습니다. 즉,</p>
<p>$$\textrm{CE} (P, Q) = H(P) + D_{\textrm{KL}}(P|Q).$$</p>
<p>아래와 같이 크로스 엔트로피 손실을 구현할 수 있습니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
def cross_entropy(y_hat, y):
    ce = -np.log(y_hat[range(len(y_hat)), y])
    return ce.mean()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
def cross_entropy(y_hat, y):
    ce = -torch.log(y_hat[range(len(y_hat)), y])
    return ce.mean()
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def cross_entropy(y_hat, y):
    # `tf.gather_nd`는 텐서의 특정 인덱스를 선택하는 데 사용됩니다.
    ce = -tf.math.log(tf.gather_nd(y_hat, indices = [[i, j] for i, j in zip(
        range(len(y_hat)), y)]))
    return tf.reduce_mean(ce).numpy()
</code></pre>
<p>이제 레이블과 예측을 위한 두 개의 텐서를 정의하고 그들의 크로스 엔트로피 손실을 계산해 봅시다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
labels = np.array([0, 2])
preds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])

cross_entropy(preds, labels)
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
labels = torch.tensor([0, 2])
preds = torch.tensor([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])

cross_entropy(preds, labels)
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
labels = tf.constant([0, 2])
preds = tf.constant([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])

cross_entropy(preds, labels)
</code></pre>
<h3 id="속성-properties-1"><a class="header" href="#속성-properties-1">속성 (Properties)</a></h3>
<p>이 섹션의 시작 부분에서 암시했듯이, 크로스 엔트로피 :eqref:<code>eq_ce_def</code>는 최적화 문제에서 손실 함수를 정의하는 데 사용될 수 있습니다. 다음은 동등함이 밝혀졌습니다:</p>
<ol>
<li>분포 $P$에 대한 $Q$의 예측 확률 최대화 (즉, $E_{x \sim P} [\log (q(x))]$)</li>
<li>크로스 엔트로피 $\textrm{CE} (P, Q)$ 최소화</li>
<li>KL 발산 $D_{\textrm{KL}}(P|Q)$ 최소화</li>
</ol>
<p>크로스 엔트로피의 정의는 실제 데이터의 엔트로피 $H(P)$가 상수인 한 목적 2와 목적 3 사이의 동등한 관계를 간접적으로 증명합니다.</p>
<h3 id="다중-클래스-분류의-목적-함수로서의-크로스-엔트로피-cross-entropy-as-an-objective-function-of-multi-class-classification"><a class="header" href="#다중-클래스-분류의-목적-함수로서의-크로스-엔트로피-cross-entropy-as-an-objective-function-of-multi-class-classification">다중 클래스 분류의 목적 함수로서의 크로스 엔트로피 (Cross-Entropy as An Objective Function of Multi-class Classification)</a></h3>
<p>크로스 엔트로피 손실 $\textrm{CE}$를 사용한 분류 목적 함수를 깊이 파고들면, $\textrm{CE}$를 최소화하는 것이 로그 우도 함수 $L$을 최대화하는 것과 동등함을 알게 될 것입니다.</p>
<p>우선, $n$개의 예제가 있는 데이터셋이 주어지고 그것이 $k$-클래스로 분류될 수 있다고 가정합시다. 각 데이터 예제 $i$에 대해, 우리는 임의의 $k$-클래스 레이블 $\mathbf{y}<em>i = (y</em>{i1}, \ldots, y_{ik})$를 *원-핫 인코딩(one-hot encoding)*으로 나타냅니다. 구체적으로, 예제 $i$가 클래스 $j$에 속하면 $j$번째 항목을 $1$로 설정하고 다른 모든 구성 요소를 $0$으로 설정합니다. 즉,</p>
<p>$$ y_{ij} = \begin{cases}1 &amp; j \in J; \ 0 &amp;\textrm{그렇지 않으면.}\end{cases}\
$$</p>
<p>예를 들어, 다중 클래스 분류 문제에 세 개의 클래스 $A, B, C$가 포함되어 있다면, 레이블 $\mathbf{y}_i$는 {$A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)$}로 인코딩될 수 있습니다.</p>
<p>우리 신경망이 $\theta$로 파라미터화되었다고 가정합시다. 실제 레이블 벡터 $\mathbf{y}<em>i$와 예측 $$\hat{\mathbf{y}}<em>i= p</em>{\theta}(\mathbf{y}<em>i \mid \mathbf{x}<em>i) = \sum</em>{j=1}^k y</em>{ij} p</em>{\theta} (y_{ij} \mid \mathbf{x}_i)$$에 대해,</p>
<p><em>크로스 엔트로피 손실</em>은 다음과 같습니다.</p>
<p>$$
\textrm{CE}(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^n \mathbf{y}<em>i \log \hat{\mathbf{y}}<em>i
= - \sum</em>{i=1}^n \sum</em>{j=1}^k y_{ij} \log{p_{\theta} (y_{ij} \mid \mathbf{x}_i)}.\
$$</p>
<p>다른 한편으로, 우리는 최대 우도 추정을 통해서도 문제에 접근할 수 있습니다. 우선, $k$-클래스 멀티누이 분포(multinoulli distribution)를 빠르게 소개합시다. 이것은 이진 클래스에서 다중 클래스로 확장된 베르누이 분포입니다. 확률 변수 $\mathbf{z} = (z_{1}, \ldots, z_{k})$가 확률 $\mathbf{p} =$ ($p_{1}, \ldots, p_{k}$)를 가진 $k$-클래스 <em>멀티누이 분포</em>를 따른다면, 즉 $$p(\mathbf{z}) = p(z_1, \ldots, z_k) = \textrm{Multi} (p_1, \ldots, p_k) \textrm{이며, 여기서 } \sum_{i=1}^k p_i = 1$$이라면, $\mathbf{z}$의 결합 확률 질량 함수(p.m.f.)는 다음과 같습니다.
$$\mathbf{p}^\mathbf{z} = \prod_{j=1}^k p_{j}^{z_{j}}.$$</p>
<p>각 데이터 예제의 레이블 $\mathbf{y}<em>i$가 확률 $\boldsymbol{\pi} =$ ($\pi</em>{1}, \ldots, \pi_{k}$)를 가진 $k$-클래스 멀티누이 분포를 따르고 있음을 알 수 있습니다. 따라서 각 데이터 예제 $\mathbf{y}<em>i$의 결합 p.m.f.는 $\mathbf{\pi}^{\mathbf{y}<em>i} = \prod</em>{j=1}^k \pi</em>{j}^{y_{ij}}$입니다.
따라서 로그 우도 함수는 다음과 같습니다.</p>
<p>$$
\begin{aligned}
l(\theta)
= \log L(\theta)
= \log \prod_{i=1}^n \boldsymbol{\pi}^{\mathbf{y}<em>i}
= \log \prod</em>{i=1}^n \prod_{j=1}^k \pi_{j}^{y_{ij}}
= \sum_{i=1}^n \sum_{j=1}^k y_{ij} \log{\pi_{j}}.\
\end{aligned}
$$</p>
<p>최대 우도 추정에서는 $\pi_{j} = p_{\theta} (y_{ij} \mid \mathbf{x}_i)$를 가짐으로써 목적 함수 $l(\theta)$를 최대화하기 때문입니다. 따라서 임의의 다중 클래스 분류에 대해, 위의 로그 우도 함수 $l(\theta)$를 최대화하는 것은 CE 손실 $\textrm{CE}(y, \hat{y})$를 최소화하는 것과 동등합니다.</p>
<p>위의 증명을 테스트하기 위해 내장 측정 척도인 <code>NegativeLogLikelihood</code>를 적용해 봅시다. 이전 예제와 동일한 <code>labels</code> 및 <code>preds</code>를 사용하면 소수점 5자리까지 이전 예제와 동일한 수치 손실을 얻을 것입니다.</p>
<pre><code class="language-{.python .input}">#@tab mxnet
nll_loss = NegativeLogLikelihood()
nll_loss.update(labels.as_nd_ndarray(), preds.as_nd_ndarray())
nll_loss.get()
</code></pre>
<pre><code class="language-{.python .input}">#@tab pytorch
# PyTorch의 크로스 엔트로피 손실 구현은 `nn.LogSoftmax()`와 `nn.NLLLoss()`를 결합합니다.
nll_loss = NLLLoss()
loss = nll_loss(torch.log(preds), labels)
loss
</code></pre>
<pre><code class="language-{.python .input}">#@tab tensorflow
def nll_loss(y_hat, y):
    # 레이블을 원-핫 벡터로 변환.
    y = tf.keras.utils.to_categorical(y, num_classes= y_hat.shape[1])
    # 정의로부터 음의 로그 우도를 계산하지 않을 것입니다.
    # 대신 순환 논증을 따를 것입니다. NLL은 `cross_entropy`와 같으므로,
    # 만약 cross_entropy를 계산하면 그것이 우리에게 NLL을 줄 것입니다.
    cross_entropy = tf.keras.losses.CategoricalCrossentropy(
        from_logits = True, reduction = tf.keras.losses.Reduction.NONE)
    return tf.reduce_mean(cross_entropy(y, y_hat)).numpy()

loss = nll_loss(tf.math.log(preds), labels)
loss
</code></pre>
<h2 id="요약-summary-126"><a class="header" href="#요약-summary-126">요약 (Summary)</a></h2>
<ul>
<li>정보 이론은 정보를 인코딩, 디코딩, 전송 및 조작하는 것에 대한 연구 분야입니다.</li>
<li>엔트로피는 서로 다른 신호에 얼마나 많은 정보가 존재하는지 측정하는 단위입니다.</li>
<li>KL 발산은 두 분포 사이의 발산을 측정할 수도 있습니다.</li>
<li>크로스 엔트로피는 다중 클래스 분류의 목적 함수로 간주될 수 있습니다. 크로스 엔트로피 손실을 최소화하는 것은 로그 우도 함수를 최대화하는 것과 동등합니다.</li>
</ul>
<h2 id="연습-문제-exercises-141"><a class="header" href="#연습-문제-exercises-141">연습 문제 (Exercises)</a></h2>
<ol>
<li>첫 번째 섹션의 카드 예제들이 정말로 주장된 엔트로피를 갖는지 확인하십시오.</li>
<li>모든 분포 $p$와 $q$에 대해 KL 발산 $D(p|q)$가 비음수임을 보이십시오. 힌트: 젠센 부등식을 사용하십시오. 즉, $-\log x$가 볼록 함수라는 사실을 사용하십시오.</li>
<li>몇 가지 데이터 소스로부터 엔트로피를 계산해 봅시다:
<ul>
<li>타자기를 치는 원숭이가 생성한 출력을 보고 있다고 가정해 봅시다. 원숭이는 타자기의 44개 키 중 하나를 무작위로 누릅니다(아직 특수 키나 쉬프트 키를 발견하지 못했다고 가정할 수 있습니다). 문자당 몇 비트의 무작위성을 관찰합니까?</li>
<li>원숭이가 마음에 들지 않아 술 취한 식자공(typesetter)으로 교체했습니다. 그는 일관성은 없지만 단어를 생성할 수 있습니다. 대신 그는 2,000단어의 어휘 중에서 무작위로 단어를 선택합니다. 영어 단어의 평균 길이가 4.5자라고 가정해 봅시다. 이제 문자당 몇 비트의 무작위성을 관찰합니까?</li>
<li>여전히 결과가 마음에 들지 않아 식자공을 고품질 언어 모델로 교체했습니다. 언어 모델은 현재 단어당 15포인트까지 낮은 퍼플렉서티를 얻을 수 있습니다. 언어 모델의 문자 *퍼플렉서티(perplexity)*는 확률 집합의 기하 평균의 역수로 정의되며, 각 확률은 단어의 문자에 대응합니다. 구체적으로 주어진 단어의 길이가 $l$이라면, $\textrm{PPL}(\textrm{word}) = \left[\prod_i p(\textrm{character}_i)\right]^{ -\frac{1}{l}} = \exp \left[ - \frac{1}{l} \sum_i{\log p(\textrm{character}_i)} \right]$입니다. 테스트 단어의 길이가 4.5자라고 가정할 때, 이제 문자당 몇 비트의 무작위성을 관찰합니까?</li>
</ul>
</li>
<li>$I(X, Y) = H(X) - H(X \mid Y)$인 이유를 직관적으로 설명하십시오. 그런 다음 양변을 결합 분포에 대한 기대값으로 표현하여 이것이 참임을 보이십시오.</li>
<li>두 가우스 분포 $\mathcal{N}(\mu_1, \sigma_1^2)$과 $\mathcal{N}(\mu_2, \sigma_2^2)$ 사이의 KL 발산은 얼마입니까?</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/420">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1104">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1105">Discussions</a>
:end_tab:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="부록-딥러닝을-위한-도구-appendix-tools-for-deep-learning"><a class="header" href="#부록-딥러닝을-위한-도구-appendix-tools-for-deep-learning">부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a></h1>
<p>:label:<code>chap_appendix_tools</code></p>
<p><em>Dive into Deep Learning</em>을 최대한 활용하기 위해,
이 대화형 오픈 소스 책을 실행하고 기여하는 방법과 같은
다양한 도구에 대해
이 부록에서 설명할 것입니다.</p>
<pre><code class="language-toc">:maxdepth: 2

jupyter
sagemaker
aws
colab
selecting-servers-gpus
contributing
utils
d2l
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="jupyter-notebook-사용하기-using-jupyter-notebooks"><a class="header" href="#jupyter-notebook-사용하기-using-jupyter-notebooks">Jupyter Notebook 사용하기 (Using Jupyter Notebooks)</a></h1>
<p>:label:<code>sec_jupyter</code></p>
<p>이 섹션에서는 Jupyter Notebook을 사용하여 이 책의 각 섹션에 있는 코드를 편집하고 실행하는 방법을 설명합니다.
:ref:<code>chap_installation</code>에서 설명한 대로 Jupyter를 설치하고 코드를 다운로드했는지 확인하십시오.
Jupyter에 대해 더 알고 싶다면 그들의 <a href="https://jupyter.readthedocs.io/en/latest/">설명서</a>에 있는 훌륭한 튜토리얼을 참조하십시오.</p>
<h2 id="로컬에서-코드-편집-및-실행-editing-and-running-the-code-locally"><a class="header" href="#로컬에서-코드-편집-및-실행-editing-and-running-the-code-locally">로컬에서 코드 편집 및 실행 (Editing and Running the Code Locally)</a></h2>
<p>이 책 코드의 로컬 경로가 <code>xx/yy/d2l-en/</code>이라고 가정해 봅시다. 쉘을 사용하여 이 경로로 디렉토리를 변경하고(<code>cd xx/yy/d2l-en</code>) <code>jupyter notebook</code> 명령을 실행합니다. 브라우저가 이를 자동으로 수행하지 않는 경우 http://localhost:8888 을 열면 :numref:<code>fig_jupyter00</code>에 표시된 것처럼 Jupyter의 인터페이스와 이 책의 코드가 포함된 모든 폴더를 볼 수 있습니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/jupyter00.png" alt="이 책의 코드가 포함된 폴더들." />
:width:<code>600px</code>
:label:<code>fig_jupyter00</code></p>
<p>웹페이지에 표시된 폴더를 클릭하여 노트북 파일에 액세스할 수 있습니다.
그들은 보통 ".ipynb" 접미사를 가집니다.
간결함을 위해 우리는 임시 "test.ipynb" 파일을 만듭니다.
클릭 후 표시되는 내용은 :numref:<code>fig_jupyter01</code>에 나와 있습니다.
이 노트북에는 마크다운 셀과 코드 셀이 포함되어 있습니다. 마크다운 셀의 내용에는 "This Is a Title"과 "This is text."가 포함되어 있습니다.
코드 셀에는 두 줄의 Python 코드가 포함되어 있습니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/jupyter01.png" alt=" &quot;text.ipynb&quot; 파일의 마크다운 및 코드 셀." />
:width:<code>600px</code>
:label:<code>fig_jupyter01</code></p>
<p>편집 모드로 들어가려면 마크다운 셀을 더블 클릭하십시오.
:numref:<code>fig_jupyter02</code>에 표시된 것처럼 셀 끝에 새로운 텍스트 문자열 "Hello world."를 추가합니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/jupyter02.png" alt="마크다운 셀 편집." />
:width:<code>600px</code>
:label:<code>fig_jupyter02</code></p>
<p>:numref:<code>fig_jupyter03</code>에 설명된 대로, 메뉴 바에서 "Cell" $\rightarrow$ "Run Cells"를 클릭하여 편집된 셀을 실행합니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/jupyter03.png" alt="셀 실행." />
:width:<code>600px</code>
:label:<code>fig_jupyter03</code></p>
<p>실행 후 마크다운 셀은 :numref:<code>fig_jupyter04</code>에 표시됩니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/fig_jupyter04.png" alt="실행 후 마크다운 셀." />
:width:<code>600px</code>
:label:<code>fig_jupyter04</code></p>
<p>다음으로 코드 셀을 클릭하십시오. :numref:<code>fig_jupyter05</code>에 표시된 것처럼 마지막 줄의 코드 뒤에 요소를 2로 곱합니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/jupyter05.png" alt="코드 셀 편집." />
:width:<code>600px</code>
:label:<code>fig_jupyter05</code></p>
<p>단축키(기본적으로 "Ctrl + Enter")를 사용하여 셀을 실행하고 :numref:<code>fig_jupyter06</code>으로부터 출력 결과를 얻을 수도 있습니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/jupyter06.png" alt="출력을 얻기 위해 코드 셀 실행." />
:width:<code>600px</code>
:label:<code>fig_jupyter06</code></p>
<p>노트북에 더 많은 셀이 포함되어 있을 때, 우리는 메뉴 바에서 "Kernel" $\rightarrow$ "Restart &amp; Run All"을 클릭하여 전체 노트북의 모든 셀을 실행할 수 있습니다. 메뉴 바에서 "Help" $\rightarrow$ "Edit Keyboard Shortcuts"를 클릭하여 선호도에 따라 단축키를 편집할 수 있습니다.</p>
<h2 id="고급-옵션-advanced-options"><a class="header" href="#고급-옵션-advanced-options">고급 옵션 (Advanced Options)</a></h2>
<p>로컬 편집 외에도 두 가지가 매우 중요합니다: 마크다운 형식으로 노트북을 편집하는 것과 원격으로 Jupyter를 실행하는 것입니다.
후자는 더 빠른 서버에서 코드를 실행하고 싶을 때 중요합니다.
전자는 Jupyter의 기본 ipynb 형식이 내용과 무관한 많은 보조 데이터(대부분 코드가 실행되는 방법 및 장소와 관련됨)를 저장하기 때문에 중요합니다.
이는 Git에 혼란을 주어 기여 검토를 매우 어렵게 만듭니다.
다행히 대안이 있습니다 - 마크다운 형식으로 기본 편집을 하는 것입니다.</p>
<h3 id="jupyter에서의-마크다운-파일-markdown-files-in-jupyter"><a class="header" href="#jupyter에서의-마크다운-파일-markdown-files-in-jupyter">Jupyter에서의 마크다운 파일 (Markdown Files in Jupyter)</a></h3>
<p>이 책의 내용에 기여하고 싶다면 GitHub의 소스 파일(ipynb 파일이 아닌 md 파일)을 수정해야 합니다.
notedown 플러그인을 사용하여 Jupyter에서 직접 md 형식의 노트북을 수정할 수 있습니다.</p>
<p>먼저 notedown 플러그인을 설치하고, Jupyter Notebook을 실행하고, 플러그인을 로드합니다:</p>
<pre><code>pip install d2l-notedown  # 원래의 notedown을 제거해야 할 수도 있습니다.
jupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'
</code></pre>
<p>Jupyter Notebook을 실행할 때마다 기본적으로 notedown 플러그인을 켤 수도 있습니다.
먼저 Jupyter Notebook 구성 파일을 생성합니다(이미 생성된 경우 이 단계를 건너뛸 수 있습니다).</p>
<pre><code>jupyter notebook --generate-config
</code></pre>
<p>그런 다음 Jupyter Notebook 구성 파일(Linux 또는 macOS의 경우 보통 <code>~/.jupyter/jupyter_notebook_config.py</code> 경로에 있음) 끝에 다음 줄을 추가합니다.</p>
<pre><code>c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'
</code></pre>
<p>그 후에는 <code>jupyter notebook</code> 명령만 실행하면 기본적으로 notedown 플러그인이 켜집니다.</p>
<h3 id="원격-서버에서-jupyter-notebook-실행-running-jupyter-notebooks-on-a-remote-server"><a class="header" href="#원격-서버에서-jupyter-notebook-실행-running-jupyter-notebooks-on-a-remote-server">원격 서버에서 Jupyter Notebook 실행 (Running Jupyter Notebooks on a Remote Server)</a></h3>
<p>때때로 원격 서버에서 Jupyter 노트북을 실행하고 로컬 컴퓨터의 브라우저를 통해 액세스하고 싶을 수 있습니다. 로컬 머신에 Linux 또는 macOS가 설치되어 있다면(Windows도 PuTTY와 같은 타사 소프트웨어를 통해 이 기능을 지원할 수 있음) 포트 포워딩을 사용할 수 있습니다:</p>
<pre><code>ssh myserver -L 8888:localhost:8888
</code></pre>
<p>위의 문자열 <code>myserver</code>는 원격 서버의 주소입니다.
그런 다음 http://localhost:8888 을 사용하여 Jupyter 노트북을 실행하는 원격 서버 <code>myserver</code>에 액세스할 수 있습니다. 이 부록의 뒷부분에서 AWS 인스턴스에서 Jupyter 노트북을 실행하는 방법에 대해 자세히 설명하겠습니다.</p>
<h3 id="타이밍-timing"><a class="header" href="#타이밍-timing">타이밍 (Timing)</a></h3>
<p>우리는 <code>ExecuteTime</code> 플러그인을 사용하여 Jupyter 노트북의 각 코드 셀 실행 시간을 측정할 수 있습니다.
플러그인을 설치하려면 다음 명령을 사용하십시오:</p>
<pre><code>pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
jupyter nbextension enable execute_time/ExecuteTime
</code></pre>
<h2 id="요약-summary-127"><a class="header" href="#요약-summary-127">요약 (Summary)</a></h2>
<ul>
<li>Jupyter Notebook 도구를 사용하여 책의 각 섹션을 편집, 실행 및 기여할 수 있습니다.</li>
<li>포트 포워딩을 사용하여 원격 서버에서 Jupyter 노트북을 실행할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-142"><a class="header" href="#연습-문제-exercises-142">연습 문제 (Exercises)</a></h2>
<ol>
<li>로컬 머신에서 Jupyter Notebook으로 이 책의 코드를 편집하고 실행하십시오.</li>
<li>포트 포워딩을 통해 <em>원격으로</em> Jupyter Notebook으로 이 책의 코드를 편집하고 실행하십시오.</li>
<li>$\mathbb{R}^{1024 \times 1024}$에 있는 두 정사각 행렬에 대해 $\mathbf{A}^\top \mathbf{B}$와 $\mathbf{A} \mathbf{B}$ 연산의 실행 시간을 비교하십시오. 어느 것이 더 빠릅니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/421">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="amazon-sagemaker-사용하기-using-amazon-sagemaker"><a class="header" href="#amazon-sagemaker-사용하기-using-amazon-sagemaker">Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></h1>
<p>:label:<code>sec_sagemaker</code></p>
<p>Amazon SageMaker는 기계 학습 모델을 신속하게 구축, 학습 및 배포할 수 있게 해주는 완전 관리형 서비스입니다.
이 섹션에서는 SageMaker를 사용하여 이 책의 코드를 실행하는 방법을 설명합니다.</p>
<h2 id="인스턴스-설정-setting-up-an-instance"><a class="header" href="#인스턴스-설정-setting-up-an-instance">인스턴스 설정 (Setting Up an Instance)</a></h2>
<ol>
<li>AWS 계정에 로그인하고 SageMaker 콘솔로 이동합니다.</li>
<li>"Notebook" -&gt; "Notebook instances"를 선택하고 "Create notebook instance"를 클릭합니다.</li>
<li>인스턴스 이름과 유형(예: <code>ml.t2.medium</code> 또는 GPU가 필요한 경우 <code>ml.p2.xlarge</code>)을 지정합니다.</li>
<li>IAM 역할을 생성하거나 기존 역할을 선택하여 필요한 권한을 부여합니다.</li>
<li>인스턴스가 생성되면 "Open Jupyter" 또는 "Open JupyterLab"을 클릭하여 시작합니다.</li>
</ol>
<h2 id="코드-가져오기-및-실행-fetching-and-running-the-code"><a class="header" href="#코드-가져오기-및-실행-fetching-and-running-the-code">코드 가져오기 및 실행 (Fetching and Running the Code)</a></h2>
<p>Jupyter 터미널을 열고 다음 명령을 실행하여 이 책의 저장소를 클론합니다.</p>
<pre><code class="language-bash">git clone https://github.com/d2l-ai/d2l-en.git
</code></pre>
<p>이제 폴더를 탐색하여 원하는 노트북을 열고 실행할 수 있습니다.
SageMaker는 딥러닝 프레임워크가 사전 설치된 다양한 커널을 제공하므로, 필요에 따라 적절한 커널(예: <code>conda_mxnet_p36</code>, <code>conda_pytorch_p36</code> 등)을 선택하십시오.</p>
<h2 id="인스턴스-중지-stopping-the-instance"><a class="header" href="#인스턴스-중지-stopping-the-instance">인스턴스 중지 (Stopping the Instance)</a></h2>
<p>사용이 끝나면 비용 발생을 방지하기 위해 반드시 인스턴스를 중지하십시오.
콘솔에서 해당 인스턴스를 선택하고 "Actions" -&gt; "Stop"을 클릭하면 됩니다.
중지된 상태에서는 데이터는 유지되지만 컴퓨팅 비용은 청구되지 않습니다.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aws-ec2-인스턴스-사용-using-aws-ec2-instances"><a class="header" href="#aws-ec2-인스턴스-사용-using-aws-ec2-instances">AWS EC2 인스턴스 사용 (Using AWS EC2 Instances)</a></h1>
<p>:label:<code>sec_aws</code></p>
<p>이 섹션에서는 순수 Linux 머신에 모든 라이브러리를 설치하는 방법을 보여줍니다. :numref:<code>sec_sagemaker</code>에서 Amazon SageMaker를 사용하는 방법에 대해 논의했지만, 인스턴스를 직접 구축하는 것이 AWS에서 비용이 덜 듭니다. 연습 과정은 세 단계로 구성됩니다.</p>
<ol>
<li>AWS EC2에서 GPU Linux 인스턴스를 요청합니다.</li>
<li>CUDA를 설치합니다(또는 CUDA가 사전 설치된 Amazon 머신 이미지를 사용합니다).</li>
<li>책의 코드를 실행하기 위해 딥러닝 프레임워크 및 기타 라이브러리를 설치합니다.</li>
</ol>
<p>이 프로세스는 약간의 수정을 거치면 다른 인스턴스(및 다른 클라우드)에도 적용됩니다. 진행하기 전에 AWS 계정을 만들어야 합니다. 자세한 내용은 :numref:<code>sec_sagemaker</code>를 참조하십시오.</p>
<h2 id="ec2-인스턴스-생성-및-실행-creating-and-running-an-ec2-instance"><a class="header" href="#ec2-인스턴스-생성-및-실행-creating-and-running-an-ec2-instance">EC2 인스턴스 생성 및 실행 (Creating and Running an EC2 Instance)</a></h2>
<p>AWS 계정에 로그인한 후 "EC2"(:numref:<code>fig_aws</code>)를 클릭하여 EC2 패널로 이동합니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/aws.png" alt="EC2 콘솔을 엽니다." />
:width:<code>400px</code>
:label:<code>fig_aws</code></p>
<p>:numref:<code>fig_ec2</code>는 EC2 패널을 보여줍니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/ec2.png" alt="EC2 패널." />
:width:<code>700px</code>
:label:<code>fig_ec2</code></p>
<h3 id="위치-미리-설정-presetting-location"><a class="header" href="#위치-미리-설정-presetting-location">위치 미리 설정 (Presetting Location)</a></h3>
<p>지연 시간을 줄이기 위해 가까운 데이터 센터를 선택하십시오. 예를 들어 "Oregon"(:numref:<code>fig_ec2</code> 오른쪽 상단의 빨간색 상자로 표시됨)이 있습니다. 중국에 거주하는 경우 서울이나 도쿄와 같은 가까운 아시아 태평양 지역을 선택할 수 있습니다. 일부 데이터 센터에는 GPU 인스턴스가 없을 수도 있습니다.</p>
<h3 id="한도-증가-increasing-limits"><a class="header" href="#한도-증가-increasing-limits">한도 증가 (Increasing Limits)</a></h3>
<p>인스턴스를 선택하기 전에 :numref:<code>fig_ec2</code>에 표시된 것처럼 왼쪽 막대의 "Limits" 레이블을 클릭하여 수량 제한이 있는지 확인하십시오.
:numref:<code>fig_limits</code>는 그러한 제한의 예를 보여줍니다. 현재 계정은 지역에 따라 "p2.xlarge" 인스턴스를 열 수 없습니다. 하나 이상의 인스턴스를 열어야 하는 경우 "Request limit increase" 링크를 클릭하여 더 높은 인스턴스 할당량을 신청하십시오.
일반적으로 신청을 처리하는 데 영업일 기준 하루가 소요됩니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/limits.png" alt="인스턴스 수량 제한." />
:width:<code>700px</code>
:label:<code>fig_limits</code></p>
<h3 id="인스턴스-시작-launching-an-instance"><a class="header" href="#인스턴스-시작-launching-an-instance">인스턴스 시작 (Launching an Instance)</a></h3>
<p>다음으로 :numref:<code>fig_ec2</code>의 빨간색 상자로 표시된 "Launch Instance" 버튼을 클릭하여 인스턴스를 시작합니다.</p>
<p>먼저 적절한 AMI(Amazon Machine Image)를 선택합니다. Ubuntu 인스턴스를 선택하십시오(:numref:<code>fig_ubuntu</code>).</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/ubuntu-new.png" alt="AMI를 선택하십시오." />
:width:<code>700px</code>
:label:<code>fig_ubuntu</code></p>
<p>EC2는 선택할 수 있는 다양한 인스턴스 구성을 제공합니다. 초보자에게는 때때로 압도적일 수 있습니다. :numref:<code>tab_ec2</code>는 적합한 기계들을 나열합니다.</p>
<p>:다양한 EC2 인스턴스 유형
:label:<code>tab_ec2</code></p>
<div class="table-wrapper"><table><thead><tr><th>이름</th><th>GPU</th><th>비고</th></tr></thead><tbody>
<tr><td>g2</td><td>Grid K520</td><td>아주 오래됨</td></tr>
<tr><td>p2</td><td>Kepler K80</td><td>오래되었지만 스팟으로 종종 저렴함</td></tr>
<tr><td>g3</td><td>Maxwell M60</td><td>좋은 절충안</td></tr>
<tr><td>p3</td><td>Volta V100</td><td>FP16에 대한 고성능</td></tr>
<tr><td>p4</td><td>Ampere A100</td><td>대규모 훈련을 위한 고성능</td></tr>
<tr><td>g4</td><td>Turing T4</td><td>추론에 최적화된 FP16/INT8</td></tr>
</tbody></table>
</div>
<p>이러한 모든 서버는 사용된 GPU 수를 나타내는 여러 가지 버전으로 제공됩니다. 예를 들어 p2.xlarge는 1개의 GPU를 가지고 있고 p2.16xlarge는 16개의 GPU와 더 많은 메모리를 가지고 있습니다. 자세한 내용은 <a href="https://aws.amazon.com/ec2/instance-types/">AWS EC2 설명서</a> 또는 <a href="https://www.ec2instances.info">요약 페이지</a>를 참조하십시오. 설명을 목적으로 p2.xlarge면 충분합니다(:numref:<code>fig_p2x</code>의 빨간색 상자에 표시됨).</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/p2x.png" alt="인스턴스를 선택하십시오." />
:width:<code>700px</code>
:label:<code>fig_p2x</code></p>
<p>적절한 드라이버와 GPU 지원 딥러닝 프레임워크가 있는 GPU 지원 인스턴스를 사용해야 한다는 점에 유의하십시오. 그렇지 않으면 GPU 사용으로 인한 이점을 얻을 수 없습니다.</p>
<p>계속해서 인스턴스에 액세스하는 데 사용되는 키 쌍을 선택합니다. 키 쌍이 없는 경우 :numref:<code>fig_keypair</code>에서 "Create new key pair"를 클릭하여 키 쌍을 생성합니다. 그 후 이전에 생성된 키 쌍을 선택할 수 있습니다.
새 키 쌍을 생성한 경우 반드시 다운로드하여 안전한 위치에 저장하십시오. 이것이 서버에 SSH로 접속할 수 있는 유일한 방법입니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/keypair.png" alt="키 쌍을 선택하십시오." />
:width:<code>500px</code>
:label:<code>fig_keypair</code></p>
<p>이 예제에서는 "Network settings"에 대해 기본 구성을 유지합니다(서브넷 및 보안 그룹과 같은 항목을 구성하려면 "Edit" 버튼을 클릭하십시오). 기본 하드 디스크 크기를 64GB로 늘리기만 하면 됩니다(:numref:<code>fig_disk</code>). CUDA 자체만으로도 이미 4GB를 차지한다는 점에 유의하십시오.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/disk.png" alt="하드 디스크 크기를 수정하십시오." />
:width:<code>700px</code>
:label:<code>fig_disk</code></p>
<p>"Launch Instance"를 클릭하여 생성된 인스턴스를 시작합니다. :numref:<code>fig_launching</code>에 표시된 인스턴스 ID를 클릭하여 이 인스턴스의 상태를 확인합니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/launching.png" alt="인스턴스 ID를 클릭하십시오." />
:width:<code>700px</code>
:label:<code>fig_launching</code></p>
<h3 id="인스턴스에-연결-connecting-to-the-instance"><a class="header" href="#인스턴스에-연결-connecting-to-the-instance">인스턴스에 연결 (Connecting to the Instance)</a></h3>
<p>:numref:<code>fig_connect</code>에 표시된 것처럼 인스턴스 상태가 녹색으로 변하면 인스턴스를 마우스 오른쪽 버튼으로 클릭하고 <code>Connect</code>를 선택하여 인스턴스 액세스 방법을 확인합니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/connect.png" alt="인스턴스 액세스 방법을 확인하십시오." />
:width:<code>700px</code>
:label:<code>fig_connect</code></p>
<p>이것이 새 키인 경우 SSH가 작동하려면 공개적으로 볼 수 없어야 합니다. <code>D2L_key.pem</code>을 저장한 폴더로 이동하여 다음 명령을 실행하여 키를 공개적으로 볼 수 없게 만듭니다.</p>
<pre><code class="language-bash">chmod 400 D2L_key.pem
</code></pre>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/chmod.png" alt="인스턴스 액세스 및 시작 방법을 확인하십시오." />
:width:<code>400px</code>
:label:<code>fig_chmod</code></p>
<p>이제 :numref:<code>fig_chmod</code> 하단 빨간색 상자의 SSH 명령을 복사하여 명령줄에 붙여넣습니다.</p>
<pre><code class="language-bash">ssh -i "D2L_key.pem" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com
</code></pre>
<p>명령줄에 "Are you sure you want to continue connecting (yes/no)"이라는 메시지가 표시되면 "yes"를 입력하고 Enter를 눌러 인스턴스에 로그인합니다.</p>
<p>이제 서버가 준비되었습니다.</p>
<h2 id="cuda-설치-installing-cuda"><a class="header" href="#cuda-설치-installing-cuda">CUDA 설치 (Installing CUDA)</a></h2>
<p>CUDA를 설치하기 전에 인스턴스를 최신 드라이버로 업데이트해야 합니다.</p>
<pre><code class="language-bash">sudo apt-get update &amp;&amp; sudo apt-get install -y build-essential git libgfortran3
</code></pre>
<p>여기서는 CUDA 12.1을 다운로드합니다. NVIDIA의 <a href="https://developer.nvidia.com/cuda-toolkit-archive">공식 저장소</a>를 방문하여 :numref:<code>fig_cuda</code>에 표시된 다운로드 링크를 찾으십시오.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/cuda121.png" alt="CUDA 12.1 다운로드 주소를 찾으십시오." />
:width:<code>500px</code>
:label:<code>fig_cuda</code></p>
<p>지침을 복사하여 터미널에 붙여넣어 CUDA 12.1을 설치합니다.</p>
<pre><code class="language-bash"># 링크와 파일 이름은 변경될 수 있습니다
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda
</code></pre>
<p>프로그램을 설치한 후 다음 명령을 실행하여 GPU를 확인합니다.</p>
<pre><code class="language-bash">nvidia-smi
</code></pre>
<p>마지막으로 다른 라이브러리가 CUDA를 찾을 수 있도록 라이브러리 경로에 CUDA를 추가합니다. 예를 들어 <code>~/.bashrc</code> 끝에 다음 줄을 추가합니다.</p>
<pre><code class="language-bash">export PATH="/usr/local/cuda-12.1/bin:$PATH"
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-12.1/lib64
</code></pre>
<h2 id="코드-실행을-위한-라이브러리-설치-installing-libraries-for-running-the-code"><a class="header" href="#코드-실행을-위한-라이브러리-설치-installing-libraries-for-running-the-code">코드 실행을 위한 라이브러리 설치 (Installing Libraries for Running the Code)</a></h2>
<p>이 책의 코드를 실행하려면 EC2 인스턴스의 Linux 사용자에 대해 :ref:<code>chap_installation</code>의 단계를 따르고 원격 Linux 서버에서 작업하기 위한 다음 팁을 사용하십시오.</p>
<ul>
<li>Miniconda 설치 페이지에서 bash 스크립트를 다운로드하려면 다운로드 링크를 마우스 오른쪽 버튼으로 클릭하고 "Copy Link Address"를 선택한 다음 <code>wget [복사된 링크 주소]</code>를 실행합니다.</li>
<li><code>~/miniconda3/bin/conda init</code>을 실행한 후 현재 쉘을 닫았다가 다시 여는 대신 <code>source ~/.bashrc</code>를 실행할 수 있습니다.</li>
</ul>
<h2 id="원격으로-jupyter-notebook-실행-running-the-jupyter-notebook-remotely"><a class="header" href="#원격으로-jupyter-notebook-실행-running-the-jupyter-notebook-remotely">원격으로 Jupyter Notebook 실행 (Running the Jupyter Notebook remotely)</a></h2>
<p>Jupyter Notebook을 원격으로 실행하려면 SSH 포트 포워딩을 사용해야 합니다. 결국 클라우드의 서버에는 모니터나 키보드가 없기 때문입니다. 이를 위해 다음과 같이 데스크탑(또는 노트북)에서 서버에 로그인하십시오.</p>
<pre><code># 이 명령은 로컬 명령줄에서 실행해야 합니다
ssh -i "/path/to/key.pem" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com -L 8889:localhost:8888
</code></pre>
<p>다음으로 EC2 인스턴스에서 다운로드한 이 책의 코드 위치로 이동하여 다음을 실행합니다.</p>
<pre><code>conda activate d2l
jupyter notebook
</code></pre>
<p>:numref:<code>fig_jupyter</code>는 Jupyter Notebook을 실행한 후의 가능한 출력을 보여줍니다. 마지막 줄은 포트 8888에 대한 URL입니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/jupyter.png" alt="Jupyter Notebook 실행 후 출력. 마지막 줄은 포트 8888에 대한 URL입니다." />
:width:<code>700px</code>
:label:<code>fig_jupyter</code></p>
<p>포트 8889로 포트 포워딩을 사용했으므로 :numref:<code>fig_jupyter</code>의 빨간색 상자에 있는 마지막 줄을 복사하여 URL에서 "8888"을 "8889"로 바꾸고 로컬 브라우저에서 엽니다.</p>
<h2 id="사용하지-않는-인스턴스-닫기-closing-unused-instances"><a class="header" href="#사용하지-않는-인스턴스-닫기-closing-unused-instances">사용하지 않는 인스턴스 닫기 (Closing Unused Instances)</a></h2>
<p>클라우드 서비스는 사용 시간에 따라 요금이 부과되므로 사용하지 않는 인스턴스는 닫아야 합니다. 대안이 있음에 유의하십시오.</p>
<ul>
<li>인스턴스를 "중지(Stopping)"하면 나중에 다시 시작할 수 있습니다. 이는 일반 서버의 전원을 끄는 것과 비슷합니다. 그러나 중지된 인스턴스는 유지된 하드 디스크 공간에 대해 소액의 요금이 계속 부과됩니다.</li>
<li>인스턴스를 "종료(Terminating)"하면 해당 인스턴스와 관련된 모든 데이터가 삭제됩니다. 여기에는 디스크가 포함되므로 다시 시작할 수 없습니다. 나중에 필요하지 않을 것임을 확신하는 경우에만 이 작업을 수행하십시오.</li>
</ul>
<p>인스턴스를 더 많은 인스턴스를 위한 템플릿으로 사용하려면 :numref:<code>fig_connect</code>의 예제를 마우스 오른쪽 버튼으로 클릭하고 "Image" $
ightarrow$ "Create"를 선택하여 인스턴스의 이미지를 만듭니다. 이 작업이 완료되면 "Instance State" $
ightarrow$ "Terminate"를 선택하여 인스턴스를 종료합니다. 다음에 이 인스턴스를 사용하고 싶을 때 이 섹션의 단계를 따라 저장된 이미지를 기반으로 인스턴스를 생성할 수 있습니다. 유일한 차이점은 :numref:<code>fig_ubuntu</code>에 표시된 "1. Choose AMI"에서 왼쪽의 "My AMIs" 옵션을 사용하여 저장된 이미지를 선택해야 한다는 것입니다. 생성된 인스턴스는 이미지 하드 디스크에 저장된 정보를 유지합니다. 예를 들어 CUDA 및 기타 런타임 환경을 다시 설치할 필요가 없습니다.</p>
<h2 id="요약-summary-128"><a class="header" href="#요약-summary-128">요약 (Summary)</a></h2>
<ul>
<li>우리는 자신의 컴퓨터를 사고 만들 필요 없이 필요에 따라 인스턴스를 시작하고 중지할 수 있습니다.</li>
<li>GPU 지원 딥러닝 프레임워크를 사용하기 전에 CUDA를 설치해야 합니다.</li>
<li>포트 포워딩을 사용하여 원격 서버에서 Jupyter Notebook을 실행할 수 있습니다.</li>
</ul>
<h2 id="연습-문제-exercises-143"><a class="header" href="#연습-문제-exercises-143">연습 문제 (Exercises)</a></h2>
<ol>
<li>클라우드는 편의성을 제공하지만 저렴하지는 않습니다. 비용을 절감하는 방법을 알아보기 위해 <a href="https://aws.amazon.com/ec2/spot/">스팟 인스턴스(spot instances)</a>를 시작하는 방법을 찾아보십시오.</li>
<li>다양한 GPU 서버를 실험해 보십시오. 얼마나 빠릅니까?</li>
<li>멀티 GPU 서버를 실험해 보십시오. 얼마나 잘 확장할 수 있습니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/423">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="google-colab-사용하기-using-google-colab"><a class="header" href="#google-colab-사용하기-using-google-colab">Google Colab 사용하기 (Using Google Colab)</a></h1>
<p>:label:<code>sec_colab</code></p>
<p>우리는 :numref:<code>sec_sagemaker</code>와 :numref:<code>sec_aws</code>에서 AWS에서 이 책을 실행하는 방법을 소개했습니다. 다른 옵션은 Google 계정이 있는 경우 <a href="https://colab.research.google.com/">Google Colab</a>에서 이 책을 실행하는 것입니다.</p>
<p>Colab에서 섹션의 코드를 실행하려면 :numref:<code>fig_colab</code>에 표시된 것처럼 <code>Colab</code> 버튼을 클릭하기만 하면 됩니다.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/colab.png" alt="Colab에서 섹션의 코드를 실행하십시오." />
:width:<code>300px</code>
:label:<code>fig_colab</code></p>
<p>코드 셀을 처음 실행하는 경우,
:numref:<code>fig_colab2</code>와 같은 경고 메시지를 받게 됩니다.
무시하려면 "RUN ANYWAY"를 클릭하십시오.</p>
<p><img src="chapter_appendix-tools-for-deep-learning/../img/colab-2.png" alt="무시하려면 &quot;RUN ANYWAY&quot;를 클릭하십시오." />
:width:<code>300px</code>
:label:<code>fig_colab2</code></p>
<p>다음으로, Colab은 이 섹션의 코드를 실행하기 위해 인스턴스에 연결해 줄 것입니다.
특히 GPU가 필요한 경우,
GPU 인스턴스에 연결하기 위해 Colab이 자동으로 요청될 것입니다.</p>
<h2 id="요약-summary-129"><a class="header" href="#요약-summary-129">요약 (Summary)</a></h2>
<ul>
<li>Google Colab을 사용하여 이 책의 각 섹션 코드를 실행할 수 있습니다.</li>
<li>이 책의 모든 섹션에서 GPU가 필요한 경우 GPU 인스턴스에 연결하기 위해 Colab이 요청될 것입니다.</li>
</ul>
<h2 id="연습-문제-exercises-144"><a class="header" href="#연습-문제-exercises-144">연습 문제 (Exercises)</a></h2>
<ol>
<li>Google Colab을 사용하여 이 책의 임의의 섹션을 여십시오.</li>
<li>Google Colab을 사용하여 GPU가 필요한 임의의 섹션을 편집하고 실행하십시오.</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/424">Discussions</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="서버-및-gpu-선택하기-selecting-servers-and-gpus"><a class="header" href="#서버-및-gpu-선택하기-selecting-servers-and-gpus">서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></h1>
<p>:label:<code>sec_selecting-servers-gpus</code></p>
<p>딥러닝을 위한 하드웨어 선택은 예산과 성능 사이의 균형을 맞추는 중요한 결정입니다.
이 섹션에서는 적절한 서버와 GPU를 선택하기 위한 몇 가지 가이드를 제공합니다.</p>
<h2 id="gpu-선택-가이드-gpu-selection-guide"><a class="header" href="#gpu-선택-가이드-gpu-selection-guide">GPU 선택 가이드 (GPU Selection Guide)</a></h2>
<ul>
<li><strong>메모리(VRAM)</strong>: 모델 크기와 배치 크기를 결정하는 핵심 요소입니다. 최소 8GB 이상을 권장하며, 최신 대규모 모델의 경우 16GB 또는 24GB 이상이 필요할 수 있습니다.</li>
<li><strong>코어 수 및 아키텍처</strong>: CUDA 코어와 텐서 코어의 수는 계산 속도에 직접적인 영향을 미칩니다. NVIDIA의 최신 아키텍처(예: Ampere, Ada Lovelace)를 선택하는 것이 유리합니다.</li>
<li><strong>부동 소수점 성능</strong>: FP16 또는 BF16 연산 성능이 훈련 속도에 중요합니다.</li>
</ul>
<h2 id="서버-구성-시-고려-사항-server-configuration"><a class="header" href="#서버-구성-시-고려-사항-server-configuration">서버 구성 시 고려 사항 (Server Configuration)</a></h2>
<ul>
<li><strong>CPU</strong>: GPU에 데이터를 공급하는 속도가 중요하므로, 충분한 PCIe 레인을 지원하는 고성능 CPU를 선택하십시오.</li>
<li><strong>PCIe 슬롯</strong>: 다중 GPU를 사용할 경우 PCIe 대역폭(예: PCIe 4.0 x16)이 충분한지 확인하십시오.</li>
<li><strong>전원 및 냉각</strong>: GPU는 전력을 많이 소비하고 열이 많이 발생하므로, 충분한 용량의 파워 서플라이와 강력한 냉각 시스템이 필수적입니다.</li>
</ul>
<h2 id="클라우드-vs-로컬-cloud-vs-local"><a class="header" href="#클라우드-vs-로컬-cloud-vs-local">클라우드 vs 로컬 (Cloud vs. Local)</a></h2>
<ul>
<li><strong>클라우드</strong>: 초기 비용이 없고 필요할 때만 고성능 자원을 사용할 수 있어 유연합니다(AWS, Google Cloud, Azure 등).</li>
<li><strong>로컬 서버</strong>: 장기적으로 대량의 연산을 수행할 경우 비용 효율적이며 데이터 보안 측면에서 유리할 수 있습니다.</li>
</ul>
<p>자신의 상황에 맞는 최적의 하드웨어 전략을 세워 보시기 바랍니다.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="기여하기-contributing-to-dive-into-deep-learning"><a class="header" href="#기여하기-contributing-to-dive-into-deep-learning">기여하기 (Contributing to Dive into Deep Learning)</a></h1>
<p>:label:<code>sec_contributing</code></p>
<p>이 책은 커뮤니티의 기여를 통해 성장하는 오픈 소스 프로젝트입니다.
오타 수정부터 새로운 섹션 작성까지, 모든 기여를 환영합니다.
기여하는 방법에 대한 자세한 가이드는 프로젝트의 <a href="https://github.com/d2l-ai/d2l-en/blob/master/CONTRIBUTING.md">기여 가이드</a>를 참조하십시오.</p>
<h2 id="기여-절차-general-workflow"><a class="header" href="#기여-절차-general-workflow">기여 절차 (General Workflow)</a></h2>
<ol>
<li>GitHub에서 저장소를 포크(fork)합니다.</li>
<li>로컬 머신에 포크한 저장소를 클론(clone)합니다.</li>
<li>새로운 브랜치를 생성하여 작업을 진행합니다.</li>
<li>변경 사항을 커밋(commit)하고 포크한 저장소에 푸시(push)합니다.</li>
<li>원본 저장소로 풀 리퀘스트(Pull Request)를 보냅니다.</li>
</ol>
<h2 id="유용한-팁-tips"><a class="header" href="#유용한-팁-tips">유용한 팁 (Tips)</a></h2>
<ul>
<li>오타나 간단한 수정은 GitHub 웹 인터페이스에서 직접 수행할 수 있습니다.</li>
<li>큰 변경 사항의 경우, 작업을 시작하기 전에 이슈(issue)를 열어 논의하는 것이 좋습니다.</li>
<li>스타일 가이드와 서식 규칙을 준수해 주십시오.</li>
</ul>
<p>여러분의 소중한 기여가 이 책을 더 좋게 만듭니다!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="유틸리티-utility-functions-and-classes"><a class="header" href="#유틸리티-utility-functions-and-classes">유틸리티 (Utility Functions and Classes)</a></h1>
<p>:label:<code>sec_utils</code></p>
<p>이 섹션에서는 이 책의 코드 예제에서 보조적으로 사용되는 몇 가지 유틸리티 함수와 클래스에 대해 설명합니다.
이들은 주로 데이터 처리, 시각화, 그리고 성능 측정의 편의를 위해 설계되었습니다.</p>
<h2 id="주요-유틸리티-common-utilities"><a class="header" href="#주요-유틸리티-common-utilities">주요 유틸리티 (Common Utilities)</a></h2>
<ul>
<li><strong>Timer</strong>: 코드의 실행 시간을 측정하기 위한 클래스입니다.</li>
<li><strong>Accumulator</strong>: 여러 변수의 합계를 효율적으로 누적하기 위한 도구입니다.</li>
<li><strong>Animator</strong>: 훈련 과정 중 손실이나 정확도 변화를 실시간으로 그래프로 그리기 위한 클래스입니다.</li>
</ul>
<p>이러한 도구들은 <code>d2l</code> 패키지에 포함되어 있으며, 각 장에서 핵심 로직에 집중할 수 있도록 복잡한 구현 세부 사항을 숨겨주는 역할을 합니다.
상세한 구현 내용은 저장소의 소스 코드를 통해 확인할 수 있습니다.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="d2l-패키지-the-d2l-package"><a class="header" href="#d2l-패키지-the-d2l-package">d2l 패키지 (The d2l Package)</a></h1>
<p>:label:<code>sec_d2l</code></p>
<p>이 책에서는 코드의 간결함을 유지하고 핵심 개념에 집중하기 위해, 자주 사용되는 함수와 클래스를 <code>d2l</code> 패키지에 모아두었습니다.
이 패키지는 딥러닝 프레임워크(MXNet, PyTorch, TensorFlow)에 따라 각각 다른 버전을 제공합니다.</p>
<h2 id="패키지-설치-installing-the-package"><a class="header" href="#패키지-설치-installing-the-package">패키지 설치 (Installing the Package)</a></h2>
<p>일반적으로 다음과 같이 pip를 통해 설치할 수 있습니다.</p>
<pre><code class="language-bash">pip install d2l
</code></pre>
<h2 id="주요-기능-key-functionalities"><a class="header" href="#주요-기능-key-functionalities">주요 기능 (Key Functionalities)</a></h2>
<p><code>d2l</code> 패키지에는 다음과 같은 유형의 도구들이 포함되어 있습니다.</p>
<ul>
<li><strong>데이터 로더</strong>: 대표적인 데이터셋을 다운로드하고 읽어오는 함수들.</li>
<li><strong>모델 정의</strong>: 시연에 자주 사용되는 표준 모델 아키텍처.</li>
<li><strong>훈련 루프</strong>: 모델을 훈련하고 성능을 측정하는 유틸리티.</li>
<li><strong>시각화</strong>: 손실 곡선, 정확도, 이미지 등을 그리기 위한 함수.</li>
</ul>
<p>각 장의 코드 예제에서 <code>#@save</code> 주석이 달린 함수나 클래스는 자동으로 <code>d2l</code> 패키지에 포함되도록 설계되었습니다.
패키지의 소스 코드는 프로젝트의 GitHub 저장소에서 확인할 수 있습니다.</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-eval_rst">
.. only:: html

   References
   ==========

</code></pre>
<p>:bibliography:<code>../d2l.bib</code></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
