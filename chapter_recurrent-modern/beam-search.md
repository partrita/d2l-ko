# 빔 검색 (Beam Search)
:label:`sec_beam-search`

:numref:`sec_seq2seq`에서 우리는 인코더-디코더 아키텍처와 이를 엔드-투-엔드로 훈련하기 위한 표준 기술을 소개했습니다. 그러나 테스트 시간 예측과 관련하여, 우리는 어떤 타임 스텝에서 특수 문장 끝("&lt;eos&gt;") 토큰을 예측할 때까지 다음에 올 예측 확률이 가장 높은 토큰을 각 타임 스텝에서 선택하는 *그리디(greedy)* 전략만 언급했습니다. 
이 섹션에서는 먼저 이 *그리디 검색(greedy search)* 전략을 공식화하고 실무자들이 흔히 겪는 몇 가지 문제를 식별하는 것으로 시작합니다. 
그 후, 이 전략을 두 가지 대안인 *전수 검색(exhaustive search)* (예시적이지만 실용적이지 않음) 및 *빔 검색(beam search)* (실무에서의 표준 방법)과 비교합니다.

:numref:`sec_seq2seq`의 관례를 빌려 수학적 표기법을 설정해 보겠습니다. 
임의의 타임 스텝 $t'$에서 디코더는 이전 토큰들 $y_1, \ldots, y_{t'}$와 인코더가 입력 시퀀스를 나타내기 위해 생성한 문맥 변수 $\mathbf{c}$에 조건부로 설정된, 어휘의 각 토큰이 시퀀스에서 다음에 올 확률( $y_{t'+1}$의 가능성 있는 값)을 나타내는 예측을 출력합니다. 
계산 비용을 정량화하기 위해, $\mathcal{Y}$를 출력 어휘(특수 문장 끝 토큰 "&lt;eos&gt;" 포함)라고 합시다. 
또한 출력 시퀀스의 최대 토큰 수를 $T'$로 지정합시다. 
우리의 목표는 가능한 모든 $\mathcal{O}(\left|\mathcal{Y}\right|^{T'})$ 출력 시퀀스 중에서 이상적인 출력을 찾는 것입니다. 
이는 "&lt;eos&gt;" 토큰이 발생하면 후속 토큰이 없기 때문에 고유한 출력의 수를 약간 과대평가하는 것입니다. 
그러나 우리의 목적을 위해 이 숫자는 검색 공간의 크기를 대략적으로 포착합니다.


## 그리디 검색 (Greedy Search)

:numref:`sec_seq2seq`의 간단한 *그리디 검색* 전략을 고려해 보십시오. 
여기서 임의의 타임 스텝 $t'$에 대해 우리는 단순히 $\mathcal{Y}$에서 가장 높은 조건부 확률을 가진 토큰을 선택합니다. 즉,

$$y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y_{t'-1}, \mathbf{c}).$$

모델이 "&lt;eos&gt;"를 출력하거나 최대 길이 $T'$에 도달하면 출력 시퀀스가 완료됩니다.

이 전략은 합리적으로 보일 수 있으며, 실제로 그리 나쁘지 않습니다! 
계산적으로 얼마나 부담이 없는지를 고려하면, 가성비가 매우 좋다고 할 수 있습니다. 
그러나 효율성을 잠시 제쳐둔다면, (그리디하게 선택된) *가장 가능성 있는 토큰*의 시퀀스가 아니라 *가장 가능성 있는 시퀀스*를 찾는 것이 더 합리적으로 보일 수 있습니다. 
이 두 객체는 상당히 다를 수 있음이 밝혀졌습니다. 
가장 가능성 있는 시퀀스는 표현식 $\prod_{t'=1}^{T'} P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$를 최대화하는 시퀀스입니다. 
기계 번역 예제에서 디코더가 기저의 생성 프로세스 확률을 진정으로 복구했다면, 이것은 우리에게 가장 가능성 있는 번역을 제공할 것입니다. 
불행히도 그리디 검색이 이 시퀀스를 제공할 것이라는 보장은 없습니다.

예를 들어 설명해 보겠습니다. 
출력 사전에 "A", "B", "C", "&lt;eos&gt;"라는 네 개의 토큰이 있다고 가정해 봅시다. 
:numref:`fig_s2s-prob1`에서 각 타임 스텝 아래의 네 숫자는 해당 타임 스텝에서 각각 "A", "B", "C", "&lt;eos&gt;"를 생성할 조건부 확률을 나타냅니다.

![각 타임 스텝에서 그리디 검색은 조건부 확률이 가장 높은 토큰을 선택합니다.](../img/s2s-prob1.svg)
:label:`fig_s2s-prob1`

각 타임 스텝에서 그리디 검색은 가장 높은 조건부 확률을 가진 토큰을 선택합니다. 
따라서 출력 시퀀스 "A", "B", "C", "&lt;eos&gt;"가 예측될 것입니다 (:numref:`fig_s2s-prob1`). 
이 출력 시퀀스의 조건부 확률은 $0.5\times0.4\times0.4\times0.6 = 0.048$입니다.


다음으로 :numref:`fig_s2s-prob2`의 다른 예를 살펴봅시다. 
:numref:`fig_s2s-prob1`과 달리 타임 스텝 2에서 우리는 *두 번째*로 높은 조건부 확률을 가진 토큰 "C"를 선택합니다.

![각 타임 스텝 아래의 네 숫자는 해당 타임 스텝에서 "A", "B", "C", "&lt;eos&gt;"를 생성할 조건부 확률을 나타냅니다. 타임 스텝 2에서 두 번째로 높은 조건부 확률을 가진 토큰 "C"가 선택됩니다.](../img/s2s-prob2.svg)
:label:`fig_s2s-prob2`

타임 스텝 3의 기반이 되는 타임 스텝 1과 2의 출력 하위 시퀀스가 :numref:`fig_s2s-prob1`의 "A"와 "B"에서 :numref:`fig_s2s-prob2`의 "A"와 "C"로 변경되었으므로, 
타임 스텝 3에서 각 토큰의 조건부 확률도 :numref:`fig_s2s-prob2`에서 변경되었습니다. 
타임 스텝 3에서 토큰 "B"를 선택한다고 가정합시다. 
이제 타임 스텝 4는 처음 세 타임 스텝의 출력 하위 시퀀스인 "A", "C", "B"에 조건부이며, 이는 :numref:`fig_s2s-prob1`의 "A", "B", "C"에서 변경되었습니다. 
따라서 :numref:`fig_s2s-prob2`의 타임 스텝 4에서 각 토큰을 생성할 조건부 확률도 :numref:`fig_s2s-prob1`의 것과 다릅니다. 
결과적으로 :numref:`fig_s2s-prob2`에서 출력 시퀀스 "A", "C", "B", "&lt;eos&gt;"의 조건부 확률은 $0.5\times0.3 \times0.6\times0.6=0.054$이며, 
이는 :numref:`fig_s2s-prob1`의 그리디 검색보다 큽니다. 
이 예에서 그리디 검색에 의해 얻은 출력 시퀀스 "A", "B", "C", "&lt;eos&gt;"는 최적이 아닙니다.



## 전수 검색 (Exhaustive Search)

목표가 가장 가능성 있는 시퀀스를 얻는 것이라면, 우리는 *전수 검색*을 사용하는 것을 고려할 수 있습니다: 
가능한 모든 출력 시퀀스를 조건부 확률과 함께 열거한 다음, 가장 높은 예측 확률을 기록하는 시퀀스를 출력합니다.


이것이 분명히 우리가 원하는 것을 제공하겠지만, 
어휘 크기에 의해 주어지는 거대한 베이스를 가진 시퀀스 길이에 기하급수적인 $\mathcal{O}(\left|\mathcal{Y}\right|^{T'})$의 감당할 수 없는 계산 비용이 수반될 것입니다. 
예를 들어 $|y|=10000$이고 $T'=10$일 때(실제 응용 사례와 비교하면 둘 다 작은 숫자임), 우리는 $10000^{10} = 10^{40}$개의 시퀀스를 평가해야 하는데, 이는 이미 예측 가능한 어떤 컴퓨터의 능력도 벗어납니다. 
반면 그리디 검색의 계산 비용은 $\mathcal{O}(\left|\mathcal{Y}\right|T')$입니다: 기적적으로 저렴하지만 최적과는 거리가 멉니다. 
예를 들어 $|y|=10000$이고 $T'=10$일 때, 우리는 $10000\times10=10^5$개의 시퀀스만 평가하면 됩니다.


## 빔 검색 (Beam Search)

시퀀스 디코딩 전략을 하나의 스펙트럼으로 볼 수 있는데, *빔 검색*은 그리디 검색의 효율성과 전수 검색의 최적성 사이에서 타협을 취합니다. 
빔 검색의 가장 간단한 버전은 단일 하이퍼파라미터인 *빔 크기(beam size)* $k$로 특징지어집니다. 
이 용어를 설명해 보겠습니다. 
타임 스텝 1에서 우리는 가장 높은 예측 확률을 가진 $k$개의 토큰을 선택합니다. 
각각은 각각 $k$개의 후보 출력 시퀀스의 첫 번째 토큰이 됩니다. 
각 후속 타임 스텝에서 이전 타임 스텝의 $k$개 후보 출력 시퀀스를 기반으로, 
우리는 $k\left|\mathcal{Y}\right|$개의 가능한 선택지 중에서 가장 높은 예측 확률을 가진 $k$개의 후보 출력 시퀀스를 계속 선택합니다.

![빔 검색 과정(빔 크기 $=2$, 출력 시퀀스의 최대 길이 $=3$). 후보 출력 시퀀스는 $\mathit{A}$, $\mathit{C}$, $\mathit{AB}$, $\mathit{CE}$, $\mathit{ABD}$, $\mathit{CED}$입니다.](../img/beam-search.svg)
:label:`fig_beam-search`


:numref:`fig_beam-search`는 빔 검색의 과정을 예로 들어 보여줍니다. 
출력 어휘에 5개의 요소만 포함되어 있다고 가정합시다: $\mathcal{Y} = \{A, B, C, D, E\}$이며, 그중 하나는 “&lt;eos&gt;”입니다. 
빔 크기를 2로 하고 출력 시퀀스의 최대 길이를 3으로 합시다. 
타임 스텝 1에서 
조건부 확률 $P(y_1 \mid \mathbf{c})$가 가장 높은 토큰이 $A$와 $C$라고 가정합니다. 
타임 스텝 2에서 모든 $y_2 \in \mathcal{Y}$에 대해 다음을 계산합니다.

$$\begin{aligned}P(A, y_2 \mid \mathbf{c}) = P(A \mid \mathbf{c})P(y_2 \mid A, \mathbf{c}),\ P(C, y_2 \mid \mathbf{c}) = P(C \mid \mathbf{c})P(y_2 \mid C, \mathbf{c}),\end{aligned}$$  

그리고 이 10개 값 중 가장 큰 두 개, 예를 들어 $P(A, B \mid \mathbf{c})$와 $P(C, E \mid \mathbf{c})$를 고릅니다. 
그런 다음 타임 스텝 3에서 모든 $y_3 \in \mathcal{Y}$에 대해 다음을 계산합니다.

$$\begin{aligned}P(A, B, y_3 \mid \mathbf{c}) = P(A, B \mid \mathbf{c})P(y_3 \mid A, B, \mathbf{c}),\P(C, E, y_3 \mid \mathbf{c}) = P(C, E \mid \mathbf{c})P(y_3 \mid C, E, \mathbf{c}),\end{aligned}$$ 

그리고 이 10개 값 중 가장 큰 두 개, 예를 들어 $P(A, B, D \mid \mathbf{c})$와 $P(C, E, D \mid  \mathbf{c})$를 고릅니다. 
결과적으로 (i) $A$, (ii) $C$, (iii) $A, B$, (iv) $C, E$, (v) $A, B, D$, (vi) $C, E, D$라는 6개의 후보 출력 시퀀스를 얻게 됩니다.


마지막으로, 이 6개 시퀀스를 기반으로 최종 후보 출력 시퀀스 집합을 얻습니다(예: “&lt;eos&gt;”를 포함한 그 이후 부분 폐기). 
그런 다음 다음 점수를 최대화하는 출력 시퀀스를 선택합니다:

$$ \frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c});$$
:eqlabel:`eq_beam-search-score`

여기서 $L$은 최종 후보 시퀀스의 길이이고 $\alpha$는 보통 0.75로 설정됩니다. 
긴 시퀀스는 :eqref:`eq_beam-search-score`의 합산에서 더 많은 로그 항을 갖기 때문에, 분모의 $L^\alpha$ 항은 긴 시퀀스에 페널티를 줍니다.

빔 검색의 계산 비용은 $\mathcal{O}(k\left|\mathcal{Y}\right|T')$입니다. 
이 결과는 그리디 검색과 전수 검색의 중간에 있습니다. 
그리디 검색은 빔 크기를 1로 설정했을 때 발생하는 빔 검색의 특수한 경우로 취급될 수 있습니다.



## 요약 (Summary)

시퀀스 검색 전략에는 그리디 검색, 전수 검색, 빔 검색이 포함됩니다. 
빔 검색은 빔 크기의 유연한 선택을 통해 정확도와 계산 비용 사이의 트레이드오프를 제공합니다.


## 연습 문제 (Exercises)

1. 전수 검색을 빔 검색의 특수한 유형으로 취급할 수 있습니까? 왜 그런가요 혹은 왜 아닌가요?
2. :numref:`sec_seq2seq`의 기계 번역 문제에 빔 검색을 적용하십시오. 빔 크기가 번역 결과와 예측 속도에 어떤 영향을 미칩니까?
3. 우리는 :numref:`sec_rnn-scratch`에서 사용자 제공 접두사를 따르는 텍스트를 생성하기 위해 언어 모델링을 사용했습니다. 그것은 어떤 종류의 검색 전략을 사용합니까? 그것을 개선할 수 있습니까?

[토론](https://discuss.d2l.ai/t/338)