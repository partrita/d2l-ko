# 언어 모델 (Language Models)
:label:`sec_language-model`

:numref:`sec_text-sequence`에서 우리는 텍스트 시퀀스를 토큰으로 매핑하는 방법을 보았습니다. 여기서 토큰은 단어나 문자와 같은 이산적인 관찰의 시퀀스로 간주될 수 있습니다. 
길이가 $T$인 텍스트 시퀀스의 토큰이 차례로 $x_1, x_2, \ldots, x_T$라고 가정합시다. 
*언어 모델(Language models)*의 목표는 전체 시퀀스의 결합 확률을 추정하는 것입니다:

$$P(x_1, x_2, \ldots, x_T),$$

여기서 :numref:`sec_sequence`의 통계 도구를 적용할 수 있습니다.

언어 모델은 매우 유용합니다. 예를 들어, 이상적인 언어 모델은 단순히 한 번에 하나의 토큰 $x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$을 뽑는 것만으로 자연스러운 텍스트를 스스로 생성해야 합니다. 
타자기를 사용하는 원숭이와는 달리, 그러한 모델에서 나오는 모든 텍스트는 영문 텍스트와 같은 자연어처럼 보일 것입니다. 
더 나아가 이전 대화 조각에 텍스트를 조건부로 하여 의미 있는 대화를 생성하는 데에도 충분할 것입니다. 
분명히 우리는 문법적으로 합리적인 콘텐츠를 생성하는 것이 아니라 텍스트를 *이해*해야 하므로 그러한 시스템을 설계하는 것과는 아직 거리가 멉니다.

그럼에도 불구하고 언어 모델은 제한된 형태에서도 큰 도움이 됩니다. 
예를 들어 "to recognize speech"와 "to wreck a nice beach"라는 구절은 매우 비슷하게 들립니다. 
이는 음성 인식에서 모호성을 유발할 수 있는데, 언어 모델을 통해 두 번째 번역을 기이한 것으로 거부함으로써 쉽게 해결할 수 있습니다. 
마찬가지로 문서 요약 알고리즘에서는 "dog bites man"이 "man bites dog"보다 훨씬 더 빈번하다거나, "I want to eat grandma"는 다소 충격적인 진술인 반면 "I want to eat, grandma"는 훨씬 더 온화하다는 것을 아는 것이 가치가 있습니다.

```{.python .input  n=1}
%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
```

```{.python .input  n=2}
%%tab mxnet
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
```

```{.python .input  n=3}
%%tab pytorch
from d2l import torch as d2l
import torch
```

```{.python .input  n=4}
%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
```

```{.python .input}
%%tab jax
from d2l import jax as d2l
from jax import numpy as jnp
```

## 언어 모델 학습 (Learning Language Models)

분명한 질문은 문서나 토큰 시퀀스를 어떻게 모델링해야 하는가입니다. 
텍스트 데이터를 단어 수준에서 토큰화한다고 가정해 봅시다. 
기본 확률 규칙을 적용하여 시작해 봅시다:

$$P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1}).$$

예를 들어, 
네 단어를 포함하는 텍스트 시퀀스의 확률은 다음과 같이 주어집니다:

$$\begin{aligned}&P(\textrm{deep}, \textrm{learning}, \textrm{is}, \textrm{fun}) \&=P(\textrm{deep}) P(\textrm{learning}  \mid  \textrm{deep}) P(\textrm{is}  \mid  \textrm{deep}, \textrm{learning}) P(\textrm{fun}  \mid  \textrm{deep}, \textrm{learning}, \textrm{is}).\end{aligned}$$

### 마르코프 모델과 $n$-gram (Markov Models and $n$-grams)
:label:`subsec_markov-models-and-n-grams`

:numref:`sec_sequence`의 시퀀스 모델 분석 중에서, 
언어 모델링에 마르코프 모델을 적용해 봅시다. 
시퀀스에 대한 분포가 $P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$를 만족하면 1차 마르코프 속성을 만족합니다. 더 높은 차수는 더 긴 의존성에 해당합니다. 이는 시퀀스를 모델링하기 위해 적용할 수 있는 여러 가지 근사로 이어집니다:

$$
\begin{aligned}
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).
\end{aligned}
$$

하나, 둘, 세 개의 변수를 포함하는 확률 공식은 일반적으로 각각 *유니그램(unigram)*, *바이그램(bigram)*, *트라이그램(trigram)* 모델이라고 합니다. 
언어 모델을 계산하려면 단어의 확률과 이전 몇 단어가 주어졌을 때 단어의 조건부 확률을 계산해야 합니다. 
그러한 확률은 언어 모델 파라미터라는 점에 유의하십시오.



### 단어 빈도 (Word Frequency)

여기서 우리는 
훈련 데이터셋이 모든 위키백과 항목, [구텐베르크 프로젝트](https://en.wikipedia.org/wiki/Project_Gutenberg), 
웹에 게시된 모든 텍스트와 같은 대규모 텍스트 말뭉치라고 가정합니다. 
단어의 확률은 훈련 데이터셋에서 주어진 단어의 상대적 단어 빈도(relative word frequency)로부터 계산할 수 있습니다. 
예를 들어 추정치 $\hat{P}(\textrm{deep})$은 "deep"이라는 단어로 시작하는 문장의 확률로 계산할 수 있습니다. 
약간 덜 정확한 접근 방식은 "deep"이라는 단어의 모든 발생 횟수를 세고 이를 말뭉치의 총 단어 수로 나누는 것입니다. 
이것은 특히 빈번한 단어에 대해 꽤 잘 작동합니다. 계속해서 우리는 다음을 추정하려고 시도할 수 있습니다.

$$\hat{P}(\textrm{learning} \mid \textrm{deep}) = \frac{n(\textrm{deep, learning})}{n(\textrm{deep})},$$

여기서 $n(x)$와 $n(x, x')$는 각각 단일 단어와 연속된 단어 쌍의 발생 횟수입니다. 
불행히도 단어 쌍의 확률을 추정하는 것은 다소 더 어렵습니다. "deep learning"의 발생 빈도가 훨씬 적기 때문입니다. 
특히 일부 특이한 단어 조합의 경우 정확한 추정치를 얻을 만큼 충분한 발생 횟수를 찾기가 까다로울 수 있습니다. 
:numref:`subsec_natural-lang-stat`의 경험적 결과가 시사하듯이, 
세 단어 조합 및 그 이상에서는 상황이 악화됩니다. 
우리의 데이터셋에서 보지 못할 가능성이 높은 그럴듯한 세 단어 조합이 많이 있을 것입니다. 
그러한 단어 조합에 0이 아닌 카운트를 할당하는 솔루션을 제공하지 않는 한, 언어 모델에서 이를 사용할 수 없습니다. 데이터셋이 작거나 단어가 매우 드문 경우, 그중 하나도 찾지 못할 수 있습니다.

### 라플라스 평활화 (Laplace Smoothing)

일반적인 전략은 어떤 형태의 *라플라스 평활화(Laplace smoothing)*를 수행하는 것입니다. 
해결책은 모든 카운트에 작은 상수를 더하는 것입니다. 
훈련 세트의 총 단어 수를 $n$, 고유 단어 수를 $m$이라고 합시다. 
이 해결책은 예를 들어 다음과 같이 단일 단어에 도움이 됩니다.

$$\begin{aligned}
\hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \ 
\hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \ 
\hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
\end{aligned}$$

여기서 $\epsilon_1,\epsilon_2, \epsilon_3$는 하이퍼파라미터입니다. 
$\\epsilon_1$을 예로 들어봅시다: 
$\\epsilon_1 = 0$일 때 평활화가 적용되지 않습니다; 
$\\epsilon_1$이 양의 무한대에 접근하면 $\hat{P}(x)$는 균등 확률 $1/m$에 접근합니다. 
위의 내용은 다른 기술이 달성할 수 있는 것의 다소 원시적인 변형입니다 :cite:`Wood.Gasthaus.Archambeau.ea.2011`.


불행히도 이와 같은 모델은 다음과 같은 이유로 금방 다루기 힘들어집니다. 
첫째, :numref:`subsec_natural-lang-stat`에서 논의한 바와 같이 
많은 $n$-gram이 매우 드물게 발생하여 라플라스 평활화가 언어 모델링에 부적합합니다. 
둘째, 모든 카운트를 저장해야 합니다. 
셋째, 이것은 단어의 의미를 완전히 무시합니다. 예를 들어 "cat"과 "feline"은 관련된 문맥에서 발생해야 합니다. 
이러한 모델을 추가적인 문맥에 맞게 조정하는 것은 꽤 어렵습니다. 
반면 딥러닝 기반 언어 모델은 이를 고려하는 데 적합합니다. 
마지막으로 긴 단어 시퀀스는 거의 확실하게 새로운 것이므로, 이전에 본 단어 시퀀스의 빈도를 단순히 세는 모델은 거기서 성능이 떨어질 수밖에 없습니다. 
따라서 이 장의 나머지 부분에서는 언어 모델링을 위해 신경망을 사용하는 데 초점을 맞춥니다.


## 퍼플렉서티 (Perplexity)
:label:`subsec_perplexity`

다음으로 언어 모델의 품질을 측정하는 방법에 대해 논의해 보겠습니다. 이는 후속 섹션에서 모델을 평가하는 데 사용할 것입니다. 
한 가지 방법은 텍스트가 얼마나 놀라운지 확인하는 것입니다. 
좋은 언어 모델은 다음에 올 토큰을 높은 정확도로 예측할 수 있습니다. 
다음 언어 모델들이 제안한 "It is raining"이라는 구절의 이어짐을 고려해 보십시오:

1. "It is raining outside"
2. "It is raining banana tree"
3. "It is raining piouw;kcj pwepoiut"

품질 면에서 예제 1이 분명히 최고입니다. 단어들이 합리적이고 논리적으로 일관성이 있습니다. 
의미적으로 어떤 단어가 뒤따를지 정확하게 반영하지 않을 수도 있지만("in San Francisco"와 "in winter"는 완벽하게 합리적인 확장이었을 것입니다), 모델은 어떤 종류의 단어가 뒤따르는지 포착할 수 있습니다. 
예제 2는 말도 안 되는 확장을 생성하여 상당히 나쁩니다. 그럼에도 불구하고 적어도 모델은 단어 철자법과 단어 간의 어느 정도의 상관관계를 학습했습니다. 마지막으로 예제 3은 데이터에 제대로 적합하지 않은 훈련이 잘못된 모델을 나타냅니다.

시퀀스의 우도(likelihood)를 계산하여 모델의 품질을 측정할 수 있습니다. 
불행히도 이것은 이해하기 어렵고 비교하기 어려운 숫자입니다. 
결국 짧은 시퀀스가 긴 시퀀스보다 발생할 가능성이 훨씬 높으므로, 톨스토이의 대작 *전쟁과 평화*에서 모델을 평가하면 생텍쥐페리의 소설 *어린 왕자*보다 훨씬 작은 우도를 생성할 것입니다. 빠진 것은 평균에 해당하는 것입니다.

여기서 정보 이론이 유용합니다. 
우리는 소프트맥스 회귀(:numref:`subsec_info_theory_basics`)를 소개할 때 엔트로피, 놀람(surprisal), 크로스 엔트로피를 정의했습니다. 
텍스트를 압축하고 싶다면 현재 토큰 세트가 주어졌을 때 다음 토큰을 예측하는 것에 대해 물을 수 있습니다. 
더 좋은 언어 모델은 다음 토큰을 더 정확하게 예측할 수 있게 해 줄 것입니다. 
따라서 시퀀스를 압축하는 데 더 적은 비트를 소비할 수 있게 해 줄 것입니다. 
따라서 우리는 시퀀스의 모든 $n$ 토큰에 대해 평균을 낸 크로스 엔트로피 손실로 이를 측정할 수 있습니다:

$$\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),$$
:eqlabel:`eq_avg_ce_for_lm`

여기서 $P$는 언어 모델에 의해 주어지고 $x_t$는 시퀀스의 타임 스텝 $t$에서 관찰된 실제 토큰입니다. 
이것은 길이가 다른 문서의 성능을 비교할 수 있게 만듭니다. 역사적인 이유로 자연어 처리 과학자들은 *퍼플렉서티(perplexity)*라는 수량을 사용하는 것을 선호합니다. 요컨대, 이것은 :eqref:`eq_avg_ce_for_lm`의 지수입니다:

$$\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).$$ 

퍼플렉서티는 다음 토큰을 선택할 때 우리가 가진 실제 선택지 수의 기하 평균의 역수로 이해하는 것이 가장 좋습니다. 몇 가지 경우를 살펴보겠습니다:

* 최선의 시나리오에서 모델은 항상 대상 토큰의 확률을 1로 완벽하게 추정합니다. 이 경우 모델의 퍼플렉서티는 1입니다.
* 최악의 시나리오에서 모델은 항상 대상 토큰의 확률을 0으로 예측합니다. 이 상황에서 퍼플렉서티는 양의 무한대입니다.
* 기준선(baseline)에서 모델은 어휘의 모든 사용 가능한 토큰에 대해 균등 분포를 예측합니다. 이 경우 퍼플렉서티는 어휘의 고유 토큰 수와 같습니다. 사실 압축 없이 시퀀스를 저장해야 한다면 이것이 인코딩을 위해 할 수 있는 최선일 것입니다. 따라서 이는 유용한 모델이라면 반드시 깨야 하는 중요한 상한선을 제공합니다.

## 시퀀스 분할 (Partitioning Sequences)
:label:`subsec_partitioning-seqs`

우리는 신경망을 사용하여 언어 모델을 설계하고 
텍스트 시퀀스에서 현재 토큰 세트가 주어졌을 때 다음 토큰을 예측하는 모델의 성능을 평가하기 위해 퍼플렉서티를 사용할 것입니다. 
모델을 소개하기 전에, 모델이 한 번에 미리 정의된 길이의 시퀀스 미니배치를 처리한다고 가정해 봅시다. 
이제 문제는 [**입력 시퀀스와 타겟 시퀀스의 미니배치를 무작위로 읽는 방법**]입니다.


데이터셋이 `corpus`에 있는 $T$개의 토큰 인덱스 시퀀스 형태를 취한다고 가정해 봅시다. 
우리는 이것을 부분 시퀀스로 분할할 것이며, 각 부분 시퀀스는 $n$개의 토큰(타임 스텝)을 가집니다. 
각 에폭마다 전체 데이터셋의 (거의) 모든 토큰을 반복하고 가능한 모든 길이 $n$ 부분 시퀀스를 얻기 위해 무작위성을 도입할 수 있습니다. 
더 구체적으로, 각 에폭이 시작될 때 무작위로 균일하게 샘플링된 $d 
in [0,n)$개의 첫 번째 토큰을 버립니다. 
나머지 시퀀스는 $m=\lfloor (T-d)/n \rfloor$개의 부분 시퀀스로 분할됩니다. 
타임 스텝 $t$에서 토큰 $x_t$로 시작하는 길이 $n$ 부분 시퀀스를 $\mathbf x_t = [x_t, \ldots, x_{t+n-1}]$로 표시합니다. 
결과적으로 $m$개의 분할된 부분 시퀀스는 
$\mathbf x_d, \mathbf x_{d+n}, \ldots, \mathbf x_{d+n(m-1)}.$ 
각 부분 시퀀스는 언어 모델의 입력 시퀀스로 사용됩니다.


언어 모델링의 경우, 
목표는 지금까지 본 토큰을 기반으로 다음 토큰을 예측하는 것이므로 타겟(레이블)은 한 토큰만큼 이동된 원래 시퀀스입니다. 
임의의 입력 시퀀스 $\mathbf x_t$에 대한 타겟 시퀀스는 길이가 $n$인 $\mathbf x_{t+1}$입니다.

![분할된 길이 5 부분 시퀀스에서 5쌍의 입력 시퀀스와 타겟 시퀀스 얻기.](../img/lang-model-data.svg) 
:label:`fig_lang_model_data`

:numref:`fig_lang_model_data`는 $n=5$ 및 $d=2$일 때 5쌍의 입력 시퀀스와 타겟 시퀀스를 얻는 예를 보여줍니다.

```{.python .input  n=5}
%%tab all
@d2l.add_to_class(d2l.TimeMachine)  #@save
def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):
    super(d2l.TimeMachine, self).__init__()
    self.save_hyperparameters()
    corpus, self.vocab = self.build(self._download())
    array = d2l.tensor([corpus[i:i+num_steps+1] 
                        for i in range(len(corpus)-num_steps)])
    self.X, self.Y = array[:,:-1], array[:,1:]
```

언어 모델을 훈련하기 위해, 
우리는 입력 시퀀스와 타겟 시퀀스 쌍을 미니배치로 무작위로 샘플링할 것입니다. 
다음 데이터 로더는 매번 데이터셋에서 미니배치를 무작위로 생성합니다. 
인수 `batch_size`는 각 미니배치의 부분 시퀀스 예제 수를 지정하고 `num_steps`는 토큰 단위의 부분 시퀀스 길이입니다.

```{.python .input  n=6}
%%tab all
@d2l.add_to_class(d2l.TimeMachine)  #@save
def get_dataloader(self, train):
    idx = slice(0, self.num_train) if train else slice(
        self.num_train, self.num_train + self.num_val)
    return self.get_tensorloader([self.X, self.Y], train, idx)
```

다음에서 볼 수 있듯이, 
타겟 시퀀스의 미니배치는 
입력 시퀀스를 한 토큰만큼 이동시켜 얻을 수 있습니다.

```{.python .input  n=7}
%%tab all
data = d2l.TimeMachine(batch_size=2, num_steps=10)
for X, Y in data.train_dataloader():
    print('X:', X, '\nY:', Y)
    break
```

## 요약 및 토론 (Summary and Discussion)

언어 모델은 텍스트 시퀀스의 결합 확률을 추정합니다. 긴 시퀀스의 경우 $n$-gram은 의존성을 잘라냄으로써 편리한 모델을 제공합니다. 그러나 라플라스 평활화를 통해 빈번하지 않은 단어 조합을 효율적으로 처리하기에는 구조는 많지만 빈도가 충분하지 않습니다. 따라서 후속 섹션에서는 신경망 언어 모델링에 초점을 맞출 것입니다.
언어 모델을 훈련하기 위해 입력 시퀀스와 타겟 시퀀스 쌍을 미니배치로 무작위로 샘플링할 수 있습니다. 훈련 후에는 언어 모델 품질을 측정하기 위해 퍼플렉서티를 사용할 것입니다.

언어 모델은 데이터 크기, 모델 크기, 훈련 컴퓨팅 양을 늘리면 확장될 수 있습니다. 대규모 언어 모델은 입력 텍스트 지침이 주어졌을 때 출력 텍스트를 예측함으로써 원하는 작업을 수행할 수 있습니다. 나중에 논의하겠지만(예: :numref:`sec_large-pretraining-transformers`), 
현재 대규모 언어 모델은 다양한 작업에서 최첨단 시스템의 기반을 형성합니다.


## 연습 문제 (Exercises)

1. 훈련 데이터셋에 100,000개의 단어가 있다고 가정합니다. 4-gram은 얼마나 많은 단어 빈도와 다중 단어 인접 빈도를 저장해야 합니까?
2. 대화를 어떻게 모델링하시겠습니까?
3. 긴 시퀀스 데이터를 읽기 위해 생각할 수 있는 다른 방법은 무엇입니까? 
4. 각 에폭 시작 시 처음 몇 개의 토큰을 무작위로 버리는 우리의 방법을 고려해 보십시오. 
    1. 그것이 정말로 문서의 시퀀스에 대해 완벽하게 균일한 분포로 이어집니까?
    2. 상황을 더 균일하게 만들기 위해 무엇을 해야 합니까? 
5. 시퀀스 예제가 완전한 문장이 되기를 원한다면 미니배치 샘플링에 어떤 문제가 발생합니까? 어떻게 해결할 수 있습니까? 

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/117)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/118)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/1049)
:end_tab:

:begin_tab:`jax`
[Discussions](https://discuss.d2l.ai/t/18012)
:end_tab: