<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>BPTT (Backpropagation Through Time) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/bptt.html" class="active"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="시간-경과에-따른-역전파-backpropagation-through-time"><a class="header" href="#시간-경과에-따른-역전파-backpropagation-through-time">시간 경과에 따른 역전파 (Backpropagation Through Time)</a></h1>
<p>:label:<code>sec_bptt</code></p>
<p>:numref:<code>sec_rnn-scratch</code>의 연습 문제를 완료했다면, 가끔 발생하는 엄청난 기울기로 인해 훈련이 불안정해지는 것을 방지하기 위해 기울기 클리핑이 필수적이라는 것을 알았을 것입니다.
우리는 폭발하는 기울기가 긴 시퀀스를 가로질러 역전파하는 데서 비롯된다고 암시했습니다.
수많은 현대 RNN 아키텍처를 소개하기 전에, 시퀀스 모델에서 <em>역전파</em>가 어떻게 작동하는지 수학적으로 자세히 살펴보겠습니다.
바라건대 이 논의가 <em>사라지는(vanishing)</em> 기울기와 <em>폭발하는(exploding)</em> 기울기의 개념에 어느 정도 정확성을 가져다줄 것입니다.
:numref:<code>sec_backprop</code>에서 MLP를 소개할 때 계산 그래프를 통한 순전파 및 역전파에 대한 논의를 기억한다면,
RNN의 순전파는 비교적 간단할 것입니다.
RNN에 역전파를 적용하는 것을 *시간 경과에 따른 역전파(backpropagation through time)*라고 합니다 :cite:<code>Werbos.1990</code>.
이 절차는 RNN의 계산 그래프를 한 번에 한 타임 스텝씩 확장(또는 펼치기)해야 합니다.
펼쳐진 RNN은 본질적으로
펼쳐진 네트워크 전체에서 동일한 파라미터가 반복되어
각 타임 스텝에 나타나는 특수한 속성을 가진 피드포워드 신경망입니다.
그런 다음 다른 피드포워드 신경망과 마찬가지로 연쇄 법칙을 적용하여 펼쳐진 네트워크를 통해 기울기를 역전파할 수 있습니다.
각 파라미터에 대한 기울기는 펼쳐진 네트워크에서 파라미터가 발생하는 모든 위치에 걸쳐 합산되어야 합니다.
이러한 가중치 묶음을 처리하는 것은 합성곱 신경망에 대한 장에서 익숙할 것입니다.</p>
<p>시퀀스가 꽤 길 수 있기 때문에 복잡한 문제가 발생합니다.
천 개 이상의 토큰으로 구성된 텍스트 시퀀스로 작업하는 것은 드문 일이 아닙니다.
이것은 계산(너무 많은 메모리) 및 최적화(수치적 불안정성) 관점 모두에서 문제를 제기합니다.
첫 번째 단계의 입력은 출력에 도달하기 전에 1000개 이상의 행렬 곱을 통과하며,
기울기를 계산하기 위해 또 다른 1000개의 행렬 곱이 필요합니다.
이제 무엇이 잘못될 수 있고 실제로 어떻게 해결해야 하는지 분석해 보겠습니다.</p>
<h2 id="rnn의-기울기-분석-analysis-of-gradients-in-rnns"><a class="header" href="#rnn의-기울기-분석-analysis-of-gradients-in-rnns">RNN의 기울기 분석 (Analysis of Gradients in RNNs)</a></h2>
<p>:label:<code>subsec_bptt_analysis</code></p>
<p>RNN이 작동하는 방식에 대한 단순화된 모델로 시작합니다.
이 모델은 은닉 상태의 세부 사항과 업데이트 방법에 대한 세부 사항을 무시합니다.
여기서 수학적 표기법은 스칼라, 벡터, 행렬을 명시적으로 구분하지 않습니다.
우리는 단지 약간의 직관을 개발하려고 노력하고 있습니다.
이 단순화된 모델에서 타임 스텝 $t$에서의 은닉 상태를 $h_t$, 입력을 $x_t$, 출력을 $o_t$로 표시합니다.
:numref:<code>subsec_rnn_w_hidden_states</code>에서의 논의를 상기하면,
입력과 은닉 상태는 은닉층의 하나의 가중치 변수와 곱해지기 전에 연결될 수 있습니다.
따라서 우리는 $w_\textrm{h}$와 $w_\textrm{o}$를 사용하여 각각 은닉층과 출력 레이어의 가중치를 나타냅니다.
결과적으로 각 타임 스텝에서의 은닉 상태와 출력은 다음과 같습니다.</p>
<p>$$\begin{aligned}h_t &amp;= f(x_t, h_{t-1}, w_\textrm{h}),\o_t &amp;= g(h_t, w_\textrm{o}),\end{aligned}$$:eqlabel:<code>eq_bptt_ht_ot</code></p>
<p>여기서 $f$와 $g$는 각각 은닉층과 출력 레이어의 변환입니다.
따라서 우리는 순환 계산을 통해 서로 의존하는 값의 체인 {\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \ldots}를 갖습니다.
순전파는 꽤 간단합니다.
우리가 필요한 것은 $(x_t, h_t, o_t)$ 삼중쌍을 한 번에 한 타임 스텝씩 반복하는 것입니다.
출력 $o_t$와 원하는 타겟 $y_t$ 사이의 불일치는 다음과 같이 모든 $T$ 타임 스텝에 걸쳐 목적 함수에 의해 평가됩니다.</p>
<p>$$L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_\textrm{h}, w_\textrm{o}) = \frac{1}{T}\sum_{t=1}^T l(y_t, o_t).$$</p>
<p>역전파의 경우, 특히 목적 함수 $L$의 파라미터 $w_\textrm{h}$에 대한 기울기를 계산할 때 문제는 좀 더 까다롭습니다.
구체적으로 연쇄 법칙에 의해,</p>
<p>$$\begin{aligned}\frac{\partial L}{\partial w_\textrm{h}}  &amp; = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_\textrm{h}}  \&amp; = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_\textrm{o})}{\partial h_t}  \frac{\partial h_t}{\partial w_\textrm{h}}.\end{aligned}$$:eqlabel:<code>eq_bptt_partial_L_wh</code></p>
<p>:eqref:<code>eq_bptt_partial_L_wh</code>에 있는 곱의 첫 번째와 두 번째 인수는 계산하기 쉽습니다.
세 번째 인수 $\partial h_t/\partial w_\textrm{h}$는 상황이 까다로워지는 부분인데, $h_t$에 대한 파라미터 $w_\textrm{h}$의 효과를 순환적으로 계산해야 하기 때문입니다.
:eqref:<code>eq_bptt_ht_ot</code>의 순환 계산에 따르면,
$h_t$는 $h_{t-1}$과 $w_\textrm{h}$ 모두에 의존하며,
여기서 $h_{t-1}$의 계산도 $w_\textrm{h}$에 의존합니다.
따라서 연쇄 법칙을 사용하여 $w_\textrm{h}$에 대한 $h_t$의 전도함수(total derivate)를 평가하면 다음과 같습니다.</p>
<p>$$\frac{\partial h_t}{\partial w_\textrm{h}}= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}} +\frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_\textrm{h}}.$$:eqlabel:<code>eq_bptt_partial_ht_wh_recur</code></p>
<p>위의 기울기를 유도하기 위해, $t=1, 2,\ldots$에 대해 $a_{0}=0$ 및 $a_{t}=b_{t}+c_{t}a_{t-1}$을 만족하는 세 시퀀스 {a_{t}},{b_{t}},{c_{t}}가 있다고 가정합니다.
그러면 $t\geq 1$에 대해 다음을 보이는 것은 쉽습니다.</p>
<p>$$a_{t}=b_{t}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t}c_{j}\right)b_{i}.$$:eqlabel:<code>eq_bptt_at</code></p>
<p>다음에 따라 $a_t$, $b_t$, $c_t$를 대입하면</p>
<p>$$\begin{aligned}a_t &amp;= \frac{\partial h_t}{\partial w_\textrm{h}},\b_t &amp;= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}}, \c_t &amp;= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial h_{t-1}},
\end{aligned}$$</p>
<p>:eqref:<code>eq_bptt_partial_ht_wh_recur</code>의 기울기 계산은 $a_{t}=b_{t}+c_{t}a_{t-1}$을 만족합니다.
따라서 :eqref:<code>eq_bptt_at</code>에 따라 다음을 사용하여 :eqref:<code>eq_bptt_partial_ht_wh_recur</code>의 순환 계산을 제거할 수 있습니다.</p>
<p>$$\frac{\partial h_t}{\partial w_\textrm{h}}=\frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_\textrm{h})}{\partial h_{j-1}} \right) \frac{\partial f(x_{i},h_{i-1},w_\textrm{h})}{\partial w_\textrm{h}}.$$:eqlabel:<code>eq_bptt_partial_ht_wh_gen</code></p>
<p>연쇄 법칙을 사용하여 $\partial h_t/\partial w_\textrm{h}$를 재귀적으로 계산할 수 있지만, $t$가 클 때마다 이 체인이 매우 길어질 수 있습니다.
이 문제를 다루기 위한 몇 가지 전략을 논의해 봅시다.</p>
<h3 id="전체-계산-full-computation"><a class="header" href="#전체-계산-full-computation">전체 계산 (Full Computation)</a></h3>
<p>한 가지 아이디어는 :eqref:<code>eq_bptt_partial_ht_wh_gen</code>에서 전체 합을 계산하는 것일 수 있습니다.
하지만 이것은 매우 느리고 기울기가 폭발할 수 있습니다.
초기 조건의 미묘한 변화가 결과에 큰 영향을 미칠 수 있기 때문입니다.
즉, 초기 조건의 미미한 변화가 결과의 불균형한 변화로 이어지는 나비 효과와 유사한 것을 볼 수 있습니다.
이것은 일반적으로 바람직하지 않습니다.
결국 우리는 잘 일반화되는 강력한 추정기를 찾고 있습니다.
따라서 이 전략은 실제로 거의 사용되지 않습니다.</p>
<h3 id="타임-스텝-자르기-truncating-time-steps"><a class="header" href="#타임-스텝-자르기-truncating-time-steps">타임 스텝 자르기 (Truncating Time Steps)###</a></h3>
<p>대안으로,
우리는 $\tau$ 단계 후 :eqref:<code>eq_bptt_partial_ht_wh_gen</code>의 합을 자를 수 있습니다.
이것이 지금까지 우리가 논의해 온 것입니다.
이것은 $\partial h_{t-\tau}/\partial w_\textrm{h}$에서 합을 종료함으로써 실제 기울기의 <em>근사치</em>로 이어집니다.
실제로 이것은 꽤 잘 작동합니다.
이것은 일반적으로 절단된 시간 경과에 따른 역전파(truncated backpropagation through time)라고 불리는 것입니다 :cite:<code>Jaeger.2002</code>.
이것의 결과 중 하나는 모델이 장기적인 결과보다는 주로 단기적인 영향에 초점을 맞춘다는 것입니다.
이것은 실제로 <em>바람직</em>한데, 추정치를 더 단순하고 안정적인 모델 쪽으로 편향시키기 때문입니다.</p>
<h3 id="무작위-자르기-randomized-truncation"><a class="header" href="#무작위-자르기-randomized-truncation">무작위 자르기 (Randomized Truncation)</a></h3>
<p>마지막으로, 우리는 $\partial h_t/\partial w_\textrm{h}$를 기댓값에서는 정확하지만 시퀀스를 자르는 확률 변수로 대체할 수 있습니다.
이것은 미리 정의된 $0 \leq \pi_t \leq 1$을 갖는 $\xi_t$ 시퀀스를 사용하여 달성됩니다.
여기서 $P(\xi_t = 0) = 1-\pi_t$이고 $P(\xi_t = \pi_t^{-1}) = \pi_t$이므로 $E[\xi_t] = 1$입니다.
우리는 이것을 사용하여 :eqref:<code>eq_bptt_partial_ht_wh_recur</code>의 기울기 $\partial h_t/\partial w_\textrm{h}$를 다음과 같이 대체합니다.</p>
<p>$$z_t= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}} +\xi_t \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_\textrm{h}}.$$</p>
<p>$\xi_t$의 정의에서 $E[z_t] = \partial h_t/\partial w_\textrm{h}$가 따릅니다.
$\xi_t = 0$일 때마다 순환 계산은 해당 타임 스텝 $t$에서 종료됩니다.
이것은 다양한 길이의 시퀀스의 가중 합으로 이어지며, 긴 시퀀스는 드물지만 적절하게 가중치가 부여됩니다.
이 아이디어는 :citet:<code>Tallec.Ollivier.2017</code>에 의해 제안되었습니다.</p>
<h3 id="전략-비교-comparing-strategies"><a class="header" href="#전략-비교-comparing-strategies">전략 비교 (Comparing Strategies)</a></h3>
<p><img src="../img/truncated-bptt.svg" alt="RNN에서 기울기를 계산하기 위한 전략 비교. 위에서 아래로: 무작위 자르기, 일반 자르기, 전체 계산." />
:label:<code>fig_truncated_bptt</code></p>
<p>:numref:<code>fig_truncated_bptt</code>는 RNN에 대한 시간 경과에 따른 역전파를 사용하여 <em>타임 머신</em>의 처음 몇 글자를 분석할 때 세 가지 전략을 보여줍니다:</p>
<ul>
<li>첫 번째 행은 텍스트를 다양한 길이의 세그먼트로 분할하는 무작위 자르기입니다.</li>
<li>두 번째 행은 텍스트를 동일한 길이의 하위 시퀀스로 나누는 일반 자르기입니다. 이것이 우리가 RNN 실험에서 해왔던 것입니다.</li>
<li>세 번째 행은 계산적으로 실행 불가능한 표현으로 이어지는 전체 시간 경과에 따른 역전파입니다.</li>
</ul>
<p>불행히도 이론적으로는 매력적이지만 무작위 자르기는 일반 자르기보다 훨씬 더 잘 작동하지 않는데, 이는 여러 요인 때문일 가능성이 큽니다.
첫째, 과거로의 여러 역전파 단계 후 관찰의 효과는 실제로 의존성을 포착하기에 충분합니다.
둘째, 증가된 분산은 더 많은 단계에서 기울기가 더 정확하다는 사실을 상쇄합니다.
셋째, 우리는 실제로 짧은 범위의 상호 작용만 있는 모델을 <em>원합니다</em>.
따라서 정기적으로 자른 시간 경과에 따른 역전파는 바람직할 수 있는 약간의 정규화 효과를 갖습니다.</p>
<h2 id="시간-경과에-따른-역전파-상세-backpropagation-through-time-in-detail"><a class="header" href="#시간-경과에-따른-역전파-상세-backpropagation-through-time-in-detail">시간 경과에 따른 역전파 상세 (Backpropagation Through Time in Detail)</a></h2>
<p>일반적인 원칙을 논의한 후, 시간 경과에 따른 역전파에 대해 자세히 논의해 봅시다.
:numref:<code>subsec_bptt_analysis</code>의 분석과 대조적으로, 다음에서는 분해된 모든 모델 파라미터에 대한 목적 함수의 기울기를 계산하는 방법을 보여줄 것입니다.
일을 단순하게 유지하기 위해 편향 파라미터가 없고 은닉층의 활성화 함수가 항등 매핑($\phi(x)=x$)을 사용하는 RNN을 고려합니다.
타임 스텝 $t$에 대해 단일 예제 입력과 타겟을 각각 $\mathbf{x}_t \in \mathbb{R}^d$와 $y_t$라고 합시다.
은닉 상태 $\mathbf{h}_t \in \mathbb{R}^h$와 출력 $\mathbf{o}_t \in \mathbb{R}^q$는 다음과 같이 계산됩니다.</p>
<p>$$\begin{aligned}\mathbf{h}<em>t &amp;= \mathbf{W}</em>\textrm{hx} \mathbf{x}<em>t + \mathbf{W}</em>\textrm{hh} \mathbf{h}<em>{t-1},\\mathbf{o}<em>t &amp;= \mathbf{W}</em>\textrm{qh} \mathbf{h}</em>{t},\end{aligned}$$</p>
<p>여기서 $\mathbf{W}<em>\textrm{hx} \in \mathbb{R}^{h \times d}$, $\mathbf{W}</em>\textrm{hh} \in \mathbb{R}^{h \times h}$, $\mathbf{W}_\textrm{qh} \in \mathbb{R}^{q \times h}$는 가중치 파라미터입니다.
$l(\mathbf{o}_t, y_t)$를 타임 스텝 $t$에서의 손실이라고 합시다.
우리의 목적 함수, 시퀀스 시작부터 $T$ 타임 스텝에 걸친 손실은 다음과 같습니다.</p>
<p>$$L = \frac{1}{T} \sum_{t=1}^T l(\mathbf{o}_t, y_t).$$</p>
<p>RNN 계산 중 모델 변수와 파라미터 간의 종속성을 시각화하기 위해, :numref:<code>fig_rnn_bptt</code>와 같이 모델에 대한 계산 그래프를 그릴 수 있습니다.
예를 들어 타임 스텝 3의 은닉 상태 $\mathbf{h}<em>3$의 계산은 모델 파라미터 $\mathbf{W}</em>\textrm{hx}$와 $\mathbf{W}_\textrm{hh}$, 이전 타임 스텝의 은닉 상태 $\mathbf{h}_2$, 현재 타임 스텝의 입력 $\mathbf{x}_3$에 의존합니다.</p>
<p><img src="../img/rnn-bptt.svg" alt="3개의 타임 스텝을 가진 RNN 모델에 대한 종속성을 보여주는 계산 그래프. 상자는 변수(음영 없음) 또는 파라미터(음영 있음)를 나타내고 원은 연산자를 나타냅니다." />
:label:<code>fig_rnn_bptt</code></p>
<p>방금 언급했듯이 :numref:<code>fig_rnn_bptt</code>의 모델 파라미터는 $\mathbf{W}<em>\textrm{hx}$, $\mathbf{W}</em>\textrm{hh}$, $\mathbf{W}<em>\textrm{qh}$입니다.
일반적으로 이 모델을 훈련하려면 이러한 파라미터에 대한 기울기 계산 $\partial L/\partial \mathbf{W}</em>\textrm{hx}$, $\partial L/\partial \mathbf{W}<em>\textrm{hh}$, $\partial L/\partial \mathbf{W}</em>\textrm{qh}$가 필요합니다.
:numref:<code>fig_rnn_bptt</code>의 종속성에 따라 화살표의 반대 방향으로 순회하여 기울기를 차례로 계산하고 저장할 수 있습니다.
연쇄 법칙에서 모양이 다른 행렬, 벡터, 스칼라의 곱셈을 유연하게 표현하기 위해 :numref:<code>sec_backprop</code>에서 설명한 대로 $\textrm{prod}$ 연산자를 계속 사용합니다.</p>
<p>우선, 임의의 타임 스텝 $t$에서 모델 출력에 대한 목적 함수를 미분하는 것은 꽤 간단합니다:</p>
<p>$$\frac{\partial L}{\partial \mathbf{o}_t} =  \frac{\partial l (\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t} \in \mathbb{R}^q.$$:eqlabel:<code>eq_bptt_partial_L_ot</code></p>
<p>이제 출력 레이어의 파라미터 $\mathbf{W}<em>\textrm{qh}$에 대한 목적 함수의 기울기를 계산할 수 있습니다: $\partial L/\partial \mathbf{W}</em>\textrm{qh} \in \mathbb{R}^{q \times h}$.
:numref:<code>fig_rnn_bptt</code>를 기반으로 목적 함수 $L$은 $\mathbf{o}_1, \ldots, \mathbf{o}<em>T$를 통해 $\mathbf{W}</em>\textrm{qh}$에 의존합니다.
연쇄 법칙을 사용하면 다음을 얻습니다.</p>
<p>$$
\frac{\partial L}{\partial \mathbf{W}<em>\textrm{qh}}
= \sum</em>{t=1}^T \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{o}<em>t}, \frac{\partial \mathbf{o}<em>t}{\partial \mathbf{W}</em>\textrm{qh}}\right)
= \sum</em>{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top,
$$</p>
<p>여기서 $\partial L/\partial \mathbf{o}_t$는 :eqref:<code>eq_bptt_partial_L_ot</code>에 의해 주어집니다.</p>
<p>다음으로, :numref:<code>fig_rnn_bptt</code>에 표시된 것처럼,
마지막 타임 스텝 $T$에서 목적 함수 $L$은 $\mathbf{o}_T$를 통해서만 은닉 상태 $\mathbf{h}_T$에 의존합니다.
따라서 연쇄 법칙을 사용하여 기울기 $\partial L/\partial \mathbf{h}_T \in \mathbb{R}^h$를 쉽게 찾을 수 있습니다.</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}_T} = \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}<em>T} \right) = \mathbf{W}</em>\textrm{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T}.$$:eqlabel:<code>eq_bptt_partial_L_hT_final_step</code></p>
<p>목적 함수 $L$이 $\mathbf{h}_{t+1}$과 $\mathbf{o}_t$를 통해 $\mathbf{h}_t$에 의존하는 $t &lt; T$인 타임 스텝의 경우 더 까다로워집니다.
연쇄 법칙에 따라,
임의의 타임 스텝 $t &lt; T$에서 은닉 상태의 기울기 $\partial L/\partial \mathbf{h}_t \in \mathbb{R}^h$는 다음과 같이 순환적으로 계산될 수 있습니다:</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}<em>t} = \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{h}</em>{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} \right) + \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{o}<em>t}, \frac{\partial \mathbf{o}<em>t}{\partial \mathbf{h}<em>t} \right) = \mathbf{W}</em>\textrm{hh}^\top \frac{\partial L}{\partial \mathbf{h}</em>{t+1}} + \mathbf{W}</em>\textrm{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t}.$$:eqlabel:<code>eq_bptt_partial_L_ht_recur</code></p>
<p>분석을 위해, 임의의 타임 스텝 $1 \leq t \leq T$에 대해 순환 계산을 확장하면 다음을 얻습니다.</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}<em>t}= \sum</em>{i=t}^T {\left(\mathbf{W}<em>\textrm{hh}^\top\right)}^{T-i} \mathbf{W}</em>\textrm{qh}^\top \frac{\partial L}{\partial \mathbf{o}_{T+t-i}}.$$:eqlabel:<code>eq_bptt_partial_L_ht</code></p>
<p>:eqref:<code>eq_bptt_partial_L_ht</code>에서 이 간단한 선형 예제가 이미 긴 시퀀스 모델의 몇 가지 주요 문제를 보여준다는 것을 알 수 있습니다:
잠재적으로 매우 큰 거듭제곱의 $\mathbf{W}_\textrm{hh}^\top$를 포함합니다.
그 안에서 1보다 작은 고유값은 사라지고 1보다 큰 고유값은 발산합니다.
이것은 수치적으로 불안정하며, 이는 사라지는 기울기와 폭발하는 기울기의 형태로 나타납니다.
이를 해결하는 한 가지 방법은 :numref:<code>subsec_bptt_analysis</code>에서 논의한 대로 계산적으로 편리한 크기에서 타임 스텝을 자르는 것입니다.
실제로 이 자르기는 주어진 타임 스텝 수 후에 기울기를 분리(detaching)함으로써 영향을 받을 수도 있습니다.
나중에 장단기 메모리(long short-term memory)와 같은 더 정교한 시퀀스 모델이 이를 어떻게 더 완화할 수 있는지 보게 될 것입니다.</p>
<p>마지막으로 :numref:<code>fig_rnn_bptt</code>는 목적 함수 $L$이 은닉 상태 $\mathbf{h}<em>1, \ldots, \mathbf{h}<em>T$를 통해 은닉층의 모델 파라미터 $\mathbf{W}</em>\textrm{hx}$와 $\mathbf{W}</em>\textrm{hh}$에 의존함을 보여줍니다.
이러한 파라미터에 대한 기울기 $\partial L / \partial \mathbf{W}<em>\textrm{hx} \in \mathbb{R}^{h \times d}$와 $\partial L / \partial \mathbf{W}</em>\textrm{hh} \in \mathbb{R}^{h \times h}$를 계산하기 위해 연쇄 법칙을 적용하면 다음을 얻습니다.</p>
<p>$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf{W}<em>\textrm{hx}}
&amp;= \sum</em>{t=1}^T \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{h}<em>t}, \frac{\partial \mathbf{h}<em>t}{\partial \mathbf{W}</em>\textrm{hx}}\right)
= \sum</em>{t=1}^T \frac{\partial L}{\partial \mathbf{h}<em>t} \mathbf{x}<em>t^\top,\
\frac{\partial L}{\partial \mathbf{W}</em>\textrm{hh}}
&amp;= \sum</em>{t=1}^T \textrm{prod}\left(\frac{\partial L}{\partial \mathbf{h}<em>t}, \frac{\partial \mathbf{h}<em>t}{\partial \mathbf{W}</em>\textrm{hh}}\right)
= \sum</em>{t=1}^T \frac{\partial L}{\partial \mathbf{h}<em>t} \mathbf{h}</em>{t-1}^\top,
\end{aligned}
$$</p>
<p>여기서 :eqref:<code>eq_bptt_partial_L_hT_final_step</code>과 :eqref:<code>eq_bptt_partial_L_ht_recur</code>에 의해 순환적으로 계산되는 $\partial L/\partial \mathbf{h}_t$는 수치적 안정성에 영향을 미치는 핵심 양입니다.</p>
<p>시간 경과에 따른 역전파는 RNN에 역전파를 적용하는 것이므로, :numref:<code>sec_backprop</code>에서 설명했듯이 RNN 훈련은 순전파와 시간 경과에 따른 역전파를 번갈아 가며 수행합니다.
더욱이 시간 경과에 따른 역전파는 위의 기울기를 차례로 계산하고 저장합니다.
구체적으로 $\partial L / \partial \mathbf{W}<em>\textrm{hx}$와 $\partial L / \partial \mathbf{W}</em>\textrm{hh}$의 계산 모두에 사용하기 위해 $\partial L/\partial \mathbf{h}_t$를 저장하는 것과 같이, 저장된 중간 값은 중복 계산을 피하기 위해 재사용됩니다.</p>
<h2 id="요약-summary"><a class="header" href="#요약-summary">요약 (Summary)</a></h2>
<p>시간 경과에 따른 역전파는 은닉 상태가 있는 시퀀스 모델에 역전파를 적용한 것일 뿐입니다.
계산 편의성과 수치적 안정성을 위해 정기적 또는 무작위와 같은 자르기가 필요합니다.
행렬의 높은 거듭제곱은 발산하거나 사라지는 고유값으로 이어질 수 있습니다. 이는 폭발하거나 사라지는 기울기의 형태로 나타납니다.
효율적인 계산을 위해 시간 경과에 따른 역전파 중에 중간 값이 캐시됩니다.</p>
<h2 id="연습-문제-exercises"><a class="header" href="#연습-문제-exercises">연습 문제 (Exercises)</a></h2>
<ol>
<li>고유값 $\lambda_i$와 그에 대응하는 고유 벡터 $\mathbf{v}<em>i$ ($i = 1, \ldots, n$)를 갖는 대칭 행렬 $\mathbf{M} \in \mathbb{R}^{n \times n}$이 있다고 가정합니다. 일반성을 잃지 않고 $|
\lambda_i| \geq |\lambda</em>{i+1}|$ 순서로 정렬되어 있다고 가정합니다.
<ol>
<li>$\mathbf{M}^k$가 고유값 $\lambda_i^k$를 가짐을 보이십시오.</li>
<li>무작위 벡터 $\mathbf{x} \in \mathbb{R}^n$에 대해 $\mathbf{M}^k \mathbf{x}$가 높은 확률로 $\mathbf{M}$의 고유 벡터 $\mathbf{v}_1$과 매우 잘 정렬될 것임을 증명하십시오. 이 진술을 공식화하십시오.</li>
<li>위의 결과는 RNN의 기울기에 대해 무엇을 의미합니까?</li>
</ol>
</li>
<li>기울기 클리핑 외에 순환 신경망에서 기울기 폭발에 대처할 수 있는 다른 방법을 생각할 수 있습니까?</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/334">토론</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_recurrent-neural-networks/rnn-concise.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_recurrent-modern/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_recurrent-neural-networks/rnn-concise.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_recurrent-modern/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
