<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>시퀀스 다루기 (Working with Sequences) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/sequence.html" class="active"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="시퀀스로-작업하기-working-with-sequences"><a class="header" href="#시퀀스로-작업하기-working-with-sequences">시퀀스로 작업하기 (Working with Sequences)</a></h1>
<p>:label:<code>sec_sequence</code></p>
<p>지금까지 우리는 입력이 단일 특성 벡터 $\mathbf{x} \in \mathbb{R}^d$로 구성된 모델에 초점을 맞추었습니다.
시퀀스를 처리할 수 있는 모델을 개발할 때 관점의 주요 변화는 이제 특성 벡터의 순서가 있는 리스트 $\mathbf{x}_1, \dots, \mathbf{x}_T$로 구성된 입력에 초점을 맞춘다는 것입니다.
여기서 각 특성 벡터 $\mathbf{x}_t$는 $\mathbb{R}^d$에 있는 타임 스텝 $t \in \mathbb{Z}^+$로 인덱싱됩니다.</p>
<p>일부 데이터셋은 단일 거대한 시퀀스로 구성됩니다.
예를 들어 기후 과학자들이 사용할 수 있는 센서 판독값의 매우 긴 스트림을 고려하십시오.
이러한 경우, 미리 결정된 길이의 하위 시퀀스를 무작위로 샘플링하여 훈련 데이터셋을 만들 수 있습니다.
더 자주 데이터는 시퀀스 모음으로 도착합니다.
다음 예를 고려하십시오:
(i) 문서 모음, 각 문서는 고유한 단어 시퀀스로 표현되고 고유한 길이 $T_i$를 가짐;
(ii) 병원 환자 입원의 시퀀스 표현, 각 입원은 여러 사건으로 구성되고 시퀀스 길이는 대략 입원 기간에 따라 다름.</p>
<p>이전에는 개별 입력을 다룰 때 동일한 기저 분포 $P(X)$에서 독립적으로 샘플링되었다고 가정했습니다.
우리는 여전히 전체 시퀀스(예: 전체 문서 또는 환자 궤적)가 독립적으로 샘플링된다고 가정하지만,
각 타임 스텝에 도착하는 데이터가 서로 독립적이라고 가정할 수는 없습니다.
예를 들어 문서 뒷부분에 나타날 가능성이 높은 단어는 문서 앞부분에 나타나는 단어에 크게 의존합니다.
병원 방문 10일째에 환자가 받을 가능성이 있는 약은 이전 9일 동안 일어난 일에 크게 의존합니다.</p>
<p>이것은 놀라운 일이 아닙니다.
시퀀스의 요소가 관련이 없다고 믿었다면 애초에 시퀀스로 모델링하지 않았을 것입니다.
검색 도구와 최신 이메일 클라이언트에서 널리 사용되는 자동 완성 기능의 유용성을 고려하십시오.
초기 접두사가 주어졌을 때 시퀀스의 가능한 연속이 무엇일지 예측하는 것이(불완전하지만 무작위 추측보다는 낫게) 종종 가능하기 때문에 유용합니다.
대부분의 시퀀스 모델의 경우, 시퀀스의 독립성이나 심지어 정상성(stationarity)도 요구하지 않습니다.
대신 시퀀스 자체가 전체 시퀀스에 대한 고정된 기저 분포에서 샘플링되어야 한다는 것만 요구합니다.</p>
<p>이 유연한 접근 방식은 (i) 문서가 처음과 끝에서 상당히 다르게 보이거나;
(ii) 병원 입원 기간 동안 환자 상태가 회복 또는 사망 쪽으로 진화하거나;
(iii) 추천 시스템과의 지속적인 상호 작용 과정에서 고객 취향이 예측 가능한 방식으로 진화하는 것과 같은 현상을 허용합니다.</p>
<p>때로는 순차적으로 구조화된 입력이 주어졌을 때 고정된 타겟 $y$를 예측하고 싶을 때가 있습니다(예: 영화 리뷰 기반 감성 분석).
다른 때는 고정된 입력이 주어졌을 때 순차적으로 구조화된 타겟($y_1, \ldots, y_T$)을 예측하고 싶을 때가 있습니다(예: 이미지 캡션).
또 다른 경우에는 순차적으로 구조화된 입력을 기반으로 순차적으로 구조화된 타겟을 예측하는 것이 목표입니다(예: 기계 번역 또는 비디오 캡션).
이러한 시퀀스-투-시퀀스 작업은 두 가지 형태를 취합니다:
(i) <em>정렬된(aligned)</em>: 각 타임 스텝의 입력이 해당 타겟과 정렬되는 경우(예: 품사 태깅);
(ii) <em>정렬되지 않은(unaligned)</em>: 입력과 타겟이 반드시 단계별 대응을 보이지 않는 경우(예: 기계 번역).</p>
<p>어떤 종류의 타겟을 처리하는 것에 대해 걱정하기 전에 가장 간단한 문제인 비지도 밀도 모델링(또는 <em>시퀀스 모델링</em>)을 다룰 수 있습니다.
여기서 시퀀스 모음이 주어지면 우리의 목표는 주어진 시퀀스를 볼 가능성이 얼마나 되는지, 즉 $p(\mathbf{x}_1, \ldots, \mathbf{x}_T)$를 알려주는 확률 질량 함수를 추정하는 것입니다.</p>
<pre><code class="language-{.python .input  n=6}">%load_ext d2lbook.tab
tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab mxnet
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, gluon, init
from mxnet.gluon import nn
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input  n=8}">%%tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
from torch import nn
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab jax
%matplotlib inline
from d2l import jax as d2l
import jax
from jax import numpy as jnp
import numpy as np
</code></pre>
<h2 id="자기회귀-모델-autoregressive-models"><a class="header" href="#자기회귀-모델-autoregressive-models">자기회귀 모델 (Autoregressive Models)</a></h2>
<p>순차적으로 구조화된 데이터를 처리하도록 설계된 특수 신경망을 소개하기 전에, 실제 시퀀스 데이터를 살펴보고 몇 가지 기본적인 직관과 통계 도구를 구축해 보겠습니다.
특히 FTSE 100 지수의 주가 데이터에 초점을 맞출 것입니다 (:numref:<code>fig_ftse100</code>).
각 <em>타임 스텝</em> $t \in \mathbb{Z}^+$에서 우리는 그 시점의 지수 가격 $x_t$를 관찰합니다.</p>
<p><img src="../img/ftse100.png" alt="약 30년 동안의 FTSE 100 지수." />
:width:<code>400px</code>
:label:<code>fig_ftse100</code></p>
<p>이제 트레이더가 다음 타임 스텝에서 지수가 상승할지 하락할지 믿는 것에 따라 전략적으로 지수에 진입하거나 빠져나오면서 단기 거래를 하고 싶다고 가정해 봅시다.
다른 특성(뉴스, 재무 보고 데이터 등)이 없는 경우, 후속 값을 예측하는 데 사용할 수 있는 유일한 신호는 현재까지의 가격 역사입니다.
따라서 트레이더는 다음 타임 스텝에서 지수가 취할 수 있는 가격에 대한 확률 분포</p>
<p>$$P(x_t \mid x_{t-1}, \ldots, x_1)$$</p>
<p>를 아는 데 관심이 있습니다.
연속적인 값을 갖는 확률 변수에 대한 전체 분포를 추정하는 것은 어려울 수 있지만, 트레이더는 분포의 몇 가지 주요 통계, 특히 기댓값과 분산에 집중하는 것으로 만족할 것입니다.
조건부 기댓값</p>
<p>$$\mathbb{E}[(x_t \mid x_{t-1}, \ldots, x_1)],$$</p>
<p>을 추정하기 위한 한 가지 간단한 전략은 선형 회귀 모델을 적용하는 것일 수 있습니다(:numref:<code>sec_linear_regression</code> 상기).
신호 값을 동일한 신호의 이전 값으로 회귀하는 이러한 모델을 자연스럽게 *자기회귀 모델(autoregressive models)*이라고 합니다.
한 가지 큰 문제가 있습니다: 입력 수 $x_{t-1}, \ldots, x_1$이 $t$에 따라 달라집니다.
즉, 우리가 마주치는 데이터의 양에 따라 입력 수가 증가합니다.
따라서 과거 데이터를 훈련 세트로 취급하려면 각 예제마다 특성 수가 다르다는 문제에 봉착하게 됩니다.
이 장의 나머지 내용 대부분은 관심 대상이 $P(x_t \mid x_{t-1}, \ldots, x_1)$ 또는 이 분포의 일부 통계인 <em>자기회귀</em> 모델링 문제에 참여할 때 이러한 문제를 극복하기 위한 기술을 중심으로 전개될 것입니다.</p>
<p>몇 가지 전략이 자주 반복됩니다.
우선, 긴 시퀀스 $x_{t-1}, \ldots, x_1$을 사용할 수 있지만 가까운 미래를 예측할 때 역사적으로 그렇게 멀리 되돌아볼 필요는 없을 수 있다고 믿을 수 있습니다.
이 경우 우리는 길이 $\tau$의 어떤 윈도우에 조건을 걸고 $x_{t-1}, \ldots, x_{t-\tau}$ 관찰만 사용하는 것으로 만족할 수 있습니다.
즉각적인 이점은 이제 적어도 $t &gt; \tau$에 대해 인수 수가 항상 동일하다는 것입니다.
이를 통해 고정 길이 벡터를 입력으로 필요로 하는 모든 선형 모델 또는 심층 네트워크를 훈련할 수 있습니다.
둘째, 과거 관찰의 요약 $h_t$를 유지하는 모델을 개발할 수 있습니다(:numref:<code>fig_sequence-model</code> 참조). 동시에 예측 $\hat{x}<em>t$ 외에 $h_t$를 업데이트합니다.
이것은 $\hat{x}<em>t = P(x_t \mid h</em>{t})$로 $x_t$를 추정할 뿐만 아니라 $h_t = g(h</em>{t-1}, x_{t-1})$ 형태의 업데이트도 수행하는 모델로 이어집니다.
$h_t$는 결코 관찰되지 않으므로 이러한 모델을 *잠재 자기회귀 모델(latent autoregressive models)*이라고도 합니다.</p>
<p><img src="../img/sequence-model.svg" alt="잠재 자기회귀 모델." />
:label:<code>fig_sequence-model</code></p>
<p>과거 데이터로부터 훈련 데이터를 구성하기 위해 일반적으로 윈도우를 무작위로 샘플링하여 예제를 생성합니다.
일반적으로 우리는 시간이 멈출 것이라고 기대하지 않습니다.
그러나 $x_t$의 특정 값은 바뀔 수 있지만, 이전 관찰이 주어졌을 때 각 후속 관찰이 생성되는 역학은 변하지 않는다고 종종 가정합니다.
통계학자들은 변하지 않는 역학을 *정상적(stationary)*이라고 부릅니다.</p>
<h2 id="시퀀스-모델-sequence-models"><a class="header" href="#시퀀스-모델-sequence-models">시퀀스 모델 (Sequence Models)</a></h2>
<p>때로는 특히 언어 작업을 할 때 전체 시퀀스의 결합 확률을 추정하고 싶을 때가 있습니다.
이것은 단어와 같은 이산 <em>토큰</em>으로 구성된 시퀀스로 작업할 때 일반적인 작업입니다.
일반적으로 이렇게 추정된 함수를 <em>시퀀스 모델</em>이라고 하며 자연어 데이터의 경우 <em>언어 모델</em>이라고 합니다.
시퀀스 모델링 분야는 자연어 처리에 의해 너무 많이 주도되어 비언어 데이터를 다룰 때조차 시퀀스 모델을 종종 "언어 모델"이라고 설명합니다.
언어 모델은 온갖 이유로 유용함이 입증되었습니다.
때로는 문장의 우도(likelihood)를 평가하고 싶을 때가 있습니다.
예를 들어 기계 번역 시스템이나 음성 인식 시스템이 생성한 두 후보 출력의 자연스러움을 비교하고 싶을 수 있습니다.
하지만 언어 모델링은 우도를 <em>평가</em>할 수 있는 능력뿐만 아니라 시퀀스를 <em>샘플링</em>하고 가장 가능성 있는 시퀀스에 대해 최적화할 수 있는 능력도 제공합니다.</p>
<p>언어 모델링이 언뜻 보기에는 자기회귀 문제처럼 보이지 않을 수 있지만,
확률의 연쇄 법칙을 적용하여 시퀀스의 결합 밀도 $p(x_1, \ldots, x_T)$를 왼쪽에서 오른쪽 방식으로 조건부 밀도의 곱으로 분해함으로써 언어 모델링을 자기회귀 예측으로 줄일 수 있습니다:</p>
<p>$$P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1).$$</p>
<p>단어와 같은 이산 신호로 작업하는 경우,
자기회귀 모델은 확률적 분류기여야 하며,
왼쪽 문맥이 주어졌을 때 다음에 올 단어에 대해 어휘 전체에 걸친 전체 확률 분포를 출력해야 한다는 점에 유의하십시오.</p>
<h3 id="마르코프-모델-markov-models"><a class="header" href="#마르코프-모델-markov-models">마르코프 모델 (Markov Models)</a></h3>
<p>:label:<code>subsec_markov-models</code></p>
<p>이제 전체 시퀀스 역사 $x_{t-1}, \ldots, x_1$보다는 $\tau$개의 이전 타임 스텝, 즉 $x_{t-1}, \ldots, x_{t-\tau}$에만 조건을 거는 위에서 언급한 전략을 채택하고 싶다고 가정해 봅시다.
예측력의 손실 없이 이전 $\tau$ 단계 너머의 역사를 버릴 수 있을 때마다,
우리는 시퀀스가 <em>마르코프 조건</em>을 만족한다고 말합니다. 즉, <em>미래는 최근 역사가 주어졌을 때 과거와 조건부 독립</em>입니다.
$\tau = 1$일 때 데이터는 <em>1차 마르코프 모델</em>로 특징지어진다고 말하고, $\tau = k$일 때 데이터는 <em>$k$차 마르코프 모델</em>로 특징지어진다고 말합니다.
1차 마르코프 조건이 성립할 때($\tau = 1$), 결합 확률의 인수분해는 이전 <em>단어</em>가 주어졌을 때 각 단어의 확률의 곱이 됩니다:</p>
<p>$$P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}).$$</p>
<p>우리는 마르코프 조건이 <em>대략적으로</em>만 사실이라는 것을 알고 있을 때조차도 마치 마르코프 조건이 충족된 것처럼 진행하는 모델로 작업하는 것이 유용하다는 것을 종종 발견합니다.
실제 텍스트 문서의 경우 왼쪽 문맥을 더 많이 포함할수록 정보를 계속 얻습니다.
하지만 이러한 이득은 빠르게 줄어듭니다.
따라서 때로는 타협하여 유효성이 <em>$k$차</em> 마르코프 조건에 의존하는 모델을 훈련함으로써 계산적 및 통계적 어려움을 제거합니다.
오늘날의 거대한 RNN 및 Transformer 기반 언어 모델조차도 수천 단어 이상의 문맥을 통합하는 경우는 거의 없습니다.</p>
<p>이산 데이터의 경우, 실제 마르코프 모델은 단순히 각 문맥에서 각 단어가 발생한 횟수를 세어 $P(x_t \mid x_{t-1})$의 상대 빈도 추정치를 생성합니다.
데이터가 이산 값만 가정할 때마다(언어와 같이),
가장 가능성 있는 단어 시퀀스는 동적 프로그래밍을 사용하여 효율적으로 계산할 수 있습니다.</p>
<h3 id="디코딩-순서-the-order-of-decoding"><a class="header" href="#디코딩-순서-the-order-of-decoding">디코딩 순서 (The Order of Decoding)</a></h3>
<p>텍스트 시퀀스 $P(x_1, \ldots, x_T)$의 인수분해를 왜 왼쪽에서 오른쪽으로 가는 조건부 확률 체인으로 표현했는지 궁금할 수 있습니다.
왜 오른쪽에서 왼쪽이나 겉보기에 무작위인 순서가 아닐까요?
원칙적으로 $P(x_1, \ldots, x_T)$를 역순으로 펼치는 데는 아무런 문제가 없습니다.
결과는 유효한 인수분해입니다:</p>
<p>$$P(x_1, \ldots, x_T) = P(x_T) \prod_{t=T-1}^1 P(x_t \mid x_{t+1}, \ldots, x_T).$$</p>
<p>그러나 우리가 읽는 것과 동일한 방향(대부분의 언어에서는 왼쪽에서 오른쪽, 아랍어와 히브리어에서는 오른쪽에서 왼쪽)으로 텍스트를 인수분해하는 것이 언어 모델링 작업에 선호되는 데는 여러 가지 이유가 있습니다.
첫째, 이것은 우리가 생각하기에 더 자연스러운 방향입니다.
결국 우리는 모두 매일 텍스트를 읽으며, 이 과정은 어떤 단어와 구문이 다음에 올 가능성이 높은지 예상하는 우리의 능력에 의해 안내됩니다.
다른 사람의 문장을 얼마나 자주 완성했는지 생각해 보십시오.
따라서 이러한 순서대로의 디코딩을 선호할 다른 이유가 없더라도, 이 순서로 예측할 때 무엇이 가능성 있어야 하는지에 대한 더 나은 직관을 가지고 있기 때문에 유용할 것입니다.</p>
<p>둘째, 순서대로 인수분해함으로써 동일한 언어 모델을 사용하여 임의로 긴 시퀀스에 확률을 할당할 수 있습니다.
1단계에서 $t$까지의 확률을 단어 $t+1$까지 확장되는 확률로 변환하려면 단순히 이전 토큰이 주어졌을 때 추가 토큰의 조건부 확률을 곱하면 됩니다:
$P(x_{t+1}, \ldots, x_1) = P(x_{t}, \ldots, x_1) \cdot P(x_{t+1} \mid x_{t}, \ldots, x_1)$.</p>
<p>셋째, 우리는 임의의 다른 위치에 있는 단어보다 인접한 단어를 예측하는 데 더 강력한 예측 모델을 가지고 있습니다.
모든 순서의 인수분해가 유효하지만, 모두 똑같이 쉬운 예측 모델링 문제를 나타내는 것은 아닙니다.
이것은 언어뿐만 아니라 다른 종류의 데이터에도 해당됩니다. 예를 들어 데이터가 인과적으로 구조화된 경우입니다.
예를 들어 우리는 미래의 사건이 과거에 영향을 줄 수 없다고 믿습니다.
따라서 $x_t$를 변경하면 앞으로 $x_{t+1}$에 대해 일어나는 일에 영향을 줄 수 있지만 그 반대는 아닙니다.
즉, $x_t$를 변경해도 과거 사건에 대한 분포는 변경되지 않습니다.
일부 맥락에서 이것은 $P(x_t \mid x_{t+1})$를 예측하는 것보다 $P(x_{t+1} \mid x_t)$를 예측하는 것을 더 쉽게 만듭니다.
예를 들어 어떤 경우에는 어떤 가산적 노이즈 $\epsilon$에 대해 $x_{t+1} = f(x_t) + \epsilon$을 찾을 수 있지만, 그 역은 참이 아닙니다 :cite:<code>Hoyer.Janzing.Mooij.ea.2009</code>.
이것은 좋은 소식입니다. 우리가 추정하는 데 관심이 있는 것은 일반적으로 순방향이기 때문입니다.
:citet:<code>Peters.Janzing.Scholkopf.2017</code>의 책에는 이 주제에 대한 더 많은 내용이 포함되어 있습니다.
우리는 겉핥기만 하고 있습니다.</p>
<h2 id="훈련-training"><a class="header" href="#훈련-training">훈련 (Training)</a></h2>
<p>텍스트 데이터에 관심을 집중하기 전에 먼저 연속 값 합성 데이터로 이것을 시도해 봅시다.</p>
<p>(<strong>여기서 1000개의 합성 데이터는 타임 스텝의 0.01배에 적용된 삼각 함수 <code>sin</code>을 따릅니다.
문제를 좀 더 흥미롭게 만들기 위해 각 샘플에 가산적 노이즈를 섞습니다.</strong>)
이 시퀀스에서 각각 특성과 레이블로 구성된 훈련 예제를 추출합니다.</p>
<pre><code class="language-{.python .input  n=10}">%%tab all
class Data(d2l.DataModule):
    def __init__(self, batch_size=16, T=1000, num_train=600, tau=4):
        self.save_hyperparameters()
        self.time = d2l.arange(1, T + 1, dtype=d2l.float32)
        if tab.selected('mxnet', 'pytorch'):
            self.x = d2l.sin(0.01 * self.time) + d2l.randn(T) * 0.2
        if tab.selected('tensorflow'):
            self.x = d2l.sin(0.01 * self.time) + d2l.normal([T]) * 0.2
        if tab.selected('jax'):
            key = d2l.get_key()
            self.x = d2l.sin(0.01 * self.time) + jax.random.normal(key,
                                                                   [T]) * 0.2
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
data = Data()
d2l.plot(data.time, data.x, 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
</code></pre>
<p>시작하기 위해, 우리는 데이터가 $\tau^{\textrm{th}}$차 마르코프 조건을 만족하는 것처럼 행동하는 모델을 시도합니다.
따라서 과거 $\tau$개의 관찰만 사용하여 $x_t$를 예측합니다.
[<strong>따라서 각 타임 스텝마다 레이블 $y  = x_t$와 특성 $\mathbf{x}<em>t = [x</em>{t-\tau}, \ldots, x_{t-1}]$을 가진 예제가 있습니다.</strong>]
예리한 독자라면 $y_1, \ldots, y_\tau$에 대한 충분한 역사가 부족하기 때문에 이것이 $1000-\tau$개의 예제를 낳는다는 것을 눈치챘을 것입니다.
처음 $\tau$ 시퀀스를 0으로 채울 수도 있지만, 일을 단순하게 유지하기 위해 지금은 삭제합니다.
결과 데이터셋에는 $T - \tau$개의 예제가 포함되어 있으며, 모델에 대한 각 입력은 시퀀스 길이가 $\tau$입니다.
우리는 sin 함수의 주기를 포함하는 (<strong>처음 600개 예제에 대한 데이터 반복자를 생성</strong>)합니다.</p>
<pre><code class="language-{.python .input}">%%tab all
@d2l.add_to_class(Data)
def get_dataloader(self, train):
    features = [self.x[i : self.T-self.tau+i] for i in range(self.tau)]
    self.features = d2l.stack(features, 1)
    self.labels = d2l.reshape(self.x[self.tau:], (-1, 1))
    i = slice(0, self.num_train) if train else slice(self.num_train, None)
    return self.get_tensorloader([self.features, self.labels], train, i)
</code></pre>
<p>이 예제에서 우리 모델은 표준 선형 회귀가 될 것입니다.</p>
<pre><code class="language-{.python .input}">%%tab all
model = d2l.LinearRegression(lr=0.01)
trainer = d2l.Trainer(max_epochs=5)
trainer.fit(model, data)
</code></pre>
<h2 id="예측-prediction"><a class="header" href="#예측-prediction">예측 (Prediction)</a></h2>
<p>[<strong>모델을 평가하기 위해 먼저 1단계 앞선 예측에서 얼마나 잘 수행되는지 확인합니다</strong>].</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
onestep_preds = d2l.numpy(model(data.features))
d2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',
         legend=['labels', '1-step preds'], figsize=(6, 3))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
onestep_preds = model.apply({'params': trainer.state.params}, data.features)
d2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',
         legend=['labels', '1-step preds'], figsize=(6, 3))
</code></pre>
<p>이 예측들은 $t=1000$인 끝부분 근처에서도 좋아 보입니다.</p>
<p>하지만 타임 스텝 604(<code>n_train + tau</code>)까지만 시퀀스 데이터를 관찰했고 미래로 몇 단계 예측하고 싶다면 어떨까요?
불행히도 타임 스텝 609에 대한 1단계 앞선 예측을 직접 계산할 수 없습니다. $x_{604}$까지만 보았기 때문에 해당 입력을 모르기 때문입니다.
우리는 이전 예측을 후속 예측을 위한 모델의 입력으로 꽂아 넣고, 원하는 타임 스텝에 도달할 때까지 한 번에 한 단계씩 앞으로 투영함으로써 이 문제를 해결할 수 있습니다:</p>
<p>$$\begin{aligned}
\hat{x}<em>{605} &amp;= f(x</em>{601}, x_{602}, x_{603}, x_{604}), \n
\hat{x}<em>{606} &amp;= f(x</em>{602}, x_{603}, x_{604}, \hat{x}<em>{605}), \n
\hat{x}</em>{607} &amp;= f(x_{603}, x_{604}, \hat{x}<em>{605}, \hat{x}</em>{606}),\n
\hat{x}<em>{608} &amp;= f(x</em>{604}, \hat{x}<em>{605}, \hat{x}</em>{606}, \hat{x}<em>{607}),\n
\hat{x}</em>{609} &amp;= f(\hat{x}<em>{605}, \hat{x}</em>{606}, \hat{x}<em>{607}, \hat{x}</em>{608}),\n
&amp;\vdots
\end{aligned}$$</p>
<p>일반적으로 관찰된 시퀀스 $x_1, \ldots, x_t$에 대해, 타임 스텝 $t+k$에서의 예측 출력 $\hat{x}<em>{t+k}$를 $k$<em>-단계 앞선 예측</em>이라고 합니다.
우리는 $x</em>{604}$까지 관찰했으므로 $k$-단계 앞선 예측은 $\hat{x}_{604+k}$입니다.
다시 말해, 다단계 앞선 예측을 하려면 우리 자신의 예측을 계속 사용해야 합니다.
이것이 어떻게 진행되는지 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch
multistep_preds = d2l.zeros(data.T)
multistep_preds[:] = data.x
for i in range(data.num_train + data.tau, data.T):
    multistep_preds[i] = model(
        d2l.reshape(multistep_preds[i-data.tau : i], (1, -1)))
multistep_preds = d2l.numpy(multistep_preds)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
multistep_preds = tf.Variable(d2l.zeros(data.T))
multistep_preds[:].assign(data.x)
for i in range(data.num_train + data.tau, data.T):
    multistep_preds[i].assign(d2l.reshape(model(
        d2l.reshape(multistep_preds[i-data.tau : i], (1, -1))), ()))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
multistep_preds = d2l.zeros(data.T)
multistep_preds = multistep_preds.at[:].set(data.x)
for i in range(data.num_train + data.tau, data.T):
    pred = model.apply({'params': trainer.state.params},
                       d2l.reshape(multistep_preds[i-data.tau : i], (1, -1)))
    multistep_preds = multistep_preds.at[i].set(pred.item())
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
d2l.plot([data.time[data.tau:], data.time[data.num_train+data.tau:]],
         [onestep_preds, multistep_preds[data.num_train+data.tau:]], 'time',
         'x', legend=['1-step preds', 'multistep preds'], figsize=(6, 3))
</code></pre>
<p>불행히도 이 경우 우리는 장엄하게 실패합니다.
예측은 몇 단계 후에 꽤 빨리 상수로 감쇠합니다.
미래로 더 멀리 예측할 때 알고리즘이 왜 그렇게 훨씬 나쁘게 수행되었을까요?
궁극적으로 이것은 오류가 쌓인다는 사실 때문입니다.
1단계 후에 오류 $\epsilon_1 = \bar\epsilon$이 있다고 가정해 봅시다.
이제 2단계에 대한 <em>입력</em>이 $\epsilon_1$에 의해 섭동되므로, 어떤 상수 $c$에 대해 $\epsilon_2 = \bar\epsilon + c \epsilon_1$ 정도의 오류를 겪게 되고, 이런 식으로 계속됩니다.
예측은 실제 관찰에서 빠르게 발산할 수 있습니다.
여러분은 이미 이 일반적인 현상에 익숙할 수 있습니다.
예를 들어 향후 24시간 동안의 일기 예보는 꽤 정확한 경향이 있지만 그 이상은 정확도가 급격히 떨어집니다.
우리는 이 장과 그 이후에 걸쳐 이를 개선하기 위한 방법을 논의할 것입니다.</p>
<p>$k = 1, 4, 16, 64$에 대해 전체 시퀀스에서 예측을 계산하여 [$k$-단계 앞선 예측의 어려움을 자세히 살펴봅시다].</p>
<pre><code class="language-{.python .input}">%%tab pytorch, mxnet, tensorflow
def k_step_pred(k):
    features = []
    for i in range(data.tau):
        features.append(data.x[i : i+data.T-data.tau-k+1])
    # (i+tau)번째 요소는 (i+1)단계 앞선 예측을 저장합니다
    for i in range(k):
        preds = model(d2l.stack(features[i : i+data.tau], 1))
        features.append(d2l.reshape(preds, -1))
    return features[data.tau:]
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def k_step_pred(k):
    features = []
    for i in range(data.tau):
        features.append(data.x[i : i+data.T-data.tau-k+1])
    # (i+tau)번째 요소는 (i+1)단계 앞선 예측을 저장합니다
    for i in range(k):
        preds = model.apply({'params': trainer.state.params},
                            d2l.stack(features[i : i+data.tau], 1))
        features.append(d2l.reshape(preds, -1))
    return features[data.tau:]
</code></pre>
<pre><code class="language-{.python .input}">%%tab all
steps = (1, 4, 16, 64)
preds = k_step_pred(steps[-1])
d2l.plot(data.time[data.tau+steps[-1]-1:],
         [d2l.numpy(preds[k-1]) for k in steps], 'time', 'x',
         legend=[f'{k}-step preds' for k in steps], figsize=(6, 3))
</code></pre>
<p>이것은 미래로 더 멀리 예측하려고 할 때 예측의 품질이 어떻게 변하는지 명확하게 보여줍니다.
4단계 앞선 예측은 여전히 좋아 보이지만 그 이상은 거의 쓸모가 없습니다.</p>
<h2 id="요약-summary"><a class="header" href="#요약-summary">요약 (Summary)</a></h2>
<p>보간과 외삽 사이에는 꽤 큰 난이도 차이가 있습니다.
결과적으로 시퀀스가 있는 경우 훈련할 때 항상 데이터의 시간 순서를 존중하십시오. 즉, 미래 데이터에 대해 훈련하지 마십시오.
이런 종류의 데이터가 주어지면 시퀀스 모델은 추정을 위한 전문 통계 도구가 필요합니다.
두 가지 인기 있는 선택은 자기회귀 모델과 잠재 변수 자기회귀 모델입니다.
인과 모델(예: 시간이 앞으로 진행됨)의 경우 순방향을 추정하는 것이 일반적으로 역방향보다 훨씬 쉽습니다.
타임 스텝 $t$까지의 관찰된 시퀀스에 대해, 타임 스텝 $t+k$에서의 예측 출력은 $k$<em>-단계 앞선 예측</em>입니다.
$k$를 늘려 시간적으로 더 멀리 예측할수록 오류가 누적되고 예측 품질이 종종 극적으로 저하됩니다.</p>
<h2 id="연습-문제-exercises"><a class="header" href="#연습-문제-exercises">연습 문제 (Exercises)</a></h2>
<ol>
<li>이 섹션의 실험에서 모델을 개선하십시오.
<ol>
<li>과거 4개 이상의 관찰을 통합합니까? 실제로 몇 개가 필요합니까?</li>
<li>노이즈가 없다면 과거 관찰이 몇 개나 필요합니까? 힌트: $\sin$과 $\cos$를 미분 방정식으로 쓸 수 있습니다.</li>
<li>총 특성 수를 일정하게 유지하면서 더 오래된 관찰을 통합할 수 있습니까? 이것이 정확도를 향상시킵니까? 그 이유는 무엇입니까?</li>
<li>신경망 아키텍처를 변경하고 성능을 평가하십시오. 새 모델을 더 많은 에폭으로 훈련할 수 있습니다. 무엇을 관찰합니까?</li>
</ol>
</li>
<li>투자자가 매수할 좋은 증권을 찾고 싶어 합니다.
그들은 과거 수익률을 보고 어떤 것이 잘 될지 결정합니다.
이 전략에서 무엇이 잘못될 수 있습니까?</li>
<li>인과 관계가 텍스트에도 적용됩니까? 어느 정도까지입니까?</li>
<li>데이터의 역학을 포착하기 위해 잠재 자기회귀 모델이 필요할 수 있는 예를 드십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/113">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/114">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1048">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/18010">토론</a>
:end_tab:</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_recurrent-neural-networks/index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_recurrent-neural-networks/text-sequence.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_recurrent-neural-networks/index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_recurrent-neural-networks/text-sequence.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
