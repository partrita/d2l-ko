<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>선형 대수 (Linear Algebra) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/linear-algebra.html" class="active"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/autograd.html"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="선형-대수-linear-algebra"><a class="header" href="#선형-대수-linear-algebra">선형 대수 (Linear Algebra)</a></h1>
<p>:label:<code>sec_linear-algebra</code></p>
<p>지금까지 우리는 데이터셋을 텐서로 로드하고
기본적인 수학 연산으로 텐서를 조작할 수 있었습니다.
정교한 모델을 구축하기 시작하려면,
선형 대수의 몇 가지 도구도 필요합니다.
이 섹션은 스칼라 산술에서 시작하여 행렬 곱셈까지
가장 필수적인 개념에 대한 부드러운 소개를 제공합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import numpy as jnp
</code></pre>
<h2 id="스칼라-scalars"><a class="header" href="#스칼라-scalars">스칼라 (Scalars)</a></h2>
<p>대부분의 일상적인 수학은
숫자를 한 번에 하나씩 조작하는 것으로 구성됩니다.
공식적으로 우리는 이러한 값을 *스칼라(scalars)*라고 부릅니다.
예를 들어 팰로앨토의 온도는
화씨 $72$도의 온화한 날씨입니다.
온도를 섭씨로 변환하려면
$f$를 $72$로 설정하여 표현식 $c = \frac{5}{9}(f - 32)$를 평가합니다.
이 방정식에서 값 $5$, $9$, $32$는 상수 스칼라입니다.
변수 $c$와 $f$는 일반적으로 알려지지 않은 스칼라를 나타냅니다.</p>
<p>우리는 일반적인 소문자(예: $x$, $y$, $z$)로 스칼라를 나타내고,
모든 (연속적인) <em>실수 값</em> 스칼라의 공간을 $\mathbb{R}$로 나타냅니다.
편의를 위해 *공간(spaces)*에 대한 엄격한 정의는 건너뛰겠습니다:
표현식 $x \in \mathbb{R}$은 $x$가 실수 값 스칼라라는 것을 말하는
공식적인 방법이라는 것만 기억하십시오.
기호 $\in$ ("in"으로 발음)은 집합의 멤버십을 나타냅니다.
예를 들어 $x, y \in {0, 1}$은
$x$와 $y$가 $0$ 또는 $1$ 값만 취할 수 있는 변수임을 나타냅니다.</p>
<p>(<strong>스칼라는 하나의 요소만 포함하는 텐서로 구현됩니다.</strong>) 아래에서는 두 개의 스칼라를 할당하고
친숙한 덧셈, 곱셈, 나눗셈, 거듭제곱 연산을 수행합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.array(3.0)
y = np.array(2.0)

x + y, x * y, x / y, x ** y
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.tensor(3.0)
y = torch.tensor(2.0)

x + y, x * y, x / y, x**y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.constant(3.0)
y = tf.constant(2.0)

x + y, x * y, x / y, x**y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.array(3.0)
y = jnp.array(2.0)

x + y, x * y, x / y, x**y
</code></pre>
<h2 id="벡터-vectors"><a class="header" href="#벡터-vectors">벡터 (Vectors)</a></h2>
<p>현재 목적을 위해, [<strong>벡터를 고정 길이의 스칼라 배열로 생각할 수 있습니다.</strong>]
코드 대응물과 마찬가지로,
우리는 이러한 스칼라를 벡터의 *요소(elements)*라고 부릅니다
(<em>항목(entries)</em> 및 *성분(components)*과 동의어). 벡터가 실제 데이터셋의 예제를 나타낼 때,
그 값은 실제적인 중요성을 갖습니다.
예를 들어, 대출 채무 불이행 위험을 예측하는 모델을 훈련하는 경우,
각 신청자를 소득, 고용 기간, 이전 채무 불이행 횟수와 같은
수량에 해당하는 성분을 가진 벡터와 연관시킬 수 있습니다.
심장마비 위험을 연구하는 경우,
각 벡터는 환자를 나타낼 수 있으며
그 성분은 가장 최근의 활력 징후, 콜레스테롤 수치,
하루 운동 시간 등에 해당할 수 있습니다. 우리는 굵은 소문자(예: $\mathbf{x}$, $\mathbf{y}$, $\mathbf{z}$)로 벡터를 나타냅니다.</p>
<p>벡터는 1차 텐서로 구현됩니다. 일반적으로 이러한 텐서는 메모리 제한에 따라 임의의 길이를 가질 수 있습니다. 주의: 대부분의 프로그래밍 언어와 마찬가지로 Python에서 벡터 인덱스는 $0$에서 시작하며(<em>0 기반 인덱싱</em>이라고도 함), 선형 대수에서는 아래첨자가 $1$에서 시작합니다(<em>1 기반 인덱싱</em>).</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(3)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(3)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.range(3)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(3)
x
</code></pre>
<p>아래첨자를 사용하여 벡터의 요소를 참조할 수 있습니다.
예를 들어 $x_2$는 $\mathbf{x}$의 두 번째 요소를 나타냅니다.
$x_2$는 스칼라이므로 굵게 표시하지 않습니다.
기본적으로 우리는 요소를 수직으로 쌓아 벡터를 시각화합니다:</p>
<p>$$\mathbf{x} =\begin{bmatrix}x_{1}  \ \vdots  \x_{n}\end{bmatrix}.$$
:eqlabel:<code>eq_vec_def</code></p>
<p>여기서 $x_1, \ldots, x_n$은 벡터의 요소입니다. 나중에 우리는 이러한 *열 벡터(column vectors)*와
요소가 수평으로 쌓인 *행 벡터(row vectors)*를 구별할 것입니다. [<strong>인덱싱을 통해 텐서의 요소에 액세스한다</strong>]는 것을 상기하십시오.</p>
<pre><code class="language-{.python .input}">%%tab all
x[2]
</code></pre>
<p>벡터에 $n$개의 요소가 포함되어 있음을 나타내기 위해
$\mathbf{x} \in \mathbb{R}^n$이라고 씁니다. 공식적으로 우리는 $n$을 벡터의 *차원(dimensionality)*이라고 부릅니다.
[<strong>코드에서 이는 텐서의 길이에 해당하며</strong>], Python의 내장 <code>len</code> 함수를 통해 액세스할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
len(x)
</code></pre>
<p><code>shape</code> 속성을 통해서도 길이에 액세스할 수 있습니다.
모양은 각 축을 따른 텐서의 길이를 나타내는 튜플입니다.
(<strong>축이 하나만 있는 텐서는 하나의 요소만 있는 모양을 갖습니다.</strong>)</p>
<pre><code class="language-{.python .input}">%%tab all
x.shape
</code></pre>
<p>종종 "차원(dimension)"이라는 단어는
축의 수와 특정 축을 따른 길이를 모두 의미하도록 과부하됩니다.
이러한 혼란을 피하기 위해,
우리는 축의 수를 나타낼 때는 *차수(order)*를 사용하고,
구성 요소의 수를 나타낼 때는 *차원(dimensionality)*을 독점적으로 사용합니다.</p>
<h2 id="행렬-matrices"><a class="header" href="#행렬-matrices">행렬 (Matrices)</a></h2>
<p>스칼라가 0차 텐서이고
벡터가 1차 텐서인 것처럼,
행렬은 2차 텐서입니다. 우리는 굵은 대문자(예: $\mathbf{X}$, $\mathbf{Y}$, $\mathbf{Z}$)로 행렬을 나타내고,
코드에서는 두 개의 축을 가진 텐서로 나타냅니다. 표현식 $\mathbf{A} \in \mathbb{R}^{m \times n}$은
행렬 $\mathbf{A}$가 $m$개의 행과 $n$개의 열로 배열된
$m \times n$개의 실수 값 스칼라를 포함함을 나타냅니다. $m = n$일 때, 우리는 행렬이 *정사각(square)*이라고 말합니다. 시각적으로 우리는 모든 행렬을 표로 설명할 수 있습니다. 개별 요소를 참조하려면 행과 열 인덱스를 모두 아래첨자로 사용합니다. 예:
$a_{ij}$는 $\mathbf{A}$의 $i$번째 행과 $j$번째 열에 속하는 값입니다:</p>
<p>$$\mathbf{A}=\begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{bmatrix}.$$
:eqlabel:<code>eq_matrix_def</code></p>
<p>코드에서 우리는 행렬 $\mathbf{A} \in \mathbb{R}^{m \times n}$을
모양 ($m$, $n$)을 가진 2차 텐서로 나타냅니다. 원하는 모양을 <code>reshape</code>에 전달하여
[<strong>적절한 크기의 $m \times n$ 텐서를 $m \times n$ 행렬로 변환할 수 있습니다</strong>]:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A = np.arange(6).reshape(3, 2)
A
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A = torch.arange(6).reshape(3, 2)
A
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A = tf.reshape(tf.range(6), (3, 2))
A
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A = jnp.arange(6).reshape(3, 2)
A
</code></pre>
<p>때때로 우리는 축을 뒤집고 싶을 때가 있습니다. 행렬의 행과 열을 교환하면,
그 결과를 *전치(transpose)*라고 합니다. 공식적으로 행렬 $\mathbf{A}$의 전치를 $\mathbf{A}^\top$로 표시하고,
$\mathbf{B} = \mathbf{A}^\top$이면 모든 $i$와 $j$에 대해 $b_{ij} = a_{ji}$입니다. 따라서 $m \times n$ 행렬의 전치는 $n \times m$ 행렬입니다:</p>
<p>$$ \mathbf{A}^\top = \begin{bmatrix} a_{11} &amp; a_{21} &amp; \dots  &amp; a_{m1} \ a_{12} &amp; a_{22} &amp; \dots  &amp; a_{m2} \ \vdots &amp; \vdots &amp; \ddots  &amp; \vdots \ a_{1n} &amp; a_{2n} &amp; \dots  &amp; a_{mn} \end{bmatrix}. $$</p>
<p>코드에서는 다음과 같이 모든 (<strong>행렬의 전치</strong>)에 액세스할 수 있습니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.T
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.transpose(A)
</code></pre>
<p>[<strong>대칭 행렬(Symmetric matrices)은 자신의 전치와 동일한 정사각 행렬의 하위 집합입니다:
$\mathbf{A} = \mathbf{A}^\top$.</strong>] 다음 행렬은 대칭입니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == A.T
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == A.T
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A = tf.constant([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == tf.transpose(A)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A = jnp.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == A.T
</code></pre>
<p>행렬은 데이터셋을 나타내는 데 유용합니다. 일반적으로 행은 개별 레코드에 해당하고
열은 별개의 속성에 해당합니다.</p>
<h2 id="텐서-tensors"><a class="header" href="#텐서-tensors">텐서 (Tensors)</a></h2>
<p>스칼라, 벡터, 행렬만으로도 머신러닝 여정을 멀리 갈 수 있지만,
결국에는 고차 [<strong>텐서</strong>]를 다뤄야 할 수도 있습니다. 텐서는 (<strong>$n$차 배열로의 확장을 설명하는 일반적인 방법을 제공합니다.</strong>) 우리는 텐서 클래스의 소프트웨어 객체도 임의의 수의 축을 가질 수 있기 때문에
정확히 "텐서"라고 부릅니다. 수학적 객체와 코드에서의 구현 모두에 <em>텐서</em>라는 단어를 사용하는 것이 혼란스러울 수 있지만,
우리의 의미는 일반적으로 문맥상 명확해야 합니다. 우리는 일반적인 텐서를 특수 글꼴(예: $\mathsf{X}$, $\mathsf{Y}$, $\mathsf{Z}$)을 사용한 대문자로 나타내며,
인덱싱 메커니즘(예: $x_{ijk}$ 및 $[\mathsf{X}]_{1, 2i-1, 3}$)은
행렬의 메커니즘을 자연스럽게 따릅니다.</p>
<p>이미지로 작업하기 시작하면 텐서가 더 중요해질 것입니다. 각 이미지는 높이, 너비, <em>채널</em>에 해당하는 축을 가진 3차 텐서로 도착합니다. 각 공간 위치에서 각 색상(빨강, 초록, 파랑)의 강도는 채널을 따라 쌓입니다. 또한 이미지 모음은 코드에서 4차 텐서로 표현되며,
여기서 별개의 이미지는 첫 번째 축을 따라 인덱싱됩니다. 고차 텐서는 벡터 및 행렬과 마찬가지로
모양 구성 요소의 수를 늘려 구성됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.arange(24).reshape(2, 3, 4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.arange(24).reshape(2, 3, 4)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reshape(tf.range(24), (2, 3, 4))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.arange(24).reshape(2, 3, 4)
</code></pre>
<h2 id="텐서-산술의-기본-속성"><a class="header" href="#텐서-산술의-기본-속성">텐서 산술의 기본 속성</a></h2>
<p>스칼라, 벡터, 행렬, 그리고 고차 텐서는
모두 몇 가지 편리한 속성을 가지고 있습니다. 예를 들어, 요소별 연산은
피연산자와 동일한 모양을 가진 출력을 생성합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A = np.arange(6).reshape(2, 3)
B = A.copy()  # 새 메모리를 할당하여 A의 사본을 B에 할당
A, A + B
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A = torch.arange(6, dtype=torch.float32).reshape(2, 3)
B = A.clone()  # 새 메모리를 할당하여 A의 사본을 B에 할당
A, A + B
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A = tf.reshape(tf.range(6, dtype=tf.float32), (2, 3))
B = A  # 새 메모리를 할당하여 A를 B에 복제하지 않음
A, A + B
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A = jnp.arange(6, dtype=jnp.float32).reshape(2, 3)
B = A
A, A + B
</code></pre>
<p>[**두 행렬의 요소별 곱을 <em>하다마드 곱(Hadamard product)<em>이라고 합니다</em></em>] ($\odot$로 표시). 두 행렬 $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}$의 하다마드 곱의 항목은 다음과 같습니다:</p>
<p>$$ \mathbf{A} \odot \mathbf{B} = \begin{bmatrix} a_{11}  b_{11} &amp; a_{12}  b_{12} &amp; \dots  &amp; a_{1n}  b_{1n} \ a_{21}  b_{21} &amp; a_{22}  b_{22} &amp; \dots  &amp; a_{2n}  b_{2n} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ a_{m1}  b_{m1} &amp; a_{m2}  b_{m2} &amp; \dots  &amp; a_{mn}  b_{mn} \end{bmatrix}. $$</p>
<pre><code class="language-{.python .input}">%%tab all
A * B
</code></pre>
<p>[<strong>스칼라와 텐서를 더하거나 곱하면</strong>] 원래 텐서와 동일한 모양을 가진 결과가 생성됩니다. 여기서 텐서의 각 요소는 스칼라에 더해지거나 곱해집니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
a = 2
X = np.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
a = 2
X = tf.reshape(tf.range(24), (2, 3, 4))
a + X, (a * X).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
a = 2
X = jnp.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
</code></pre>
<h2 id="축소-reduction"><a class="header" href="#축소-reduction">축소 (Reduction)</a></h2>
<p>:label:<code>subsec_lin-alg-reduction</code></p>
<p>종종 우리는 [<strong>텐서 요소의 합을 계산하고 싶어 합니다.</strong>] 길이가 $n$인 벡터 $\mathbf{x}$의 요소 합을 표현하기 위해
$\sum_{i=1}^n x_i$라고 씁니다. 이를 위한 간단한 함수가 있습니다:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
x = np.arange(3)
x, x.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x = torch.arange(3, dtype=torch.float32)
x, x.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.range(3, dtype=tf.float32)
x, tf.reduce_sum(x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(3, dtype=jnp.float32)
x, x.sum()
</code></pre>
<p>[<strong>임의의 모양을 가진 텐서 요소의 합</strong>]을 표현하기 위해,
우리는 단순히 모든 축에 대해 합계를 구합니다. 예를 들어, $m \times n$ 행렬 $\mathbf{A}$의 요소 합은
$\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$로 쓸 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.shape, A.sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A.shape, tf.reduce_sum(A)
</code></pre>
<p>기본적으로 합계 함수를 호출하면
텐서를 모든 축을 따라 *축소(reduce)*하여
결국 스칼라를 생성합니다. 우리의 라이브러리는 또한 [<strong>텐서가 축소되어야 할 축을 지정</strong>]할 수 있게 해줍니다. 행(축 0)을 따라 모든 요소를 합산하려면,
<code>sum</code>에서 <code>axis=0</code>을 지정합니다. 입력 행렬이 축 0을 따라 축소되어 출력 벡터를 생성하므로,
이 축은 출력 모양에서 사라집니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.shape, A.sum(axis=0).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A.shape, tf.reduce_sum(A, axis=0).shape
</code></pre>
<p><code>axis=1</code>을 지정하면 모든 열의 요소를 합산하여 열 차원(축 1)을 축소합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.shape, A.sum(axis=1).shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A.shape, tf.reduce_sum(A, axis=1).shape
</code></pre>
<p>합계를 통해 행과 열 모두를 따라 행렬을 축소하는 것은
행렬의 모든 요소를 합산하는 것과 같습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.sum(axis=[0, 1]) == A.sum()  # A.sum()과 동일
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_sum(A, axis=[0, 1]), tf.reduce_sum(A)  # tf.reduce_sum(A)와 동일
</code></pre>
<p>[<strong>관련된 양은 *평균(mean)*이며 *평균(average)*이라고도 합니다.</strong>] 우리는 합계를 총 요소 수로 나누어 평균을 계산합니다. 평균을 계산하는 것은 매우 일반적이기 때문에,
<code>sum</code>과 유사하게 작동하는 전용 라이브러리 함수가 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, jax
A.mean(), A.sum() / A.size
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A.mean(), A.sum() / A.numel()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_mean(A), tf.reduce_sum(A) / tf.size(A).numpy()
</code></pre>
<p>마찬가지로 평균을 계산하는 함수도
특정 축을 따라 텐서를 축소할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_mean(A, axis=0), tf.reduce_sum(A, axis=0) / A.shape[0]
</code></pre>
<h2 id="비축소-합계-non-reduction-sum"><a class="header" href="#비축소-합계-non-reduction-sum">비축소 합계 (Non-Reduction Sum)</a></h2>
<p>:label:<code>subsec_lin-alg-non-reduction</code></p>
<p>합계나 평균을 계산하는 함수를 호출할 때
[<strong>축의 수를 변경하지 않고 유지</strong>]하는 것이 유용할 때가 있습니다. 이것은 브로드캐스트 메커니즘을 사용하고 싶을 때 중요합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
sum_A = A.sum(axis=1, keepdims=True)
sum_A, sum_A.shape
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
sum_A = tf.reduce_sum(A, axis=1, keepdims=True)
sum_A, sum_A.shape
</code></pre>
<p>예를 들어, <code>sum_A</code>는 각 행을 합산한 후에도 두 축을 유지하므로,
우리는 (<strong>브로드캐스팅을 사용하여 <code>A</code>를 <code>sum_A</code>로 나누어</strong>) 각 행의 합이 $1$이 되는 행렬을 만들 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab all
A / sum_A
</code></pre>
<p>[<strong>어떤 축을 따라 <code>A</code> 요소의 누적 합을 계산하고 싶다면</strong>],
예를 들어 <code>axis=0</code> (행별로), <code>cumsum</code> 함수를 호출할 수 있습니다. 설계상, 이 함수는 어떤 축을 따라 입력 텐서를 축소하지 않습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet, pytorch, jax
A.cumsum(axis=0)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.cumsum(A, axis=0)
</code></pre>
<h2 id="내적-dot-products"><a class="header" href="#내적-dot-products">내적 (Dot Products)</a></h2>
<p>지금까지 우리는 요소별 연산, 합계, 평균만 수행했습니다. 이것이 우리가 할 수 있는 전부라면 선형 대수는 별도의 섹션을 가질 자격이 없을 것입니다. 다행히도 여기서부터 상황이 더 흥미로워집니다. 가장 기본적인 연산 중 하나는 내적입니다. 두 벡터 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^d$가 주어졌을 때, 그들의 <em>내적(dot product)</em> $\mathbf{x}^\top \mathbf{y}$ (<em>내적(inner product)</em> $\langle \mathbf{x}, \mathbf{y}  \rangle$라고도 함)는 동일한 위치에 있는 요소들의 곱에 대한 합입니다:
$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$.</p>
<p>[<del>두 벡터의 <em>내적</em>은 같은 위치에 있는 요소들의 곱에 대한 합입니다</del>]</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y = np.ones(3)
x, y, np.dot(x, y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
y = torch.ones(3, dtype = torch.float32)
x, y, torch.dot(x, y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
y = tf.ones(3, dtype=tf.float32)
x, y, tf.tensordot(x, y, axes=1)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = jnp.ones(3, dtype = jnp.float32)
x, y, jnp.dot(x, y)
</code></pre>
<p>동등하게, (<strong>요소별 곱셈을 수행한 다음 합계를 구하여 두 벡터의 내적을 계산할 수 있습니다:</strong>)</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.sum(x * y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.sum(x * y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_sum(x * y)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.sum(x * y)
</code></pre>
<p>내적은 광범위한 맥락에서 유용합니다. 예를 들어, 벡터 $\mathbf{x}  \in \mathbb{R}^n$으로 표시되는 일련의 값과
$\mathbf{w} \in \mathbb{R}^n$으로 표시되는 일련의 가중치가 주어졌을 때, 가중치 $\mathbf{w}$에 따른 $\mathbf{x}$ 값의 가중 합은
내적 $\mathbf{x}^\top \mathbf{w}$로 표현될 수 있습니다. 가중치가 음수가 아니고
합이 $1$일 때, 즉 $\left(\sum_{i=1}^{n} {w_i} = 1\right)$,
내적은 *가중 평균(weighted average)*을 나타냅니다. 단위 길이를 갖도록 두 벡터를 정규화한 후,
내적은 두 벡터 사이 각도의 코사인을 나타냅니다. 이 섹션의 뒷부분에서 이 <em>길이</em> 개념을 공식적으로 소개할 것입니다.</p>
<h2 id="행렬-벡터-곱-matrix--vector-products"><a class="header" href="#행렬-벡터-곱-matrix--vector-products">행렬-벡터 곱 (Matrix--Vector Products)</a></h2>
<p>이제 내적을 계산하는 방법을 알았으므로,
$m \times n$ 행렬 $\mathbf{A}$와 $n$차원 벡터 $\mathbf{x}$ 사이의 <em>곱</em>을 이해할 수 있습니다. 시작하기 위해, 우리는 행렬을 행 벡터로 시각화합니다</p>
<p>$$\mathbf{A}= \begin{bmatrix} \mathbf{a}^\top_{1} \ \mathbf{a}^\top_{2} \ \vdots \ \mathbf{a}^\top_m \ \end{bmatrix},$$</p>
<p>여기서 각 $\mathbf{a}^\top_{i} \in \mathbb{R}^n$은 행렬 $\mathbf{A}$의 $i$번째 행을 나타내는 행 벡터입니다.</p>
<p>[<strong>행렬-벡터 곱 $\mathbf{A}\mathbf{x}$는 단순히 길이가 $m$인 열 벡터이며, 그 $i$번째 요소는 내적 $\mathbf{a}^\top_i \mathbf{x}$입니다:</strong>]</p>
<p>$$ \mathbf{A}\mathbf{x} = \begin{bmatrix} \mathbf{a}^\top_{1} \ \mathbf{a}^\top_{2} \ \vdots \ \mathbf{a}^\top_m \ \end{bmatrix}\mathbf{x} = \begin{bmatrix}  \mathbf{a}^\top_{1} \mathbf{x}  \ \mathbf{a}^\top_{2} \mathbf{x} \ \vdots\ \mathbf{a}^\top_{m} \mathbf{x}\end{bmatrix}. $$</p>
<p>우리는 행렬 $\mathbf{A}\in \mathbb{R}^{m \times n}$과의 곱셈을 벡터를 $\mathbb{R}^{n}$에서 $\mathbb{R}^{m}$으로 투영하는 변환으로 생각할 수 있습니다. 이러한 변환은 놀라울 정도로 유용합니다. 예를 들어, 우리는 회전을 특정 정사각 행렬에 의한 곱셈으로 나타낼 수 있습니다. 행렬-벡터 곱은 또한 이전 레이어의 출력이 주어졌을 때 신경망의 각 레이어의 출력을 계산하는 데 관련된 핵심 계산을 설명합니다.</p>
<p>:begin_tab:<code>mxnet</code>
코드에서 행렬-벡터 곱을 표현하기 위해,
우리는 동일한 <code>dot</code> 함수를 사용합니다. 연산은 인수의 유형에 따라 추론됩니다. <code>A</code>의 열 차원(축 1을 따른 길이)은
<code>x</code>의 차원(길이)과 같아야 합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
코드에서 행렬-벡터 곱을 표현하기 위해,
우리는 <code>mv</code> 함수를 사용합니다. <code>A</code>의 열 차원(축 1을 따른 길이)은
<code>x</code>의 차원(길이)과 같아야 합니다. Python에는 행렬-벡터 및 행렬-행렬 곱을 모두 실행할 수 있는
편의 연산자 <code>@</code>가 있습니다(인수에 따라 다름). 따라서 <code>A@x</code>라고 쓸 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
코드에서 행렬-벡터 곱을 표현하기 위해,
우리는 <code>matvec</code> 함수를 사용합니다. <code>A</code>의 열 차원(축 1을 따른 길이)은
<code>x</code>의 차원(길이)과 같아야 합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
A.shape, x.shape, np.dot(A, x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
A.shape, x.shape, torch.mv(A, x), A@x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
A.shape, x.shape, tf.linalg.matvec(A, x)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
A.shape, x.shape, jnp.matmul(A, x)
</code></pre>
<h2 id="행렬-행렬-곱셈-matrix--matrix-multiplication"><a class="header" href="#행렬-행렬-곱셈-matrix--matrix-multiplication">행렬-행렬 곱셈 (Matrix--Matrix Multiplication)</a></h2>
<p>내적과 행렬-벡터 곱에 익숙해졌다면,
<em>행렬-행렬 곱셈</em>은 간단할 것입니다.</p>
<p>두 개의 행렬 $\mathbf{A} \in \mathbb{R}^{n \times k}$와
$\mathbf{B} \in \mathbb{R}^{k \times m}$이 있다고 가정해 봅시다:</p>
<p>$$\mathbf{A}=\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1k} \
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2k} \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nk} \
\end{bmatrix},
\quad
\mathbf{B}=\begin{bmatrix}
b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1m} \
b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2m} \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
b_{k1} &amp; b_{k2} &amp; \cdots &amp; b_{km} \
\end{bmatrix}.$$</p>
<p>$\mathbf{a}^\top_{i} \in \mathbb{R}^k$는
행렬 $\mathbf{A}$의 $i$번째 행을 나타내는 행 벡터를 나타내고,
$\mathbf{b}_{j} \in \mathbb{R}^k$는
행렬 $\mathbf{B}$의 $j$번째 열에서 나온 열 벡터를 나타냅니다:</p>
<p>$$\mathbf{A}= \begin{bmatrix}
\mathbf{a}^\top_{1} \
\mathbf{a}^\top_{2} \
\vdots \
\mathbf{a}^\top_n \
\end{bmatrix},
\quad
\mathbf{B}=\begin{bmatrix}
\mathbf{b}<em>{1} &amp; \mathbf{b}</em>{2} &amp; \cdots &amp; \mathbf{b}_{m} \
\end{bmatrix}. $$</p>
<p>행렬 곱 $\mathbf{C} \in \mathbb{R}^{n \times m}$을 형성하기 위해,
우리는 각 요소 $c_{ij}$를
$\mathbf{A}$의 $i$번째 행과
$\mathbf{B}$의 $j$번째 열 사이의 내적,
즉 $\mathbf{a}^\top_i \mathbf{b}_j$로 간단히 계산합니다:</p>
<p>$$\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \
\mathbf{a}^\top_{2} \
\vdots \
\mathbf{a}^\top_n \
\end{bmatrix}
\begin{bmatrix}
\mathbf{b}<em>{1} &amp; \mathbf{b}</em>{2} &amp; \cdots &amp; \mathbf{b}<em>{m} \
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top</em>{1} \mathbf{b}<em>1 &amp; \mathbf{a}^\top</em>{1}\mathbf{b}<em>2&amp; \cdots &amp; \mathbf{a}^\top</em>{1} \mathbf{b}<em>m \
\mathbf{a}^\top</em>{2}\mathbf{b}<em>1 &amp; \mathbf{a}^\top</em>{2} \mathbf{b}<em>2 &amp; \cdots &amp; \mathbf{a}^\top</em>{2} \mathbf{b}<em>m \
\vdots &amp; \vdots &amp; \ddots &amp;\vdots\
\mathbf{a}^\top</em>{n} \mathbf{b}<em>1 &amp; \mathbf{a}^\top</em>{n}\mathbf{b}<em>2&amp; \cdots&amp; \mathbf{a}^\top</em>{n} \mathbf{b}_m
\end{bmatrix}. $$</p>
<p>[<strong>행렬-행렬 곱셈 $\mathbf{AB}$를
$m$개의 행렬-벡터 곱을 수행하거나
$m \times n$개의 내적을 수행하고
결과를 연결하여 $n \times m$ 행렬을 형성하는 것으로
생각할 수 있습니다.</strong>] 다음 스니펫에서,
우리는 <code>A</code>와 <code>B</code>에 대해 행렬 곱셈을 수행합니다. 여기서 <code>A</code>는 2개의 행과 3개의 열을 가진 행렬이고,
<code>B</code>는 3개의 행과 4개의 열을 가진 행렬입니다. 곱셈 후, 우리는 2개의 행과 4개의 열을 가진 행렬을 얻습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
B = np.ones(shape=(3, 4))
np.dot(A, B)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
B = torch.ones(3, 4)
torch.mm(A, B), A@B
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
B = tf.ones((3, 4), tf.float32)
tf.matmul(A, B)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
B = jnp.ones((3, 4))
jnp.matmul(A, B)
</code></pre>
<p><em>행렬-행렬 곱셈</em>이라는 용어는 종종
<em>행렬 곱셈</em>으로 단순화되며, 하다마드 곱과 혼동해서는 안 됩니다.</p>
<h2 id="노름-norms"><a class="header" href="#노름-norms">노름 (Norms)</a></h2>
<p>:label:<code>subsec_lin-algebra-norms</code></p>
<p>선형 대수에서 가장 유용한 연산자 중 일부는 *노름(norms)*입니다. 비공식적으로, 벡터의 노름은 벡터가 얼마나 <em>큰지</em> 알려줍니다. 예를 들어, $\ell_2$ 노름은 벡터의 (유클리드) 길이를 측정합니다. 여기서 우리는 벡터의 차원이 아니라 벡터 성분의 크기와 관련된 <em>크기</em> 개념을 사용하고 있습니다.</p>
<p>노름은 벡터를 스칼라로 매핑하고 다음 세 가지 속성을 만족하는 함수 $\ | \cdot \ |$입니다:</p>
<ol>
<li>어떤 벡터 $\mathbf{x}$가 주어졌을 때, 벡터의 (모든 요소)를 스칼라 $\alpha \in \mathbb{R}$로 스케일링하면, 노름도 그에 따라 스케일링됩니다:
$\|\alpha \mathbf{x}\| = |\alpha| \|\mathbf{x}\|$.</li>
<li>어떤 벡터 $\mathbf{x}$와 $\mathbf{y}$에 대해서도:
노름은 삼각 부등식을 만족합니다:
$\|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\$.</li>
<li>벡터의 노름은 음이 아니며 벡터가 0일 때만 사라집니다:
모든 $\mathbf{x} \neq 0$에 대해 $\|\mathbf{x}\| &gt; 0$.</li>
</ol>
<p>많은 함수가 유효한 노름이며 서로 다른 노름은 서로 다른 크기 개념을 인코딩합니다. 초등학교 기하학에서 직각 삼각형의 빗변을 계산할 때 배웠던 유클리드 노름은 벡터 요소의 제곱 합의 제곱근입니다. 공식적으로 이것은 [<strong>$\ell_2$ <em>노름</em></strong>]이라고 하며 다음과 같이 표현됩니다.</p>
<p>(<strong>$$\|\mathbf{x}\|<em>2 = \sqrt{\sum</em>{i=1}^n x_i^2}.$$</strong>)</p>
<p><code>norm</code> 메서드는 $\ell_2$ 노름을 계산합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
u = np.array([3, -4])
np.linalg.norm(u)
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
u = torch.tensor([3.0, -4.0])
torch.norm(u)
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
u = tf.constant([3.0, -4.0])
tf.norm(u)
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
u = jnp.array([3.0, -4.0])
jnp.linalg.norm(u)
</code></pre>
<p>[<strong>$\ell_1$ 노름</strong>]도 일반적이며 관련된 측정값을 맨해튼 거리라고 합니다. 정의에 따라 $\ell_1$ 노름은 벡터 요소의 절댓값을 합산합니다:</p>
<p>(<strong>$$\|\mathbf{x}\|<em>1 = \sum</em>{i=1}^n \left|x_i \right|.$$</strong>)</p>
<p>$\ell_2$ 노름에 비해 이상값에 덜 민감합니다. $\ell_1$ 노름을 계산하기 위해,
우리는 절댓값 연산과 합계 연산을 구성합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.abs(u).sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.abs(u).sum()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.reduce_sum(tf.abs(u))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.linalg.norm(u, ord=1) # jnp.abs(u).sum()과 동일
</code></pre>
<p>$\ell_2$ 및 $\ell_1$ 노름은 모두 더 일반적인 $\ell_p$ <em>노름</em>의 특수한 경우입니다:</p>
<p>$$\|\mathbf{x}\|<em>p = \left(\sum</em>{i=1}^n \left|x_i \right|^p \right)^{1/p}.$$</p>
<p>행렬의 경우 문제는 더 복잡합니다. 결국 행렬은 개별 항목의 모음이자
벡터에 작용하여 다른 벡터로 변환하는 객체로 볼 수 있습니다. 예를 들어, 우리는 행렬-벡터 곱 $\mathbf{X} \mathbf{v}$가
$\mathbf{v}$에 비해 얼마나 더 길어질 수 있는지 물을 수 있습니다. 이러한 생각은 <em>스펙트럼(spectral)</em> 노름이라고 불리는 것으로 이어집니다. 지금은 [<strong>계산하기 훨씬 쉬운 <em>프로베니우스(Frobenius) 노름</em></strong>]을 소개합니다. 이것은 행렬 요소의 제곱 합의 제곱근으로 정의됩니다:</p>
<p>[<strong>$$\|\mathbf{X}\|<em>\textrm{F} = \sqrt{\sum</em>{i=1}^m \sum_{j=1}^n x_{ij}^2}.$$</strong>]</p>
<p>프로베니우스 노름은 마치 행렬 모양 벡터의 $\ell_2$ 노름인 것처럼 동작합니다. 다음 함수를 호출하면 행렬의 프로베니우스 노름이 계산됩니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
np.linalg.norm(np.ones((4, 9)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
torch.norm(torch.ones((4, 9)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
tf.norm(tf.ones((4, 9)))
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
jnp.linalg.norm(jnp.ones((4, 9)))
</code></pre>
<p>너무 앞서 나가고 싶지는 않지만, 우리는 이미 이러한 개념이 유용한 이유에 대한 직관을 심을 수 있습니다. 딥러닝에서 우리는 종종 최적화 문제를 해결하려고 합니다:
관찰된 데이터에 할당된 확률을 <em>최대화</em>합니다; 추천 모델과 관련된 수익을 <em>최대화</em>합니다; 예측과 실제 관찰 사이의 거리를 <em>최소화</em>합니다; 동일한 사람의 사진 표현 간의 거리를 <em>최소화</em>하는 동시에
다른 사람의 사진 표현 간의 거리를 <em>최대화</em>합니다. 딥러닝 알고리즘의 목표를 구성하는 이러한 거리는
종종 노름으로 표현됩니다.</p>
<h2 id="토론"><a class="header" href="#토론">토론</a></h2>
<p>이 섹션에서 우리는 현대 딥러닝의 상당 부분을 이해하는 데 필요한
모든 선형 대수를 검토했습니다. 하지만 선형 대수에는 훨씬 더 많은 내용이 있으며,
그 중 많은 부분이 머신러닝에 유용합니다. 예를 들어 행렬을 인수로 분해할 수 있으며,
이러한 분해는 실제 데이터셋의 저차원 구조를 드러낼 수 있습니다. 데이터셋의 구조를 발견하고 예측 문제를 해결하기 위해
행렬 분해와 고차 텐서로의 일반화를 사용하는 데 초점을 맞춘
머신러닝의 전체 하위 분야가 있습니다. 하지만 이 책은 딥러닝에 초점을 맞춥니다. 그리고 우리는 여러분이 실제 데이터셋에 머신러닝을 적용하며
손을 더럽히고 나면 더 많은 수학을 배우고 싶어질 것이라고 믿습니다. 따라서 나중에 더 많은 수학을 소개할 권리는 보유하지만,
이 섹션은 여기서 마무리합니다.</p>
<p>선형 대수를 더 배우고 싶다면,
훌륭한 책과 온라인 리소스가 많이 있습니다. 더 고급 집중 코스를 원한다면
:citet:<code>Strang.1993</code>, :citet:<code>Kolter.2008</code>, 및 :citet:<code>Petersen.Pedersen.ea.2008</code>을 확인해 보십시오.</p>
<p>요약하자면:</p>
<ul>
<li>스칼라, 벡터, 행렬, 텐서는
선형 대수에서 사용되는 기본 수학적 객체이며
각각 0개, 1개, 2개, 그리고 임의의 수의 축을 가지고 있습니다.</li>
<li>텐서는 인덱싱을 통해 특정 축을 따라 슬라이스하거나,
<code>sum</code> 및 <code>mean</code>과 같은 연산을 통해 축소할 수 있습니다.</li>
<li>요소별 곱을 하다마드 곱이라고 합니다.
반면 내적, 행렬-벡터 곱, 행렬-행렬 곱은
요소별 연산이 아니며 일반적으로 피연산자와 다른 모양을 가진 객체를 반환합니다.</li>
<li>하다마드 곱에 비해 행렬-행렬 곱은
계산하는 데 상당히 더 오래 걸립니다(2차 시간이 아닌 3차 시간).</li>
<li>노름은 벡터(또는 행렬)의 크기에 대한 다양한 개념을 포착하며,
일반적으로 두 벡터 사이의 거리를 측정하기 위해 두 벡터의 차이에 적용됩니다.</li>
<li>일반적인 벡터 노름에는 $\ell_1$ 및 $\ell_2$ 노름이 포함되며,
일반적인 행렬 노름에는 <em>스펙트럼</em> 및 <em>프로베니우스</em> 노름이 포함됩니다.</li>
</ul>
<h2 id="연습-문제"><a class="header" href="#연습-문제">연습 문제</a></h2>
<ol>
<li>행렬 전치의 전치는 행렬 자체임을 증명하십시오: $(\mathbf{A}^\top)^\top = \mathbf{A}$.</li>
<li>두 행렬 $\mathbf{A}$와 $\mathbf{B}$가 주어졌을 때, 합과 전치가 교환 가능함을 보이십시오: $\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top$.</li>
<li>임의의 정사각 행렬 $\mathbf{A}$에 대해, $\mathbf{A} + \mathbf{A}^\top$는 항상 대칭입니까? 이전 두 연습 문제의 결과만 사용하여 결과를 증명할 수 있습니까?</li>
<li>우리는 이 섹션에서 모양 (2, 3, 4)의 텐서 <code>X</code>를 정의했습니다. <code>len(X)</code>의 출력은 무엇입니까? 코드를 구현하지 말고 답을 쓴 다음 코드를 사용하여 답을 확인하십시오.</li>
<li>임의의 모양을 가진 텐서 <code>X</code>의 경우, <code>len(X)</code>는 항상 <code>X</code>의 특정 축의 길이에 해당합니까? 그 축은 무엇입니까?</li>
<li><code>A / A.sum(axis=1)</code>을 실행하고 무슨 일이 일어나는지 확인하십시오. 결과를 분석할 수 있습니까?</li>
<li>맨해튼 시내의 두 지점 사이를 이동할 때, 좌표 측면에서, 즉 거리와 거리(avenues and streets) 측면에서 커버해야 하는 거리는 얼마입니까? 대각선으로 이동할 수 있습니까?</li>
<li>모양 (2, 3, 4)의 텐서를 고려하십시오. 축 0, 1, 2를 따른 합계 출력의 모양은 무엇입니까?</li>
<li>3개 이상의 축을 가진 텐서를 <code>linalg.norm</code> 함수에 입력하고 출력을 관찰하십시오. 이 함수는 임의의 모양을 가진 텐서에 대해 무엇을 계산합니까?</li>
<li>가우스 무작위 변수로 초기화된 $\mathbf{A} \in \mathbb{R}^{2^{10} \times 2^{16}}$, $\mathbf{B} \in \mathbb{R}^{2^{16} \times 2^{5}}$, $\mathbf{C} \in \mathbb{R}^{2^{5} \times 2^{14}}$의 세 개의 큰 행렬을 고려하십시오. 곱 $\mathbf{A} \mathbf{B} \mathbf{C}$를 계산하려고 합니다. $(\mathbf{A} \mathbf{B}) \mathbf{C}$를 계산하는지 아니면 $\mathbf{A} (\mathbf{B} \mathbf{C})$를 계산하는지에 따라 메모리 사용량과 속도에 차이가 있습니까? 왜 그렇습니까?</li>
<li>세 개의 큰 행렬 $\mathbf{A} \in \mathbb{R}^{2^{10} \times 2^{16}}$, $\mathbf{B} \in \mathbb{R}^{2^{16} \times 2^{5}}$, $\mathbf{C} \in \mathbb{R}^{2^{5} \times 2^{16}}$을 고려하십시오. $\mathbf{A} \mathbf{B}$를 계산하는지 아니면 $\mathbf{A} \mathbf{C}^\top$를 계산하는지에 따라 속도에 차이가 있습니까? 왜 그렇습니까? 메모리를 복제하지 않고 $\mathbf{C} = \mathbf{B}^\top$를 초기화하면 어떻게 변합니까? 왜 그렇습니까?</li>
<li>세 행렬 $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{100 \times 200}$을 고려하십시오. $[\mathbf{A}, \mathbf{B}, \mathbf{C}]$를 쌓아서 3개의 축을 가진 텐서를 만듭니다. 차원(dimensionality)은 무엇입니까? 세 번째 축의 두 번째 좌표를 슬라이스하여 $\mathbf{B}$를 복구하십시오. 답이 맞는지 확인하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/30">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/31">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/196">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17968">토론</a>
:end_tab:</p>
<pre><code></code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_preliminaries/pandas.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_preliminaries/calculus.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_preliminaries/pandas.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_preliminaries/calculus.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
