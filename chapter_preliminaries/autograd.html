<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>자동 미분 (Automatic Differentiation) - Dive into Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../static/d2l.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">딥러닝 (Deep Learning)</a></li><li class="chapter-item expanded "><a href="../chapter_preface/index.html"><strong aria-hidden="true">1.</strong> 서문 (Preface)</a></li><li class="chapter-item expanded "><a href="../chapter_installation/index.html"><strong aria-hidden="true">2.</strong> 설치 (Installation)</a></li><li class="chapter-item expanded "><a href="../chapter_notation/index.html"><strong aria-hidden="true">3.</strong> 표기법 (Notation)</a></li><li class="chapter-item expanded "><a href="../chapter_introduction/index.html"><strong aria-hidden="true">4.</strong> 소개 (Introduction)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/index.html"><strong aria-hidden="true">5.</strong> 예비 지식 (Preliminaries)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_preliminaries/ndarray.html"><strong aria-hidden="true">5.1.</strong> 데이터 조작 (Data Manipulation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/pandas.html"><strong aria-hidden="true">5.2.</strong> 데이터 전처리 (Data Preprocessing)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/linear-algebra.html"><strong aria-hidden="true">5.3.</strong> 선형 대수 (Linear Algebra)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/calculus.html"><strong aria-hidden="true">5.4.</strong> 미적분 (Calculus)</a></li><li class="chapter-item expanded "><a href="../chapter_preliminaries/autograd.html" class="active"><strong aria-hidden="true">5.5.</strong> 자동 미분 (Automatic Differentiation)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/probability.html"><strong aria-hidden="true">5.6.</strong> 확률과 통계 (Probability and Statistics)</a></li><li class="chapter-item "><a href="../chapter_preliminaries/lookup-api.html"><strong aria-hidden="true">5.7.</strong> 문서화 (Documentation)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-regression/index.html"><strong aria-hidden="true">6.</strong> 회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression.html"><strong aria-hidden="true">6.1.</strong> 선형 회귀 (Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/oo-design.html"><strong aria-hidden="true">6.2.</strong> 구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/synthetic-regression-data.html"><strong aria-hidden="true">6.3.</strong> 합성 회귀 데이터 (Synthetic Regression Data)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-scratch.html"><strong aria-hidden="true">6.4.</strong> 밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/linear-regression-concise.html"><strong aria-hidden="true">6.5.</strong> 선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/generalization.html"><strong aria-hidden="true">6.6.</strong> 일반화 (Generalization)</a></li><li class="chapter-item "><a href="../chapter_linear-regression/weight-decay.html"><strong aria-hidden="true">6.7.</strong> 가중치 감쇠 (Weight Decay)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_linear-classification/index.html"><strong aria-hidden="true">7.</strong> 분류를 위한 선형 신경망 (Linear Neural Networks for Classification)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression.html"><strong aria-hidden="true">7.1.</strong> 소프트맥스 회귀 (Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/image-classification-dataset.html"><strong aria-hidden="true">7.2.</strong> 이미지 분류 데이터셋 (The Image Classification Dataset)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/classification.html"><strong aria-hidden="true">7.3.</strong> 기본 분류 모델 (The Base Classification Model)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-scratch.html"><strong aria-hidden="true">7.4.</strong> 밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/softmax-regression-concise.html"><strong aria-hidden="true">7.5.</strong> 소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/generalization-classification.html"><strong aria-hidden="true">7.6.</strong> 분류에서의 일반화 (Generalization in Classification)</a></li><li class="chapter-item "><a href="../chapter_linear-classification/environment-and-distribution-shift.html"><strong aria-hidden="true">7.7.</strong> 환경 및 분포 이동 (Environment and Distribution Shift)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_multilayer-perceptrons/index.html"><strong aria-hidden="true">8.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp.html"><strong aria-hidden="true">8.1.</strong> 다층 퍼셉트론 (Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/mlp-implementation.html"><strong aria-hidden="true">8.2.</strong> 다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/backprop.html"><strong aria-hidden="true">8.3.</strong> 순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html"><strong aria-hidden="true">8.4.</strong> 수치적 안정성과 초기화 (Numerical Stability and Initialization)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/generalization-deep.html"><strong aria-hidden="true">8.5.</strong> 딥러닝에서의 일반화 (Generalization in Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/dropout.html"><strong aria-hidden="true">8.6.</strong> 드롭아웃 (Dropout)</a></li><li class="chapter-item "><a href="../chapter_multilayer-perceptrons/kaggle-house-price.html"><strong aria-hidden="true">8.7.</strong> Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_builders-guide/index.html"><strong aria-hidden="true">9.</strong> 빌더 가이드 (Builders' Guide)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_builders-guide/model-construction.html"><strong aria-hidden="true">9.1.</strong> 레이어와 모듈 (Layers and Modules)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/parameters.html"><strong aria-hidden="true">9.2.</strong> 파라미터 관리 (Parameter Management)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/init-param.html"><strong aria-hidden="true">9.3.</strong> 파라미터 초기화 (Parameter Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/lazy-init.html"><strong aria-hidden="true">9.4.</strong> 지연 초기화 (Lazy Initialization)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/custom-layer.html"><strong aria-hidden="true">9.5.</strong> 사용자 정의 레이어 (Custom Layers)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/read-write.html"><strong aria-hidden="true">9.6.</strong> 파일 I/O (File I/O)</a></li><li class="chapter-item "><a href="../chapter_builders-guide/use-gpu.html"><strong aria-hidden="true">9.7.</strong> GPU (GPUs)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-neural-networks/index.html"><strong aria-hidden="true">10.</strong> 합성곱 신경망 (Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/why-conv.html"><strong aria-hidden="true">10.1.</strong> 완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/conv-layer.html"><strong aria-hidden="true">10.2.</strong> 이미지를 위한 합성곱 (Convolutions for Images)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/padding-and-strides.html"><strong aria-hidden="true">10.3.</strong> 패딩과 스트라이드 (Padding and Stride)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/channels.html"><strong aria-hidden="true">10.4.</strong> 다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/pooling.html"><strong aria-hidden="true">10.5.</strong> 풀링 (Pooling)</a></li><li class="chapter-item "><a href="../chapter_convolutional-neural-networks/lenet.html"><strong aria-hidden="true">10.6.</strong> 합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_convolutional-modern/index.html"><strong aria-hidden="true">11.</strong> 현대 합성곱 신경망 (Modern Convolutional Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_convolutional-modern/alexnet.html"><strong aria-hidden="true">11.1.</strong> 심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/vgg.html"><strong aria-hidden="true">11.2.</strong> 블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/nin.html"><strong aria-hidden="true">11.3.</strong> 네트워크 속의 네트워크 (NiN) (Network in Network (NiN))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/googlenet.html"><strong aria-hidden="true">11.4.</strong> 다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/batch-norm.html"><strong aria-hidden="true">11.5.</strong> 배치 정규화 (Batch Normalization)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/resnet.html"><strong aria-hidden="true">11.6.</strong> 잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/densenet.html"><strong aria-hidden="true">11.7.</strong> 밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))</a></li><li class="chapter-item "><a href="../chapter_convolutional-modern/cnn-design.html"><strong aria-hidden="true">11.8.</strong> 합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-neural-networks/index.html"><strong aria-hidden="true">12.</strong> 순환 신경망 (Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/sequence.html"><strong aria-hidden="true">12.1.</strong> 시퀀스 다루기 (Working with Sequences)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/text-sequence.html"><strong aria-hidden="true">12.2.</strong> 원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/language-model.html"><strong aria-hidden="true">12.3.</strong> 언어 모델 (Language Models)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn.html"><strong aria-hidden="true">12.4.</strong> 순환 신경망 (Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-scratch.html"><strong aria-hidden="true">12.5.</strong> 밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/rnn-concise.html"><strong aria-hidden="true">12.6.</strong> 순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-neural-networks/bptt.html"><strong aria-hidden="true">12.7.</strong> BPTT (Backpropagation Through Time)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recurrent-modern/index.html"><strong aria-hidden="true">13.</strong> 현대 순환 신경망 (Modern Recurrent Neural Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recurrent-modern/lstm.html"><strong aria-hidden="true">13.1.</strong> 장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/gru.html"><strong aria-hidden="true">13.2.</strong> 게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/deep-rnn.html"><strong aria-hidden="true">13.3.</strong> 심층 순환 신경망 (Deep Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/bi-rnn.html"><strong aria-hidden="true">13.4.</strong> 양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/machine-translation-and-dataset.html"><strong aria-hidden="true">13.5.</strong> 기계 번역과 데이터셋 (Machine Translation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/encoder-decoder.html"><strong aria-hidden="true">13.6.</strong> 인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/seq2seq.html"><strong aria-hidden="true">13.7.</strong> 기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)</a></li><li class="chapter-item "><a href="../chapter_recurrent-modern/beam-search.html"><strong aria-hidden="true">13.8.</strong> 빔 검색 (Beam Search)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_attention-mechanisms-and-transformers/index.html"><strong aria-hidden="true">14.</strong> 어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html"><strong aria-hidden="true">14.1.</strong> 쿼리, 키, 그리고 값 (Queries, Keys, and Values)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html"><strong aria-hidden="true">14.2.</strong> 유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html"><strong aria-hidden="true">14.3.</strong> 어텐션 점수 함수 (Attention Scoring Functions)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html"><strong aria-hidden="true">14.4.</strong> Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html"><strong aria-hidden="true">14.5.</strong> 다중 헤드 어텐션 (Multi-Head Attention)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"><strong aria-hidden="true">14.6.</strong> 자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/transformer.html"><strong aria-hidden="true">14.7.</strong> 트랜스포머 아키텍처 (The Transformer Architecture)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html"><strong aria-hidden="true">14.8.</strong> 비전을 위한 트랜스포머 (Transformers for Vision)</a></li><li class="chapter-item "><a href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html"><strong aria-hidden="true">14.9.</strong> 트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_optimization/index.html"><strong aria-hidden="true">15.</strong> 최적화 알고리즘 (Optimization Algorithms)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_optimization/optimization-intro.html"><strong aria-hidden="true">15.1.</strong> 최적화와 딥러닝 (Optimization and Deep Learning)</a></li><li class="chapter-item "><a href="../chapter_optimization/convexity.html"><strong aria-hidden="true">15.2.</strong> 볼록성 (Convexity)</a></li><li class="chapter-item "><a href="../chapter_optimization/gd.html"><strong aria-hidden="true">15.3.</strong> 경사 하강법 (Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/sgd.html"><strong aria-hidden="true">15.4.</strong> 확률적 경사 하강법 (Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/minibatch-sgd.html"><strong aria-hidden="true">15.5.</strong> 미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)</a></li><li class="chapter-item "><a href="../chapter_optimization/momentum.html"><strong aria-hidden="true">15.6.</strong> 모멘텀 (Momentum)</a></li><li class="chapter-item "><a href="../chapter_optimization/adagrad.html"><strong aria-hidden="true">15.7.</strong> Adagrad</a></li><li class="chapter-item "><a href="../chapter_optimization/rmsprop.html"><strong aria-hidden="true">15.8.</strong> RMSProp</a></li><li class="chapter-item "><a href="../chapter_optimization/adadelta.html"><strong aria-hidden="true">15.9.</strong> Adadelta</a></li><li class="chapter-item "><a href="../chapter_optimization/adam.html"><strong aria-hidden="true">15.10.</strong> Adam</a></li><li class="chapter-item "><a href="../chapter_optimization/lr-scheduler.html"><strong aria-hidden="true">15.11.</strong> 학습률 스케줄링 (Learning Rate Scheduling)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computational-performance/index.html"><strong aria-hidden="true">16.</strong> 계산 성능 (Computational Performance)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computational-performance/hybridize.html"><strong aria-hidden="true">16.1.</strong> 컴파일러와 인터프리터 (Compilers and Interpreters)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/async-computation.html"><strong aria-hidden="true">16.2.</strong> 비동기 계산 (Asynchronous Computation)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/auto-parallelism.html"><strong aria-hidden="true">16.3.</strong> 자동 병렬화 (Automatic Parallelism)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/hardware.html"><strong aria-hidden="true">16.4.</strong> 하드웨어 (Hardware)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus.html"><strong aria-hidden="true">16.5.</strong> 다중 GPU에서의 훈련 (Training on Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/multiple-gpus-concise.html"><strong aria-hidden="true">16.6.</strong> 다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)</a></li><li class="chapter-item "><a href="../chapter_computational-performance/parameterserver.html"><strong aria-hidden="true">16.7.</strong> 파라미터 서버 (Parameter Servers)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_computer-vision/index.html"><strong aria-hidden="true">17.</strong> 컴퓨터 비전 (Computer Vision)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_computer-vision/image-augmentation.html"><strong aria-hidden="true">17.1.</strong> 이미지 증강 (Image Augmentation)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fine-tuning.html"><strong aria-hidden="true">17.2.</strong> 미세 조정 (Fine-Tuning)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/bounding-box.html"><strong aria-hidden="true">17.3.</strong> 객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/anchor.html"><strong aria-hidden="true">17.4.</strong> 앵커 박스 (Anchor Boxes)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/multiscale-object-detection.html"><strong aria-hidden="true">17.5.</strong> 멀티스케일 객체 탐지 (Multiscale Object Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/object-detection-dataset.html"><strong aria-hidden="true">17.6.</strong> 객체 탐지 데이터셋 (The Object Detection Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/ssd.html"><strong aria-hidden="true">17.7.</strong> 싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/rcnn.html"><strong aria-hidden="true">17.8.</strong> 영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))</a></li><li class="chapter-item "><a href="../chapter_computer-vision/semantic-segmentation-and-dataset.html"><strong aria-hidden="true">17.9.</strong> 시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/transposed-conv.html"><strong aria-hidden="true">17.10.</strong> 전치 합성곱 (Transposed Convolution)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/fcn.html"><strong aria-hidden="true">17.11.</strong> 완전 합성곱 네트워크 (Fully Convolutional Networks)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/neural-style.html"><strong aria-hidden="true">17.12.</strong> 신경 스타일 전송 (Neural Style Transfer)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-cifar10.html"><strong aria-hidden="true">17.13.</strong> Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)</a></li><li class="chapter-item "><a href="../chapter_computer-vision/kaggle-dog.html"><strong aria-hidden="true">17.14.</strong> Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-pretraining/index.html"><strong aria-hidden="true">18.</strong> 자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec.html"><strong aria-hidden="true">18.1.</strong> 단어 임베딩 (word2vec) (Word Embedding (word2vec))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/approx-training.html"><strong aria-hidden="true">18.2.</strong> 근사 훈련 (Approximate Training)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html"><strong aria-hidden="true">18.3.</strong> 단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html"><strong aria-hidden="true">18.4.</strong> word2vec 사전 훈련 (Pretraining word2vec)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/glove.html"><strong aria-hidden="true">18.5.</strong> GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/subword-embedding.html"><strong aria-hidden="true">18.6.</strong> 서브워드 임베딩 (Subword Embedding)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/similarity-analogy.html"><strong aria-hidden="true">18.7.</strong> 단어 유사도와 유추 (Word Similarity and Analogy)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert.html"><strong aria-hidden="true">18.8.</strong> 트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-dataset.html"><strong aria-hidden="true">18.9.</strong> BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-pretraining/bert-pretraining.html"><strong aria-hidden="true">18.10.</strong> BERT 사전 훈련 (Pretraining BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_natural-language-processing-applications/index.html"><strong aria-hidden="true">19.</strong> 자연어 처리: 응용 (Natural Language Processing: Applications)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html"><strong aria-hidden="true">19.1.</strong> 감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html"><strong aria-hidden="true">19.2.</strong> 감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html"><strong aria-hidden="true">19.3.</strong> 감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html"><strong aria-hidden="true">19.4.</strong> 자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html"><strong aria-hidden="true">19.5.</strong> 자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/finetuning-bert.html"><strong aria-hidden="true">19.6.</strong> 시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)</a></li><li class="chapter-item "><a href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html"><strong aria-hidden="true">19.7.</strong> 자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_reinforcement-learning/index.html"><strong aria-hidden="true">20.</strong> 강화 학습 (Reinforcement Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_reinforcement-learning/mdp.html"><strong aria-hidden="true">20.1.</strong> 마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/value-iter.html"><strong aria-hidden="true">20.2.</strong> 가치 반복 (Value Iteration)</a></li><li class="chapter-item "><a href="../chapter_reinforcement-learning/qlearning.html"><strong aria-hidden="true">20.3.</strong> Q-러닝 (Q-Learning)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_gaussian-processes/index.html"><strong aria-hidden="true">21.</strong> 가우스 과정 (Gaussian Processes)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-intro.html"><strong aria-hidden="true">21.1.</strong> 가우스 과정 소개 (Introduction to Gaussian Processes)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-priors.html"><strong aria-hidden="true">21.2.</strong> 가우스 과정 사전 분포 (Gaussian Process Priors)</a></li><li class="chapter-item "><a href="../chapter_gaussian-processes/gp-inference.html"><strong aria-hidden="true">21.3.</strong> 가우스 과정 추론 (Gaussian Process Inference)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_hyperparameter-optimization/index.html"><strong aria-hidden="true">22.</strong> 하이퍼파라미터 최적화 (Hyperparameter Optimization)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-intro.html"><strong aria-hidden="true">22.1.</strong> 하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/hyperopt-api.html"><strong aria-hidden="true">22.2.</strong> 하이퍼파라미터 최적화 API (Hyperparameter Optimization API)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/rs-async.html"><strong aria-hidden="true">22.3.</strong> 비동기 무작위 검색 (Asynchronous Random Search)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-intro.html"><strong aria-hidden="true">22.4.</strong> 다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)</a></li><li class="chapter-item "><a href="../chapter_hyperparameter-optimization/sh-async.html"><strong aria-hidden="true">22.5.</strong> 비동기 연속 반감법 (Asynchronous Successive Halving)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_generative-adversarial-networks/index.html"><strong aria-hidden="true">23.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/gan.html"><strong aria-hidden="true">23.1.</strong> 생성적 적대 신경망 (Generative Adversarial Networks)</a></li><li class="chapter-item "><a href="../chapter_generative-adversarial-networks/dcgan.html"><strong aria-hidden="true">23.2.</strong> 심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_recommender-systems/index.html"><strong aria-hidden="true">24.</strong> 추천 시스템 (Recommender Systems)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_recommender-systems/recsys-intro.html"><strong aria-hidden="true">24.1.</strong> 추천 시스템 개요 (Overview of Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/movielens.html"><strong aria-hidden="true">24.2.</strong> MovieLens 데이터셋 (The MovieLens Dataset)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/mf.html"><strong aria-hidden="true">24.3.</strong> 행렬 분해 (Matrix Factorization)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/autorec.html"><strong aria-hidden="true">24.4.</strong> AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ranking.html"><strong aria-hidden="true">24.5.</strong> 추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/neumf.html"><strong aria-hidden="true">24.6.</strong> 개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/seqrec.html"><strong aria-hidden="true">24.7.</strong> 시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/ctr.html"><strong aria-hidden="true">24.8.</strong> 풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/fm.html"><strong aria-hidden="true">24.9.</strong> 인수 분해 머신 (Factorization Machines)</a></li><li class="chapter-item "><a href="../chapter_recommender-systems/deepfm.html"><strong aria-hidden="true">24.10.</strong> 심층 인수 분해 머신 (Deep Factorization Machines)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-mathematics-for-deep-learning/index.html"><strong aria-hidden="true">25.</strong> 부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html"><strong aria-hidden="true">25.1.</strong> 기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html"><strong aria-hidden="true">25.2.</strong> 고유 분해 (Eigendecompositions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html"><strong aria-hidden="true">25.3.</strong> 단일 변수 미적분 (Single Variable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"><strong aria-hidden="true">25.4.</strong> 다변수 미적분 (Multivariable Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html"><strong aria-hidden="true">25.5.</strong> 적분 (Integral Calculus)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html"><strong aria-hidden="true">25.6.</strong> 확률 변수 (Random Variables)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html"><strong aria-hidden="true">25.7.</strong> 최대 우도 (Maximum Likelihood)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/distributions.html"><strong aria-hidden="true">25.8.</strong> 분포 (Distributions)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html"><strong aria-hidden="true">25.9.</strong> 나이브 베이즈 (Naive Bayes)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/statistics.html"><strong aria-hidden="true">25.10.</strong> 통계 (Statistics)</a></li><li class="chapter-item "><a href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html"><strong aria-hidden="true">25.11.</strong> 정보 이론 (Information Theory)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_appendix-tools-for-deep-learning/index.html"><strong aria-hidden="true">26.</strong> 부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/jupyter.html"><strong aria-hidden="true">26.1.</strong> 주피터 노트북 사용하기 (Using Jupyter Notebooks)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/sagemaker.html"><strong aria-hidden="true">26.2.</strong> Amazon SageMaker 사용하기 (Using Amazon SageMaker)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/aws.html"><strong aria-hidden="true">26.3.</strong> AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/colab.html"><strong aria-hidden="true">26.4.</strong> Google Colab 사용하기 (Using Google Colab)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html"><strong aria-hidden="true">26.5.</strong> 서버 및 GPU 선택하기 (Selecting Servers and GPUs)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/contributing.html"><strong aria-hidden="true">26.6.</strong> 이 책에 기여하기 (Contributing to This Book)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/utils.html"><strong aria-hidden="true">26.7.</strong> 유틸리티 함수 및 클래스 (Utility Functions and Classes)</a></li><li class="chapter-item "><a href="../chapter_appendix-tools-for-deep-learning/d2l.html"><strong aria-hidden="true">26.8.</strong> d2l API 문서 (The d2l API Document)</a></li></ol></li><li class="chapter-item expanded "><a href="../chapter_references/zreferences.html"><strong aria-hidden="true">27.</strong> 참고 문헌 (chapter_references/zreferences.md)</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Dive into Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/d2l-ai/d2l-en" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <pre><code class="language-{.python .input}">%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])
</code></pre>
<h1 id="자동-미분-automatic-differentiation"><a class="header" href="#자동-미분-automatic-differentiation">자동 미분 (Automatic Differentiation)</a></h1>
<p>:label:<code>sec_autograd</code></p>
<p>:numref:<code>sec_calculus</code>에서
도함수 계산이 심층 신경망을 훈련하는 데 사용할
모든 최적화 알고리즘의 중요한 단계라는 것을 상기해 보십시오.
계산은 간단하지만, 손으로 계산하는 것은 지루하고 오류가 발생하기 쉬우며,
모델이 더 복잡해짐에 따라 이러한 문제는 커질 뿐입니다.</p>
<p>다행히도 모든 최신 딥러닝 프레임워크는
<em>자동 미분(automatic differentiation)</em> (종종 <em>autograd</em>로 줄임)을 제공하여
이 작업을 대신해 줍니다.
각 연속적인 함수를 통해 데이터를 전달할 때,
프레임워크는 각 값이 다른 값에 어떻게 의존하는지 추적하는
*계산 그래프(computational graph)*를 구축합니다.
도함수를 계산하기 위해, 자동 미분은
연쇄 법칙을 적용하여 이 그래프를 역방향으로 진행합니다.
이런 방식으로 연쇄 법칙을 적용하는 계산 알고리즘을 *역전파(backpropagation)*라고 합니다.</p>
<p>autograd 라이브러리는 지난 10년 동안 뜨거운 관심사가 되었지만,
오랜 역사를 가지고 있습니다.
사실 autograd에 대한 최초의 언급은 반세기 전으로 거슬러 올라갑니다 :cite:<code>Wengert.1964</code>.
현대 역전파의 핵심 아이디어는 1980년 박사 학위 논문 :cite:<code>Speelpenning.1980</code>으로 거슬러 올라가며
1980년대 후반에 더욱 발전되었습니다 :cite:<code>Griewank.1989</code>.
역전파는 기울기를 계산하는 기본 방법이 되었지만 유일한 옵션은 아닙니다.
예를 들어 Julia 프로그래밍 언어는 순방향 전파를 사용합니다 :cite:<code>Revels.Lubin.Papamarkou.2016</code>.
방법을 탐구하기 전에, 먼저 autograd 패키지를 마스터해 봅시다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
from mxnet import autograd, np, npx
npx.set_np()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
import torch
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
import tensorflow as tf
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import numpy as jnp
</code></pre>
<h2 id="간단한-함수-a-simple-function"><a class="header" href="#간단한-함수-a-simple-function">간단한 함수 (A Simple Function)</a></h2>
<p>우리가 열 벡터 $\mathbf{x}$에 대해
(<strong>함수 $y = 2\mathbf{x}^{\top}\mathbf{x}$를 미분하는 데</strong>) 관심이 있다고 가정해 봅시다.
시작하기 위해 <code>x</code>에 초기 값을 할당합니다.</p>
<pre><code class="language-{.python .input  n=1}">%%tab mxnet
x = np.arange(4.0)
x
</code></pre>
<pre><code class="language-{.python .input  n=7}">%%tab pytorch
x = torch.arange(4.0)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.range(4, dtype=tf.float32)
x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x = jnp.arange(4.0)
x
</code></pre>
<p>:begin_tab:<code>mxnet, pytorch, tensorflow</code>
[<strong>$\mathbf{x}$에 대한 $y$의 기울기를 계산하기 전에,
그것을 저장할 장소가 필요합니다.</strong>] 일반적으로 우리는 도함수를 취할 때마다 새 메모리를 할당하는 것을 피합니다.
왜냐하면 딥러닝은 동일한 파라미터에 대해
도함수를 계속해서 매우 여러 번 계산해야 하기 때문이며,
메모리가 부족할 위험이 있습니다. 벡터 $\mathbf{x}$에 대한 스칼라 값 함수의 기울기는
$\mathbf{x}$와 동일한 모양을 가진 벡터 값입니다.
:end_tab:</p>
<pre><code class="language-{.python .input  n=8}">%%tab mxnet
# `attach_grad`를 호출하여 텐서의 기울기를 위한 메모리를 할당합니다
x.attach_grad()
# `x`에 대해 취한 기울기를 계산한 후, 0으로 초기화된 값을 가진
# `grad` 속성을 통해 액세스할 수 있습니다
x.grad
</code></pre>
<pre><code class="language-{.python .input  n=9}">%%tab pytorch
# x = torch.arange(4.0, requires_grad=True)를 생성할 수도 있습니다
x.requires_grad_(True)
x.grad  # 기울기는 기본적으로 None입니다
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x = tf.Variable(x)
</code></pre>
<p>(<strong>이제 <code>x</code>의 함수를 계산하고 결과를 <code>y</code>에 할당합니다.</strong>)</p>
<pre><code class="language-{.python .input  n=10}">%%tab mxnet
# 코드는 계산 그래프를 구축하기 위해 `autograd.record` 범위 안에 있습니다
with autograd.record():
    y = 2 * np.dot(x, x)
y
</code></pre>
<pre><code class="language-{.python .input  n=11}">%%tab pytorch
y = 2 * torch.dot(x, x)
y
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# 모든 계산을 테이프에 기록합니다
with tf.GradientTape() as t:
    y = 2 * tf.tensordot(x, x, axes=1)
y
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = lambda x: 2 * jnp.dot(x, x)
y(x)
</code></pre>
<p>:begin_tab:<code>mxnet</code>
<code>backward</code> 메서드를 호출하여
[<strong>이제 <code>x</code>에 대한 <code>y</code>의 기울기를 취할 수 있습니다</strong>].
다음으로, <code>x</code>의 <code>grad</code> 속성을 통해 기울기에 액세스할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<code>backward</code> 메서드를 호출하여
[<strong>이제 <code>x</code>에 대한 <code>y</code>의 기울기를 취할 수 있습니다</strong>].
다음으로, <code>x</code>의 <code>grad</code> 속성을 통해 기울기에 액세스할 수 있습니다.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<code>gradient</code> 메서드를 호출하여
[<strong>이제 <code>x</code>에 대한 <code>y</code>의 기울기를 계산할 수 있습니다</strong>].
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<code>grad</code> 변환을 통과시켜
[<strong>이제 <code>x</code>에 대한 <code>y</code>의 기울기를 취할 수 있습니다</strong>].
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
x.grad
</code></pre>
<pre><code class="language-{.python .input  n=12}">%%tab pytorch
y.backward()
x.grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x_grad = t.gradient(y, x)
x_grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import grad
# `grad` 변환은 원래 함수의 기울기를 계산하는 Python 함수를 반환합니다
x_grad = grad(y)(x)
x_grad
</code></pre>
<p>(<strong>우리는 이미 $\mathbf{x}$에 대한 함수 $y = 2\mathbf{x}^{\top}\mathbf{x}$의 기울기가
$4\mathbf{x}$여야 한다는 것을 알고 있습니다.</strong>) 이제 자동 기울기 계산과 예상 결과가 동일한지 확인할 수 있습니다.</p>
<pre><code class="language-{.python .input  n=13}">%%tab mxnet
x.grad == 4 * x
</code></pre>
<pre><code class="language-{.python .input  n=14}">%%tab pytorch
x.grad == 4 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
x_grad == 4 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
x_grad == 4 * x
</code></pre>
<p>:begin_tab:<code>mxnet</code>
[<strong>이제 <code>x</code>의 다른 함수를 계산하고
기울기를 취해 봅시다.</strong>] MXNet은 우리가 새 기울기를 기록할 때마다
기울기 버퍼를 재설정한다는 점에 유의하십시오.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
[<strong>이제 <code>x</code>의 다른 함수를 계산하고
기울기를 취해 봅시다.</strong>] PyTorch는 우리가 새 기울기를 기록할 때
자동으로 기울기 버퍼를 재설정하지 않는다는 점에 유의하십시오.
대신, 새 기울기가 이미 저장된 기울기에 추가됩니다.
이 동작은 여러 목적 함수의 합을 최적화하고 싶을 때 유용합니다.
기울기 버퍼를 재설정하려면 다음과 같이 <code>x.grad.zero_()</code>를 호출할 수 있습니다:
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
[<strong>이제 <code>x</code>의 다른 함수를 계산하고
기울기를 취해 봅시다.</strong>] TensorFlow는 우리가 새 기울기를 기록할 때마다
기울기 버퍼를 재설정한다는 점에 유의하십시오.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = x.sum()
y.backward()
x.grad  # 새로 계산된 기울기로 덮어쓰여짐
</code></pre>
<pre><code class="language-{.python .input  n=20}">%%tab pytorch
x.grad.zero_()  # 기울기 재설정
y = x.sum()
y.backward()
x.grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = tf.reduce_sum(x)
t.gradient(y, x)  # 새로 계산된 기울기로 덮어쓰여짐
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = lambda x: x.sum()
grad(y)(x)
</code></pre>
<h2 id="비스칼라-변수의-역전파-backward-for-non-scalar-variables"><a class="header" href="#비스칼라-변수의-역전파-backward-for-non-scalar-variables">비스칼라 변수의 역전파 (Backward for Non-Scalar Variables)</a></h2>
<p><code>y</code>가 벡터일 때,
벡터 <code>x</code>에 대한 <code>y</code>의 도함수의 가장 자연스러운 표현은
<code>y</code>의 각 성분의 <code>x</code>의 각 성분에 대한 편도함수를 포함하는
*자코비안(Jacobian)*이라는 행렬입니다.
마찬가지로 고차 <code>y</code>와 <code>x</code>의 경우 미분 결과는 더 높은 차수의 텐서가 될 수 있습니다.</p>
<p>자코비안은 일부 고급 머신러닝 기술에 등장하지만,
더 일반적으로 우리는 전체 벡터 <code>x</code>에 대한
<code>y</code>의 각 성분의 기울기를 합산하여
<code>x</code>와 동일한 모양의 벡터를 생성하기를 원합니다.
예를 들어, 우리는 종종 훈련 예제의 <em>배치</em> 중 각 예제에 대해
개별적으로 계산된 손실 함수의 값을 나타내는 벡터를 가지고 있습니다.
여기서 우리는 단지 (<strong>각 예제에 대해 개별적으로 계산된 기울기를 합산</strong>)하기를 원합니다.</p>
<p>:begin_tab:<code>mxnet</code>
MXNet은 기울기를 계산하기 전에 합계를 통해 모든 텐서를 스칼라로 축소하여 이 문제를 처리합니다.
즉, 자코비안 $\partial_{\mathbf{x}} \mathbf{y}$를 반환하는 대신,
합의 기울기 $\partial_{\mathbf{x}} \sum_i y_i$를 반환합니다.
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
딥러닝 프레임워크마다 비스칼라 텐서의 기울기를 해석하는 방식이 다르기 때문에,
PyTorch는 혼란을 피하기 위해 몇 가지 조치를 취합니다.
비스칼라에서 <code>backward</code>를 호출하면 PyTorch에 객체를 스칼라로 축소하는 방법을 알려주지 않는 한 오류가 발생합니다.
더 공식적으로, 우리는 <code>backward</code>가 $\partial_{\mathbf{x}} \mathbf{y}$ 대신
$\mathbf{v}^\top \partial_{\mathbf{x}} \mathbf{y}$를 계산하도록 하는 어떤 벡터 $\mathbf{v}$를 제공해야 합니다.
이 다음 부분은 혼란스러울 수 있지만, 나중에 명확해질 이유들로 인해
이 인수($\mathbf{v}$를 나타냄)의 이름은 <code>gradient</code>입니다. 자세한 설명은 Yang Zhang의 <a href="https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29">Medium 게시물</a>을 참조하십시오.
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
기본적으로 TensorFlow는 합의 기울기를 반환합니다.
즉, 자코비안 $\partial_{\mathbf{x}} \mathbf{y}$를 반환하는 대신,
합의 기울기 $\partial_{\mathbf{x}} \sum_i y_i$를 반환합니다.
:end_tab:</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = x * x  
y.backward()
x.grad  # y = sum(x * x)의 기울기와 동일
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x.grad.zero_()
y = x * x
y.backward(gradient=torch.ones(len(y)))  # 더 빠름: y.sum().backward()
x.grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
with tf.GradientTape() as t:
    y = x * x
t.gradient(y, x)  # y = tf.reduce_sum(x * x)와 동일
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
y = lambda x: x * x
# grad는 스칼라 출력 함수에 대해서만 정의됨
grad(lambda x: y(x).sum())(x)
</code></pre>
<h2 id="계산-분리-detaching-computation"><a class="header" href="#계산-분리-detaching-computation">계산 분리 (Detaching Computation)</a></h2>
<p>때때로 우리는 [<strong>일부 계산을 기록된 계산 그래프 외부로 이동</strong>]하고 싶을 때가 있습니다.
예를 들어, 입력을 사용하여 기울기를 계산하고 싶지 않은
일부 보조 중간 항을 만든다고 가정해 봅시다.
이 경우, 최종 결과에서 해당 계산 그래프를 *분리(detach)*해야 합니다.
다음의 장난감 예제는 이것을 더 명확하게 만듭니다:
<code>z = x * y</code>이고 <code>y = x * x</code>이지만
<code>y</code>를 통해 전달되는 영향보다는 <code>z</code>에 대한 <code>x</code>의 <em>직접적인</em> 영향에 초점을 맞추고 싶다고 가정해 봅시다.
이 경우, <code>y</code>와 동일한 값을 갖지만
<em>출처(provenance)</em> (어떻게 생성되었는지)가 지워진
새 변수 <code>u</code>를 만들 수 있습니다.
따라서 <code>u</code>는 그래프에 조상이 없으며 기울기는 <code>u</code>를 통해 <code>x</code>로 흐르지 않습니다.
예를 들어 <code>z = x * u</code>의 기울기를 취하면 결과 <code>u</code>를 산출합니다
(<code>z = x * x * x</code>이기 때문에 예상할 수 있는 <code>3 * x * x</code>가 아님).</p>
<pre><code class="language-{.python .input}">%%tab mxnet
with autograd.record():
    y = x * x
    u = y.detach()
    z = u * x
z.backward()
x.grad == u
</code></pre>
<pre><code class="language-{.python .input  n=21}">%%tab pytorch
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
# persistent=True로 설정하여 계산 그래프를 보존합니다.
# 이를 통해 t.gradient를 두 번 이상 실행할 수 있습니다.
with tf.GradientTape(persistent=True) as t:
    y = x * x
    u = tf.stop_gradient(y)
    z = u * x

x_grad = t.gradient(z, x)
x_grad == u
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
import jax

y = lambda x: x * x
# jax.lax 기본 요소는 XLA 연산에 대한 Python 래퍼입니다
u = jax.lax.stop_gradient(y(x))
z = lambda x: u * x

grad(lambda x: z(x).sum())(x) == y(x)
</code></pre>
<p>이 절차가 <code>z</code>로 이어지는 그래프에서
<code>y</code>의 조상을 분리하지만, <code>y</code>로 이어지는 계산 그래프는 유지되므로
<code>x</code>에 대한 <code>y</code>의 기울기를 계산할 수 있다는 점에 유의하십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
y.backward()
x.grad == 2 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
t.gradient(y, x) == 2 * x
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
grad(lambda x: y(x).sum())(x) == 2 * x
</code></pre>
<h2 id="기울기와-python-제어-흐름-gradients-and-python-control-flow"><a class="header" href="#기울기와-python-제어-흐름-gradients-and-python-control-flow">기울기와 Python 제어 흐름 (Gradients and Python Control Flow)</a></h2>
<p>지금까지 우리는 <code>z = x * x * x</code>와 같은 함수를 통해
입력에서 출력까지의 경로가 잘 정의된 사례를 검토했습니다.
프로그래밍은 결과를 계산하는 방법에 있어 훨씬 더 많은 자유를 제공합니다.
예를 들어, 보조 변수에 의존하게 하거나 중간 결과에 대한 선택을 조건부로 만들 수 있습니다.
자동 미분을 사용하는 이점 중 하나는
(<strong>함수의 계산 그래프를 구축하는 데 Python 제어 흐름의 미로를 통과해야 하더라도</strong>)
(예: 조건문, 루프, 임의 함수 호출), [<strong>결과 변수의 기울기를 여전히 계산할 수 있다는 것입니다.</strong>]
이를 설명하기 위해 <code>while</code> 루프의 반복 횟수와 <code>if</code> 문의 평가가
모두 입력 <code>a</code>의 값에 의존하는 다음 코드 스니펫을 고려해 보십시오.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
def f(a):
    b = a * 2
    while np.linalg.norm(b) &lt; 1000:
        b = b * 2
    if b.sum() &gt; 0:
        c = b
    else:
        c = 100 * b
    return c
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
def f(a):
    b = a * 2
    while b.norm() &lt; 1000:
        b = b * 2
    if b.sum() &gt; 0:
        c = b
    else:
        c = 100 * b
    return c
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
def f(a):
    b = a * 2
    while tf.norm(b) &lt; 1000:
        b = b * 2
    if tf.reduce_sum(b) &gt; 0:
        c = b
    else:
        c = 100 * b
    return c
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
def f(a):
    b = a * 2
    while jnp.linalg.norm(b) &lt; 1000:
        b = b * 2
    if b.sum() &gt; 0:
        c = b
    else:
        c = 100 * b
    return c
</code></pre>
<p>아래에서 우리는 무작위 값을 입력으로 전달하여 이 함수를 호출합니다.
입력이 확률 변수이므로, 우리는 계산 그래프가 어떤 형태를 취할지 모릅니다.
그러나 특정 입력에 대해 <code>f(a)</code>를 실행할 때마다
특정 계산 그래프를 실현하고 이후에 <code>backward</code>를 실행할 수 있습니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
a = np.random.normal()
a.attach_grad()
with autograd.record():
    d = f(a)
d.backward()
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
a = tf.Variable(tf.random.normal(shape=()))
with tf.GradientTape() as t:
    d = f(a)
d_grad = t.gradient(d, a)
d_grad
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
from jax import random
a = random.normal(random.PRNGKey(1), ()) 
d = f(a)
d_grad = grad(f)(a)
</code></pre>
<p>비록 우리 함수 <code>f</code>가 데모 목적으로 약간 작위적이지만,
입력에 대한 의존성은 꽤 간단합니다: 그것은 부분적으로 정의된 스케일을 가진 <code>a</code>의 <em>선형</em> 함수입니다.
따라서 <code>f(a) / a</code>는 상수 항목의 벡터이며, 게다가 <code>f(a) / a</code>는 <code>a</code>에 대한 <code>f(a)</code>의 기울기와 일치해야 합니다.</p>
<pre><code class="language-{.python .input}">%%tab mxnet
a.grad == d / a
</code></pre>
<pre><code class="language-{.python .input}">%%tab pytorch
a.grad == d / a
</code></pre>
<pre><code class="language-{.python .input}">%%tab tensorflow
d_grad == d / a
</code></pre>
<pre><code class="language-{.python .input}">%%tab jax
d_grad == d / a
</code></pre>
<p>동적 제어 흐름은 딥러닝에서 매우 일반적입니다.
예를 들어 텍스트를 처리할 때 계산 그래프는 입력 길이에 따라 달라집니다.
이러한 경우, 기울기를 <em>선험적으로(a priori)</em> 계산하는 것이 불가능하기 때문에
자동 미분은 통계적 모델링에 필수적이 됩니다.</p>
<h2 id="토론"><a class="header" href="#토론">토론</a></h2>
<p>여러분은 이제 자동 미분의 힘을 맛보았습니다.
도함수를 자동으로 그리고 효율적으로 계산하는 라이브러리의 개발은
딥러닝 실무자들에게 엄청난 생산성 향상 요인이 되어, 그들이 덜 하찮은 일에 집중할 수 있게 해주었습니다.
게다가 autograd를 사용하면 펜과 종이로 기울기를 계산하는 것이
엄두도 못 낼 정도로 시간이 많이 걸리는 거대한 모델을 설계할 수 있습니다.
흥미롭게도, 우리는 모델을 (통계적 의미에서) <em>최적화</em>하기 위해 autograd를 사용하지만,
autograd 라이브러리 자체의 (계산적 의미에서의) <em>최적화</em>는
프레임워크 설계자들에게 매우 중요한 관심사인 풍부한 주제입니다.
여기서 가장 신속하고 메모리 효율적인 방식으로 결과를 계산하기 위해
컴파일러와 그래프 조작 도구가 활용됩니다.</p>
<p>지금은 다음 기본 사항을 기억해 두십시오: (i) 도함수를 원하는 변수에 기울기를 연결합니다; (ii) 목표 값의 계산을 기록합니다; (iii) 역전파 함수를 실행합니다; (iv) 결과 기울기에 액세스합니다.</p>
<h2 id="연습-문제"><a class="header" href="#연습-문제">연습 문제</a></h2>
<ol>
<li>2계 도함수가 1계 도함수보다 계산 비용이 훨씬 더 많이 드는 이유는 무엇입니까?</li>
<li>역전파 함수를 실행한 후 즉시 다시 실행하면 어떻게 됩니까? 조사해 보십시오.</li>
<li><code>a</code>에 대한 <code>d</code>의 도함수를 계산하는 제어 흐름 예제에서 변수 <code>a</code>를 무작위 벡터나 행렬로 변경하면 어떻게 됩니까? 이 시점에서 계산 <code>f(a)</code>의 결과는 더 이상 스칼라가 아닙니다. 결과에 어떤 일이 발생합니까? 이것을 어떻게 분석합니까?</li>
<li>$f(x) = \sin(x)$라고 합시다. $f$와 도함수 $f'$의 그래프를 플롯하십시오. $f'(x) = \cos(x)$라는 사실을 이용하지 말고 자동 미분을 사용하여 결과를 얻으십시오.</li>
<li>$f(x) = ((\log x^2) \cdot \sin x) + x^{-1}$이라고 합시다. $x$에서 $f(x)$까지 결과를 추적하는 종속성 그래프를 작성하십시오.</li>
<li>연쇄 법칙을 사용하여 앞서 언급한 함수의 도함수 $\frac{df}{dx}$를 계산하고, 이전에 구성한 종속성 그래프에 각 항을 배치하십시오.</li>
<li>그래프와 중간 도함수 결과가 주어졌을 때, 기울기를 계산할 때 여러 가지 옵션이 있습니다. $x$에서 $f$로 시작하여 한 번, $f$에서 $x$로 추적하여 한 번 결과를 평가하십시오. $x$에서 $f$로의 경로는 일반적으로 *순방향 미분(forward differentiation)*으로 알려져 있으며, $f$에서 $x$로의 경로는 *역방향 미분(backward differentiation)*으로 알려져 있습니다.</li>
<li>언제 순방향 미분을 사용하고 언제 역방향 미분을 사용하고 싶으십니까? 힌트: 필요한 중간 데이터의 양, 단계를 병렬화하는 능력, 관련된 행렬 및 벡터의 크기를 고려하십시오.</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/34">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/35">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/200">토론</a>
:end_tab:</p>
<p>:begin_tab:<code>jax</code>
<a href="https://discuss.d2l.ai/t/17970">토론</a>
:end_tab:</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapter_preliminaries/calculus.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapter_preliminaries/probability.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapter_preliminaries/calculus.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapter_preliminaries/probability.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
