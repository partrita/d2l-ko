# 강화 학습 (Reinforcement Learning)
:label:`chap_reinforcement_learning`


**Pratik Chaudhari** (*University of Pennsylvania and Amazon*), **Rasool Fakoor** (*Amazon*), 및 **Kavosh Asadi** (*Amazon*)

강화 학습(Reinforcement Learning, RL)은 순차적으로 결정을 내리는 머신러닝 시스템을 구축할 수 있게 해주는 기술 모음입니다. 예를 들어, 온라인 소매업체에서 구매한 새 옷이 들어 있는 패키지가 일련의 결정, 즉 소매업체가 집에서 가장 가까운 창고에서 옷을 찾고, 옷을 상자에 넣고, 육로 또는 항공으로 상자를 운송하고, 도시 내 집으로 배달한 후 문앞에 도착합니다. 그 과정에서 패키지 배송에 영향을 미치는 많은 변수가 있습니다. 예를 들어 창고에 옷이 있었는지 여부, 상자를 운송하는 데 걸린 시간, 일일 배달 트럭이 출발하기 전에 도시에 도착했는지 여부 등입니다. 핵심 아이디어는 각 단계에서 우리가 종종 통제하지 않는 이러한 변수가 미래의 전체 사건 순서에 영향을 미친다는 것입니다. 예를 들어 창고에서 상자를 포장하는 데 지연이 발생하면 소매업체는 적시 배송을 보장하기 위해 지상 대신 항공으로 패키지를 보내야 할 수도 있습니다. 강화 학습 방법을 사용하면 순차적 의사 결정 문제의 각 단계에서 적절한 조치를 취하여 결국 어떤 효용, 예를 들어 패키지의 적시 배송을 극대화할 수 있습니다.

이러한 순차적 의사 결정 문제는 수많은 다른 곳에서도 볼 수 있습니다. 예를 들어 [바둑(Go)](https://en.wikipedia.org/wiki/Go_(game))을 두는 동안 현재의 움직임이 다음 움직임을 결정하고 상대방의 움직임은 제어할 수 없는 변수입니다... 일련의 움직임이 결국 승패를 결정합니다; Netflix가 현재 추천하는 영화는 당신이 무엇을 볼지 결정하고, 당신이 그 영화를 좋아할지 여부는 Netflix에 알려지지 않았으며, 결국 일련의 영화 추천은 당신이 Netflix에 얼마나 만족하는지 결정합니다. 오늘날 이러한 문제에 대한 효과적인 솔루션을 개발하기 위해 강화 학습이 사용되고 있습니다 :cite:`mnih2013playing,Silver.Huang.Maddison.ea.2016`. 강화 학습과 표준 딥러닝의 주요 차이점은 표준 딥러닝에서는 하나의 테스트 데이터에 대한 훈련된 모델의 예측이 미래의 테스트 데이터에 대한 예측에 영향을 미치지 않는다는 것입니다; 강화 학습에서는 미래의 순간(RL에서는 결정을 행동(action)이라고도 함)에서의 결정이 과거에 내린 결정에 의해 영향을 받습니다.

이 장에서는 강화 학습의 기초를 개발하고 널리 사용되는 강화 학습 방법을 구현하는 실무 경험을 얻을 것입니다. 먼저 그러한 순차적 의사 결정 문제를 생각할 수 있게 해주는 마르코프 결정 과정(Markov Decision Process, MDP)이라는 개념을 개발할 것입니다. 가치 반복(Value Iteration)이라는 알고리즘은 MDP의 통제되지 않는 변수(RL에서는 이러한 제어된 변수를 환경이라고 함)가 일반적으로 어떻게 행동하는지 알고 있다는 가정 하에 강화 학습 문제를 해결하는 첫 번째 통찰력이 될 것입니다. 가치 반복의 더 일반적인 버전인 Q-러닝(Q-Learning)이라는 알고리즘을 사용하면 환경에 대한 완전한 지식이 없더라도 적절한 조치를 취할 수 있습니다. 그런 다음 전문가의 행동을 모방하여 강화 학습 문제에 딥 네트워크를 사용하는 방법을 연구할 것입니다. 마지막으로, 딥 네트워크를 사용하여 알려지지 않은 환경에서 행동을 취하는 강화 학습 방법을 개발할 것입니다. 이러한 기술은 오늘날 다양한 실제 응용 분야에서 사용되는 더 발전된 RL 알고리즘의 기초를 형성하며, 그 중 일부는 장에서 지적할 것입니다.

![강화 학습 구조](../img/RL_main.png)
:width:`400px`
:label:`fig_rl_big`

```toc
:maxdepth: 2

mdp
value-iter
qlearning
```