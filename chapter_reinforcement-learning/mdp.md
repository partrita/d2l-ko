# 마르코프 결정 과정 (Markov Decision Process, MDP)
:label:`sec_mdp`
이 섹션에서는 마르코프 결정 과정(MDP)을 사용하여 강화 학습 문제를 공식화하는 방법에 대해 논의하고 MDP의 다양한 구성 요소를 자세히 설명합니다.

## MDP의 정의

마르코프 결정 과정(MDP) :cite:`BellmanMDP`은 시스템에 다양한 행동이 적용될 때 시스템의 상태가 어떻게 진화하는지에 대한 모델입니다. 몇 가지 다른 수량들이 함께 모여 MDP를 형성합니다.

![간단한 그리드 월드 내비게이션 작업. 로봇은 목표 위치(녹색 집으로 표시됨)로 가는 길을 찾아야 할 뿐만 아니라 함정 위치(빨간색 십자가로 표시됨)를 피해야 합니다.](../img/mdp.png)
:width:`250px`
:label:`fig_mdp`

* $\mathcal{S}$를 MDP의 상태 집합이라고 합시다. 구체적인 예로 :numref:`fig_mdp`를 참조하십시오. 로봇이 그리드 월드를 탐색하고 있습니다. 이 경우, $\mathcal{S}$는 주어진 타임스텝에서 로봇이 있을 수 있는 위치 집합에 해당합니다.
* $\mathcal{A}$를 로봇이 각 상태에서 취할 수 있는 행동 집합이라고 합시다. 예: "앞으로 이동", "오른쪽으로 회전", "왼쪽으로 회전", "같은 위치에 머무르기" 등. 행동은 로봇의 현재 상태를 $\mathcal{S}$ 내의 다른 상태로 변경할 수 있습니다.
* 로봇이 *정확히* 어떻게 움직이는지 모르고 대략적으로만 알 수도 있습니다. 강화 학습에서는 이 상황을 다음과 같이 모델링합니다: 로봇이 "앞으로 이동" 행동을 취하면 현재 상태에 머무를 확률이 작고, "왼쪽으로 회전"할 확률도 작을 수 있습니다. 수학적으로 이것은 "전이 함수(transition function)" $T: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0,1]$를 정의하는 것과 같습니다. 로봇이 상태 $s$에 있고 행동 $a$를 취했을 때 상태 $s'$에 도달할 조건부 확률을 사용하여 $T(s, a, s') = P(s' \mid s, a)$로 정의합니다. 전이 함수는 확률 분포이므로 모든 $s \in \mathcal{S}$ 및 $a \in \mathcal{A}$에 대해 $\sum_{s' \in \mathcal{S}} T(s, a, s') = 1$입니다. 즉, 로봇이 행동을 취하면 어떤 상태로든 가야 합니다.
* 이제 "보상(reward)" $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ 개념을 사용하여 어떤 행동이 유용하고 어떤 행동이 유용하지 않은지에 대한 개념을 구성합니다. 로봇이 상태 $s$에서 행동 $a$를 취하면 보상 $r(s,a)$를 받는다고 말합니다. 보상 $r(s, a)$가 크면 상태 $s$에서 행동 $a$를 취하는 것이 로봇의 목표(예: 녹색 집으로 이동)를 달성하는 데 더 유용함을 나타냅니다. 보상 $r(s, a)$가 작으면 행동 $a$는 이 목표를 달성하는 데 덜 유용합니다. 보상은 목표를 염두에 두고 사용자(강화 학습 알고리즘을 만드는 사람)가 설계한다는 점에 유의하는 것이 중요합니다.

## 반환값 및 할인율 (Return and Discount Factor)

위의 다양한 구성 요소가 함께 마르코프 결정 과정(MDP)을 형성합니다.
$$\textrm{MDP}: (\mathcal{S}, \mathcal{A}, T, r).$$ 

이제 로봇이 특정 상태 $s_0 \in \mathcal{S}$에서 시작하여 행동을 계속 취하여 궤적을 만드는 상황을 고려해 봅시다.
$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, a_2, r_2, \ldots).$$ 

각 타임스텝 $t$에서 로봇은 상태 $s_t$에 있고 행동 $a_t$를 취하며 그 결과 보상 $r_t = r(s_t, a_t)$를 얻습니다. 궤적의 *반환값(return)*은 로봇이 해당 궤적을 따라 얻은 총 보상입니다.
$$R(\tau) = r_0 + r_1 + r_2 + \cdots.$$ 

강화 학습의 목표는 가장 큰 *반환값*을 갖는 궤적을 찾는 것입니다.

로봇이 목표 위치에 도달하지 못하고 그리드 월드를 계속 여행하는 상황을 생각해 봅시다. 이 경우 궤적의 상태 및 행동 시퀀스는 무한히 길 수 있으며, 그러한 무한히 긴 궤적의 *반환값*은 무한대가 될 것입니다. 강화 학습 공식을 그러한 궤적에 대해서도 의미 있게 유지하기 위해 할인율(discount factor) $\gamma < 1$의 개념을 도입합니다. 할인된 *반환값*은 다음과 같이 씁니다.
$$R(\tau) = r_0 + \gamma r_1 + \gamma^2 r_2 + \cdots = \sum_{t=0}^\infty \gamma^t r_t.$$ 

$\\gamma$가 매우 작으면 먼 미래(예: $t = 1000$)에 로봇이 얻는 보상은 $\\gamma^{1000}$만큼 크게 할인됩니다. 이것은 로봇이 목표, 즉 그리드 월드 예제(:numref:`fig_mdp` 참조)에서 녹색 집으로 가는 목표를 달성하는 짧은 궤적을 선택하도록 장려합니다. 할인율이 큰 값(예: $\\gamma = 0.99$)이면 로봇이 *탐험*하고 목표 위치로 가는 최상의 궤적을 찾도록 장려됩니다.

## 마르코프 가정에 대한 논의

상태 $s_t$가 위와 같이 위치이지만 행동 $a_t$가 "앞으로 이동"과 같은 추상적인 명령이 아니라 로봇이 바퀴에 적용하는 가속도인 새로운 로봇을 생각해 봅시다. 이 로봇이 상태 $s_t$에서 0이 아닌 속도를 갖는다면, 다음 위치 $s_{t+1}$은 과거 위치 $s_t$, 가속도 $a_t$, 그리고 $s_t - s_{t-1}$에 비례하는 시간 $t$에서의 로봇 속도의 함수입니다. 이것은 다음을 의미합니다.

$$s_{t+1} = \textrm{some function}(s_t, a_t, s_{t-1});$$ 

우리 경우 "어떤 함수"는 뉴턴의 운동 법칙일 것입니다. 이것은 단순히 $s_t$와 $a_t$에만 의존하는 전이 함수와는 매우 다릅니다.

마르코프 시스템은 다음 상태 $s_{t+1}$이 현재 상태 $s_t$와 현재 상태에서 취한 행동 $a_t$의 함수일 뿐인 모든 시스템입니다. 마르코프 시스템에서 다음 상태는 과거에 취한 행동이나 과거에 로봇이 있었던 상태에 의존하지 않습니다. 예를 들어, 위에서 가속도를 행동으로 하는 새로운 로봇은 다음 위치 $s_{t+1}$이 속도를 통해 이전 상태 $s_{t-1}$에 의존하기 때문에 마르코프적이지 않습니다. 시스템의 마르코프적 특성이 제한적인 가정인 것처럼 보일 수 있지만 그렇지 않습니다. 마르코프 결정 과정은 여전히 매우 큰 클래스의 실제 시스템을 모델링할 수 있습니다. 예를 들어, 새로운 로봇의 경우 상태 $s_t$를 튜플 $(\textrm{location}, \textrm{velocity})$로 선택하면 다음 상태 $(\textrm{location}_{t+1}, \textrm{velocity}_{t+1})$이 현재 상태 $(\textrm{location}_t, \textrm{velocity}_t)$와 현재 상태에서의 행동 $a_t$에만 의존하기 때문에 시스템은 마르코프적입니다.

## 요약
강화 학습 문제는 일반적으로 마르코프 결정 과정을 사용하여 모델링됩니다. 마르코프 결정 과정(MDP)은 네 가지 엔터티 $(\mathcal{S}, \mathcal{A}, T, r)$의 튜플로 정의됩니다. 여기서 $\mathcal{S}$는 상태 공간, $\mathcal{A}$는 행동 공간, $T$는 MDP의 전이 확률을 인코딩하는 전이 함수, $r$은 특정 상태에서 행동을 취하여 얻은 즉각적인 보상입니다.


## 연습 문제

1. [MountainCar](https://www.gymlibrary.dev/environments/classic_control/mountain_car/) 문제를 모델링하기 위해 MDP를 설계하고 싶다고 가정해 봅시다.
    1. 상태 집합은 무엇입니까?
    2. 행동 집합은 무엇입니까?
    3. 가능한 보상 함수는 무엇입니까?
2. [Pong 게임](https://www.gymlibrary.dev/environments/atari/pong/)과 같은 Atari 게임을 위한 MDP를 어떻게 설계하시겠습니까?

:begin_tab:`pytorch`
[토론](https://discuss.d2l.ai/t/12084)
:end_tab: