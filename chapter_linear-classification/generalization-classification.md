# 분류에서의 일반화 (Generalization in Classification)

:label:`chap_classification_generalization`



지금까지 우리는 여러 개의 출력과 소프트맥스 함수를 가진 (선형) 신경망을 훈련하여 다중 클래스 분류 문제를 다루는 방법에 집중했습니다. 모델의 출력을 확률론적 예측으로 해석하여, 모델이(고정된 파라미터 세트에 대해) 실제 레이블에 할당하는 음의 로그 우도를 계산하는 크로스 엔트로피 손실 함수를 유도하고 동기를 부여했습니다. 마지막으로 이러한 도구들을 훈련 세트에 모델을 맞춤으로써 실습에 적용했습니다. 하지만 언제나 그렇듯 우리의 목표는 이전에 본 적 없는 데이터(테스트 세트)에서 경험적으로 평가된 *일반적인 패턴*을 학습하는 것입니다. 훈련 세트에서의 높은 정확도는 아무 의미가 없습니다. 각 입력이 고유할 때마다(실제로 대부분의 고차원 데이터셋에서 그러함), 첫 번째 훈련 에폭에서 데이터셋을 단순히 암기한 다음 새로운 이미지를 볼 때마다 레이블을 찾아보는 것만으로 훈련 세트에서 완벽한 정확도를 얻을 수 있습니다. 하지만 정확한 훈련 예제와 관련된 정확한 레이블을 암기하는 것은 새로운 예제를 어떻게 분류해야 하는지 알려주지 않습니다. 추가적인 지침이 없다면 새로운 예제를 만날 때마다 무작위 추측에 의존해야 할 수도 있습니다.

몇 가지 시급한 질문들이 즉각적인 관심을 요구합니다:

1. 모집단에 대한 분류기의 정확도를 잘 추정하기 위해 얼마나 많은 테스트 예제가 필요할까요?
2. 동일한 테스트 세트에서 모델을 반복해서 계속 평가하면 어떻게 될까요?
3. 훈련 세트에 우리 선형 모델을 맞추는 것이 단순한 암기 체계보다 더 나을 것이라고 왜 기대해야 할까요?


:numref:`sec_generalization_basics`가 선형 회귀의 맥락에서 과대적합과 일반화의 기초를 소개했다면, 이 장에서는 조금 더 깊이 들어가 통계적 학습 이론의 기초적인 아이디어 몇 가지를 소개할 것입니다. 우리는 종종 사후적으로 일반화를 보장할 수 있는 것으로 밝혀졌습니다: 많은 모델에 대해, 그리고 원하는 일반화 갭의 상한 $\epsilon$에 대해, 훈련 세트가 최소 $n$개의 샘플을 포함한다면 *어떠한 데이터 생성 분포에 대해서도* 우리의 경험적 오차가 실제 오차의 $\epsilon$ 범위 내에 있게 되는 필요한 샘플 수 $n$을 결정할 수 있는 경우가 많습니다. 불행히도 이러한 종류의 보장은 심오한 지적 토대를 제공하지만, 딥러닝 실무자에게는 실제적인 유용성이 제한적이라는 사실도 밝혀졌습니다. 간단히 말해, 이러한 보장은 심층 신경망의 일반화를 *사전적으로* 보장하려면 (아마도 수조 개 이상의) 터무니없는 수의 예제가 필요함을 시사합니다. 하지만 우리가 관심을 갖는 작업에서 심층 신경망은 일반적으로 훨씬 적은 수의 예제(수천 개)로도 놀라울 정도로 잘 일반화된다는 것을 발견합니다. 따라서 딥러닝 실무자들은 종종 사전적 보장을 아예 포기하고, 대신 과거에 유사한 문제에서 잘 일반화되었던 방법들을 채택하며, 경험적 평가를 통해 *사후적으로* 일반화를 인증합니다. :numref:`chap_perceptrons`에 도달하면 일반화를 다시 다루고, 왜 심층 신경망이 실제 상황에서 일반화되는지 설명하려는 시도에서 생겨난 방대한 과학 문헌에 대해 가볍게 소개할 것입니다.

## 테스트 세트 (The Test Set)

우리는 이미 일반화 오차를 평가하는 표준 방법으로서 테스트 세트에 의존하기 시작했으므로, 그러한 오차 추정치의 속성에 대해 논의하며 시작해 봅시다. 어떻게 얻었는지에 대해서는 걱정하지 말고 고정된 분류기 $f$에 집중해 봅시다. 또한 분류기 $f$를 훈련하는 데 사용되지 않은 예제들의 *신선한* 데이터셋 $\mathcal{D} = {(\mathbf{x}^{(i)},y^{(i)})}_{i=1}^n$을 가지고 있다고 가정합시다. $\mathcal{D}$에서 우리 분류기 $f$의 *경험적 오차(empirical error)*는 단순히 예측 $f(\mathbf{x}^{(i)})$가 실제 레이블 $y^{(i)}$와 일치하지 않는 인스턴스의 비율이며, 다음 식으로 주어집니다:

$$\epsilon_\mathcal{D}(f) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(f(\mathbf{x}^{(i)}) \neq y^{(i)}).$$ 

대조적으로, *모집단 오차(population error)*는 기저 모집단(확률 밀도 함수 $p(\mathbf{x},y)$로 특징지어지는 어떤 분포 $P(X,Y)$)에서 우리 분류기가 실제 레이블과 일치하지 않는 예제들의 *기대* 비율입니다:

$$\epsilon(f) =  E_{(\mathbf{x}, y) \sim P} \mathbf{1}(f(\mathbf{x}) \neq y) = 
\int\int \mathbf{1}(f(\mathbf{x}) \neq y) p(\mathbf{x}, y) \;d\mathbf{x} dy.$$ 

$\epsilon(f)$가 우리가 실제로 관심을 갖는 양이지만, 모든 사람을 측정하지 않고는 큰 모집단의 평균 키를 직접 관찰할 수 없는 것처럼 이를 직접 관찰할 수는 없습니다. 우리는 샘플을 바탕으로 이 양을 추정할 수 있을 뿐입니다. 우리 테스트 세트 $\mathcal{D}$가 기저 모집단을 통계적으로 대표하기 때문에, $\epsilon_\mathcal{D}(f)$를 모집단 오차 $\epsilon(f)$의 통계적 추정량(statistical estimator)으로 볼 수 있습니다. 게다가 우리가 관심 있는 양 $\epsilon(f)$가 (확률 변수 $\mathbf{1}(f(X) \neq Y)$의) 기댓값이고 해당 추정량 $\epsilon_\mathcal{D}(f)$가 표본 평균이므로, 모집단 오차를 추정하는 것은 단순히 :numref:`sec_prob`에서 기억하실 수 있는 고전적인 평균 추정 문제입니다.

확률 이론의 중요한 고전적 결과인 *중심 극한 정리(central limit theorem)*는 평균 $\mu$와 표준 편차 $\sigma$를 가진 임의의 분포에서 추출된 $n$개의 무작위 샘플 $a_1, ..., a_n$을 가질 때마다, 샘플 수 $n$이 무한대에 가까워짐에 따라 표본 평균 $\hat{\mu}$가 실제 평균을 중심으로 하고 표준 편차가 $\sigma/\sqrt{n}$인 정규 분포에 대략적으로 근접함을 보장합니다. 이미 이는 중요한 사실을 말해줍니다: 예제 수가 많아짐에 따라 테스트 오차 $\epsilon_\mathcal{D}(f)$는 $\mathcal{O}(1/\sqrt{n})$의 비율로 실제 오차 $\epsilon(f)$에 접근해야 한다는 것입니다. 따라서 테스트 오차를 두 배 더 정확하게 추정하려면 테스트 세트를 4배 더 크게 수집해야 합니다. 테스트 오차를 100분의 1로 줄이려면 테스트 세트를 만 배 더 크게 수집해야 합니다. 일반적으로 그러한 $\mathcal{O}(1/\sqrt{n})$의 비율은 통계학에서 우리가 바랄 수 있는 최선의 결과인 경우가 많습니다.

이제 테스트 오차 $\epsilon_\mathcal{D}(f)$가 실제 오차 $\epsilon(f)$로 수렴하는 점진적 비율에 대해 알게 되었으므로, 몇 가지 중요한 세부 사항을 확대해 봅시다. 관심 있는 확률 변수 $\mathbf{1}(f(X) \neq Y)$가 0과 1 값만 취할 수 있으므로 이는 베르누이 확률 변수이며, 값 1을 취할 확률을 나타내는 파라미터로 특징지어집니다. 여기서 1은 우리 분류기가 오차를 범했음을 의미하므로, 우리 확률 변수의 파라미터는 실제로는 실제 오차율 $\epsilon(f)$입니다. 베르누이의 분산 $\sigma^2$은 식 $\epsilon(f)(1-\epsilon(f))$에 따라 그 파라미터(여기서는 $\epsilon(f)$)에 의존합니다. $\epsilon(f)$는 처음에 알려지지 않았지만 1보다 클 수 없음을 압니다. 이 함수를 조금 조사해 보면 실제 오차율이 0.5에 가까울 때 분산이 가장 높고, 0이나 1에 가까울 때 훨씬 낮아질 수 있음을 알 수 있습니다. 이는 ($n$개의 테스트 샘플 선택에 따른) 오차 $\epsilon(f)$에 대한 우리 추정치 $\epsilon_\mathcal{D}(f)$의 점진적 표준 편차가 $\sqrt{0.25/n}$보다 클 수 없음을 알려줍니다.

이 비율이 유한한 샘플을 가질 때가 아니라 테스트 세트 크기가 무한대에 가까워질 때의 행동을 특징짓는다는 사실을 무시한다면, 이는 테스트 오차 $\epsilon_\mathcal{D}(f)$가 모집단 오차 $\epsilon(f)$에 근사하여 한 표준 편차가 $\pm 0.01$의 구간에 해당하기를 원한다면 대략 2500개의 샘플을 수집해야 함을 알려줍니다. 만약 그 범위에 두 표준 편차를 맞추어 $\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$임을 95% 확신하고 싶다면 10,000개의 샘플이 필요할 것입니다!

이것이 머신러닝의 많은 인기 있는 벤치마크 테스트 세트 크기로 밝혀졌습니다. 매년 수천 편의 응용 딥러닝 논문이 발표되면서 0.01 이하의 오차율 개선을 대단하게 다루는 것을 보고 놀라실 수도 있습니다. 물론 오차율이 0에 훨씬 가까울 때는 0.01의 개선이 정말 큰 일일 수 있습니다.


지금까지 우리 분석의 한 가지 성가신 특징은 이것이 실제로는 점진적(asymptotics)인 것, 즉 샘플 크기가 무한대로 갈 때 $\epsilon_\mathcal{D}$와 $\epsilon$ 사이의 관계가 어떻게 진화하는지에 대해서만 알려준다는 점입니다. 다행히 우리 확률 변수가 유계(bounded)이므로, Hoeffding(1963)의 부등식을 적용하여 유효한 유한 샘플 경계를 얻을 수 있습니다:

$$P(\epsilon_\mathcal{D}(f) - \epsilon(f) \geq t) < \exp\left( - 2n t^2 \right).$$ 

우리 추정치 $\epsilon_\mathcal{D}(f)$와 실제 오차율 $\epsilon(f)$ 사이의 거리 $t$가 0.01을 넘지 않는다고 95% 확신할 수 있게 해주는 가장 작은 데이터셋 크기를 구해보면, 위 점진적 분석에서 제안된 10,000개 예제에 비해 대략 15,000개의 예제가 필요함을 알게 될 것입니다. 통계학을 더 깊이 파고들면 이러한 경향이 일반적으로 유지됨을 알게 될 것입니다. 유한 샘플에서도 유지되는 보장은 일반적으로 약간 더 보수적입니다. 대세로 볼 때 이러한 수치들이 그렇게 멀지 않다는 점에 유의하십시오. 이는 점진적 분석이 비록 법정으로 가져갈 수 있는 보장은 아니더라도 우리에게 대략적인 수치를 제공하는 데 일반적으로 유용함을 반영합니다.

## 테스트 세트 재사용 (Test Set Reuse)

어떤 의미에서 여러분은 이제 경험적 머신러닝 연구를 수행하는 데 성공할 준비가 되었습니다. 거의 모든 실제 모델은 테스트 세트 성능을 기반으로 개발되고 검증되며, 여러분은 이제 테스트 세트의 마스터입니다. 고정된 분류기 $f$에 대해 테스트 오차 $\epsilon_\mathcal{D}(f)$를 평가하는 방법을 알고, 그 모집단 오차 $\epsilon(f)$에 대해 무엇을 말할 수 있는지(그리고 없는지) 정확히 압니다.

따라서 여러분이 이 지식을 가지고 첫 번째 모델 $f_1$을 훈련할 준비를 한다고 합시다. 분류기의 오차율 성능에 대해 얼마나 확신이 필요한지 알고 있으므로, 테스트 세트를 위해 떼어놓을 적절한 예제 수를 결정하기 위해 위 분석을 적용합니다. 더욱이 여러분이 :numref:`sec_generalization_basics`의 교훈을 가슴에 새기고 모든 예비 분석, 하이퍼파라미터 튜닝, 심지어 경쟁하는 여러 모델 아키텍처 중 선택까지 검증 세트에서 수행함으로써 테스트 세트의 신성함을 지켰다고 가정해 봅시다. 마지막으로 테스트 세트에서 모델 $f_1$을 평가하고 관련 신뢰 구간과 함께 모집단 오차의 불편 추정치를 보고합니다.

지금까지는 모든 것이 잘 진행되는 것 같습니다. 하지만 그날 밤 새벽 3시에 새로운 모델링 접근 방식에 대한 기막힌 아이디어가 떠올라 잠에서 깹니다. 다음 날 새 모델을 코딩하고 검증 세트에서 하이퍼파라미터를 튜닝하며, 새 모델 $f_2$가 작동할 뿐만 아니라 오차율이 $f_1$보다 훨씬 낮아 보입니다. 하지만 최종 평가를 준비하면서 갑자기 발견의 전율이 사라집니다. 여러분에게는 테스트 세트가 없습니다!

비록 원래 테스트 세트 $\mathcal{D}$가 여전히 서버에 남아 있더라도, 이제 여러분은 두 가지 만만치 않은 문제에 직면합니다. 첫째, 테스트 세트를 수집할 때 여러분은 단일 분류기 $f$를 평가한다는 가정하에 필요한 정밀도 수준을 결정했습니다. 하지만 동일한 테스트 세트에서 여러 분류기 $f_1, ..., f_k$를 평가하는 일을 하게 된다면, 거짓 발견(false discovery) 문제를 고려해야 합니다. 이전에는 단일 분류기 $f$에 대해 $\epsilon_\mathcal{D}(f) \in \epsilon(f) \pm 0.01$임을 95% 확신했을 수 있고, 따라서 잘못된 결과가 나올 확률은 단 5%였습니다. $k$개의 분류기가 섞여 있으면, 그들 중 단 하나도 테스트 세트 성능이 오해를 불러일으키지 않는다고 보장하기 어려울 수 있습니다. 고려 중인 분류기가 20개라면, 그들 중 최소 하나가 오해의 소지가 있는 점수를 받았을 가능성을 배제할 힘이 전혀 없을 수도 있습니다. 이 문제는 다중 가설 검정(multiple hypothesis testing)과 관련이 있으며, 통계학의 방대한 문헌에도 불구하고 과학 연구를 괴롭히는 고질적인 문제로 남아 있습니다.


그것만으로 걱정하기 부족하다면, 후속 평가에서 얻는 결과를 불신해야 할 특별한 이유가 있습니다. 테스트 세트 성능에 대한 우리 분석은 분류기가 테스트 세트와의 접촉 없이 선택되었다는 가정에 근거했으므로 테스트 세트를 기저 모집단에서 무작위로 추출된 것으로 볼 수 있었다는 점을 상기하십시오. 여기서는 여러 함수를 테스트하고 있을 뿐만 아니라, 후속 함수 $f_2$가 $f_1$의 테스트 세트 성능을 관찰한 *후에* 선택되었습니다. 테스트 세트의 정보가 모델러에게 유출되고 나면, 엄격한 의미에서 그것은 더 이상 진정한 테스트 세트가 될 수 없습니다. 이 문제를 *적응형 과대적합(adaptive overfitting)*이라고 하며 최근 학습 이론가들과 통계학자들 사이에서 강렬한 관심 주제로 부상했습니다 :cite:`dwork2015preserving`. 다행히 홀드아웃 세트에서 모든 정보를 유출하는 것이 가능하고 이론적인 최악의 시나리오는 암울하지만, 이러한 분석들은 너무 보수적일 수 있습니다. 실제로는 진짜 테스트 세트를 만들고, 가능한 한 드물게 참조하며, 신뢰 구간을 보고할 때 다중 가설 검정을 고려하고, 이해관계가 높고 데이터셋 크기가 작을 때 경계심을 더 공격적으로 높이도록 주의하십시오. 일련의 벤치마크 챌린지를 실행할 때, 매 라운드 후에 오래된 테스트 세트를 검증 세트로 강등시킬 수 있도록 여러 테스트 세트를 유지하는 것이 종종 좋은 관행입니다.




## 통계적 학습 이론 (Statistical Learning Theory)

단순히 말해서 *테스트 세트가 우리가 가진 전부*이지만, 이 사실은 이상하게도 불만족스럽게 느껴집니다. 첫째, 우리는 진정한 테스트 세트를 소유하는 경우가 드뭅니다. 우리가 데이터셋을 직접 만드는 사람이 아니라면, 다른 누군가가 이미 우리의 표면적인 "테스트 세트"에서 자신의 분류기를 평가했을 가능성이 높습니다. 그리고 우리가 우선권(first dibs)을 가졌을 때조차, 곧 우리 숫자를 믿을 수 없다는 찝찝한 기분 없이 후속 모델링 시도를 평가할 수 있기를 바라며 좌절하게 됩니다. 더욱이 진정한 테스트 세트조차 분류기가 실제로 모집단에 일반화되었는지를 *사후적으로* 알려줄 뿐이며, 일반화되어야 할 *사전적인* 이유가 있는지는 알려주지 않습니다.

이러한 우려를 염두에 둔다면, 여러분은 이제 통계적 학습 이론(statistical learning theory)의 매력을 볼 준비가 충분히 된 것일 수 있습니다. 통계적 학습 이론은 머신러닝의 수학적 하위 분야로, 실무자들이 경험적 데이터로 훈련된 모델이 왜/언제 보지 못한 데이터로 일반화될 수 있는/있을 것인지 설명하는 근본적인 원리를 밝히는 것을 목표로 합니다. 통계적 학습 연구자들의 주요 목표 중 하나는 모델 클래스의 속성을 데이터셋의 샘플 수와 연관 지어 일반화 갭을 제한(bound)하는 것이었습니다.

학습 이론가들은 훈련 세트 $\mathcal{S}$에서 훈련되고 평가된 학습된 분류기 $f_\mathcal{S}$의 *경험적 오차* $\epsilon_\mathcal{S}(f_\mathcal{S})$와, 기저 모집단에 대한 동일한 분류기의 실제 오차 $\epsilon(f_\mathcal{S})$ 사이의 차이를 제한하는 것을 목표로 합니다. 이는 우리가 방금 다룬 평가 문제와 비슷해 보일 수 있지만 큰 차이가 있습니다. 앞서 분류기 $f$는 고정되어 있었고 우리는 평가 목적으로만 데이터셋이 필요했습니다. 그리고 실제로 어떤 고정된 분류기는 일반화됩니다: (이전에 보지 못한) 데이터셋에서의 오차는 모집단 오차의 불편 추정치입니다. 하지만 분류기가 동일한 데이터셋에서 훈련되고 평가될 때는 무엇을 말할 수 있을까요? 훈련 오차가 테스트 오차와 가까울 것이라고 확신할 수 있을까요?


우리 학습된 분류기 $f_\mathcal{S}$가 미리 지정된 함수 집합 $\mathcal{F}$에서 선택되어야 한다고 가정합시다. 테스트 세트에 대한 논의에서 단일 분류기의 오차를 추정하는 것은 쉽지만, 분류기 모음을 고려하기 시작하면 상황이 까다로워진다는 점을 상기하십시오. 비록 어떤 하나의 (고정된) 분류기의 경험적 오차가 높은 확률로 실제 오차와 가까울지라도, 분류기 모음을 고려하게 되면 그들 중 *단 하나라도* 오차가 잘못 추정될 가능성을 걱정해야 합니다. 걱정되는 점은 우리가 그러한 분류기를 선택하여 모집단 오차를 크게 과소평가하게 될 수도 있다는 것입니다. 더욱이 선형 모델의 경우에도 그 파라미터가 연속적인 값을 갖기 때문에, 우리는 일반적으로 무한한 함수 클래스($|\mathcal{F}| = \infty$)에서 선택하게 됩니다.

이 문제에 대한 한 가지 야심 찬 해결책은 균등 수렴(uniform convergence)을 증명하기 위한 분석 도구를 개발하는 것입니다. 즉, 높은 확률로 클래스 $f\in\mathcal{F}$ 내의 모든 분류기에 대한 경험적 오차율이 실제 오차율로 *동시에* 수렴한다는 것입니다. 다시 말해, 우리는 (어떤 작은 $\delta$에 대해) 적어도 $1-\delta$의 확률로 클래스 $\mathcal{F}$ 내의 어떤 분류기의 오차율 $\epsilon(f)$도 어떤 작은 양 $\alpha$보다 더 크게 잘못 추정되지 않을 것이라고 말할 수 있게 해주는 이론적 원리를 찾습니다. 분명히 우리는 모든 모델 클래스 $\mathcal{F}$에 대해 그러한 진술을 할 수 없습니다. 항상 경험적 오차 0을 달성하지만 기저 모집단에 대해 무작위 추측보다 결코 나은 성능을 내지 못하는 암기 기계 클래스를 상기해 보십시오.

어떤 의미에서 암기자 클래스는 너무 유연합니다. 그러한 균등 수렴 결과가 유지될 수 없습니다. 반면에 고정된 분류기는 쓸모없습니다. 완벽하게 일반화되지만 훈련 데이터에도 테스트 데이터에도 맞지 않기 때문입니다. 따라서 학습의 중심 질문은 역사적으로 훈련 데이터에 더 잘 맞지만 과대적합의 위험이 있는 더 유연한(높은 분산) 모델 클래스와, 잘 일반화되지만 과소적합의 위험이 있는 더 경직된(높은 편향) 모델 클래스 사이의 절충으로 틀이 잡혀 왔습니다. 학습 이론의 핵심 질문은 모델이 이 스펙트럼의 어디에 위치하는지 정량화하고 관련 보장을 제공하기 위한 적절한 수학적 분석을 개발하는 것이었습니다.

일련의 중대한 논문들에서 Vapnik과 Chervonenkis는 상대 빈도의 수렴에 관한 이론을 더 일반적인 함수 클래스로 확장했습니다 :cite:`VapChe64,VapChe68,VapChe71,VapChe74b,VapChe81,VapChe91`. 이 연구 라인의 주요 기여 중 하나는 모델 클래스의 복잡성(유연성)을 측정하는 (하나의 개념인) Vapnik--Chervonenkis (VC) 차원입니다. 더욱이 그들의 주요 결과 중 하나는 경험적 오차와 모집단 오차 사이의 차이를 VC 차원과 샘플 수의 함수로 제한합니다:

$$P\left(R[p, f] - R_\textrm{emp}[\mathbf{X}, \mathbf{Y}, f] < \alpha\right) \geq 1-\delta
\ 	extrm{ 여기서 }\ \alpha \geq c \sqrt{(\textrm{VC} - \log \delta)/n}.$$ 

여기서 $\delta > 0$은 경계가 위반될 확률이고, $\alpha$는 일반화 갭의 상한이며, $n$은 데이터셋 크기입니다. 마지막으로 $c > 0$은 발생할 수 있는 손실의 규모에만 의존하는 상수입니다. 이 경계의 한 가지 용도는 원하는 $\delta$와 $\alpha$ 값을 대입하여 수집할 샘플 수를 결정하는 것일 수 있습니다. VC 차원은 임의의 (이진) 레이블링을 할당할 수 있고 각 레이블링에 대해 해당 레이블링과 일치하는 클래스 내의 어떤 모델 $f$를 찾을 수 있는 데이터 포인트의 최대 수를 정량화합니다. 예를 들어, $d$차원 입력에 대한 선형 모델은 VC 차원이 $d+1$입니다. 직선이 2차원에서 세 점에 대해서는 가능한 모든 레이블링을 할당할 수 있지만, 네 점에 대해서는 그렇지 못함을 쉽게 알 수 있습니다. 불행히도 이 이론은 더 복잡한 모델에 대해 지나치게 비관적인 경향이 있으며, 이 보장을 얻으려면 일반적으로 원하는 오차율을 달성하는 데 실제로 필요한 것보다 훨씬 더 많은 예제가 필요합니다. 또한 모델 클래스와 $\delta$를 고정하면, 우리 오차율은 다시 통상적인 $\mathcal{O}(1/\sqrt{n})$ 비율로 감소한다는 점에 유의하십시오. $n$ 측면에서 이보다 더 잘하기는 어려워 보입니다. 하지만 모델 클래스를 변경함에 따라 VC 차원은 일반화 갭에 대해 비관적인 그림을 제시할 수 있습니다.





## 요약 (Summary)

모델을 평가하는 가장 직접적인 방법은 이전에 본 적 없는 데이터로 구성된 테스트 세트를 참조하는 것입니다. 테스트 세트 평가는 실제 오차의 불편 추정치를 제공하며 테스트 세트가 커짐에 따라 원하는 $\mathcal{O}(1/\sqrt{n})$ 비율로 수렴합니다. 우리는 정확한 점진적 분포를 기반으로 대략적인 신뢰 구간을 제공하거나, (더 보수적인) 유한 샘플 보장을 기반으로 유효한 유한 샘플 신뢰 구간을 제공할 수 있습니다. 실제로 테스트 세트 평가는 현대 머신러닝 연구의 토대입니다. 하지만 테스트 세트가 (여러 연구자에 의해 반복해서 사용되는) 진정한 테스트 세트인 경우는 드뭅니다. 동일한 테스트 세트가 여러 모델을 평가하는 데 사용되고 나면, 거짓 발견을 제어하기 어려울 수 있습니다. 이는 이론적으로 큰 문제를 일으킬 수 있습니다. 실제로는 문제의 심각성이 해당 홀드아웃 세트의 크기와 그것이 단순히 하이퍼파라미터를 선택하는 데 사용되는지 아니면 정보를 더 직접적으로 유출하고 있는지에 따라 달라집니다. 그럼에도 불구하고 진짜 테스트 세트(또는 여러 개)를 관리하고 그것들이 사용되는 빈도에 대해 가능한 한 보수적인 태도를 취하는 것이 좋은 관행입니다.


더 만족스러운 해결책을 제공하기를 바라며, 통계적 학습 이론가들은 모델 클래스에 대한 균등 수렴을 보장하기 위한 방법들을 개발했습니다. 만약 실제로 모든 모델의 경험적 오차가 동시에 실제 오차로 수렴한다면, 우리는 훈련 오차를 최소화하여 가장 잘 수행되는 모델을 자유롭게 선택할 수 있으며, 그 모델이 홀드아웃 데이터에서도 비슷하게 잘 수행될 것임을 알 수 있습니다. 결정적으로, 그러한 결과들 중 어느 하나라도 모델 클래스의 어떤 속성에 의존해야 합니다. Vladimir Vapnik과 Alexey Chernovenkis는 VC 차원을 도입하여 VC 클래스의 모든 모델에 대해 유지되는 균등 수렴 결과를 제시했습니다. 클래스 내의 모든 모델에 대한 훈련 오차는 (동시에) 실제 오차와 가까울 것으로 보장되며, $\mathcal{O}(1/\sqrt{n})$의 비율로 더욱 가까워질 것으로 보장됩니다. VC 차원의 혁명적인 발견 이후, 수많은 대안적인 복잡도 척도들이 제안되었으며 각각은 유사한 일반화 보장을 용이하게 합니다. 함수 복잡도를 측정하는 여러 고급 방법에 대한 자세한 논의는 :citet:`boucheron2005theory`를 참조하십시오. 불행히도 이러한 복잡도 척도들은 통계 이론에서 폭넓게 유용한 도구가 되었지만, (직접적으로 적용될 때) 왜 심층 신경망이 일반화되는지 설명하는 데는 무력한 것으로 드러났습니다. 심층 신경망은 종종 수백만 개(또는 그 이상)의 파라미터를 가지며 큰 포인트 모음에 무작위 레이블을 쉽게 할당할 수 있습니다. 그럼에도 불구하고 실제 문제에서 잘 일반화되며, 놀랍게도 더 높은 VC 차원을 유발함에도 불구하고 더 크고 깊을 때 종종 더 잘 일반화됩니다. 다음 장에서는 딥러닝의 맥락에서 일반화를 다시 살펴보겠습니다.

## 연습 문제 (Exercises)

1. 고정된 모델 $f$의 오차를 99.9% 이상의 확률로 0.0001 이내로 추정하고 싶다면 몇 개의 샘플이 필요할까요?
2. 다른 누군가가 레이블이 지정된 테스트 세트 $\mathcal{D}$를 소유하고 있으며 레이블이 없는 입력(특성)만 제공한다고 가정해 봅시다. 이제 여러분이 (모델 클래스에 어떠한 제한도 두지 않고) 각 레이블이 없는 입력에 대해 모델 $f$를 실행하고 그에 대응하는 오차 $\epsilon_\mathcal{D}(f)$를 받음으로써만 테스트 세트 레이블에 접근할 수 있다고 가정해 봅시다. 여러분의 실제 오차와 상관없이 전체 테스트 세트를 유출하여 오차 0인 것처럼 보이게 하려면 얼마나 많은 모델을 평가해야 할까요?
3. 5차 다항식 클래스의 VC 차원은 얼마입니까?
4. 2차원 데이터에서 축에 평행한 직사각형(axis-aligned rectangles) 클래스의 VC 차원은 얼마입니까?

[토론](https://discuss.d2l.ai/t/6829)