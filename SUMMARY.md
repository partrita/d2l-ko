# 요약 (Summary)

[딥러닝 (Deep Learning)](index.md)

- [서문 (Preface)](chapter_preface/index.md)
- [설치 (Installation)](chapter_installation/index.md)
- [표기법 (Notation)](chapter_notation/index.md)
- [소개 (Introduction)](chapter_introduction/index.md)
- [예비 지식 (Preliminaries)](chapter_preliminaries/index.md)
    - [데이터 조작 (Data Manipulation)](chapter_preliminaries/ndarray.md)
    - [데이터 전처리 (Data Preprocessing)](chapter_preliminaries/pandas.md)
    - [선형 대수 (Linear Algebra)](chapter_preliminaries/linear-algebra.md)
    - [미적분 (Calculus)](chapter_preliminaries/calculus.md)
    - [자동 미분 (Automatic Differentiation)](chapter_preliminaries/autograd.md)
    - [확률과 통계 (Probability and Statistics)](chapter_preliminaries/probability.md)
    - [문서화 (Documentation)](chapter_preliminaries/lookup-api.md)
- [회귀를 위한 선형 신경망 (Linear Neural Networks for Regression)](chapter_linear-regression/index.md)
    - [선형 회귀 (Linear Regression)](chapter_linear-regression/linear-regression.md)
    - [구현을 위한 객체 지향 설계 (Object-Oriented Design for Implementation)](chapter_linear-regression/oo-design.md)
    - [합성 회귀 데이터 (Synthetic Regression Data)](chapter_linear-regression/synthetic-regression-data.md)
    - [밑바닥부터 시작하는 선형 회귀 구현 (Linear Regression Implementation from Scratch)](chapter_linear-regression/linear-regression-scratch.md)
    - [선형 회귀의 간결한 구현 (Concise Implementation of Linear Regression)](chapter_linear-regression/linear-regression-concise.md)
    - [일반화 (Generalization)](chapter_linear-regression/generalization.md)
    - [가중치 감쇠 (Weight Decay)](chapter_linear-regression/weight-decay.md)
- [분류를 위한 선형 신경망 (Linear Neural Networks for Classification)](chapter_linear-classification/index.md)
    - [소프트맥스 회귀 (Softmax Regression)](chapter_linear-classification/softmax-regression.md)
    - [이미지 분류 데이터셋 (The Image Classification Dataset)](chapter_linear-classification/image-classification-dataset.md)
    - [기본 분류 모델 (The Base Classification Model)](chapter_linear-classification/classification.md)
    - [밑바닥부터 시작하는 소프트맥스 회귀 구현 (Softmax Regression Implementation from Scratch)](chapter_linear-classification/softmax-regression-scratch.md)
    - [소프트맥스 회귀의 간결한 구현 (Concise Implementation of Softmax Regression)](chapter_linear-classification/softmax-regression-concise.md)
    - [분류에서의 일반화 (Generalization in Classification)](chapter_linear-classification/generalization-classification.md)
    - [환경 및 분포 이동 (Environment and Distribution Shift)](chapter_linear-classification/environment-and-distribution-shift.md)
- [다층 퍼셉트론 (Multilayer Perceptrons)](chapter_multilayer-perceptrons/index.md)
    - [다층 퍼셉트론 (Multilayer Perceptrons)](chapter_multilayer-perceptrons/mlp.md)
    - [다층 퍼셉트론의 구현 (Implementation of Multilayer Perceptrons)](chapter_multilayer-perceptrons/mlp-implementation.md)
    - [순전파, 역전파, 그리고 계산 그래프 (Forward Propagation, Backward Propagation, and Computational Graphs)](chapter_multilayer-perceptrons/backprop.md)
    - [수치적 안정성과 초기화 (Numerical Stability and Initialization)](chapter_multilayer-perceptrons/numerical-stability-and-init.md)
    - [딥러닝에서의 일반화 (Generalization in Deep Learning)](chapter_multilayer-perceptrons/generalization-deep.md)
    - [드롭아웃 (Dropout)](chapter_multilayer-perceptrons/dropout.md)
    - [Kaggle에서 주택 가격 예측하기 (Predicting House Prices on Kaggle)](chapter_multilayer-perceptrons/kaggle-house-price.md)
- [빌더 가이드 (Builders' Guide)](chapter_builders-guide/index.md)
    - [레이어와 모듈 (Layers and Modules)](chapter_builders-guide/model-construction.md)
    - [파라미터 관리 (Parameter Management)](chapter_builders-guide/parameters.md)
    - [파라미터 초기화 (Parameter Initialization)](chapter_builders-guide/init-param.md)
    - [지연 초기화 (Lazy Initialization)](chapter_builders-guide/lazy-init.md)
    - [사용자 정의 레이어 (Custom Layers)](chapter_builders-guide/custom-layer.md)
    - [파일 I/O (File I/O)](chapter_builders-guide/read-write.md)
    - [GPU (GPUs)](chapter_builders-guide/use-gpu.md)
- [합성곱 신경망 (Convolutional Neural Networks)](chapter_convolutional-neural-networks/index.md)
    - [완전 연결 레이어에서 합성곱으로 (From Fully Connected Layers to Convolutions)](chapter_convolutional-neural-networks/why-conv.md)
    - [이미지를 위한 합성곱 (Convolutions for Images)](chapter_convolutional-neural-networks/conv-layer.md)
    - [패딩과 스트라이드 (Padding and Stride)](chapter_convolutional-neural-networks/padding-and-strides.md)
    - [다중 입력 및 다중 출력 채널 (Multiple Input and Multiple Output Channels)](chapter_convolutional-neural-networks/channels.md)
    - [풀링 (Pooling)](chapter_convolutional-neural-networks/pooling.md)
    - [합성곱 신경망 (LeNet) (Convolutional Neural Networks (LeNet))](chapter_convolutional-neural-networks/lenet.md)
- [현대 합성곱 신경망 (Modern Convolutional Neural Networks)](chapter_convolutional-modern/index.md)
    - [심층 합성곱 신경망 (AlexNet) (Deep Convolutional Neural Networks (AlexNet))](chapter_convolutional-modern/alexnet.md)
    - [블록을 사용하는 네트워크 (VGG) (Networks Using Blocks (VGG))](chapter_convolutional-modern/vgg.md)
    - [네트워크 속의 네트워크 (NiN) (Network in Network (NiN))](chapter_convolutional-modern/nin.md)
    - [다중 분기 네트워크 (GoogLeNet) (Multi-Branch Networks (GoogLeNet))](chapter_convolutional-modern/googlenet.md)
    - [배치 정규화 (Batch Normalization)](chapter_convolutional-modern/batch-norm.md)
    - [잔차 네트워크 (ResNet)와 ResNeXt (Residual Networks (ResNet) and ResNeXt)](chapter_convolutional-modern/resnet.md)
    - [밀집 연결 네트워크 (DenseNet) (Densely Connected Networks (DenseNet))](chapter_convolutional-modern/densenet.md)
    - [합성곱 네트워크 아키텍처 설계 (Designing Convolution Network Architectures)](chapter_convolutional-modern/cnn-design.md)
- [순환 신경망 (Recurrent Neural Networks)](chapter_recurrent-neural-networks/index.md)
    - [시퀀스 다루기 (Working with Sequences)](chapter_recurrent-neural-networks/sequence.md)
    - [원시 텍스트를 시퀀스 데이터로 변환하기 (Converting Raw Text into Sequence Data)](chapter_recurrent-neural-networks/text-sequence.md)
    - [언어 모델 (Language Models)](chapter_recurrent-neural-networks/language-model.md)
    - [순환 신경망 (Recurrent Neural Networks)](chapter_recurrent-neural-networks/rnn.md)
    - [밑바닥부터 시작하는 순환 신경망 구현 (Recurrent Neural Network Implementation from Scratch)](chapter_recurrent-neural-networks/rnn-scratch.md)
    - [순환 신경망의 간결한 구현 (Concise Implementation of Recurrent Neural Networks)](chapter_recurrent-neural-networks/rnn-concise.md)
    - [BPTT (Backpropagation Through Time)](chapter_recurrent-neural-networks/bptt.md)
- [현대 순환 신경망 (Modern Recurrent Neural Networks)](chapter_recurrent-modern/index.md)
    - [장단기 메모리 (LSTM) (Long Short-Term Memory (LSTM))](chapter_recurrent-modern/lstm.md)
    - [게이트 순환 유닛 (GRU) (Gated Recurrent Units (GRU))](chapter_recurrent-modern/gru.md)
    - [심층 순환 신경망 (Deep Recurrent Neural Networks)](chapter_recurrent-modern/deep-rnn.md)
    - [양방향 순환 신경망 (Bidirectional Recurrent Neural Networks)](chapter_recurrent-modern/bi-rnn.md)
    - [기계 번역과 데이터셋 (Machine Translation and the Dataset)](chapter_recurrent-modern/machine-translation-and-dataset.md)
    - [인코더-디코더 아키텍처 (The Encoder--Decoder Architecture)](chapter_recurrent-modern/encoder-decoder.md)
    - [기계 번역을 위한 시퀀스-투-시퀀스 학습 (Sequence-to-Sequence Learning for Machine Translation)](chapter_recurrent-modern/seq2seq.md)
    - [빔 검색 (Beam Search)](chapter_recurrent-modern/beam-search.md)
- [어텐션 메커니즘과 트랜스포머 (Attention Mechanisms and Transformers)](chapter_attention-mechanisms-and-transformers/index.md)
    - [쿼리, 키, 그리고 값 (Queries, Keys, and Values)](chapter_attention-mechanisms-and-transformers/queries-keys-values.md)
    - [유사도에 의한 어텐션 풀링 (Attention Pooling by Similarity)](chapter_attention-mechanisms-and-transformers/attention-pooling.md)
    - [어텐션 점수 함수 (Attention Scoring Functions)](chapter_attention-mechanisms-and-transformers/attention-scoring-functions.md)
    - [Bahdanau 어텐션 메커니즘 (The Bahdanau Attention Mechanism)](chapter_attention-mechanisms-and-transformers/bahdanau-attention.md)
    - [다중 헤드 어텐션 (Multi-Head Attention)](chapter_attention-mechanisms-and-transformers/multihead-attention.md)
    - [자기 어텐션과 위치 인코딩 (Self-Attention and Positional Encoding)](chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.md)
    - [트랜스포머 아키텍처 (The Transformer Architecture)](chapter_attention-mechanisms-and-transformers/transformer.md)
    - [비전을 위한 트랜스포머 (Transformers for Vision)](chapter_attention-mechanisms-and-transformers/vision-transformer.md)
    - [트랜스포머를 이용한 대규모 사전 훈련 (Large-Scale Pretraining with Transformers)](chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.md)
- [최적화 알고리즘 (Optimization Algorithms)](chapter_optimization/index.md)
    - [최적화와 딥러닝 (Optimization and Deep Learning)](chapter_optimization/optimization-intro.md)
    - [볼록성 (Convexity)](chapter_optimization/convexity.md)
    - [경사 하강법 (Gradient Descent)](chapter_optimization/gd.md)
    - [확률적 경사 하강법 (Stochastic Gradient Descent)](chapter_optimization/sgd.md)
    - [미니배치 확률적 경사 하강법 (Minibatch Stochastic Gradient Descent)](chapter_optimization/minibatch-sgd.md)
    - [모멘텀 (Momentum)](chapter_optimization/momentum.md)
    - [Adagrad](chapter_optimization/adagrad.md)
    - [RMSProp](chapter_optimization/rmsprop.md)
    - [Adadelta](chapter_optimization/adadelta.md)
    - [Adam](chapter_optimization/adam.md)
    - [학습률 스케줄링 (Learning Rate Scheduling)](chapter_optimization/lr-scheduler.md)
- [계산 성능 (Computational Performance)](chapter_computational-performance/index.md)
    - [컴파일러와 인터프리터 (Compilers and Interpreters)](chapter_computational-performance/hybridize.md)
    - [비동기 계산 (Asynchronous Computation)](chapter_computational-performance/async-computation.md)
    - [자동 병렬화 (Automatic Parallelism)](chapter_computational-performance/auto-parallelism.md)
    - [하드웨어 (Hardware)](chapter_computational-performance/hardware.md)
    - [다중 GPU에서의 훈련 (Training on Multiple GPUs)](chapter_computational-performance/multiple-gpus.md)
    - [다중 GPU를 위한 간결한 구현 (Concise Implementation for Multiple GPUs)](chapter_computational-performance/multiple-gpus-concise.md)
    - [파라미터 서버 (Parameter Servers)](chapter_computational-performance/parameterserver.md)
- [컴퓨터 비전 (Computer Vision)](chapter_computer-vision/index.md)
    - [이미지 증강 (Image Augmentation)](chapter_computer-vision/image-augmentation.md)
    - [미세 조정 (Fine-Tuning)](chapter_computer-vision/fine-tuning.md)
    - [객체 탐지와 바운딩 박스 (Object Detection and Bounding Boxes)](chapter_computer-vision/bounding-box.md)
    - [앵커 박스 (Anchor Boxes)](chapter_computer-vision/anchor.md)
    - [멀티스케일 객체 탐지 (Multiscale Object Detection)](chapter_computer-vision/multiscale-object-detection.md)
    - [객체 탐지 데이터셋 (The Object Detection Dataset)](chapter_computer-vision/object-detection-dataset.md)
    - [싱글 샷 멀티박스 탐지 (SSD) (Single Shot Multibox Detection)](chapter_computer-vision/ssd.md)
    - [영역 기반 CNN (R-CNNs) (Region-based CNNs (R-CNNs))](chapter_computer-vision/rcnn.md)
    - [시맨틱 세그멘테이션과 데이터셋 (Semantic Segmentation and the Dataset)](chapter_computer-vision/semantic-segmentation-and-dataset.md)
    - [전치 합성곱 (Transposed Convolution)](chapter_computer-vision/transposed-conv.md)
    - [완전 합성곱 네트워크 (Fully Convolutional Networks)](chapter_computer-vision/fcn.md)
    - [신경 스타일 전송 (Neural Style Transfer)](chapter_computer-vision/neural-style.md)
    - [Kaggle에서의 이미지 분류 (CIFAR-10) (Image Classification (CIFAR-10) on Kaggle)](chapter_computer-vision/kaggle-cifar10.md)
    - [Kaggle에서의 견종 식별 (ImageNet Dogs) (Dog Breed Identification (ImageNet Dogs) on Kaggle)](chapter_computer-vision/kaggle-dog.md)
- [자연어 처리: 사전 훈련 (Natural Language Processing: Pretraining)](chapter_natural-language-processing-pretraining/index.md)
    - [단어 임베딩 (word2vec) (Word Embedding (word2vec))](chapter_natural-language-processing-pretraining/word2vec.md)
    - [근사 훈련 (Approximate Training)](chapter_natural-language-processing-pretraining/approx-training.md)
    - [단어 임베딩 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining Word Embeddings)](chapter_natural-language-processing-pretraining/word-embedding-dataset.md)
    - [word2vec 사전 훈련 (Pretraining word2vec)](chapter_natural-language-processing-pretraining/word2vec-pretraining.md)
    - [GloVe를 이용한 단어 임베딩 (Word Embedding with Global Vectors (GloVe))](chapter_natural-language-processing-pretraining/glove.md)
    - [서브워드 임베딩 (Subword Embedding)](chapter_natural-language-processing-pretraining/subword-embedding.md)
    - [단어 유사도와 유추 (Word Similarity and Analogy)](chapter_natural-language-processing-pretraining/similarity-analogy.md)
    - [트랜스포머의 양방향 인코더 표현 (BERT) (Bidirectional Encoder Representations from Transformers (BERT))](chapter_natural-language-processing-pretraining/bert.md)
    - [BERT 사전 훈련을 위한 데이터셋 (The Dataset for Pretraining BERT)](chapter_natural-language-processing-pretraining/bert-dataset.md)
    - [BERT 사전 훈련 (Pretraining BERT)](chapter_natural-language-processing-pretraining/bert-pretraining.md)
- [자연어 처리: 응용 (Natural Language Processing: Applications)](chapter_natural-language-processing-applications/index.md)
    - [감성 분석과 데이터셋 (Sentiment Analysis and the Dataset)](chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.md)
    - [감성 분석: 순환 신경망 사용 (Sentiment Analysis: Using Recurrent Neural Networks)](chapter_natural-language-processing-applications/sentiment-analysis-rnn.md)
    - [감성 분석: 합성곱 신경망 사용 (Sentiment Analysis: Using Convolutional Neural Networks)](chapter_natural-language-processing-applications/sentiment-analysis-cnn.md)
    - [자연어 추론과 데이터셋 (Natural Language Inference and the Dataset)](chapter_natural-language-processing-applications/natural-language-inference-and-dataset.md)
    - [자연어 추론: 어텐션 사용 (Natural Language Inference: Using Attention)](chapter_natural-language-processing-applications/natural-language-inference-attention.md)
    - [시퀀스 레벨 및 토큰 레벨 응용을 위한 BERT 미세 조정 (Fine-Tuning BERT for Sequence-Level and Token-Level Applications)](chapter_natural-language-processing-applications/finetuning-bert.md)
    - [자연어 추론: BERT 미세 조정 (Natural Language Inference: Fine-Tuning BERT)](chapter_natural-language-processing-applications/natural-language-inference-bert.md)
- [강화 학습 (Reinforcement Learning)](chapter_reinforcement-learning/index.md)
    - [마르코프 결정 과정 (MDP) (Markov Decision Process (MDP))](chapter_reinforcement-learning/mdp.md)
    - [가치 반복 (Value Iteration)](chapter_reinforcement-learning/value-iter.md)
    - [Q-러닝 (Q-Learning)](chapter_reinforcement-learning/qlearning.md)
- [가우스 과정 (Gaussian Processes)](chapter_gaussian-processes/index.md)
    - [가우스 과정 소개 (Introduction to Gaussian Processes)](chapter_gaussian-processes/gp-intro.md)
    - [가우스 과정 사전 분포 (Gaussian Process Priors)](chapter_gaussian-processes/gp-priors.md)
    - [가우스 과정 추론 (Gaussian Process Inference)](chapter_gaussian-processes/gp-inference.md)
- [하이퍼파라미터 최적화 (Hyperparameter Optimization)](chapter_hyperparameter-optimization/index.md)
    - [하이퍼파라미터 최적화란 무엇인가? (What Is Hyperparameter Optimization?)](chapter_hyperparameter-optimization/hyperopt-intro.md)
    - [하이퍼파라미터 최적화 API (Hyperparameter Optimization API)](chapter_hyperparameter-optimization/hyperopt-api.md)
    - [비동기 무작위 검색 (Asynchronous Random Search)](chapter_hyperparameter-optimization/rs-async.md)
    - [다중 충실도 하이퍼파라미터 최적화 (Multi-Fidelity Hyperparameter Optimization)](chapter_hyperparameter-optimization/sh-intro.md)
    - [비동기 연속 반감법 (Asynchronous Successive Halving)](chapter_hyperparameter-optimization/sh-async.md)
- [생성적 적대 신경망 (Generative Adversarial Networks)](chapter_generative-adversarial-networks/index.md)
    - [생성적 적대 신경망 (Generative Adversarial Networks)](chapter_generative-adversarial-networks/gan.md)
    - [심층 합성곱 생성적 적대 신경망 (Deep Convolutional Generative Adversarial Networks)](chapter_generative-adversarial-networks/dcgan.md)
- [추천 시스템 (Recommender Systems)](chapter_recommender-systems/index.md)
    - [추천 시스템 개요 (Overview of Recommender Systems)](chapter_recommender-systems/recsys-intro.md)
    - [MovieLens 데이터셋 (The MovieLens Dataset)](chapter_recommender-systems/movielens.md)
    - [행렬 분해 (Matrix Factorization)](chapter_recommender-systems/mf.md)
    - [AutoRec: 오토인코더를 이용한 평점 예측 (AutoRec: Rating Prediction with Autoencoders)](chapter_recommender-systems/autorec.md)
    - [추천 시스템을 위한 개인화된 순위 지정 (Personalized Ranking for Recommender Systems)](chapter_recommender-systems/ranking.md)
    - [개인화된 순위 지정을 위한 신경 협업 필터링 (Neural Collaborative Filtering for Personalized Ranking)](chapter_recommender-systems/neumf.md)
    - [시퀀스 인식 추천 시스템 (Sequence-Aware Recommender Systems)](chapter_recommender-systems/seqrec.md)
    - [풍부한 특성 추천 시스템 (Feature-Rich Recommender Systems)](chapter_recommender-systems/ctr.md)
    - [인수 분해 머신 (Factorization Machines)](chapter_recommender-systems/fm.md)
    - [심층 인수 분해 머신 (Deep Factorization Machines)](chapter_recommender-systems/deepfm.md)
- [부록: 딥러닝을 위한 수학 (Appendix: Mathematics for Deep Learning)](chapter_appendix-mathematics-for-deep-learning/index.md)
    - [기하학 및 선형 대수 연산 (Geometry and Linear Algebraic Operations)](chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.md)
    - [고유 분해 (Eigendecompositions)](chapter_appendix-mathematics-for-deep-learning/eigendecomposition.md)
    - [단일 변수 미적분 (Single Variable Calculus)](chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.md)
    - [다변수 미적분 (Multivariable Calculus)](chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.md)
    - [적분 (Integral Calculus)](chapter_appendix-mathematics-for-deep-learning/integral-calculus.md)
    - [확률 변수 (Random Variables)](chapter_appendix-mathematics-for-deep-learning/random-variables.md)
    - [최대 우도 (Maximum Likelihood)](chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.md)
    - [분포 (Distributions)](chapter_appendix-mathematics-for-deep-learning/distributions.md)
    - [나이브 베이즈 (Naive Bayes)](chapter_appendix-mathematics-for-deep-learning/naive-bayes.md)
    - [통계 (Statistics)](chapter_appendix-mathematics-for-deep-learning/statistics.md)
    - [정보 이론 (Information Theory)](chapter_appendix-mathematics-for-deep-learning/information-theory.md)
- [부록: 딥러닝을 위한 도구 (Appendix: Tools for Deep Learning)](chapter_appendix-tools-for-deep-learning/index.md)
    - [주피터 노트북 사용하기 (Using Jupyter Notebooks)](chapter_appendix-tools-for-deep-learning/jupyter.md)
    - [Amazon SageMaker 사용하기 (Using Amazon SageMaker)](chapter_appendix-tools-for-deep-learning/sagemaker.md)
    - [AWS EC2 인스턴스 사용하기 (Using AWS EC2 Instances)](chapter_appendix-tools-for-deep-learning/aws.md)
    - [Google Colab 사용하기 (Using Google Colab)](chapter_appendix-tools-for-deep-learning/colab.md)
    - [서버 및 GPU 선택하기 (Selecting Servers and GPUs)](chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.md)
    - [이 책에 기여하기 (Contributing to This Book)](chapter_appendix-tools-for-deep-learning/contributing.md)
    - [유틸리티 함수 및 클래스 (Utility Functions and Classes)](chapter_appendix-tools-for-deep-learning/utils.md)
    - [`d2l` API 문서 (The `d2l` API Document)](chapter_appendix-tools-for-deep-learning/d2l.md)
- [참고 문헌 (chapter_references/zreferences.md)](chapter_references/zreferences.md)
